/home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb/
2021-05-16 11:52:24,563 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb
2021-05-16 11:52:24,563 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb/sent_train.txt
2021-05-16 11:52:24,563 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb/sent_dev.txt
2021-05-16 11:52:24,563 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb/sent_test.txt
Corpus: 2538 train + 541 dev + 490 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 11:52:45,595 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,601 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 11:52:45,603 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,603 Corpus: "Corpus: 2538 train + 541 dev + 490 test sentences"
2021-05-16 11:52:45,603 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,603 Parameters:
2021-05-16 11:52:45,603  - learning_rate: "0.1"
2021-05-16 11:52:45,603  - mini_batch_size: "16"
2021-05-16 11:52:45,603  - patience: "3"
2021-05-16 11:52:45,603  - anneal_factor: "0.5"
2021-05-16 11:52:45,603  - max_epochs: "30"
2021-05-16 11:52:45,603  - shuffle: "True"
2021-05-16 11:52:45,604  - train_with_dev: "False"
2021-05-16 11:52:45,604  - batch_growth_annealing: "False"
2021-05-16 11:52:45,604 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,604 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb"
2021-05-16 11:52:45,604 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,604 Device: cuda:0
2021-05-16 11:52:45,604 ----------------------------------------------------------------------------------------------------
2021-05-16 11:52:45,604 Embeddings storage mode: cpu
2021-05-16 11:52:45,607 ----------------------------------------------------------------------------------------------------
2021-05-16 11:53:07,314 epoch 1 - iter 15/159 - loss 8.03736699 - samples/sec: 11.06 - lr: 0.100000
2021-05-16 11:53:30,749 epoch 1 - iter 30/159 - loss 6.63934180 - samples/sec: 10.24 - lr: 0.100000
2021-05-16 11:53:53,096 epoch 1 - iter 45/159 - loss 6.00938438 - samples/sec: 10.74 - lr: 0.100000
2021-05-16 11:54:15,074 epoch 1 - iter 60/159 - loss 5.72151108 - samples/sec: 10.92 - lr: 0.100000
2021-05-16 11:54:37,363 epoch 1 - iter 75/159 - loss 5.48535022 - samples/sec: 10.77 - lr: 0.100000
2021-05-16 11:55:00,888 epoch 1 - iter 90/159 - loss 5.41461057 - samples/sec: 10.20 - lr: 0.100000
2021-05-16 11:55:23,926 epoch 1 - iter 105/159 - loss 5.28759820 - samples/sec: 10.42 - lr: 0.100000
2021-05-16 11:55:45,608 epoch 1 - iter 120/159 - loss 5.19891032 - samples/sec: 11.07 - lr: 0.100000
2021-05-16 11:56:08,939 epoch 1 - iter 135/159 - loss 5.08414632 - samples/sec: 10.29 - lr: 0.100000
2021-05-16 11:56:32,833 epoch 1 - iter 150/159 - loss 5.01235718 - samples/sec: 10.04 - lr: 0.100000
2021-05-16 11:56:46,553 ----------------------------------------------------------------------------------------------------
2021-05-16 11:56:46,554 EPOCH 1 done: loss 4.9718 - lr 0.1000000
2021-05-16 11:57:22,547 DEV : loss 4.372793674468994 - score 0.0
2021-05-16 11:57:22,647 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 11:57:33,536 ----------------------------------------------------------------------------------------------------
2021-05-16 11:57:43,526 epoch 2 - iter 15/159 - loss 4.53248459 - samples/sec: 24.03 - lr: 0.100000
2021-05-16 11:57:52,740 epoch 2 - iter 30/159 - loss 4.19500479 - samples/sec: 26.05 - lr: 0.100000
2021-05-16 11:58:01,388 epoch 2 - iter 45/159 - loss 4.19585666 - samples/sec: 27.76 - lr: 0.100000
2021-05-16 11:58:09,400 epoch 2 - iter 60/159 - loss 4.08220653 - samples/sec: 29.96 - lr: 0.100000
2021-05-16 11:58:18,049 epoch 2 - iter 75/159 - loss 4.15707929 - samples/sec: 27.75 - lr: 0.100000
2021-05-16 11:58:26,826 epoch 2 - iter 90/159 - loss 4.14133737 - samples/sec: 27.35 - lr: 0.100000
2021-05-16 11:58:34,900 epoch 2 - iter 105/159 - loss 4.12605852 - samples/sec: 29.73 - lr: 0.100000
2021-05-16 11:58:43,702 epoch 2 - iter 120/159 - loss 4.10456845 - samples/sec: 27.27 - lr: 0.100000
2021-05-16 11:58:51,655 epoch 2 - iter 135/159 - loss 4.08127326 - samples/sec: 30.18 - lr: 0.100000
2021-05-16 11:59:00,642 epoch 2 - iter 150/159 - loss 4.04075983 - samples/sec: 26.71 - lr: 0.100000
2021-05-16 11:59:05,523 ----------------------------------------------------------------------------------------------------
2021-05-16 11:59:05,524 EPOCH 2 done: loss 4.0589 - lr 0.1000000
2021-05-16 11:59:12,786 DEV : loss 4.2010273933410645 - score 0.0
2021-05-16 11:59:12,885 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 11:59:24,541 ----------------------------------------------------------------------------------------------------
2021-05-16 11:59:32,842 epoch 3 - iter 15/159 - loss 3.69413951 - samples/sec: 28.92 - lr: 0.100000
2021-05-16 11:59:41,181 epoch 3 - iter 30/159 - loss 3.84703369 - samples/sec: 28.78 - lr: 0.100000
2021-05-16 11:59:49,615 epoch 3 - iter 45/159 - loss 3.91149402 - samples/sec: 28.46 - lr: 0.100000
2021-05-16 11:59:58,226 epoch 3 - iter 60/159 - loss 3.91979210 - samples/sec: 27.88 - lr: 0.100000
2021-05-16 12:00:07,480 epoch 3 - iter 75/159 - loss 4.00159686 - samples/sec: 25.94 - lr: 0.100000
2021-05-16 12:00:16,122 epoch 3 - iter 90/159 - loss 4.01339697 - samples/sec: 27.78 - lr: 0.100000
2021-05-16 12:00:25,292 epoch 3 - iter 105/159 - loss 3.97579458 - samples/sec: 26.17 - lr: 0.100000
2021-05-16 12:00:34,330 epoch 3 - iter 120/159 - loss 3.99514177 - samples/sec: 26.56 - lr: 0.100000
2021-05-16 12:00:43,459 epoch 3 - iter 135/159 - loss 3.94494007 - samples/sec: 26.29 - lr: 0.100000
2021-05-16 12:00:52,184 epoch 3 - iter 150/159 - loss 3.91480471 - samples/sec: 27.51 - lr: 0.100000
2021-05-16 12:00:57,079 ----------------------------------------------------------------------------------------------------
2021-05-16 12:00:57,080 EPOCH 3 done: loss 3.9039 - lr 0.1000000
2021-05-16 12:01:04,972 DEV : loss 4.039122581481934 - score 0.0
2021-05-16 12:01:05,101 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:01:16,436 ----------------------------------------------------------------------------------------------------
2021-05-16 12:01:27,472 epoch 4 - iter 15/159 - loss 3.73120891 - samples/sec: 21.75 - lr: 0.100000
2021-05-16 12:01:37,828 epoch 4 - iter 30/159 - loss 3.66403999 - samples/sec: 23.18 - lr: 0.100000
2021-05-16 12:01:46,560 epoch 4 - iter 45/159 - loss 3.62984822 - samples/sec: 27.49 - lr: 0.100000
2021-05-16 12:01:55,046 epoch 4 - iter 60/159 - loss 3.64547866 - samples/sec: 28.29 - lr: 0.100000
2021-05-16 12:02:04,520 epoch 4 - iter 75/159 - loss 3.65317740 - samples/sec: 25.33 - lr: 0.100000
2021-05-16 12:02:14,538 epoch 4 - iter 90/159 - loss 3.72855538 - samples/sec: 23.96 - lr: 0.100000
2021-05-16 12:02:23,319 epoch 4 - iter 105/159 - loss 3.75795102 - samples/sec: 27.34 - lr: 0.100000
2021-05-16 12:02:32,314 epoch 4 - iter 120/159 - loss 3.77175715 - samples/sec: 26.72 - lr: 0.100000
2021-05-16 12:02:41,383 epoch 4 - iter 135/159 - loss 3.75680608 - samples/sec: 26.47 - lr: 0.100000
2021-05-16 12:02:51,227 epoch 4 - iter 150/159 - loss 3.74422322 - samples/sec: 24.38 - lr: 0.100000
2021-05-16 12:02:57,366 ----------------------------------------------------------------------------------------------------
2021-05-16 12:02:57,366 EPOCH 4 done: loss 3.7386 - lr 0.1000000
2021-05-16 12:03:05,750 DEV : loss 3.8532774448394775 - score 0.0
2021-05-16 12:03:05,850 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:03:16,820 ----------------------------------------------------------------------------------------------------
2021-05-16 12:03:24,805 epoch 5 - iter 15/159 - loss 3.48796887 - samples/sec: 30.06 - lr: 0.100000
2021-05-16 12:03:33,414 epoch 5 - iter 30/159 - loss 3.71112323 - samples/sec: 27.88 - lr: 0.100000
2021-05-16 12:03:42,328 epoch 5 - iter 45/159 - loss 3.69440710 - samples/sec: 26.93 - lr: 0.100000
2021-05-16 12:03:51,999 epoch 5 - iter 60/159 - loss 3.65457598 - samples/sec: 24.82 - lr: 0.100000
2021-05-16 12:04:00,810 epoch 5 - iter 75/159 - loss 3.64584617 - samples/sec: 27.24 - lr: 0.100000
2021-05-16 12:04:10,073 epoch 5 - iter 90/159 - loss 3.76103739 - samples/sec: 25.91 - lr: 0.100000
2021-05-16 12:04:18,178 epoch 5 - iter 105/159 - loss 3.75373479 - samples/sec: 29.62 - lr: 0.100000
2021-05-16 12:04:27,082 epoch 5 - iter 120/159 - loss 3.73813440 - samples/sec: 26.96 - lr: 0.100000
2021-05-16 12:04:35,480 epoch 5 - iter 135/159 - loss 3.78334511 - samples/sec: 28.58 - lr: 0.100000
2021-05-16 12:04:44,155 epoch 5 - iter 150/159 - loss 3.77706150 - samples/sec: 27.67 - lr: 0.100000
2021-05-16 12:04:49,102 ----------------------------------------------------------------------------------------------------
2021-05-16 12:04:49,103 EPOCH 5 done: loss 3.7913 - lr 0.1000000
2021-05-16 12:04:57,458 DEV : loss 3.395667552947998 - score 0.0
2021-05-16 12:04:57,582 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:05:09,011 ----------------------------------------------------------------------------------------------------
2021-05-16 12:05:17,830 epoch 6 - iter 15/159 - loss 3.62610381 - samples/sec: 27.22 - lr: 0.100000
2021-05-16 12:05:27,025 epoch 6 - iter 30/159 - loss 3.64449141 - samples/sec: 26.11 - lr: 0.100000
2021-05-16 12:05:35,076 epoch 6 - iter 45/159 - loss 3.67408175 - samples/sec: 29.81 - lr: 0.100000
2021-05-16 12:05:43,607 epoch 6 - iter 60/159 - loss 3.66206464 - samples/sec: 28.14 - lr: 0.100000
2021-05-16 12:05:52,580 epoch 6 - iter 75/159 - loss 3.66442555 - samples/sec: 26.75 - lr: 0.100000
2021-05-16 12:06:01,027 epoch 6 - iter 90/159 - loss 3.75048652 - samples/sec: 28.42 - lr: 0.100000
2021-05-16 12:06:10,319 epoch 6 - iter 105/159 - loss 3.73361820 - samples/sec: 25.83 - lr: 0.100000
2021-05-16 12:06:19,816 epoch 6 - iter 120/159 - loss 3.75698558 - samples/sec: 25.28 - lr: 0.100000
2021-05-16 12:06:28,184 epoch 6 - iter 135/159 - loss 3.73527506 - samples/sec: 28.69 - lr: 0.100000
2021-05-16 12:06:37,362 epoch 6 - iter 150/159 - loss 3.76214570 - samples/sec: 26.15 - lr: 0.100000
2021-05-16 12:06:42,417 ----------------------------------------------------------------------------------------------------
2021-05-16 12:06:42,417 EPOCH 6 done: loss 3.7613 - lr 0.1000000
2021-05-16 12:06:51,101 DEV : loss 3.6112935543060303 - score 0.0
2021-05-16 12:06:51,201 BAD EPOCHS (no improvement): 1
2021-05-16 12:06:51,201 ----------------------------------------------------------------------------------------------------
2021-05-16 12:07:00,258 epoch 7 - iter 15/159 - loss 3.93418045 - samples/sec: 26.50 - lr: 0.100000
2021-05-16 12:07:08,982 epoch 7 - iter 30/159 - loss 3.67728730 - samples/sec: 27.51 - lr: 0.100000
2021-05-16 12:07:18,276 epoch 7 - iter 45/159 - loss 3.67304690 - samples/sec: 25.83 - lr: 0.100000
2021-05-16 12:07:27,556 epoch 7 - iter 60/159 - loss 3.82345289 - samples/sec: 25.86 - lr: 0.100000
2021-05-16 12:07:36,313 epoch 7 - iter 75/159 - loss 3.83625454 - samples/sec: 27.41 - lr: 0.100000
2021-05-16 12:07:45,215 epoch 7 - iter 90/159 - loss 3.77763665 - samples/sec: 26.96 - lr: 0.100000
2021-05-16 12:07:54,032 epoch 7 - iter 105/159 - loss 3.75870891 - samples/sec: 27.23 - lr: 0.100000
2021-05-16 12:08:03,255 epoch 7 - iter 120/159 - loss 3.75848123 - samples/sec: 26.02 - lr: 0.100000
2021-05-16 12:08:12,139 epoch 7 - iter 135/159 - loss 3.78770855 - samples/sec: 27.03 - lr: 0.100000
2021-05-16 12:08:20,875 epoch 7 - iter 150/159 - loss 3.77726236 - samples/sec: 27.48 - lr: 0.100000
2021-05-16 12:08:25,565 ----------------------------------------------------------------------------------------------------
2021-05-16 12:08:25,566 EPOCH 7 done: loss 3.7644 - lr 0.1000000
2021-05-16 12:08:33,350 DEV : loss 4.009126663208008 - score 0.0
2021-05-16 12:08:33,449 BAD EPOCHS (no improvement): 2
2021-05-16 12:08:33,450 ----------------------------------------------------------------------------------------------------
2021-05-16 12:08:42,142 epoch 8 - iter 15/159 - loss 3.49810041 - samples/sec: 27.62 - lr: 0.100000
2021-05-16 12:08:50,208 epoch 8 - iter 30/159 - loss 3.69816137 - samples/sec: 29.76 - lr: 0.100000
2021-05-16 12:08:59,004 epoch 8 - iter 45/159 - loss 3.86196103 - samples/sec: 27.29 - lr: 0.100000
2021-05-16 12:09:08,185 epoch 8 - iter 60/159 - loss 3.75910669 - samples/sec: 26.14 - lr: 0.100000
2021-05-16 12:09:17,679 epoch 8 - iter 75/159 - loss 3.76309104 - samples/sec: 25.28 - lr: 0.100000
2021-05-16 12:09:26,612 epoch 8 - iter 90/159 - loss 3.72059411 - samples/sec: 26.87 - lr: 0.100000
2021-05-16 12:09:36,977 epoch 8 - iter 105/159 - loss 3.70498125 - samples/sec: 23.16 - lr: 0.100000
2021-05-16 12:09:45,764 epoch 8 - iter 120/159 - loss 3.67902753 - samples/sec: 27.32 - lr: 0.100000
2021-05-16 12:09:55,104 epoch 8 - iter 135/159 - loss 3.66029700 - samples/sec: 25.70 - lr: 0.100000
2021-05-16 12:10:05,606 epoch 8 - iter 150/159 - loss 3.65733404 - samples/sec: 22.86 - lr: 0.100000
2021-05-16 12:10:11,585 ----------------------------------------------------------------------------------------------------
2021-05-16 12:10:11,585 EPOCH 8 done: loss 3.6532 - lr 0.1000000
2021-05-16 12:10:20,798 DEV : loss 3.80291748046875 - score 0.0
2021-05-16 12:10:20,899 BAD EPOCHS (no improvement): 3
2021-05-16 12:10:20,900 ----------------------------------------------------------------------------------------------------
2021-05-16 12:10:31,455 epoch 9 - iter 15/159 - loss 3.30980711 - samples/sec: 22.74 - lr: 0.100000
2021-05-16 12:10:40,490 epoch 9 - iter 30/159 - loss 3.54697684 - samples/sec: 26.58 - lr: 0.100000
2021-05-16 12:10:49,254 epoch 9 - iter 45/159 - loss 3.63081270 - samples/sec: 27.39 - lr: 0.100000
2021-05-16 12:10:57,668 epoch 9 - iter 60/159 - loss 3.63811005 - samples/sec: 28.53 - lr: 0.100000
2021-05-16 12:11:06,494 epoch 9 - iter 75/159 - loss 3.70575978 - samples/sec: 27.20 - lr: 0.100000
2021-05-16 12:11:16,915 epoch 9 - iter 90/159 - loss 3.67885739 - samples/sec: 23.03 - lr: 0.100000
2021-05-16 12:11:25,277 epoch 9 - iter 105/159 - loss 3.70715496 - samples/sec: 28.70 - lr: 0.100000
2021-05-16 12:11:34,633 epoch 9 - iter 120/159 - loss 3.72899296 - samples/sec: 25.66 - lr: 0.100000
2021-05-16 12:11:43,197 epoch 9 - iter 135/159 - loss 3.72437209 - samples/sec: 28.03 - lr: 0.100000
2021-05-16 12:11:52,522 epoch 9 - iter 150/159 - loss 3.71644365 - samples/sec: 25.74 - lr: 0.100000
2021-05-16 12:11:57,840 ----------------------------------------------------------------------------------------------------
2021-05-16 12:11:57,841 EPOCH 9 done: loss 3.7222 - lr 0.1000000
2021-05-16 12:12:05,583 DEV : loss 3.6780052185058594 - score 0.0
Epoch     9: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 12:12:05,682 BAD EPOCHS (no improvement): 4
2021-05-16 12:12:05,682 ----------------------------------------------------------------------------------------------------
2021-05-16 12:12:14,246 epoch 10 - iter 15/159 - loss 3.26959054 - samples/sec: 28.03 - lr: 0.050000
2021-05-16 12:12:23,581 epoch 10 - iter 30/159 - loss 3.49748302 - samples/sec: 25.71 - lr: 0.050000
2021-05-16 12:12:33,332 epoch 10 - iter 45/159 - loss 3.44766813 - samples/sec: 24.62 - lr: 0.050000
2021-05-16 12:12:42,816 epoch 10 - iter 60/159 - loss 3.40711200 - samples/sec: 25.31 - lr: 0.050000
2021-05-16 12:12:51,679 epoch 10 - iter 75/159 - loss 3.41351760 - samples/sec: 27.08 - lr: 0.050000
2021-05-16 12:13:01,169 epoch 10 - iter 90/159 - loss 3.39840955 - samples/sec: 25.29 - lr: 0.050000
2021-05-16 12:13:09,745 epoch 10 - iter 105/159 - loss 3.38913790 - samples/sec: 27.99 - lr: 0.050000
2021-05-16 12:13:17,972 epoch 10 - iter 120/159 - loss 3.40972473 - samples/sec: 29.18 - lr: 0.050000
2021-05-16 12:13:26,804 epoch 10 - iter 135/159 - loss 3.39003664 - samples/sec: 27.18 - lr: 0.050000
2021-05-16 12:13:35,554 epoch 10 - iter 150/159 - loss 3.39098009 - samples/sec: 27.44 - lr: 0.050000
2021-05-16 12:13:40,641 ----------------------------------------------------------------------------------------------------
2021-05-16 12:13:40,643 EPOCH 10 done: loss 3.3955 - lr 0.0500000
2021-05-16 12:13:48,532 DEV : loss 3.3803980350494385 - score 0.0
2021-05-16 12:13:48,632 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:13:59,936 ----------------------------------------------------------------------------------------------------
2021-05-16 12:14:09,218 epoch 11 - iter 15/159 - loss 3.48723761 - samples/sec: 25.86 - lr: 0.050000
2021-05-16 12:14:17,955 epoch 11 - iter 30/159 - loss 3.50294994 - samples/sec: 27.47 - lr: 0.050000
2021-05-16 12:14:28,195 epoch 11 - iter 45/159 - loss 3.43359110 - samples/sec: 23.44 - lr: 0.050000
2021-05-16 12:14:36,530 epoch 11 - iter 60/159 - loss 3.45121222 - samples/sec: 28.80 - lr: 0.050000
2021-05-16 12:14:45,395 epoch 11 - iter 75/159 - loss 3.45420280 - samples/sec: 27.08 - lr: 0.050000
2021-05-16 12:14:53,962 epoch 11 - iter 90/159 - loss 3.39219699 - samples/sec: 28.02 - lr: 0.050000
2021-05-16 12:15:03,735 epoch 11 - iter 105/159 - loss 3.36779812 - samples/sec: 24.56 - lr: 0.050000
2021-05-16 12:15:12,876 epoch 11 - iter 120/159 - loss 3.39830687 - samples/sec: 26.26 - lr: 0.050000
2021-05-16 12:15:21,834 epoch 11 - iter 135/159 - loss 3.40704057 - samples/sec: 26.80 - lr: 0.050000
2021-05-16 12:15:30,836 epoch 11 - iter 150/159 - loss 3.39931179 - samples/sec: 26.68 - lr: 0.050000
2021-05-16 12:15:36,171 ----------------------------------------------------------------------------------------------------
2021-05-16 12:15:36,171 EPOCH 11 done: loss 3.3959 - lr 0.0500000
2021-05-16 12:15:44,693 DEV : loss 3.379225730895996 - score 0.0
2021-05-16 12:15:44,795 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:15:55,784 ----------------------------------------------------------------------------------------------------
2021-05-16 12:16:05,653 epoch 12 - iter 15/159 - loss 3.51265459 - samples/sec: 24.32 - lr: 0.050000
2021-05-16 12:16:14,434 epoch 12 - iter 30/159 - loss 3.54551002 - samples/sec: 27.34 - lr: 0.050000
2021-05-16 12:16:24,011 epoch 12 - iter 45/159 - loss 3.49130181 - samples/sec: 25.06 - lr: 0.050000
2021-05-16 12:16:32,541 epoch 12 - iter 60/159 - loss 3.50750462 - samples/sec: 28.14 - lr: 0.050000
2021-05-16 12:16:41,001 epoch 12 - iter 75/159 - loss 3.47553144 - samples/sec: 28.38 - lr: 0.050000
2021-05-16 12:16:49,986 epoch 12 - iter 90/159 - loss 3.43377668 - samples/sec: 26.72 - lr: 0.050000
2021-05-16 12:16:58,630 epoch 12 - iter 105/159 - loss 3.38909607 - samples/sec: 27.77 - lr: 0.050000
2021-05-16 12:17:07,315 epoch 12 - iter 120/159 - loss 3.39200201 - samples/sec: 27.64 - lr: 0.050000
2021-05-16 12:17:16,041 epoch 12 - iter 135/159 - loss 3.41668012 - samples/sec: 27.51 - lr: 0.050000
2021-05-16 12:17:24,386 epoch 12 - iter 150/159 - loss 3.40155019 - samples/sec: 28.77 - lr: 0.050000
2021-05-16 12:17:29,331 ----------------------------------------------------------------------------------------------------
2021-05-16 12:17:29,332 EPOCH 12 done: loss 3.3974 - lr 0.0500000
2021-05-16 12:17:37,902 DEV : loss 3.4092230796813965 - score 0.0
2021-05-16 12:17:38,001 BAD EPOCHS (no improvement): 1
2021-05-16 12:17:38,002 ----------------------------------------------------------------------------------------------------
2021-05-16 12:17:48,071 epoch 13 - iter 15/159 - loss 3.24660401 - samples/sec: 23.84 - lr: 0.050000
2021-05-16 12:17:58,165 epoch 13 - iter 30/159 - loss 3.26728758 - samples/sec: 23.78 - lr: 0.050000
2021-05-16 12:18:07,445 epoch 13 - iter 45/159 - loss 3.38749218 - samples/sec: 25.87 - lr: 0.050000
2021-05-16 12:18:16,784 epoch 13 - iter 60/159 - loss 3.40514687 - samples/sec: 25.70 - lr: 0.050000
2021-05-16 12:18:25,247 epoch 13 - iter 75/159 - loss 3.42757625 - samples/sec: 28.37 - lr: 0.050000
2021-05-16 12:18:33,600 epoch 13 - iter 90/159 - loss 3.40651979 - samples/sec: 28.73 - lr: 0.050000
2021-05-16 12:18:41,464 epoch 13 - iter 105/159 - loss 3.42706328 - samples/sec: 30.53 - lr: 0.050000
2021-05-16 12:18:50,193 epoch 13 - iter 120/159 - loss 3.40810687 - samples/sec: 27.50 - lr: 0.050000
2021-05-16 12:18:59,357 epoch 13 - iter 135/159 - loss 3.41886912 - samples/sec: 26.21 - lr: 0.050000
2021-05-16 12:19:08,501 epoch 13 - iter 150/159 - loss 3.40574420 - samples/sec: 26.25 - lr: 0.050000
2021-05-16 12:19:13,521 ----------------------------------------------------------------------------------------------------
2021-05-16 12:19:13,521 EPOCH 13 done: loss 3.4038 - lr 0.0500000
2021-05-16 12:19:22,183 DEV : loss 3.4065968990325928 - score 0.0
2021-05-16 12:19:22,282 BAD EPOCHS (no improvement): 2
2021-05-16 12:19:22,282 ----------------------------------------------------------------------------------------------------
2021-05-16 12:19:31,076 epoch 14 - iter 15/159 - loss 3.50046560 - samples/sec: 27.30 - lr: 0.050000
2021-05-16 12:19:39,721 epoch 14 - iter 30/159 - loss 3.50370921 - samples/sec: 27.76 - lr: 0.050000
2021-05-16 12:19:48,710 epoch 14 - iter 45/159 - loss 3.46521373 - samples/sec: 26.70 - lr: 0.050000
2021-05-16 12:19:57,520 epoch 14 - iter 60/159 - loss 3.44356482 - samples/sec: 27.25 - lr: 0.050000
2021-05-16 12:20:07,165 epoch 14 - iter 75/159 - loss 3.47795765 - samples/sec: 24.89 - lr: 0.050000
2021-05-16 12:20:16,707 epoch 14 - iter 90/159 - loss 3.43691242 - samples/sec: 25.15 - lr: 0.050000
2021-05-16 12:20:26,225 epoch 14 - iter 105/159 - loss 3.41546054 - samples/sec: 25.22 - lr: 0.050000
2021-05-16 12:20:35,311 epoch 14 - iter 120/159 - loss 3.37682178 - samples/sec: 26.42 - lr: 0.050000
2021-05-16 12:20:45,001 epoch 14 - iter 135/159 - loss 3.39172030 - samples/sec: 24.77 - lr: 0.050000
2021-05-16 12:20:55,305 epoch 14 - iter 150/159 - loss 3.39811660 - samples/sec: 23.30 - lr: 0.050000
2021-05-16 12:21:00,938 ----------------------------------------------------------------------------------------------------
2021-05-16 12:21:00,938 EPOCH 14 done: loss 3.4036 - lr 0.0500000
2021-05-16 12:21:09,031 DEV : loss 3.387791395187378 - score 0.0
2021-05-16 12:21:09,121 BAD EPOCHS (no improvement): 3
2021-05-16 12:21:09,122 ----------------------------------------------------------------------------------------------------
2021-05-16 12:21:18,217 epoch 15 - iter 15/159 - loss 3.39043503 - samples/sec: 26.39 - lr: 0.050000
2021-05-16 12:21:27,862 epoch 15 - iter 30/159 - loss 3.50213123 - samples/sec: 24.89 - lr: 0.050000
2021-05-16 12:21:36,890 epoch 15 - iter 45/159 - loss 3.44489161 - samples/sec: 26.59 - lr: 0.050000
2021-05-16 12:21:46,752 epoch 15 - iter 60/159 - loss 3.39597619 - samples/sec: 24.34 - lr: 0.050000
2021-05-16 12:21:55,770 epoch 15 - iter 75/159 - loss 3.37374235 - samples/sec: 26.62 - lr: 0.050000
2021-05-16 12:22:04,792 epoch 15 - iter 90/159 - loss 3.33699293 - samples/sec: 26.61 - lr: 0.050000
2021-05-16 12:22:13,175 epoch 15 - iter 105/159 - loss 3.34255827 - samples/sec: 28.63 - lr: 0.050000
2021-05-16 12:22:23,151 epoch 15 - iter 120/159 - loss 3.35932490 - samples/sec: 24.06 - lr: 0.050000
2021-05-16 12:22:32,157 epoch 15 - iter 135/159 - loss 3.37824517 - samples/sec: 26.65 - lr: 0.050000
2021-05-16 12:22:40,672 epoch 15 - iter 150/159 - loss 3.39644225 - samples/sec: 28.19 - lr: 0.050000
2021-05-16 12:22:45,558 ----------------------------------------------------------------------------------------------------
2021-05-16 12:22:45,559 EPOCH 15 done: loss 3.3947 - lr 0.0500000
2021-05-16 12:22:55,100 DEV : loss 3.620047092437744 - score 0.0
Epoch    15: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 12:22:55,201 BAD EPOCHS (no improvement): 4
2021-05-16 12:22:55,201 ----------------------------------------------------------------------------------------------------
2021-05-16 12:23:04,444 epoch 16 - iter 15/159 - loss 3.17981626 - samples/sec: 25.97 - lr: 0.025000
2021-05-16 12:23:13,317 epoch 16 - iter 30/159 - loss 3.12907073 - samples/sec: 27.06 - lr: 0.025000
2021-05-16 12:23:22,649 epoch 16 - iter 45/159 - loss 3.20469640 - samples/sec: 25.72 - lr: 0.025000
2021-05-16 12:23:31,884 epoch 16 - iter 60/159 - loss 3.28197639 - samples/sec: 26.00 - lr: 0.025000
2021-05-16 12:23:40,776 epoch 16 - iter 75/159 - loss 3.29677499 - samples/sec: 26.99 - lr: 0.025000
2021-05-16 12:23:49,849 epoch 16 - iter 90/159 - loss 3.31295172 - samples/sec: 26.46 - lr: 0.025000
2021-05-16 12:23:59,985 epoch 16 - iter 105/159 - loss 3.34683604 - samples/sec: 23.68 - lr: 0.025000
2021-05-16 12:24:09,165 epoch 16 - iter 120/159 - loss 3.35837867 - samples/sec: 26.15 - lr: 0.025000
2021-05-16 12:24:18,063 epoch 16 - iter 135/159 - loss 3.36019401 - samples/sec: 26.97 - lr: 0.025000
2021-05-16 12:24:27,151 epoch 16 - iter 150/159 - loss 3.37422101 - samples/sec: 26.41 - lr: 0.025000
2021-05-16 12:24:32,872 ----------------------------------------------------------------------------------------------------
2021-05-16 12:24:32,872 EPOCH 16 done: loss 3.3764 - lr 0.0250000
2021-05-16 12:24:41,223 DEV : loss 3.436584234237671 - score 0.0
2021-05-16 12:24:41,323 BAD EPOCHS (no improvement): 1
2021-05-16 12:24:41,324 ----------------------------------------------------------------------------------------------------
2021-05-16 12:24:50,211 epoch 17 - iter 15/159 - loss 3.41146819 - samples/sec: 27.01 - lr: 0.025000
2021-05-16 12:24:59,497 epoch 17 - iter 30/159 - loss 3.45960628 - samples/sec: 25.85 - lr: 0.025000
2021-05-16 12:25:09,767 epoch 17 - iter 45/159 - loss 3.38514152 - samples/sec: 23.37 - lr: 0.025000
2021-05-16 12:25:19,919 epoch 17 - iter 60/159 - loss 3.34726358 - samples/sec: 23.64 - lr: 0.025000
2021-05-16 12:25:29,749 epoch 17 - iter 75/159 - loss 3.38541772 - samples/sec: 24.42 - lr: 0.025000
2021-05-16 12:25:39,678 epoch 17 - iter 90/159 - loss 3.38900378 - samples/sec: 24.18 - lr: 0.025000
2021-05-16 12:25:49,999 epoch 17 - iter 105/159 - loss 3.43500110 - samples/sec: 23.26 - lr: 0.025000
2021-05-16 12:25:59,889 epoch 17 - iter 120/159 - loss 3.40011818 - samples/sec: 24.27 - lr: 0.025000
2021-05-16 12:26:09,762 epoch 17 - iter 135/159 - loss 3.38734447 - samples/sec: 24.31 - lr: 0.025000
2021-05-16 12:26:18,330 epoch 17 - iter 150/159 - loss 3.38529651 - samples/sec: 28.02 - lr: 0.025000
2021-05-16 12:26:24,178 ----------------------------------------------------------------------------------------------------
2021-05-16 12:26:24,184 EPOCH 17 done: loss 3.3674 - lr 0.0250000
2021-05-16 12:26:34,291 DEV : loss 3.38067364692688 - score 0.0
2021-05-16 12:26:34,393 BAD EPOCHS (no improvement): 2
2021-05-16 12:26:34,393 ----------------------------------------------------------------------------------------------------
2021-05-16 12:26:43,664 epoch 18 - iter 15/159 - loss 3.49269204 - samples/sec: 25.89 - lr: 0.025000
2021-05-16 12:26:53,526 epoch 18 - iter 30/159 - loss 3.45268975 - samples/sec: 24.34 - lr: 0.025000
2021-05-16 12:27:03,121 epoch 18 - iter 45/159 - loss 3.38786722 - samples/sec: 25.02 - lr: 0.025000
2021-05-16 12:27:11,725 epoch 18 - iter 60/159 - loss 3.40334127 - samples/sec: 27.90 - lr: 0.025000
2021-05-16 12:27:20,300 epoch 18 - iter 75/159 - loss 3.35939321 - samples/sec: 27.99 - lr: 0.025000
2021-05-16 12:27:30,009 epoch 18 - iter 90/159 - loss 3.35134796 - samples/sec: 24.73 - lr: 0.025000
2021-05-16 12:27:38,941 epoch 18 - iter 105/159 - loss 3.33993581 - samples/sec: 26.87 - lr: 0.025000
2021-05-16 12:27:47,712 epoch 18 - iter 120/159 - loss 3.38162605 - samples/sec: 27.37 - lr: 0.025000
2021-05-16 12:27:56,091 epoch 18 - iter 135/159 - loss 3.39306808 - samples/sec: 28.65 - lr: 0.025000
2021-05-16 12:28:05,054 epoch 18 - iter 150/159 - loss 3.35373888 - samples/sec: 26.78 - lr: 0.025000
2021-05-16 12:28:10,654 ----------------------------------------------------------------------------------------------------
2021-05-16 12:28:10,662 EPOCH 18 done: loss 3.3681 - lr 0.0250000
2021-05-16 12:28:20,270 DEV : loss 3.3807122707366943 - score 0.0
2021-05-16 12:28:20,371 BAD EPOCHS (no improvement): 3
2021-05-16 12:28:20,372 ----------------------------------------------------------------------------------------------------
2021-05-16 12:28:30,009 epoch 19 - iter 15/159 - loss 3.36488132 - samples/sec: 24.91 - lr: 0.025000
2021-05-16 12:28:39,032 epoch 19 - iter 30/159 - loss 3.41064034 - samples/sec: 26.60 - lr: 0.025000
2021-05-16 12:28:49,322 epoch 19 - iter 45/159 - loss 3.41423133 - samples/sec: 23.33 - lr: 0.025000
2021-05-16 12:28:59,442 epoch 19 - iter 60/159 - loss 3.39637327 - samples/sec: 23.72 - lr: 0.025000
2021-05-16 12:29:08,825 epoch 19 - iter 75/159 - loss 3.36380895 - samples/sec: 25.58 - lr: 0.025000
2021-05-16 12:29:16,981 epoch 19 - iter 90/159 - loss 3.35148905 - samples/sec: 29.43 - lr: 0.025000
2021-05-16 12:29:27,384 epoch 19 - iter 105/159 - loss 3.34959188 - samples/sec: 23.07 - lr: 0.025000
2021-05-16 12:29:37,489 epoch 19 - iter 120/159 - loss 3.36644360 - samples/sec: 23.76 - lr: 0.025000
2021-05-16 12:29:47,340 epoch 19 - iter 135/159 - loss 3.38056252 - samples/sec: 24.36 - lr: 0.025000
2021-05-16 12:29:56,420 epoch 19 - iter 150/159 - loss 3.38517978 - samples/sec: 26.44 - lr: 0.025000
2021-05-16 12:30:01,474 ----------------------------------------------------------------------------------------------------
2021-05-16 12:30:01,474 EPOCH 19 done: loss 3.3706 - lr 0.0250000
2021-05-16 12:30:11,840 DEV : loss 3.394745111465454 - score 0.0
Epoch    19: reducing learning rate of group 0 to 1.2500e-02.
2021-05-16 12:30:11,980 BAD EPOCHS (no improvement): 4
2021-05-16 12:30:11,980 ----------------------------------------------------------------------------------------------------
2021-05-16 12:30:21,480 epoch 20 - iter 15/159 - loss 3.35401268 - samples/sec: 25.27 - lr: 0.012500
2021-05-16 12:30:31,613 epoch 20 - iter 30/159 - loss 3.44121744 - samples/sec: 23.69 - lr: 0.012500
2021-05-16 12:30:42,046 epoch 20 - iter 45/159 - loss 3.46663442 - samples/sec: 23.01 - lr: 0.012500
2021-05-16 12:30:51,190 epoch 20 - iter 60/159 - loss 3.36792401 - samples/sec: 26.25 - lr: 0.012500
2021-05-16 12:31:00,430 epoch 20 - iter 75/159 - loss 3.38136293 - samples/sec: 25.98 - lr: 0.012500
2021-05-16 12:31:09,831 epoch 20 - iter 90/159 - loss 3.40710447 - samples/sec: 25.53 - lr: 0.012500
2021-05-16 12:31:18,286 epoch 20 - iter 105/159 - loss 3.39999126 - samples/sec: 28.40 - lr: 0.012500
2021-05-16 12:31:26,989 epoch 20 - iter 120/159 - loss 3.37516422 - samples/sec: 27.58 - lr: 0.012500
2021-05-16 12:31:35,620 epoch 20 - iter 135/159 - loss 3.36201988 - samples/sec: 27.81 - lr: 0.012500
2021-05-16 12:31:44,680 epoch 20 - iter 150/159 - loss 3.34732846 - samples/sec: 26.50 - lr: 0.012500
2021-05-16 12:31:50,359 ----------------------------------------------------------------------------------------------------
2021-05-16 12:31:50,360 EPOCH 20 done: loss 3.3638 - lr 0.0125000
2021-05-16 12:31:58,678 DEV : loss 3.403066873550415 - score 0.0
2021-05-16 12:31:58,778 BAD EPOCHS (no improvement): 1
2021-05-16 12:31:58,778 ----------------------------------------------------------------------------------------------------
2021-05-16 12:32:07,557 epoch 21 - iter 15/159 - loss 3.59487856 - samples/sec: 27.34 - lr: 0.012500
2021-05-16 12:32:16,197 epoch 21 - iter 30/159 - loss 3.50888972 - samples/sec: 27.78 - lr: 0.012500
2021-05-16 12:32:25,122 epoch 21 - iter 45/159 - loss 3.35454768 - samples/sec: 26.89 - lr: 0.012500
2021-05-16 12:32:34,137 epoch 21 - iter 60/159 - loss 3.34415007 - samples/sec: 26.63 - lr: 0.012500
2021-05-16 12:32:42,759 epoch 21 - iter 75/159 - loss 3.36773410 - samples/sec: 27.84 - lr: 0.012500
2021-05-16 12:32:51,422 epoch 21 - iter 90/159 - loss 3.34419685 - samples/sec: 27.71 - lr: 0.012500
2021-05-16 12:33:01,394 epoch 21 - iter 105/159 - loss 3.35746428 - samples/sec: 24.07 - lr: 0.012500
2021-05-16 12:33:09,685 epoch 21 - iter 120/159 - loss 3.34502740 - samples/sec: 28.95 - lr: 0.012500
2021-05-16 12:33:18,325 epoch 21 - iter 135/159 - loss 3.38037786 - samples/sec: 27.78 - lr: 0.012500
2021-05-16 12:33:26,668 epoch 21 - iter 150/159 - loss 3.36809520 - samples/sec: 28.77 - lr: 0.012500
2021-05-16 12:33:31,397 ----------------------------------------------------------------------------------------------------
2021-05-16 12:33:31,398 EPOCH 21 done: loss 3.3594 - lr 0.0125000
2021-05-16 12:33:38,833 DEV : loss 3.3914506435394287 - score 0.0
2021-05-16 12:33:38,932 BAD EPOCHS (no improvement): 2
2021-05-16 12:33:38,933 ----------------------------------------------------------------------------------------------------
2021-05-16 12:33:48,338 epoch 22 - iter 15/159 - loss 3.16359577 - samples/sec: 25.52 - lr: 0.012500
2021-05-16 12:33:57,716 epoch 22 - iter 30/159 - loss 3.35304029 - samples/sec: 25.60 - lr: 0.012500
2021-05-16 12:34:07,670 epoch 22 - iter 45/159 - loss 3.39705591 - samples/sec: 24.11 - lr: 0.012500
2021-05-16 12:34:15,998 epoch 22 - iter 60/159 - loss 3.35875548 - samples/sec: 28.82 - lr: 0.012500
2021-05-16 12:34:25,668 epoch 22 - iter 75/159 - loss 3.37616035 - samples/sec: 24.82 - lr: 0.012500
2021-05-16 12:34:36,080 epoch 22 - iter 90/159 - loss 3.38969122 - samples/sec: 23.05 - lr: 0.012500
2021-05-16 12:34:47,283 epoch 22 - iter 105/159 - loss 3.35729203 - samples/sec: 21.43 - lr: 0.012500
2021-05-16 12:34:56,656 epoch 22 - iter 120/159 - loss 3.39729882 - samples/sec: 25.61 - lr: 0.012500
2021-05-16 12:35:05,733 epoch 22 - iter 135/159 - loss 3.36701707 - samples/sec: 26.44 - lr: 0.012500
2021-05-16 12:35:14,558 epoch 22 - iter 150/159 - loss 3.36823426 - samples/sec: 27.20 - lr: 0.012500
2021-05-16 12:35:19,776 ----------------------------------------------------------------------------------------------------
2021-05-16 12:35:19,777 EPOCH 22 done: loss 3.3595 - lr 0.0125000
2021-05-16 12:35:28,168 DEV : loss 3.378418207168579 - score 0.0
2021-05-16 12:35:28,267 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:35:39,244 ----------------------------------------------------------------------------------------------------
2021-05-16 12:35:48,885 epoch 23 - iter 15/159 - loss 3.33139187 - samples/sec: 24.90 - lr: 0.012500
2021-05-16 12:35:57,955 epoch 23 - iter 30/159 - loss 3.24541503 - samples/sec: 26.46 - lr: 0.012500
2021-05-16 12:36:07,068 epoch 23 - iter 45/159 - loss 3.32896824 - samples/sec: 26.34 - lr: 0.012500
2021-05-16 12:36:16,310 epoch 23 - iter 60/159 - loss 3.37542035 - samples/sec: 25.97 - lr: 0.012500
2021-05-16 12:36:25,814 epoch 23 - iter 75/159 - loss 3.38869558 - samples/sec: 25.26 - lr: 0.012500
2021-05-16 12:36:35,114 epoch 23 - iter 90/159 - loss 3.33108829 - samples/sec: 25.81 - lr: 0.012500
2021-05-16 12:36:44,843 epoch 23 - iter 105/159 - loss 3.36412900 - samples/sec: 24.67 - lr: 0.012500
2021-05-16 12:36:54,528 epoch 23 - iter 120/159 - loss 3.33991753 - samples/sec: 24.78 - lr: 0.012500
2021-05-16 12:37:05,041 epoch 23 - iter 135/159 - loss 3.33550534 - samples/sec: 22.83 - lr: 0.012500
2021-05-16 12:37:14,965 epoch 23 - iter 150/159 - loss 3.34240517 - samples/sec: 24.19 - lr: 0.012500
2021-05-16 12:37:19,642 ----------------------------------------------------------------------------------------------------
2021-05-16 12:37:19,642 EPOCH 23 done: loss 3.3563 - lr 0.0125000
2021-05-16 12:37:27,068 DEV : loss 3.386911392211914 - score 0.0
2021-05-16 12:37:27,168 BAD EPOCHS (no improvement): 1
2021-05-16 12:37:27,169 ----------------------------------------------------------------------------------------------------
2021-05-16 12:37:36,429 epoch 24 - iter 15/159 - loss 3.17376981 - samples/sec: 25.92 - lr: 0.012500
2021-05-16 12:37:45,302 epoch 24 - iter 30/159 - loss 3.23259771 - samples/sec: 27.05 - lr: 0.012500
2021-05-16 12:37:53,657 epoch 24 - iter 45/159 - loss 3.33021860 - samples/sec: 28.73 - lr: 0.012500
2021-05-16 12:38:03,039 epoch 24 - iter 60/159 - loss 3.31008658 - samples/sec: 25.58 - lr: 0.012500
2021-05-16 12:38:11,905 epoch 24 - iter 75/159 - loss 3.31331421 - samples/sec: 27.07 - lr: 0.012500
2021-05-16 12:38:20,215 epoch 24 - iter 90/159 - loss 3.35825367 - samples/sec: 28.89 - lr: 0.012500
2021-05-16 12:38:29,445 epoch 24 - iter 105/159 - loss 3.39244296 - samples/sec: 26.00 - lr: 0.012500
2021-05-16 12:38:38,166 epoch 24 - iter 120/159 - loss 3.36631595 - samples/sec: 27.52 - lr: 0.012500
2021-05-16 12:38:46,468 epoch 24 - iter 135/159 - loss 3.36266558 - samples/sec: 28.91 - lr: 0.012500
2021-05-16 12:38:55,301 epoch 24 - iter 150/159 - loss 3.34373566 - samples/sec: 27.18 - lr: 0.012500
2021-05-16 12:39:00,825 ----------------------------------------------------------------------------------------------------
2021-05-16 12:39:00,826 EPOCH 24 done: loss 3.3564 - lr 0.0125000
2021-05-16 12:39:07,963 DEV : loss 3.3838353157043457 - score 0.0
2021-05-16 12:39:08,065 BAD EPOCHS (no improvement): 2
2021-05-16 12:39:08,065 ----------------------------------------------------------------------------------------------------
2021-05-16 12:39:16,956 epoch 25 - iter 15/159 - loss 3.45691549 - samples/sec: 27.00 - lr: 0.012500
2021-05-16 12:39:26,205 epoch 25 - iter 30/159 - loss 3.45488581 - samples/sec: 25.95 - lr: 0.012500
2021-05-16 12:39:36,540 epoch 25 - iter 45/159 - loss 3.37228799 - samples/sec: 23.22 - lr: 0.012500
2021-05-16 12:39:44,733 epoch 25 - iter 60/159 - loss 3.37604992 - samples/sec: 29.30 - lr: 0.012500
2021-05-16 12:39:53,702 epoch 25 - iter 75/159 - loss 3.35856197 - samples/sec: 26.77 - lr: 0.012500
2021-05-16 12:40:02,723 epoch 25 - iter 90/159 - loss 3.35031382 - samples/sec: 26.61 - lr: 0.012500
2021-05-16 12:40:11,246 epoch 25 - iter 105/159 - loss 3.37197765 - samples/sec: 28.16 - lr: 0.012500
2021-05-16 12:40:20,056 epoch 25 - iter 120/159 - loss 3.36304180 - samples/sec: 27.25 - lr: 0.012500
2021-05-16 12:40:28,831 epoch 25 - iter 135/159 - loss 3.36582923 - samples/sec: 27.35 - lr: 0.012500
2021-05-16 12:40:38,159 epoch 25 - iter 150/159 - loss 3.36587931 - samples/sec: 25.73 - lr: 0.012500
2021-05-16 12:40:43,427 ----------------------------------------------------------------------------------------------------
2021-05-16 12:40:43,427 EPOCH 25 done: loss 3.3577 - lr 0.0125000
2021-05-16 12:40:51,480 DEV : loss 3.3785645961761475 - score 0.0
2021-05-16 12:40:51,579 BAD EPOCHS (no improvement): 3
2021-05-16 12:40:51,580 ----------------------------------------------------------------------------------------------------
2021-05-16 12:41:00,536 epoch 26 - iter 15/159 - loss 3.54416134 - samples/sec: 26.80 - lr: 0.012500
2021-05-16 12:41:09,672 epoch 26 - iter 30/159 - loss 3.43214094 - samples/sec: 26.28 - lr: 0.012500
2021-05-16 12:41:18,511 epoch 26 - iter 45/159 - loss 3.33776599 - samples/sec: 27.16 - lr: 0.012500
2021-05-16 12:41:29,332 epoch 26 - iter 60/159 - loss 3.32391998 - samples/sec: 22.18 - lr: 0.012500
2021-05-16 12:41:40,087 epoch 26 - iter 75/159 - loss 3.30998532 - samples/sec: 22.32 - lr: 0.012500
2021-05-16 12:41:49,770 epoch 26 - iter 90/159 - loss 3.36767083 - samples/sec: 24.80 - lr: 0.012500
2021-05-16 12:41:59,640 epoch 26 - iter 105/159 - loss 3.37410155 - samples/sec: 24.32 - lr: 0.012500
2021-05-16 12:42:10,207 epoch 26 - iter 120/159 - loss 3.36098352 - samples/sec: 22.71 - lr: 0.012500
2021-05-16 12:42:19,445 epoch 26 - iter 135/159 - loss 3.36644293 - samples/sec: 25.99 - lr: 0.012500
2021-05-16 12:42:30,146 epoch 26 - iter 150/159 - loss 3.36175479 - samples/sec: 22.43 - lr: 0.012500
2021-05-16 12:42:35,473 ----------------------------------------------------------------------------------------------------
2021-05-16 12:42:35,474 EPOCH 26 done: loss 3.3554 - lr 0.0125000
2021-05-16 12:42:46,572 DEV : loss 3.3845584392547607 - score 0.0
Epoch    26: reducing learning rate of group 0 to 6.2500e-03.
2021-05-16 12:42:46,696 BAD EPOCHS (no improvement): 4
2021-05-16 12:42:46,696 ----------------------------------------------------------------------------------------------------
2021-05-16 12:42:55,050 epoch 27 - iter 15/159 - loss 3.02352287 - samples/sec: 28.73 - lr: 0.006250
2021-05-16 12:43:04,577 epoch 27 - iter 30/159 - loss 3.16771217 - samples/sec: 25.21 - lr: 0.006250
2021-05-16 12:43:14,083 epoch 27 - iter 45/159 - loss 3.24893868 - samples/sec: 25.25 - lr: 0.006250
2021-05-16 12:43:23,059 epoch 27 - iter 60/159 - loss 3.29659347 - samples/sec: 26.76 - lr: 0.006250
2021-05-16 12:43:31,785 epoch 27 - iter 75/159 - loss 3.29607464 - samples/sec: 27.51 - lr: 0.006250
2021-05-16 12:43:40,557 epoch 27 - iter 90/159 - loss 3.30432867 - samples/sec: 27.36 - lr: 0.006250
2021-05-16 12:43:49,168 epoch 27 - iter 105/159 - loss 3.31450547 - samples/sec: 27.88 - lr: 0.006250
2021-05-16 12:43:58,441 epoch 27 - iter 120/159 - loss 3.30475445 - samples/sec: 25.88 - lr: 0.006250
2021-05-16 12:44:07,112 epoch 27 - iter 135/159 - loss 3.33817428 - samples/sec: 27.68 - lr: 0.006250
2021-05-16 12:44:15,959 epoch 27 - iter 150/159 - loss 3.35466759 - samples/sec: 27.14 - lr: 0.006250
2021-05-16 12:44:21,372 ----------------------------------------------------------------------------------------------------
2021-05-16 12:44:21,372 EPOCH 27 done: loss 3.3510 - lr 0.0062500
2021-05-16 12:44:29,000 DEV : loss 3.379631280899048 - score 0.0
2021-05-16 12:44:29,099 BAD EPOCHS (no improvement): 1
2021-05-16 12:44:29,100 ----------------------------------------------------------------------------------------------------
2021-05-16 12:44:37,809 epoch 28 - iter 15/159 - loss 3.41642200 - samples/sec: 27.56 - lr: 0.006250
2021-05-16 12:44:46,766 epoch 28 - iter 30/159 - loss 3.24924037 - samples/sec: 26.80 - lr: 0.006250
2021-05-16 12:44:55,997 epoch 28 - iter 45/159 - loss 3.31496523 - samples/sec: 26.00 - lr: 0.006250
2021-05-16 12:45:05,229 epoch 28 - iter 60/159 - loss 3.36440862 - samples/sec: 26.00 - lr: 0.006250
2021-05-16 12:45:14,253 epoch 28 - iter 75/159 - loss 3.30105124 - samples/sec: 26.60 - lr: 0.006250
2021-05-16 12:45:23,295 epoch 28 - iter 90/159 - loss 3.34573851 - samples/sec: 26.55 - lr: 0.006250
2021-05-16 12:45:32,437 epoch 28 - iter 105/159 - loss 3.35100802 - samples/sec: 26.26 - lr: 0.006250
2021-05-16 12:45:42,442 epoch 28 - iter 120/159 - loss 3.36620831 - samples/sec: 23.99 - lr: 0.006250
2021-05-16 12:45:51,813 epoch 28 - iter 135/159 - loss 3.34226445 - samples/sec: 25.61 - lr: 0.006250
2021-05-16 12:46:00,723 epoch 28 - iter 150/159 - loss 3.34030196 - samples/sec: 26.94 - lr: 0.006250
2021-05-16 12:46:05,179 ----------------------------------------------------------------------------------------------------
2021-05-16 12:46:05,179 EPOCH 28 done: loss 3.3556 - lr 0.0062500
2021-05-16 12:46:13,350 DEV : loss 3.384953498840332 - score 0.0
2021-05-16 12:46:13,450 BAD EPOCHS (no improvement): 2
2021-05-16 12:46:13,450 ----------------------------------------------------------------------------------------------------
2021-05-16 12:46:22,479 epoch 29 - iter 15/159 - loss 3.39323271 - samples/sec: 26.58 - lr: 0.006250
2021-05-16 12:46:31,131 epoch 29 - iter 30/159 - loss 3.32762651 - samples/sec: 27.74 - lr: 0.006250
2021-05-16 12:46:39,880 epoch 29 - iter 45/159 - loss 3.34601633 - samples/sec: 27.44 - lr: 0.006250
2021-05-16 12:46:48,637 epoch 29 - iter 60/159 - loss 3.31500291 - samples/sec: 27.41 - lr: 0.006250
2021-05-16 12:46:57,197 epoch 29 - iter 75/159 - loss 3.32372216 - samples/sec: 28.04 - lr: 0.006250
2021-05-16 12:47:06,245 epoch 29 - iter 90/159 - loss 3.31113948 - samples/sec: 26.53 - lr: 0.006250
2021-05-16 12:47:14,974 epoch 29 - iter 105/159 - loss 3.34477147 - samples/sec: 27.50 - lr: 0.006250
2021-05-16 12:47:24,115 epoch 29 - iter 120/159 - loss 3.33249390 - samples/sec: 26.26 - lr: 0.006250
2021-05-16 12:47:32,202 epoch 29 - iter 135/159 - loss 3.33380784 - samples/sec: 29.68 - lr: 0.006250
2021-05-16 12:47:40,540 epoch 29 - iter 150/159 - loss 3.34967563 - samples/sec: 28.79 - lr: 0.006250
2021-05-16 12:47:45,877 ----------------------------------------------------------------------------------------------------
2021-05-16 12:47:45,877 EPOCH 29 done: loss 3.3541 - lr 0.0062500
2021-05-16 12:47:53,945 DEV : loss 3.3779423236846924 - score 0.0
2021-05-16 12:47:54,045 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:48:04,931 ----------------------------------------------------------------------------------------------------
2021-05-16 12:48:13,548 epoch 30 - iter 15/159 - loss 3.25531111 - samples/sec: 27.86 - lr: 0.006250
2021-05-16 12:48:21,939 epoch 30 - iter 30/159 - loss 3.36335927 - samples/sec: 28.60 - lr: 0.006250
2021-05-16 12:48:30,526 epoch 30 - iter 45/159 - loss 3.33416100 - samples/sec: 27.95 - lr: 0.006250
2021-05-16 12:48:39,125 epoch 30 - iter 60/159 - loss 3.31265393 - samples/sec: 27.91 - lr: 0.006250
2021-05-16 12:48:47,632 epoch 30 - iter 75/159 - loss 3.27301733 - samples/sec: 28.22 - lr: 0.006250
2021-05-16 12:48:56,653 epoch 30 - iter 90/159 - loss 3.28106548 - samples/sec: 26.61 - lr: 0.006250
2021-05-16 12:49:05,763 epoch 30 - iter 105/159 - loss 3.31142432 - samples/sec: 26.35 - lr: 0.006250
2021-05-16 12:49:14,107 epoch 30 - iter 120/159 - loss 3.32466503 - samples/sec: 28.77 - lr: 0.006250
2021-05-16 12:49:22,473 epoch 30 - iter 135/159 - loss 3.33185606 - samples/sec: 28.69 - lr: 0.006250
2021-05-16 12:49:30,659 epoch 30 - iter 150/159 - loss 3.35167913 - samples/sec: 29.33 - lr: 0.006250
2021-05-16 12:49:35,847 ----------------------------------------------------------------------------------------------------
2021-05-16 12:49:35,848 EPOCH 30 done: loss 3.3552 - lr 0.0062500
2021-05-16 12:49:44,085 DEV : loss 3.380751132965088 - score 0.0
2021-05-16 12:49:44,183 BAD EPOCHS (no improvement): 1
2021-05-16 12:49:55,041 ----------------------------------------------------------------------------------------------------
2021-05-16 12:49:55,041 Testing using best model ...
2021-05-16 12:49:55,041 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.pdtb.cdtb/best-model.pt
2021-05-16 12:50:27,384 0.0000	0.0000	0.0000
2021-05-16 12:50:27,384 
Results:
- F1-score (micro) 0.0000
- F1-score (macro) 0.0000

By class:
SENT       tp: 0 - fp: 0 - fn: 404 - precision: 0.0000 - recall: 0.0000 - f1-score: 0.0000
2021-05-16 12:50:27,384 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb/
2021-05-16 12:50:27,405 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb
2021-05-16 12:50:27,405 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb/sent_train.txt
2021-05-16 12:50:27,405 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb/sent_dev.txt
2021-05-16 12:50:27,405 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb/sent_test.txt
Corpus: 2146 train + 371 dev + 396 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 12:50:43,534 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,540 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 12:50:43,541 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,541 Corpus: "Corpus: 2146 train + 371 dev + 396 test sentences"
2021-05-16 12:50:43,541 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,541 Parameters:
2021-05-16 12:50:43,541  - learning_rate: "0.1"
2021-05-16 12:50:43,541  - mini_batch_size: "16"
2021-05-16 12:50:43,541  - patience: "3"
2021-05-16 12:50:43,542  - anneal_factor: "0.5"
2021-05-16 12:50:43,542  - max_epochs: "30"
2021-05-16 12:50:43,542  - shuffle: "True"
2021-05-16 12:50:43,542  - train_with_dev: "False"
2021-05-16 12:50:43,542  - batch_growth_annealing: "False"
2021-05-16 12:50:43,542 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,542 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb"
2021-05-16 12:50:43,542 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,542 Device: cuda:0
2021-05-16 12:50:43,542 ----------------------------------------------------------------------------------------------------
2021-05-16 12:50:43,542 Embeddings storage mode: cpu
2021-05-16 12:50:43,546 ----------------------------------------------------------------------------------------------------
2021-05-16 12:51:04,381 epoch 1 - iter 13/135 - loss 6.66772954 - samples/sec: 9.98 - lr: 0.100000
2021-05-16 12:51:26,046 epoch 1 - iter 26/135 - loss 5.08526124 - samples/sec: 9.60 - lr: 0.100000
2021-05-16 12:51:48,077 epoch 1 - iter 39/135 - loss 4.32862653 - samples/sec: 9.44 - lr: 0.100000
2021-05-16 12:52:10,304 epoch 1 - iter 52/135 - loss 3.78847176 - samples/sec: 9.36 - lr: 0.100000
2021-05-16 12:52:32,831 epoch 1 - iter 65/135 - loss 3.39017892 - samples/sec: 9.23 - lr: 0.100000
2021-05-16 12:52:53,753 epoch 1 - iter 78/135 - loss 3.08941228 - samples/sec: 9.94 - lr: 0.100000
2021-05-16 12:53:15,100 epoch 1 - iter 91/135 - loss 2.84189768 - samples/sec: 9.74 - lr: 0.100000
2021-05-16 12:53:36,187 epoch 1 - iter 104/135 - loss 2.65159372 - samples/sec: 9.86 - lr: 0.100000
2021-05-16 12:53:57,204 epoch 1 - iter 117/135 - loss 2.47026947 - samples/sec: 9.90 - lr: 0.100000
2021-05-16 12:54:18,865 epoch 1 - iter 130/135 - loss 2.31431226 - samples/sec: 9.60 - lr: 0.100000
2021-05-16 12:54:26,905 ----------------------------------------------------------------------------------------------------
2021-05-16 12:54:26,906 EPOCH 1 done: loss 2.2726 - lr 0.1000000
2021-05-16 12:54:58,405 DEV : loss 0.33265915513038635 - score 0.8983
2021-05-16 12:54:58,474 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:55:10,083 ----------------------------------------------------------------------------------------------------
2021-05-16 12:55:18,385 epoch 2 - iter 13/135 - loss 0.84176131 - samples/sec: 25.06 - lr: 0.100000
2021-05-16 12:55:27,132 epoch 2 - iter 26/135 - loss 0.87877321 - samples/sec: 23.78 - lr: 0.100000
2021-05-16 12:55:35,146 epoch 2 - iter 39/135 - loss 0.79014518 - samples/sec: 25.96 - lr: 0.100000
2021-05-16 12:55:43,456 epoch 2 - iter 52/135 - loss 0.83825340 - samples/sec: 25.03 - lr: 0.100000
2021-05-16 12:55:52,176 epoch 2 - iter 65/135 - loss 0.84801719 - samples/sec: 23.86 - lr: 0.100000
2021-05-16 12:56:01,585 epoch 2 - iter 78/135 - loss 0.86458778 - samples/sec: 22.11 - lr: 0.100000
2021-05-16 12:56:09,584 epoch 2 - iter 91/135 - loss 0.89514161 - samples/sec: 26.01 - lr: 0.100000
2021-05-16 12:56:17,425 epoch 2 - iter 104/135 - loss 0.87811323 - samples/sec: 26.53 - lr: 0.100000
2021-05-16 12:56:26,291 epoch 2 - iter 117/135 - loss 0.87605123 - samples/sec: 23.46 - lr: 0.100000
2021-05-16 12:56:35,506 epoch 2 - iter 130/135 - loss 0.88016728 - samples/sec: 22.58 - lr: 0.100000
2021-05-16 12:56:38,132 ----------------------------------------------------------------------------------------------------
2021-05-16 12:56:38,133 EPOCH 2 done: loss 0.8683 - lr 0.1000000
2021-05-16 12:56:43,823 DEV : loss 0.16577625274658203 - score 0.965
2021-05-16 12:56:43,890 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 12:56:54,494 ----------------------------------------------------------------------------------------------------
2021-05-16 12:57:03,308 epoch 3 - iter 13/135 - loss 0.88036258 - samples/sec: 23.60 - lr: 0.100000
2021-05-16 12:57:11,144 epoch 3 - iter 26/135 - loss 0.76279605 - samples/sec: 26.55 - lr: 0.100000
2021-05-16 12:57:19,263 epoch 3 - iter 39/135 - loss 0.72454464 - samples/sec: 25.62 - lr: 0.100000
2021-05-16 12:57:27,982 epoch 3 - iter 52/135 - loss 0.69692971 - samples/sec: 23.86 - lr: 0.100000
2021-05-16 12:57:35,917 epoch 3 - iter 65/135 - loss 0.69986497 - samples/sec: 26.22 - lr: 0.100000
2021-05-16 12:57:43,896 epoch 3 - iter 78/135 - loss 0.75645914 - samples/sec: 26.07 - lr: 0.100000
2021-05-16 12:57:51,843 epoch 3 - iter 91/135 - loss 0.74575300 - samples/sec: 26.18 - lr: 0.100000
2021-05-16 12:58:00,403 epoch 3 - iter 104/135 - loss 0.75805407 - samples/sec: 24.30 - lr: 0.100000
2021-05-16 12:58:08,278 epoch 3 - iter 117/135 - loss 0.73629513 - samples/sec: 26.41 - lr: 0.100000
2021-05-16 12:58:16,998 epoch 3 - iter 130/135 - loss 0.72927158 - samples/sec: 23.86 - lr: 0.100000
2021-05-16 12:58:19,629 ----------------------------------------------------------------------------------------------------
2021-05-16 12:58:19,629 EPOCH 3 done: loss 0.7437 - lr 0.1000000
2021-05-16 12:58:25,886 DEV : loss 0.1484624445438385 - score 0.9594
2021-05-16 12:58:25,954 BAD EPOCHS (no improvement): 1
2021-05-16 12:58:25,955 ----------------------------------------------------------------------------------------------------
2021-05-16 12:58:34,063 epoch 4 - iter 13/135 - loss 0.60882795 - samples/sec: 25.66 - lr: 0.100000
2021-05-16 12:58:42,649 epoch 4 - iter 26/135 - loss 0.61723523 - samples/sec: 24.23 - lr: 0.100000
2021-05-16 12:58:50,954 epoch 4 - iter 39/135 - loss 0.59826710 - samples/sec: 25.05 - lr: 0.100000
2021-05-16 12:59:00,086 epoch 4 - iter 52/135 - loss 0.64677145 - samples/sec: 22.78 - lr: 0.100000
2021-05-16 12:59:09,012 epoch 4 - iter 65/135 - loss 0.66238817 - samples/sec: 23.30 - lr: 0.100000
2021-05-16 12:59:18,452 epoch 4 - iter 78/135 - loss 0.65861606 - samples/sec: 22.04 - lr: 0.100000
2021-05-16 12:59:27,280 epoch 4 - iter 91/135 - loss 0.64181750 - samples/sec: 23.57 - lr: 0.100000
2021-05-16 12:59:35,903 epoch 4 - iter 104/135 - loss 0.64335910 - samples/sec: 24.13 - lr: 0.100000
2021-05-16 12:59:44,240 epoch 4 - iter 117/135 - loss 0.62726137 - samples/sec: 24.95 - lr: 0.100000
2021-05-16 12:59:52,664 epoch 4 - iter 130/135 - loss 0.63213368 - samples/sec: 24.70 - lr: 0.100000
2021-05-16 12:59:55,868 ----------------------------------------------------------------------------------------------------
2021-05-16 12:59:55,868 EPOCH 4 done: loss 0.6313 - lr 0.1000000
2021-05-16 13:00:02,387 DEV : loss 0.11789421737194061 - score 0.9692
2021-05-16 13:00:02,450 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:00:13,370 ----------------------------------------------------------------------------------------------------
2021-05-16 13:00:23,030 epoch 5 - iter 13/135 - loss 0.79986595 - samples/sec: 21.54 - lr: 0.100000
2021-05-16 13:00:30,900 epoch 5 - iter 26/135 - loss 0.67733574 - samples/sec: 26.43 - lr: 0.100000
2021-05-16 13:00:40,339 epoch 5 - iter 39/135 - loss 0.65246889 - samples/sec: 22.04 - lr: 0.100000
2021-05-16 13:00:49,783 epoch 5 - iter 52/135 - loss 0.66502075 - samples/sec: 22.03 - lr: 0.100000
2021-05-16 13:00:58,088 epoch 5 - iter 65/135 - loss 0.63606638 - samples/sec: 25.05 - lr: 0.100000
2021-05-16 13:01:06,581 epoch 5 - iter 78/135 - loss 0.61381599 - samples/sec: 24.50 - lr: 0.100000
2021-05-16 13:01:13,839 epoch 5 - iter 91/135 - loss 0.59850447 - samples/sec: 28.66 - lr: 0.100000
2021-05-16 13:01:21,820 epoch 5 - iter 104/135 - loss 0.63501137 - samples/sec: 26.07 - lr: 0.100000
2021-05-16 13:01:30,702 epoch 5 - iter 117/135 - loss 0.60865326 - samples/sec: 23.42 - lr: 0.100000
2021-05-16 13:01:38,662 epoch 5 - iter 130/135 - loss 0.61663854 - samples/sec: 26.13 - lr: 0.100000
2021-05-16 13:01:41,184 ----------------------------------------------------------------------------------------------------
2021-05-16 13:01:41,185 EPOCH 5 done: loss 0.6344 - lr 0.1000000
2021-05-16 13:01:48,766 DEV : loss 0.15056318044662476 - score 0.9658
2021-05-16 13:01:48,835 BAD EPOCHS (no improvement): 1
2021-05-16 13:01:48,835 ----------------------------------------------------------------------------------------------------
2021-05-16 13:01:57,271 epoch 6 - iter 13/135 - loss 0.44439440 - samples/sec: 24.66 - lr: 0.100000
2021-05-16 13:02:05,940 epoch 6 - iter 26/135 - loss 0.48345831 - samples/sec: 24.00 - lr: 0.100000
2021-05-16 13:02:14,683 epoch 6 - iter 39/135 - loss 0.45818954 - samples/sec: 23.80 - lr: 0.100000
2021-05-16 13:02:23,393 epoch 6 - iter 52/135 - loss 0.48838299 - samples/sec: 23.88 - lr: 0.100000
2021-05-16 13:02:31,241 epoch 6 - iter 65/135 - loss 0.48146935 - samples/sec: 26.51 - lr: 0.100000
2021-05-16 13:02:39,135 epoch 6 - iter 78/135 - loss 0.48562380 - samples/sec: 26.35 - lr: 0.100000
2021-05-16 13:02:47,852 epoch 6 - iter 91/135 - loss 0.49456064 - samples/sec: 23.87 - lr: 0.100000
2021-05-16 13:02:56,657 epoch 6 - iter 104/135 - loss 0.49339645 - samples/sec: 23.62 - lr: 0.100000
2021-05-16 13:03:05,703 epoch 6 - iter 117/135 - loss 0.49377078 - samples/sec: 23.00 - lr: 0.100000
2021-05-16 13:03:13,970 epoch 6 - iter 130/135 - loss 0.50904293 - samples/sec: 25.16 - lr: 0.100000
2021-05-16 13:03:16,942 ----------------------------------------------------------------------------------------------------
2021-05-16 13:03:16,942 EPOCH 6 done: loss 0.5044 - lr 0.1000000
2021-05-16 13:03:23,568 DEV : loss 0.11060714721679688 - score 0.9786
2021-05-16 13:03:23,636 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:03:34,492 ----------------------------------------------------------------------------------------------------
2021-05-16 13:03:43,269 epoch 7 - iter 13/135 - loss 0.52059887 - samples/sec: 23.70 - lr: 0.100000
2021-05-16 13:03:52,615 epoch 7 - iter 26/135 - loss 0.62407229 - samples/sec: 22.26 - lr: 0.100000
2021-05-16 13:04:01,187 epoch 7 - iter 39/135 - loss 0.64272087 - samples/sec: 24.27 - lr: 0.100000
2021-05-16 13:04:08,978 epoch 7 - iter 52/135 - loss 0.65510071 - samples/sec: 26.72 - lr: 0.100000
2021-05-16 13:04:17,757 epoch 7 - iter 65/135 - loss 0.61161381 - samples/sec: 23.70 - lr: 0.100000
2021-05-16 13:04:26,225 epoch 7 - iter 78/135 - loss 0.58450328 - samples/sec: 24.58 - lr: 0.100000
2021-05-16 13:04:34,227 epoch 7 - iter 91/135 - loss 0.59241510 - samples/sec: 26.00 - lr: 0.100000
2021-05-16 13:04:43,481 epoch 7 - iter 104/135 - loss 0.58296723 - samples/sec: 22.48 - lr: 0.100000
2021-05-16 13:04:52,329 epoch 7 - iter 117/135 - loss 0.58697092 - samples/sec: 23.51 - lr: 0.100000
2021-05-16 13:05:00,819 epoch 7 - iter 130/135 - loss 0.57908347 - samples/sec: 24.50 - lr: 0.100000
2021-05-16 13:05:03,934 ----------------------------------------------------------------------------------------------------
2021-05-16 13:05:03,934 EPOCH 7 done: loss 0.5751 - lr 0.1000000
2021-05-16 13:05:09,577 DEV : loss 0.10998357832431793 - score 0.9749
2021-05-16 13:05:09,645 BAD EPOCHS (no improvement): 1
2021-05-16 13:05:09,646 ----------------------------------------------------------------------------------------------------
2021-05-16 13:05:18,882 epoch 8 - iter 13/135 - loss 0.69750737 - samples/sec: 22.52 - lr: 0.100000
2021-05-16 13:05:27,686 epoch 8 - iter 26/135 - loss 0.53856468 - samples/sec: 23.63 - lr: 0.100000
2021-05-16 13:05:37,074 epoch 8 - iter 39/135 - loss 0.54438370 - samples/sec: 22.16 - lr: 0.100000
2021-05-16 13:05:47,514 epoch 8 - iter 52/135 - loss 0.57940012 - samples/sec: 19.93 - lr: 0.100000
2021-05-16 13:05:56,884 epoch 8 - iter 65/135 - loss 0.56036421 - samples/sec: 22.20 - lr: 0.100000
2021-05-16 13:06:04,944 epoch 8 - iter 78/135 - loss 0.52521566 - samples/sec: 25.81 - lr: 0.100000
2021-05-16 13:06:13,255 epoch 8 - iter 91/135 - loss 0.50963366 - samples/sec: 25.03 - lr: 0.100000
2021-05-16 13:06:20,887 epoch 8 - iter 104/135 - loss 0.50638234 - samples/sec: 27.26 - lr: 0.100000
2021-05-16 13:06:28,913 epoch 8 - iter 117/135 - loss 0.51729133 - samples/sec: 25.92 - lr: 0.100000
2021-05-16 13:06:37,963 epoch 8 - iter 130/135 - loss 0.51776505 - samples/sec: 22.99 - lr: 0.100000
2021-05-16 13:06:40,538 ----------------------------------------------------------------------------------------------------
2021-05-16 13:06:40,538 EPOCH 8 done: loss 0.5348 - lr 0.1000000
2021-05-16 13:06:46,299 DEV : loss 0.25214970111846924 - score 0.9534
2021-05-16 13:06:46,366 BAD EPOCHS (no improvement): 2
2021-05-16 13:06:46,366 ----------------------------------------------------------------------------------------------------
2021-05-16 13:06:54,440 epoch 9 - iter 13/135 - loss 0.79541454 - samples/sec: 25.77 - lr: 0.100000
2021-05-16 13:07:02,460 epoch 9 - iter 26/135 - loss 0.65523029 - samples/sec: 25.94 - lr: 0.100000
2021-05-16 13:07:10,532 epoch 9 - iter 39/135 - loss 0.64455398 - samples/sec: 25.77 - lr: 0.100000
2021-05-16 13:07:19,022 epoch 9 - iter 52/135 - loss 0.60165301 - samples/sec: 24.50 - lr: 0.100000
2021-05-16 13:07:27,359 epoch 9 - iter 65/135 - loss 0.57843922 - samples/sec: 24.95 - lr: 0.100000
2021-05-16 13:07:36,237 epoch 9 - iter 78/135 - loss 0.56597699 - samples/sec: 23.43 - lr: 0.100000
2021-05-16 13:07:46,638 epoch 9 - iter 91/135 - loss 0.54779151 - samples/sec: 20.00 - lr: 0.100000
2021-05-16 13:07:58,066 epoch 9 - iter 104/135 - loss 0.55083054 - samples/sec: 18.20 - lr: 0.100000
2021-05-16 13:08:06,526 epoch 9 - iter 117/135 - loss 0.54460561 - samples/sec: 24.59 - lr: 0.100000
2021-05-16 13:08:16,122 epoch 9 - iter 130/135 - loss 0.53428884 - samples/sec: 21.69 - lr: 0.100000
2021-05-16 13:08:18,998 ----------------------------------------------------------------------------------------------------
2021-05-16 13:08:18,999 EPOCH 9 done: loss 0.5354 - lr 0.1000000
2021-05-16 13:08:26,071 DEV : loss 0.10504603385925293 - score 0.9766
2021-05-16 13:08:26,141 BAD EPOCHS (no improvement): 3
2021-05-16 13:08:26,142 ----------------------------------------------------------------------------------------------------
2021-05-16 13:08:34,608 epoch 10 - iter 13/135 - loss 0.77833907 - samples/sec: 24.57 - lr: 0.100000
2021-05-16 13:08:42,278 epoch 10 - iter 26/135 - loss 0.72021752 - samples/sec: 27.12 - lr: 0.100000
2021-05-16 13:08:50,300 epoch 10 - iter 39/135 - loss 0.61705942 - samples/sec: 25.93 - lr: 0.100000
2021-05-16 13:08:58,110 epoch 10 - iter 52/135 - loss 0.58134521 - samples/sec: 26.64 - lr: 0.100000
2021-05-16 13:09:06,095 epoch 10 - iter 65/135 - loss 0.54302932 - samples/sec: 26.05 - lr: 0.100000
2021-05-16 13:09:14,849 epoch 10 - iter 78/135 - loss 0.53342103 - samples/sec: 23.77 - lr: 0.100000
2021-05-16 13:09:23,799 epoch 10 - iter 91/135 - loss 0.54242403 - samples/sec: 23.24 - lr: 0.100000
2021-05-16 13:09:32,138 epoch 10 - iter 104/135 - loss 0.57401316 - samples/sec: 24.95 - lr: 0.100000
2021-05-16 13:09:40,171 epoch 10 - iter 117/135 - loss 0.58168313 - samples/sec: 25.90 - lr: 0.100000
2021-05-16 13:09:49,192 epoch 10 - iter 130/135 - loss 0.58025381 - samples/sec: 23.06 - lr: 0.100000
2021-05-16 13:09:52,646 ----------------------------------------------------------------------------------------------------
2021-05-16 13:09:52,647 EPOCH 10 done: loss 0.5750 - lr 0.1000000
2021-05-16 13:10:00,390 DEV : loss 0.1095864325761795 - score 0.9713
Epoch    10: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 13:10:00,459 BAD EPOCHS (no improvement): 4
2021-05-16 13:10:00,459 ----------------------------------------------------------------------------------------------------
2021-05-16 13:10:08,903 epoch 11 - iter 13/135 - loss 0.54639550 - samples/sec: 24.64 - lr: 0.050000
2021-05-16 13:10:17,835 epoch 11 - iter 26/135 - loss 0.51414697 - samples/sec: 23.29 - lr: 0.050000
2021-05-16 13:10:26,511 epoch 11 - iter 39/135 - loss 0.51769119 - samples/sec: 23.98 - lr: 0.050000
2021-05-16 13:10:35,154 epoch 11 - iter 52/135 - loss 0.50838389 - samples/sec: 24.07 - lr: 0.050000
2021-05-16 13:10:44,789 epoch 11 - iter 65/135 - loss 0.52175703 - samples/sec: 21.59 - lr: 0.050000
2021-05-16 13:10:53,510 epoch 11 - iter 78/135 - loss 0.49100399 - samples/sec: 23.85 - lr: 0.050000
2021-05-16 13:11:01,679 epoch 11 - iter 91/135 - loss 0.50792765 - samples/sec: 25.47 - lr: 0.050000
2021-05-16 13:11:10,269 epoch 11 - iter 104/135 - loss 0.49548345 - samples/sec: 24.22 - lr: 0.050000
2021-05-16 13:11:20,430 epoch 11 - iter 117/135 - loss 0.47600103 - samples/sec: 20.48 - lr: 0.050000
2021-05-16 13:11:29,599 epoch 11 - iter 130/135 - loss 0.45924102 - samples/sec: 22.69 - lr: 0.050000
2021-05-16 13:11:33,167 ----------------------------------------------------------------------------------------------------
2021-05-16 13:11:33,168 EPOCH 11 done: loss 0.4514 - lr 0.0500000
2021-05-16 13:11:41,434 DEV : loss 0.08506441116333008 - score 0.9786
2021-05-16 13:11:41,504 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:11:52,299 ----------------------------------------------------------------------------------------------------
2021-05-16 13:12:01,763 epoch 12 - iter 13/135 - loss 0.49427906 - samples/sec: 21.98 - lr: 0.050000
2021-05-16 13:12:10,560 epoch 12 - iter 26/135 - loss 0.42710749 - samples/sec: 23.65 - lr: 0.050000
2021-05-16 13:12:19,142 epoch 12 - iter 39/135 - loss 0.43323250 - samples/sec: 24.24 - lr: 0.050000
2021-05-16 13:12:27,178 epoch 12 - iter 52/135 - loss 0.39514367 - samples/sec: 25.89 - lr: 0.050000
2021-05-16 13:12:36,165 epoch 12 - iter 65/135 - loss 0.37144570 - samples/sec: 23.15 - lr: 0.050000
2021-05-16 13:12:45,395 epoch 12 - iter 78/135 - loss 0.38355329 - samples/sec: 22.54 - lr: 0.050000
2021-05-16 13:12:55,156 epoch 12 - iter 91/135 - loss 0.39850583 - samples/sec: 21.31 - lr: 0.050000
2021-05-16 13:13:03,798 epoch 12 - iter 104/135 - loss 0.39876365 - samples/sec: 24.07 - lr: 0.050000
2021-05-16 13:13:13,071 epoch 12 - iter 117/135 - loss 0.40373637 - samples/sec: 22.43 - lr: 0.050000
2021-05-16 13:13:21,913 epoch 12 - iter 130/135 - loss 0.40054428 - samples/sec: 23.53 - lr: 0.050000
2021-05-16 13:13:24,287 ----------------------------------------------------------------------------------------------------
2021-05-16 13:13:24,287 EPOCH 12 done: loss 0.4018 - lr 0.0500000
2021-05-16 13:13:30,738 DEV : loss 0.09201624989509583 - score 0.9731
2021-05-16 13:13:30,808 BAD EPOCHS (no improvement): 1
2021-05-16 13:13:30,808 ----------------------------------------------------------------------------------------------------
2021-05-16 13:13:39,379 epoch 13 - iter 13/135 - loss 0.42449889 - samples/sec: 24.27 - lr: 0.050000
2021-05-16 13:13:47,311 epoch 13 - iter 26/135 - loss 0.38920177 - samples/sec: 26.22 - lr: 0.050000
2021-05-16 13:13:55,395 epoch 13 - iter 39/135 - loss 0.33716679 - samples/sec: 25.73 - lr: 0.050000
2021-05-16 13:14:03,805 epoch 13 - iter 52/135 - loss 0.35114781 - samples/sec: 24.74 - lr: 0.050000
2021-05-16 13:14:12,616 epoch 13 - iter 65/135 - loss 0.35000615 - samples/sec: 23.61 - lr: 0.050000
2021-05-16 13:14:20,808 epoch 13 - iter 78/135 - loss 0.35695377 - samples/sec: 25.40 - lr: 0.050000
2021-05-16 13:14:30,030 epoch 13 - iter 91/135 - loss 0.36863321 - samples/sec: 22.56 - lr: 0.050000
2021-05-16 13:14:37,389 epoch 13 - iter 104/135 - loss 0.35527587 - samples/sec: 28.27 - lr: 0.050000
2021-05-16 13:14:45,022 epoch 13 - iter 117/135 - loss 0.35286126 - samples/sec: 27.26 - lr: 0.050000
2021-05-16 13:14:53,340 epoch 13 - iter 130/135 - loss 0.35949893 - samples/sec: 25.01 - lr: 0.050000
2021-05-16 13:14:55,634 ----------------------------------------------------------------------------------------------------
2021-05-16 13:14:55,635 EPOCH 13 done: loss 0.3539 - lr 0.0500000
2021-05-16 13:15:01,790 DEV : loss 0.09789064526557922 - score 0.9767
2021-05-16 13:15:01,858 BAD EPOCHS (no improvement): 2
2021-05-16 13:15:01,859 ----------------------------------------------------------------------------------------------------
2021-05-16 13:15:11,256 epoch 14 - iter 13/135 - loss 0.41812251 - samples/sec: 22.14 - lr: 0.050000
2021-05-16 13:15:19,718 epoch 14 - iter 26/135 - loss 0.46157471 - samples/sec: 24.58 - lr: 0.050000
2021-05-16 13:15:28,377 epoch 14 - iter 39/135 - loss 0.41891731 - samples/sec: 24.03 - lr: 0.050000
2021-05-16 13:15:36,403 epoch 14 - iter 52/135 - loss 0.40599989 - samples/sec: 25.92 - lr: 0.050000
2021-05-16 13:15:44,754 epoch 14 - iter 65/135 - loss 0.40755224 - samples/sec: 24.91 - lr: 0.050000
2021-05-16 13:15:53,040 epoch 14 - iter 78/135 - loss 0.37583409 - samples/sec: 25.10 - lr: 0.050000
2021-05-16 13:16:01,498 epoch 14 - iter 91/135 - loss 0.35700450 - samples/sec: 24.60 - lr: 0.050000
2021-05-16 13:16:09,782 epoch 14 - iter 104/135 - loss 0.35790003 - samples/sec: 25.11 - lr: 0.050000
2021-05-16 13:16:17,854 epoch 14 - iter 117/135 - loss 0.35019489 - samples/sec: 25.79 - lr: 0.050000
2021-05-16 13:16:26,762 epoch 14 - iter 130/135 - loss 0.34277018 - samples/sec: 23.35 - lr: 0.050000
2021-05-16 13:16:29,957 ----------------------------------------------------------------------------------------------------
2021-05-16 13:16:29,959 EPOCH 14 done: loss 0.3564 - lr 0.0500000
2021-05-16 13:16:35,447 DEV : loss 0.08428677171468735 - score 0.9712
2021-05-16 13:16:35,521 BAD EPOCHS (no improvement): 3
2021-05-16 13:16:35,522 ----------------------------------------------------------------------------------------------------
2021-05-16 13:16:43,686 epoch 15 - iter 13/135 - loss 0.23853691 - samples/sec: 25.48 - lr: 0.050000
2021-05-16 13:16:51,112 epoch 15 - iter 26/135 - loss 0.29828710 - samples/sec: 28.01 - lr: 0.050000
2021-05-16 13:16:59,343 epoch 15 - iter 39/135 - loss 0.30032173 - samples/sec: 25.27 - lr: 0.050000
2021-05-16 13:17:07,083 epoch 15 - iter 52/135 - loss 0.29045890 - samples/sec: 26.88 - lr: 0.050000
2021-05-16 13:17:15,904 epoch 15 - iter 65/135 - loss 0.30445045 - samples/sec: 23.59 - lr: 0.050000
2021-05-16 13:17:23,784 epoch 15 - iter 78/135 - loss 0.32085201 - samples/sec: 26.40 - lr: 0.050000
2021-05-16 13:17:31,959 epoch 15 - iter 91/135 - loss 0.31701971 - samples/sec: 25.45 - lr: 0.050000
2021-05-16 13:17:40,005 epoch 15 - iter 104/135 - loss 0.32508584 - samples/sec: 25.85 - lr: 0.050000
2021-05-16 13:17:48,484 epoch 15 - iter 117/135 - loss 0.34405378 - samples/sec: 24.54 - lr: 0.050000
2021-05-16 13:17:57,634 epoch 15 - iter 130/135 - loss 0.33696340 - samples/sec: 22.73 - lr: 0.050000
2021-05-16 13:18:00,132 ----------------------------------------------------------------------------------------------------
2021-05-16 13:18:00,132 EPOCH 15 done: loss 0.3368 - lr 0.0500000
2021-05-16 13:18:06,355 DEV : loss 0.08965082466602325 - score 0.977
Epoch    15: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 13:18:06,422 BAD EPOCHS (no improvement): 4
2021-05-16 13:18:06,423 ----------------------------------------------------------------------------------------------------
2021-05-16 13:18:15,393 epoch 16 - iter 13/135 - loss 0.59692243 - samples/sec: 23.19 - lr: 0.025000
2021-05-16 13:18:22,986 epoch 16 - iter 26/135 - loss 0.51417753 - samples/sec: 27.40 - lr: 0.025000
2021-05-16 13:18:30,458 epoch 16 - iter 39/135 - loss 0.46425484 - samples/sec: 27.84 - lr: 0.025000
2021-05-16 13:18:38,363 epoch 16 - iter 52/135 - loss 0.41851339 - samples/sec: 26.32 - lr: 0.025000
2021-05-16 13:18:46,273 epoch 16 - iter 65/135 - loss 0.39268474 - samples/sec: 26.30 - lr: 0.025000
2021-05-16 13:18:54,586 epoch 16 - iter 78/135 - loss 0.36113698 - samples/sec: 25.02 - lr: 0.025000
2021-05-16 13:19:03,142 epoch 16 - iter 91/135 - loss 0.35674472 - samples/sec: 24.31 - lr: 0.025000
2021-05-16 13:19:11,576 epoch 16 - iter 104/135 - loss 0.34392213 - samples/sec: 24.67 - lr: 0.025000
2021-05-16 13:19:20,306 epoch 16 - iter 117/135 - loss 0.34447445 - samples/sec: 23.83 - lr: 0.025000
2021-05-16 13:19:28,538 epoch 16 - iter 130/135 - loss 0.34858398 - samples/sec: 25.27 - lr: 0.025000
2021-05-16 13:19:31,222 ----------------------------------------------------------------------------------------------------
2021-05-16 13:19:31,223 EPOCH 16 done: loss 0.3500 - lr 0.0250000
2021-05-16 13:19:38,119 DEV : loss 0.0810013934969902 - score 0.9768
2021-05-16 13:19:38,205 BAD EPOCHS (no improvement): 1
2021-05-16 13:19:38,205 ----------------------------------------------------------------------------------------------------
2021-05-16 13:19:45,963 epoch 17 - iter 13/135 - loss 0.27380841 - samples/sec: 26.82 - lr: 0.025000
2021-05-16 13:19:55,068 epoch 17 - iter 26/135 - loss 0.24063890 - samples/sec: 22.85 - lr: 0.025000
2021-05-16 13:20:03,718 epoch 17 - iter 39/135 - loss 0.24992264 - samples/sec: 24.05 - lr: 0.025000
2021-05-16 13:20:11,656 epoch 17 - iter 52/135 - loss 0.28430774 - samples/sec: 26.21 - lr: 0.025000
2021-05-16 13:20:19,385 epoch 17 - iter 65/135 - loss 0.30795828 - samples/sec: 26.92 - lr: 0.025000
2021-05-16 13:20:27,513 epoch 17 - iter 78/135 - loss 0.30086764 - samples/sec: 25.59 - lr: 0.025000
2021-05-16 13:20:36,477 epoch 17 - iter 91/135 - loss 0.29276381 - samples/sec: 23.21 - lr: 0.025000
2021-05-16 13:20:46,440 epoch 17 - iter 104/135 - loss 0.28774053 - samples/sec: 20.88 - lr: 0.025000
2021-05-16 13:20:55,191 epoch 17 - iter 117/135 - loss 0.29367191 - samples/sec: 23.77 - lr: 0.025000
2021-05-16 13:21:04,625 epoch 17 - iter 130/135 - loss 0.30683006 - samples/sec: 22.05 - lr: 0.025000
2021-05-16 13:21:07,668 ----------------------------------------------------------------------------------------------------
2021-05-16 13:21:07,668 EPOCH 17 done: loss 0.3026 - lr 0.0250000
2021-05-16 13:21:12,842 DEV : loss 0.08197732269763947 - score 0.9749
2021-05-16 13:21:12,909 BAD EPOCHS (no improvement): 2
2021-05-16 13:21:12,910 ----------------------------------------------------------------------------------------------------
2021-05-16 13:21:21,697 epoch 18 - iter 13/135 - loss 0.20108103 - samples/sec: 23.67 - lr: 0.025000
2021-05-16 13:21:30,589 epoch 18 - iter 26/135 - loss 0.29670401 - samples/sec: 23.40 - lr: 0.025000
2021-05-16 13:21:39,808 epoch 18 - iter 39/135 - loss 0.26460458 - samples/sec: 22.56 - lr: 0.025000
2021-05-16 13:21:48,818 epoch 18 - iter 52/135 - loss 0.27450861 - samples/sec: 23.09 - lr: 0.025000
2021-05-16 13:21:56,878 epoch 18 - iter 65/135 - loss 0.26024515 - samples/sec: 25.81 - lr: 0.025000
2021-05-16 13:22:05,334 epoch 18 - iter 78/135 - loss 0.28095437 - samples/sec: 24.60 - lr: 0.025000
2021-05-16 13:22:13,168 epoch 18 - iter 91/135 - loss 0.29063015 - samples/sec: 26.55 - lr: 0.025000
2021-05-16 13:22:21,382 epoch 18 - iter 104/135 - loss 0.29480404 - samples/sec: 25.33 - lr: 0.025000
2021-05-16 13:22:29,647 epoch 18 - iter 117/135 - loss 0.29627552 - samples/sec: 25.17 - lr: 0.025000
2021-05-16 13:22:37,520 epoch 18 - iter 130/135 - loss 0.28587281 - samples/sec: 26.42 - lr: 0.025000
2021-05-16 13:22:40,799 ----------------------------------------------------------------------------------------------------
2021-05-16 13:22:40,799 EPOCH 18 done: loss 0.2837 - lr 0.0250000
2021-05-16 13:22:47,015 DEV : loss 0.08601263165473938 - score 0.9731
2021-05-16 13:22:47,083 BAD EPOCHS (no improvement): 3
2021-05-16 13:22:47,084 ----------------------------------------------------------------------------------------------------
2021-05-16 13:22:55,789 epoch 19 - iter 13/135 - loss 0.33403563 - samples/sec: 23.90 - lr: 0.025000
2021-05-16 13:23:04,732 epoch 19 - iter 26/135 - loss 0.29366309 - samples/sec: 23.26 - lr: 0.025000
2021-05-16 13:23:12,857 epoch 19 - iter 39/135 - loss 0.30713098 - samples/sec: 25.60 - lr: 0.025000
2021-05-16 13:23:21,283 epoch 19 - iter 52/135 - loss 0.29545094 - samples/sec: 24.69 - lr: 0.025000
2021-05-16 13:23:29,402 epoch 19 - iter 65/135 - loss 0.29901242 - samples/sec: 25.63 - lr: 0.025000
2021-05-16 13:23:37,593 epoch 19 - iter 78/135 - loss 0.30093960 - samples/sec: 25.41 - lr: 0.025000
2021-05-16 13:23:45,649 epoch 19 - iter 91/135 - loss 0.28684018 - samples/sec: 25.82 - lr: 0.025000
2021-05-16 13:23:53,979 epoch 19 - iter 104/135 - loss 0.28570213 - samples/sec: 24.97 - lr: 0.025000
2021-05-16 13:24:02,840 epoch 19 - iter 117/135 - loss 0.29039199 - samples/sec: 23.48 - lr: 0.025000
2021-05-16 13:24:11,400 epoch 19 - iter 130/135 - loss 0.30900550 - samples/sec: 24.30 - lr: 0.025000
2021-05-16 13:24:14,499 ----------------------------------------------------------------------------------------------------
2021-05-16 13:24:14,500 EPOCH 19 done: loss 0.3095 - lr 0.0250000
2021-05-16 13:24:20,302 DEV : loss 0.08174091577529907 - score 0.9731
Epoch    19: reducing learning rate of group 0 to 1.2500e-02.
2021-05-16 13:24:20,391 BAD EPOCHS (no improvement): 4
2021-05-16 13:24:20,391 ----------------------------------------------------------------------------------------------------
2021-05-16 13:24:28,477 epoch 20 - iter 13/135 - loss 0.26644406 - samples/sec: 25.73 - lr: 0.012500
2021-05-16 13:24:36,903 epoch 20 - iter 26/135 - loss 0.32000930 - samples/sec: 24.69 - lr: 0.012500
2021-05-16 13:24:44,754 epoch 20 - iter 39/135 - loss 0.30298271 - samples/sec: 26.50 - lr: 0.012500
2021-05-16 13:24:53,290 epoch 20 - iter 52/135 - loss 0.29999561 - samples/sec: 24.37 - lr: 0.012500
2021-05-16 13:25:01,694 epoch 20 - iter 65/135 - loss 0.27762927 - samples/sec: 24.75 - lr: 0.012500
2021-05-16 13:25:09,881 epoch 20 - iter 78/135 - loss 0.28466178 - samples/sec: 25.41 - lr: 0.012500
2021-05-16 13:25:18,247 epoch 20 - iter 91/135 - loss 0.29632101 - samples/sec: 24.87 - lr: 0.012500
2021-05-16 13:25:26,782 epoch 20 - iter 104/135 - loss 0.29753278 - samples/sec: 24.38 - lr: 0.012500
2021-05-16 13:25:34,733 epoch 20 - iter 117/135 - loss 0.28490859 - samples/sec: 26.16 - lr: 0.012500
2021-05-16 13:25:43,035 epoch 20 - iter 130/135 - loss 0.28192879 - samples/sec: 25.06 - lr: 0.012500
2021-05-16 13:25:45,857 ----------------------------------------------------------------------------------------------------
2021-05-16 13:25:45,858 EPOCH 20 done: loss 0.2885 - lr 0.0125000
2021-05-16 13:25:52,798 DEV : loss 0.08342554420232773 - score 0.975
2021-05-16 13:25:52,867 BAD EPOCHS (no improvement): 1
2021-05-16 13:25:52,867 ----------------------------------------------------------------------------------------------------
2021-05-16 13:26:02,632 epoch 21 - iter 13/135 - loss 0.26396430 - samples/sec: 21.33 - lr: 0.012500
2021-05-16 13:26:10,888 epoch 21 - iter 26/135 - loss 0.29758977 - samples/sec: 25.20 - lr: 0.012500
2021-05-16 13:26:20,730 epoch 21 - iter 39/135 - loss 0.30406660 - samples/sec: 21.14 - lr: 0.012500
2021-05-16 13:26:30,585 epoch 21 - iter 52/135 - loss 0.34423653 - samples/sec: 21.11 - lr: 0.012500
2021-05-16 13:26:38,997 epoch 21 - iter 65/135 - loss 0.31741178 - samples/sec: 24.73 - lr: 0.012500
2021-05-16 13:26:47,171 epoch 21 - iter 78/135 - loss 0.29204871 - samples/sec: 25.45 - lr: 0.012500
2021-05-16 13:26:55,566 epoch 21 - iter 91/135 - loss 0.28754895 - samples/sec: 24.78 - lr: 0.012500
2021-05-16 13:27:03,487 epoch 21 - iter 104/135 - loss 0.28623464 - samples/sec: 26.26 - lr: 0.012500
2021-05-16 13:27:11,389 epoch 21 - iter 117/135 - loss 0.28033349 - samples/sec: 26.33 - lr: 0.012500
2021-05-16 13:27:20,856 epoch 21 - iter 130/135 - loss 0.27399636 - samples/sec: 21.97 - lr: 0.012500
2021-05-16 13:27:23,572 ----------------------------------------------------------------------------------------------------
2021-05-16 13:27:23,574 EPOCH 21 done: loss 0.2812 - lr 0.0125000
2021-05-16 13:27:30,717 DEV : loss 0.08772297948598862 - score 0.9751
2021-05-16 13:27:30,787 BAD EPOCHS (no improvement): 2
2021-05-16 13:27:30,787 ----------------------------------------------------------------------------------------------------
2021-05-16 13:27:39,194 epoch 22 - iter 13/135 - loss 0.22067891 - samples/sec: 24.75 - lr: 0.012500
2021-05-16 13:27:48,385 epoch 22 - iter 26/135 - loss 0.24805166 - samples/sec: 22.63 - lr: 0.012500
2021-05-16 13:27:58,048 epoch 22 - iter 39/135 - loss 0.26020183 - samples/sec: 21.53 - lr: 0.012500
2021-05-16 13:28:07,189 epoch 22 - iter 52/135 - loss 0.27510323 - samples/sec: 22.76 - lr: 0.012500
2021-05-16 13:28:16,070 epoch 22 - iter 65/135 - loss 0.25529723 - samples/sec: 23.42 - lr: 0.012500
2021-05-16 13:28:25,357 epoch 22 - iter 78/135 - loss 0.25123137 - samples/sec: 22.40 - lr: 0.012500
2021-05-16 13:28:34,375 epoch 22 - iter 91/135 - loss 0.24832867 - samples/sec: 23.07 - lr: 0.012500
2021-05-16 13:28:44,237 epoch 22 - iter 104/135 - loss 0.24487359 - samples/sec: 21.09 - lr: 0.012500
2021-05-16 13:28:53,083 epoch 22 - iter 117/135 - loss 0.25237774 - samples/sec: 23.52 - lr: 0.012500
2021-05-16 13:29:02,169 epoch 22 - iter 130/135 - loss 0.24720841 - samples/sec: 22.90 - lr: 0.012500
2021-05-16 13:29:04,942 ----------------------------------------------------------------------------------------------------
2021-05-16 13:29:04,943 EPOCH 22 done: loss 0.2446 - lr 0.0125000
2021-05-16 13:29:11,627 DEV : loss 0.08699886500835419 - score 0.9771
2021-05-16 13:29:11,694 BAD EPOCHS (no improvement): 3
2021-05-16 13:29:11,694 ----------------------------------------------------------------------------------------------------
2021-05-16 13:29:20,934 epoch 23 - iter 13/135 - loss 0.36621693 - samples/sec: 22.51 - lr: 0.012500
2021-05-16 13:29:30,894 epoch 23 - iter 26/135 - loss 0.35311030 - samples/sec: 20.89 - lr: 0.012500
2021-05-16 13:29:40,394 epoch 23 - iter 39/135 - loss 0.31968527 - samples/sec: 21.90 - lr: 0.012500
2021-05-16 13:29:49,739 epoch 23 - iter 52/135 - loss 0.28990962 - samples/sec: 22.26 - lr: 0.012500
2021-05-16 13:29:59,768 epoch 23 - iter 65/135 - loss 0.27416131 - samples/sec: 20.74 - lr: 0.012500
2021-05-16 13:30:08,899 epoch 23 - iter 78/135 - loss 0.25945560 - samples/sec: 22.78 - lr: 0.012500
2021-05-16 13:30:19,624 epoch 23 - iter 91/135 - loss 0.25169665 - samples/sec: 19.40 - lr: 0.012500
2021-05-16 13:30:28,892 epoch 23 - iter 104/135 - loss 0.24621553 - samples/sec: 22.45 - lr: 0.012500
2021-05-16 13:30:38,541 epoch 23 - iter 117/135 - loss 0.24405778 - samples/sec: 21.56 - lr: 0.012500
2021-05-16 13:30:48,374 epoch 23 - iter 130/135 - loss 0.24185422 - samples/sec: 21.16 - lr: 0.012500
2021-05-16 13:30:51,438 ----------------------------------------------------------------------------------------------------
2021-05-16 13:30:51,438 EPOCH 23 done: loss 0.2387 - lr 0.0125000
2021-05-16 13:30:58,972 DEV : loss 0.08183392882347107 - score 0.9771
Epoch    23: reducing learning rate of group 0 to 6.2500e-03.
2021-05-16 13:30:59,041 BAD EPOCHS (no improvement): 4
2021-05-16 13:30:59,041 ----------------------------------------------------------------------------------------------------
2021-05-16 13:31:07,429 epoch 24 - iter 13/135 - loss 0.27477179 - samples/sec: 24.80 - lr: 0.006250
2021-05-16 13:31:16,305 epoch 24 - iter 26/135 - loss 0.29169245 - samples/sec: 23.44 - lr: 0.006250
2021-05-16 13:31:25,517 epoch 24 - iter 39/135 - loss 0.24618606 - samples/sec: 22.58 - lr: 0.006250
2021-05-16 13:31:33,960 epoch 24 - iter 52/135 - loss 0.27732533 - samples/sec: 24.64 - lr: 0.006250
2021-05-16 13:31:42,396 epoch 24 - iter 65/135 - loss 0.27970917 - samples/sec: 24.66 - lr: 0.006250
2021-05-16 13:31:49,798 epoch 24 - iter 78/135 - loss 0.27931064 - samples/sec: 28.11 - lr: 0.006250
2021-05-16 13:31:58,548 epoch 24 - iter 91/135 - loss 0.26944526 - samples/sec: 23.77 - lr: 0.006250
2021-05-16 13:32:07,393 epoch 24 - iter 104/135 - loss 0.26860851 - samples/sec: 23.52 - lr: 0.006250
2021-05-16 13:32:14,895 epoch 24 - iter 117/135 - loss 0.28128798 - samples/sec: 27.73 - lr: 0.006250
2021-05-16 13:32:23,476 epoch 24 - iter 130/135 - loss 0.27051305 - samples/sec: 24.24 - lr: 0.006250
2021-05-16 13:32:26,276 ----------------------------------------------------------------------------------------------------
2021-05-16 13:32:26,276 EPOCH 24 done: loss 0.2728 - lr 0.0062500
2021-05-16 13:32:31,853 DEV : loss 0.09192368388175964 - score 0.9771
2021-05-16 13:32:31,936 BAD EPOCHS (no improvement): 1
2021-05-16 13:32:31,936 ----------------------------------------------------------------------------------------------------
2021-05-16 13:32:39,901 epoch 25 - iter 13/135 - loss 0.22574859 - samples/sec: 26.12 - lr: 0.006250
2021-05-16 13:32:48,617 epoch 25 - iter 26/135 - loss 0.23537049 - samples/sec: 23.87 - lr: 0.006250
2021-05-16 13:32:56,480 epoch 25 - iter 39/135 - loss 0.24188307 - samples/sec: 26.46 - lr: 0.006250
2021-05-16 13:33:05,569 epoch 25 - iter 52/135 - loss 0.25071522 - samples/sec: 22.89 - lr: 0.006250
2021-05-16 13:33:13,762 epoch 25 - iter 65/135 - loss 0.23303279 - samples/sec: 25.39 - lr: 0.006250
2021-05-16 13:33:21,979 epoch 25 - iter 78/135 - loss 0.22885871 - samples/sec: 25.31 - lr: 0.006250
2021-05-16 13:33:30,802 epoch 25 - iter 91/135 - loss 0.24248935 - samples/sec: 23.58 - lr: 0.006250
2021-05-16 13:33:40,440 epoch 25 - iter 104/135 - loss 0.24925290 - samples/sec: 21.58 - lr: 0.006250
2021-05-16 13:33:49,770 epoch 25 - iter 117/135 - loss 0.25433641 - samples/sec: 22.30 - lr: 0.006250
2021-05-16 13:33:59,292 epoch 25 - iter 130/135 - loss 0.25604718 - samples/sec: 21.85 - lr: 0.006250
2021-05-16 13:34:02,055 ----------------------------------------------------------------------------------------------------
2021-05-16 13:34:02,056 EPOCH 25 done: loss 0.2507 - lr 0.0062500
2021-05-16 13:34:09,381 DEV : loss 0.0860348641872406 - score 0.9751
2021-05-16 13:34:09,449 BAD EPOCHS (no improvement): 2
2021-05-16 13:34:09,449 ----------------------------------------------------------------------------------------------------
2021-05-16 13:34:19,624 epoch 26 - iter 13/135 - loss 0.36683610 - samples/sec: 20.45 - lr: 0.006250
2021-05-16 13:34:28,565 epoch 26 - iter 26/135 - loss 0.25301131 - samples/sec: 23.27 - lr: 0.006250
2021-05-16 13:34:39,197 epoch 26 - iter 39/135 - loss 0.24803248 - samples/sec: 19.57 - lr: 0.006250
2021-05-16 13:34:48,716 epoch 26 - iter 52/135 - loss 0.27330844 - samples/sec: 21.86 - lr: 0.006250
2021-05-16 13:34:58,246 epoch 26 - iter 65/135 - loss 0.27045317 - samples/sec: 21.83 - lr: 0.006250
2021-05-16 13:35:07,194 epoch 26 - iter 78/135 - loss 0.26471170 - samples/sec: 23.25 - lr: 0.006250
2021-05-16 13:35:16,939 epoch 26 - iter 91/135 - loss 0.27057651 - samples/sec: 21.35 - lr: 0.006250
2021-05-16 13:35:26,862 epoch 26 - iter 104/135 - loss 0.28223602 - samples/sec: 20.96 - lr: 0.006250
2021-05-16 13:35:36,687 epoch 26 - iter 117/135 - loss 0.27466916 - samples/sec: 21.17 - lr: 0.006250
2021-05-16 13:35:45,415 epoch 26 - iter 130/135 - loss 0.26684721 - samples/sec: 23.84 - lr: 0.006250
2021-05-16 13:35:48,662 ----------------------------------------------------------------------------------------------------
2021-05-16 13:35:48,662 EPOCH 26 done: loss 0.2675 - lr 0.0062500
2021-05-16 13:35:55,674 DEV : loss 0.0749456137418747 - score 0.977
2021-05-16 13:35:55,735 BAD EPOCHS (no improvement): 3
2021-05-16 13:35:55,735 ----------------------------------------------------------------------------------------------------
2021-05-16 13:36:05,903 epoch 27 - iter 13/135 - loss 0.30702669 - samples/sec: 20.46 - lr: 0.006250
2021-05-16 13:36:15,090 epoch 27 - iter 26/135 - loss 0.28922353 - samples/sec: 22.64 - lr: 0.006250
2021-05-16 13:36:25,469 epoch 27 - iter 39/135 - loss 0.28871037 - samples/sec: 20.04 - lr: 0.006250
2021-05-16 13:36:34,893 epoch 27 - iter 52/135 - loss 0.28494950 - samples/sec: 22.07 - lr: 0.006250
2021-05-16 13:36:44,496 epoch 27 - iter 65/135 - loss 0.31590657 - samples/sec: 21.66 - lr: 0.006250
2021-05-16 13:36:55,180 epoch 27 - iter 78/135 - loss 0.29069686 - samples/sec: 19.47 - lr: 0.006250
2021-05-16 13:37:04,579 epoch 27 - iter 91/135 - loss 0.26806381 - samples/sec: 22.13 - lr: 0.006250
2021-05-16 13:37:13,907 epoch 27 - iter 104/135 - loss 0.27179135 - samples/sec: 22.30 - lr: 0.006250
2021-05-16 13:37:23,558 epoch 27 - iter 117/135 - loss 0.26583647 - samples/sec: 21.55 - lr: 0.006250
2021-05-16 13:37:32,777 epoch 27 - iter 130/135 - loss 0.27752175 - samples/sec: 22.57 - lr: 0.006250
2021-05-16 13:37:35,830 ----------------------------------------------------------------------------------------------------
2021-05-16 13:37:35,830 EPOCH 27 done: loss 0.2730 - lr 0.0062500
2021-05-16 13:37:41,776 DEV : loss 0.07974916696548462 - score 0.977
Epoch    27: reducing learning rate of group 0 to 3.1250e-03.
2021-05-16 13:37:41,844 BAD EPOCHS (no improvement): 4
2021-05-16 13:37:41,845 ----------------------------------------------------------------------------------------------------
2021-05-16 13:37:51,115 epoch 28 - iter 13/135 - loss 0.34809444 - samples/sec: 22.44 - lr: 0.003125
2021-05-16 13:38:00,982 epoch 28 - iter 26/135 - loss 0.40318150 - samples/sec: 21.08 - lr: 0.003125
2021-05-16 13:38:11,108 epoch 28 - iter 39/135 - loss 0.34598047 - samples/sec: 20.54 - lr: 0.003125
2021-05-16 13:38:20,405 epoch 28 - iter 52/135 - loss 0.31539804 - samples/sec: 22.38 - lr: 0.003125
2021-05-16 13:38:30,295 epoch 28 - iter 65/135 - loss 0.31324488 - samples/sec: 21.04 - lr: 0.003125
2021-05-16 13:38:38,505 epoch 28 - iter 78/135 - loss 0.29952914 - samples/sec: 25.34 - lr: 0.003125
2021-05-16 13:38:46,961 epoch 28 - iter 91/135 - loss 0.30800559 - samples/sec: 24.60 - lr: 0.003125
2021-05-16 13:38:56,321 epoch 28 - iter 104/135 - loss 0.29991724 - samples/sec: 22.22 - lr: 0.003125
2021-05-16 13:39:05,682 epoch 28 - iter 117/135 - loss 0.29329723 - samples/sec: 22.22 - lr: 0.003125
2021-05-16 13:39:15,609 epoch 28 - iter 130/135 - loss 0.28926807 - samples/sec: 20.95 - lr: 0.003125
2021-05-16 13:39:19,141 ----------------------------------------------------------------------------------------------------
2021-05-16 13:39:19,141 EPOCH 28 done: loss 0.2863 - lr 0.0031250
2021-05-16 13:39:26,994 DEV : loss 0.07675009220838547 - score 0.9789
2021-05-16 13:39:27,058 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:39:38,201 ----------------------------------------------------------------------------------------------------
2021-05-16 13:39:48,757 epoch 29 - iter 13/135 - loss 0.27200496 - samples/sec: 19.71 - lr: 0.003125
2021-05-16 13:39:59,351 epoch 29 - iter 26/135 - loss 0.28943262 - samples/sec: 19.63 - lr: 0.003125
2021-05-16 13:40:09,112 epoch 29 - iter 39/135 - loss 0.30686991 - samples/sec: 21.31 - lr: 0.003125
2021-05-16 13:40:19,519 epoch 29 - iter 52/135 - loss 0.29653469 - samples/sec: 19.99 - lr: 0.003125
2021-05-16 13:40:29,737 epoch 29 - iter 65/135 - loss 0.28484236 - samples/sec: 20.36 - lr: 0.003125
2021-05-16 13:40:39,823 epoch 29 - iter 78/135 - loss 0.27811117 - samples/sec: 20.63 - lr: 0.003125
2021-05-16 13:40:49,215 epoch 29 - iter 91/135 - loss 0.27174751 - samples/sec: 22.15 - lr: 0.003125
2021-05-16 13:40:58,161 epoch 29 - iter 104/135 - loss 0.26288401 - samples/sec: 23.25 - lr: 0.003125
2021-05-16 13:41:07,824 epoch 29 - iter 117/135 - loss 0.26924109 - samples/sec: 21.53 - lr: 0.003125
2021-05-16 13:41:16,571 epoch 29 - iter 130/135 - loss 0.25797104 - samples/sec: 23.78 - lr: 0.003125
2021-05-16 13:41:19,953 ----------------------------------------------------------------------------------------------------
2021-05-16 13:41:19,954 EPOCH 29 done: loss 0.2584 - lr 0.0031250
2021-05-16 13:41:28,045 DEV : loss 0.08168482780456543 - score 0.977
2021-05-16 13:41:28,132 BAD EPOCHS (no improvement): 1
2021-05-16 13:41:28,133 ----------------------------------------------------------------------------------------------------
2021-05-16 13:41:37,651 epoch 30 - iter 13/135 - loss 0.24836006 - samples/sec: 21.86 - lr: 0.003125
2021-05-16 13:41:46,414 epoch 30 - iter 26/135 - loss 0.22219931 - samples/sec: 23.74 - lr: 0.003125
2021-05-16 13:41:56,225 epoch 30 - iter 39/135 - loss 0.24235602 - samples/sec: 21.20 - lr: 0.003125
2021-05-16 13:42:05,318 epoch 30 - iter 52/135 - loss 0.23683574 - samples/sec: 22.88 - lr: 0.003125
2021-05-16 13:42:14,941 epoch 30 - iter 65/135 - loss 0.23812508 - samples/sec: 21.62 - lr: 0.003125
2021-05-16 13:42:23,031 epoch 30 - iter 78/135 - loss 0.22724969 - samples/sec: 25.71 - lr: 0.003125
2021-05-16 13:42:31,450 epoch 30 - iter 91/135 - loss 0.26104998 - samples/sec: 24.71 - lr: 0.003125
2021-05-16 13:42:39,800 epoch 30 - iter 104/135 - loss 0.25488191 - samples/sec: 24.91 - lr: 0.003125
2021-05-16 13:42:47,479 epoch 30 - iter 117/135 - loss 0.25653216 - samples/sec: 27.09 - lr: 0.003125
2021-05-16 13:42:56,565 epoch 30 - iter 130/135 - loss 0.24478526 - samples/sec: 22.89 - lr: 0.003125
2021-05-16 13:42:59,798 ----------------------------------------------------------------------------------------------------
2021-05-16 13:42:59,798 EPOCH 30 done: loss 0.2410 - lr 0.0031250
2021-05-16 13:43:06,801 DEV : loss 0.08601810038089752 - score 0.9771
2021-05-16 13:43:06,886 BAD EPOCHS (no improvement): 2
2021-05-16 13:43:17,464 ----------------------------------------------------------------------------------------------------
2021-05-16 13:43:17,464 Testing using best model ...
2021-05-16 13:43:17,465 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.rststb/best-model.pt
2021-05-16 13:43:50,897 0.9365	0.9736	0.9547
2021-05-16 13:43:50,897 
Results:
- F1-score (micro) 0.9547
- F1-score (macro) 0.9547

By class:
SENT       tp: 295 - fp: 20 - fn: 8 - precision: 0.9365 - recall: 0.9736 - f1-score: 0.9547
2021-05-16 13:43:50,897 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb/
2021-05-16 13:43:50,930 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb
2021-05-16 13:43:50,930 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb/sent_train.txt
2021-05-16 13:43:50,932 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb/sent_dev.txt
2021-05-16 13:43:50,932 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb/sent_test.txt
Corpus: 441 train + 104 dev + 165 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 13:44:05,101 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,109 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 13:44:05,110 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,111 Corpus: "Corpus: 441 train + 104 dev + 165 test sentences"
2021-05-16 13:44:05,111 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,111 Parameters:
2021-05-16 13:44:05,111  - learning_rate: "0.1"
2021-05-16 13:44:05,111  - mini_batch_size: "16"
2021-05-16 13:44:05,111  - patience: "3"
2021-05-16 13:44:05,111  - anneal_factor: "0.5"
2021-05-16 13:44:05,111  - max_epochs: "30"
2021-05-16 13:44:05,111  - shuffle: "True"
2021-05-16 13:44:05,112  - train_with_dev: "False"
2021-05-16 13:44:05,112  - batch_growth_annealing: "False"
2021-05-16 13:44:05,112 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,112 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb"
2021-05-16 13:44:05,112 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,112 Device: cuda:0
2021-05-16 13:44:05,112 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:05,112 Embeddings storage mode: cpu
2021-05-16 13:44:05,117 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:08,777 epoch 1 - iter 2/28 - loss 27.15427065 - samples/sec: 8.74 - lr: 0.100000
2021-05-16 13:44:13,422 epoch 1 - iter 4/28 - loss 16.37028152 - samples/sec: 6.90 - lr: 0.100000
2021-05-16 13:44:16,344 epoch 1 - iter 6/28 - loss 12.23668357 - samples/sec: 10.95 - lr: 0.100000
2021-05-16 13:44:19,361 epoch 1 - iter 8/28 - loss 10.82138577 - samples/sec: 10.61 - lr: 0.100000
2021-05-16 13:44:22,502 epoch 1 - iter 10/28 - loss 10.03548281 - samples/sec: 10.19 - lr: 0.100000
2021-05-16 13:44:25,747 epoch 1 - iter 12/28 - loss 9.20555375 - samples/sec: 9.86 - lr: 0.100000
2021-05-16 13:44:29,365 epoch 1 - iter 14/28 - loss 8.46028616 - samples/sec: 8.84 - lr: 0.100000
2021-05-16 13:44:33,378 epoch 1 - iter 16/28 - loss 7.91596819 - samples/sec: 7.98 - lr: 0.100000
2021-05-16 13:44:37,141 epoch 1 - iter 18/28 - loss 7.56517149 - samples/sec: 8.50 - lr: 0.100000
2021-05-16 13:44:40,258 epoch 1 - iter 20/28 - loss 7.21016533 - samples/sec: 10.27 - lr: 0.100000
2021-05-16 13:44:43,264 epoch 1 - iter 22/28 - loss 6.90144019 - samples/sec: 10.65 - lr: 0.100000
2021-05-16 13:44:46,880 epoch 1 - iter 24/28 - loss 6.91814661 - samples/sec: 8.85 - lr: 0.100000
2021-05-16 13:44:51,024 epoch 1 - iter 26/28 - loss 6.60937990 - samples/sec: 7.72 - lr: 0.100000
2021-05-16 13:44:53,752 epoch 1 - iter 28/28 - loss 6.44720686 - samples/sec: 11.73 - lr: 0.100000
2021-05-16 13:44:53,752 ----------------------------------------------------------------------------------------------------
2021-05-16 13:44:53,752 EPOCH 1 done: loss 6.4472 - lr 0.1000000
2021-05-16 13:45:00,623 DEV : loss 3.200310230255127 - score 0.0
2021-05-16 13:45:00,643 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:45:11,712 ----------------------------------------------------------------------------------------------------
2021-05-16 13:45:13,191 epoch 2 - iter 2/28 - loss 3.77475822 - samples/sec: 21.66 - lr: 0.100000
2021-05-16 13:45:14,675 epoch 2 - iter 4/28 - loss 4.54044455 - samples/sec: 21.58 - lr: 0.100000
2021-05-16 13:45:16,371 epoch 2 - iter 6/28 - loss 4.25241478 - samples/sec: 18.87 - lr: 0.100000
2021-05-16 13:45:17,671 epoch 2 - iter 8/28 - loss 4.13917792 - samples/sec: 24.63 - lr: 0.100000
2021-05-16 13:45:18,897 epoch 2 - iter 10/28 - loss 3.97888381 - samples/sec: 26.11 - lr: 0.100000
2021-05-16 13:45:20,055 epoch 2 - iter 12/28 - loss 3.98873315 - samples/sec: 27.65 - lr: 0.100000
2021-05-16 13:45:21,359 epoch 2 - iter 14/28 - loss 3.92266807 - samples/sec: 24.57 - lr: 0.100000
2021-05-16 13:45:22,793 epoch 2 - iter 16/28 - loss 3.82907368 - samples/sec: 22.32 - lr: 0.100000
2021-05-16 13:45:24,077 epoch 2 - iter 18/28 - loss 3.87610352 - samples/sec: 24.93 - lr: 0.100000
2021-05-16 13:45:25,550 epoch 2 - iter 20/28 - loss 3.87785039 - samples/sec: 21.73 - lr: 0.100000
2021-05-16 13:45:26,718 epoch 2 - iter 22/28 - loss 3.89102970 - samples/sec: 27.43 - lr: 0.100000
2021-05-16 13:45:27,968 epoch 2 - iter 24/28 - loss 3.82636994 - samples/sec: 25.61 - lr: 0.100000
2021-05-16 13:45:29,256 epoch 2 - iter 26/28 - loss 3.72556574 - samples/sec: 24.88 - lr: 0.100000
2021-05-16 13:45:30,305 epoch 2 - iter 28/28 - loss 3.70991566 - samples/sec: 30.52 - lr: 0.100000
2021-05-16 13:45:30,307 ----------------------------------------------------------------------------------------------------
2021-05-16 13:45:30,307 EPOCH 2 done: loss 3.7099 - lr 0.1000000
2021-05-16 13:45:32,175 DEV : loss 4.246613025665283 - score 0.0
2021-05-16 13:45:32,194 BAD EPOCHS (no improvement): 1
2021-05-16 13:45:32,195 ----------------------------------------------------------------------------------------------------
2021-05-16 13:45:33,555 epoch 3 - iter 2/28 - loss 3.65804327 - samples/sec: 23.53 - lr: 0.100000
2021-05-16 13:45:34,976 epoch 3 - iter 4/28 - loss 4.81718510 - samples/sec: 22.59 - lr: 0.100000
2021-05-16 13:45:36,223 epoch 3 - iter 6/28 - loss 4.71812856 - samples/sec: 25.68 - lr: 0.100000
2021-05-16 13:45:37,395 epoch 3 - iter 8/28 - loss 4.56160936 - samples/sec: 27.33 - lr: 0.100000
2021-05-16 13:45:38,750 epoch 3 - iter 10/28 - loss 4.32582211 - samples/sec: 23.63 - lr: 0.100000
2021-05-16 13:45:39,855 epoch 3 - iter 12/28 - loss 4.29355335 - samples/sec: 28.96 - lr: 0.100000
2021-05-16 13:45:41,045 epoch 3 - iter 14/28 - loss 4.19533096 - samples/sec: 26.98 - lr: 0.100000
2021-05-16 13:45:42,389 epoch 3 - iter 16/28 - loss 4.07951392 - samples/sec: 23.82 - lr: 0.100000
2021-05-16 13:45:43,580 epoch 3 - iter 18/28 - loss 3.97313572 - samples/sec: 26.88 - lr: 0.100000
2021-05-16 13:45:44,788 epoch 3 - iter 20/28 - loss 3.81182069 - samples/sec: 26.50 - lr: 0.100000
2021-05-16 13:45:45,839 epoch 3 - iter 22/28 - loss 3.81165100 - samples/sec: 30.44 - lr: 0.100000
2021-05-16 13:45:46,850 epoch 3 - iter 24/28 - loss 3.70774466 - samples/sec: 31.69 - lr: 0.100000
2021-05-16 13:45:47,981 epoch 3 - iter 26/28 - loss 3.70373632 - samples/sec: 28.31 - lr: 0.100000
2021-05-16 13:45:48,815 epoch 3 - iter 28/28 - loss 3.65637661 - samples/sec: 38.40 - lr: 0.100000
2021-05-16 13:45:48,816 ----------------------------------------------------------------------------------------------------
2021-05-16 13:45:48,816 EPOCH 3 done: loss 3.6564 - lr 0.1000000
2021-05-16 13:45:50,198 DEV : loss 3.918255567550659 - score 0.5389
2021-05-16 13:45:50,217 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:46:00,877 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:02,124 epoch 4 - iter 2/28 - loss 3.58990037 - samples/sec: 25.68 - lr: 0.100000
2021-05-16 13:46:03,107 epoch 4 - iter 4/28 - loss 3.37224966 - samples/sec: 32.58 - lr: 0.100000
2021-05-16 13:46:04,165 epoch 4 - iter 6/28 - loss 3.30755905 - samples/sec: 30.26 - lr: 0.100000
2021-05-16 13:46:05,213 epoch 4 - iter 8/28 - loss 3.11524093 - samples/sec: 30.53 - lr: 0.100000
2021-05-16 13:46:06,617 epoch 4 - iter 10/28 - loss 3.10661187 - samples/sec: 22.84 - lr: 0.100000
2021-05-16 13:46:07,780 epoch 4 - iter 12/28 - loss 3.15772917 - samples/sec: 27.60 - lr: 0.100000
2021-05-16 13:46:09,018 epoch 4 - iter 14/28 - loss 3.03309069 - samples/sec: 25.85 - lr: 0.100000
2021-05-16 13:46:10,070 epoch 4 - iter 16/28 - loss 3.05691344 - samples/sec: 30.45 - lr: 0.100000
2021-05-16 13:46:11,393 epoch 4 - iter 18/28 - loss 3.05673438 - samples/sec: 24.19 - lr: 0.100000
2021-05-16 13:46:12,740 epoch 4 - iter 20/28 - loss 3.12472926 - samples/sec: 23.81 - lr: 0.100000
2021-05-16 13:46:13,780 epoch 4 - iter 22/28 - loss 3.11640021 - samples/sec: 30.79 - lr: 0.100000
2021-05-16 13:46:14,818 epoch 4 - iter 24/28 - loss 3.07444971 - samples/sec: 30.81 - lr: 0.100000
2021-05-16 13:46:15,781 epoch 4 - iter 26/28 - loss 3.10690489 - samples/sec: 33.27 - lr: 0.100000
2021-05-16 13:46:16,971 epoch 4 - iter 28/28 - loss 3.06363667 - samples/sec: 26.92 - lr: 0.100000
2021-05-16 13:46:16,971 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:16,972 EPOCH 4 done: loss 3.0636 - lr 0.1000000
2021-05-16 13:46:18,673 DEV : loss 1.9286704063415527 - score 0.0
2021-05-16 13:46:18,692 BAD EPOCHS (no improvement): 1
2021-05-16 13:46:18,693 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:20,057 epoch 5 - iter 2/28 - loss 3.34174693 - samples/sec: 23.47 - lr: 0.100000
2021-05-16 13:46:21,157 epoch 5 - iter 4/28 - loss 3.53424895 - samples/sec: 29.11 - lr: 0.100000
2021-05-16 13:46:22,598 epoch 5 - iter 6/28 - loss 3.09506965 - samples/sec: 22.22 - lr: 0.100000
2021-05-16 13:46:23,830 epoch 5 - iter 8/28 - loss 2.81271854 - samples/sec: 25.98 - lr: 0.100000
2021-05-16 13:46:25,173 epoch 5 - iter 10/28 - loss 2.60133994 - samples/sec: 23.86 - lr: 0.100000
2021-05-16 13:46:26,403 epoch 5 - iter 12/28 - loss 2.55888357 - samples/sec: 26.02 - lr: 0.100000
2021-05-16 13:46:27,824 epoch 5 - iter 14/28 - loss 2.51859026 - samples/sec: 22.57 - lr: 0.100000
2021-05-16 13:46:28,986 epoch 5 - iter 16/28 - loss 2.53652410 - samples/sec: 27.57 - lr: 0.100000
2021-05-16 13:46:30,039 epoch 5 - iter 18/28 - loss 2.50988004 - samples/sec: 30.42 - lr: 0.100000
2021-05-16 13:46:31,079 epoch 5 - iter 20/28 - loss 2.52041554 - samples/sec: 30.79 - lr: 0.100000
2021-05-16 13:46:32,265 epoch 5 - iter 22/28 - loss 2.43738960 - samples/sec: 27.00 - lr: 0.100000
2021-05-16 13:46:33,316 epoch 5 - iter 24/28 - loss 2.47596440 - samples/sec: 30.46 - lr: 0.100000
2021-05-16 13:46:34,416 epoch 5 - iter 26/28 - loss 2.46397973 - samples/sec: 29.09 - lr: 0.100000
2021-05-16 13:46:35,309 epoch 5 - iter 28/28 - loss 2.52384786 - samples/sec: 35.87 - lr: 0.100000
2021-05-16 13:46:35,309 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:35,309 EPOCH 5 done: loss 2.5238 - lr 0.1000000
2021-05-16 13:46:37,056 DEV : loss 1.532151699066162 - score 0.4324
2021-05-16 13:46:37,075 BAD EPOCHS (no improvement): 2
2021-05-16 13:46:37,075 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:38,073 epoch 6 - iter 2/28 - loss 1.77195680 - samples/sec: 32.11 - lr: 0.100000
2021-05-16 13:46:39,059 epoch 6 - iter 4/28 - loss 2.29336584 - samples/sec: 32.47 - lr: 0.100000
2021-05-16 13:46:40,193 epoch 6 - iter 6/28 - loss 2.40767237 - samples/sec: 28.24 - lr: 0.100000
2021-05-16 13:46:41,353 epoch 6 - iter 8/28 - loss 2.27378830 - samples/sec: 27.60 - lr: 0.100000
2021-05-16 13:46:42,366 epoch 6 - iter 10/28 - loss 2.40673230 - samples/sec: 31.61 - lr: 0.100000
2021-05-16 13:46:43,386 epoch 6 - iter 12/28 - loss 2.51756624 - samples/sec: 31.39 - lr: 0.100000
2021-05-16 13:46:44,388 epoch 6 - iter 14/28 - loss 2.55450340 - samples/sec: 31.93 - lr: 0.100000
2021-05-16 13:46:45,593 epoch 6 - iter 16/28 - loss 2.41840907 - samples/sec: 26.58 - lr: 0.100000
2021-05-16 13:46:46,770 epoch 6 - iter 18/28 - loss 2.34807022 - samples/sec: 27.20 - lr: 0.100000
2021-05-16 13:46:47,820 epoch 6 - iter 20/28 - loss 2.37068986 - samples/sec: 30.48 - lr: 0.100000
2021-05-16 13:46:49,075 epoch 6 - iter 22/28 - loss 2.33416124 - samples/sec: 25.51 - lr: 0.100000
2021-05-16 13:46:50,071 epoch 6 - iter 24/28 - loss 2.34280605 - samples/sec: 32.17 - lr: 0.100000
2021-05-16 13:46:51,250 epoch 6 - iter 26/28 - loss 2.34917341 - samples/sec: 27.14 - lr: 0.100000
2021-05-16 13:46:52,171 epoch 6 - iter 28/28 - loss 2.31631796 - samples/sec: 34.81 - lr: 0.100000
2021-05-16 13:46:52,171 ----------------------------------------------------------------------------------------------------
2021-05-16 13:46:52,171 EPOCH 6 done: loss 2.3163 - lr 0.1000000
2021-05-16 13:46:54,039 DEV : loss 1.5648610591888428 - score 0.7722
2021-05-16 13:46:54,065 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:47:05,391 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:06,853 epoch 7 - iter 2/28 - loss 1.45487690 - samples/sec: 21.95 - lr: 0.100000
2021-05-16 13:47:07,960 epoch 7 - iter 4/28 - loss 1.68626916 - samples/sec: 28.92 - lr: 0.100000
2021-05-16 13:47:09,068 epoch 7 - iter 6/28 - loss 1.67967558 - samples/sec: 28.90 - lr: 0.100000
2021-05-16 13:47:10,235 epoch 7 - iter 8/28 - loss 1.78106630 - samples/sec: 27.43 - lr: 0.100000
2021-05-16 13:47:11,480 epoch 7 - iter 10/28 - loss 1.82337384 - samples/sec: 25.71 - lr: 0.100000
2021-05-16 13:47:12,874 epoch 7 - iter 12/28 - loss 2.00689598 - samples/sec: 22.96 - lr: 0.100000
2021-05-16 13:47:14,114 epoch 7 - iter 14/28 - loss 1.99557651 - samples/sec: 25.81 - lr: 0.100000
2021-05-16 13:47:15,613 epoch 7 - iter 16/28 - loss 2.19631224 - samples/sec: 21.36 - lr: 0.100000
2021-05-16 13:47:16,981 epoch 7 - iter 18/28 - loss 2.23563278 - samples/sec: 23.40 - lr: 0.100000
2021-05-16 13:47:18,178 epoch 7 - iter 20/28 - loss 2.20933929 - samples/sec: 26.76 - lr: 0.100000
2021-05-16 13:47:19,365 epoch 7 - iter 22/28 - loss 2.25744585 - samples/sec: 27.03 - lr: 0.100000
2021-05-16 13:47:20,542 epoch 7 - iter 24/28 - loss 2.25919304 - samples/sec: 27.26 - lr: 0.100000
2021-05-16 13:47:21,845 epoch 7 - iter 26/28 - loss 2.26583479 - samples/sec: 24.56 - lr: 0.100000
2021-05-16 13:47:22,914 epoch 7 - iter 28/28 - loss 2.24537990 - samples/sec: 29.98 - lr: 0.100000
2021-05-16 13:47:22,914 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:22,914 EPOCH 7 done: loss 2.2454 - lr 0.1000000
2021-05-16 13:47:24,353 DEV : loss 1.1938279867172241 - score 0.7651
2021-05-16 13:47:24,373 BAD EPOCHS (no improvement): 1
2021-05-16 13:47:24,373 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:25,494 epoch 8 - iter 2/28 - loss 1.70824766 - samples/sec: 28.58 - lr: 0.100000
2021-05-16 13:47:26,747 epoch 8 - iter 4/28 - loss 2.12882912 - samples/sec: 25.54 - lr: 0.100000
2021-05-16 13:47:28,081 epoch 8 - iter 6/28 - loss 2.04408650 - samples/sec: 24.00 - lr: 0.100000
2021-05-16 13:47:29,283 epoch 8 - iter 8/28 - loss 2.01930419 - samples/sec: 26.64 - lr: 0.100000
2021-05-16 13:47:30,420 epoch 8 - iter 10/28 - loss 1.89572418 - samples/sec: 28.16 - lr: 0.100000
2021-05-16 13:47:31,525 epoch 8 - iter 12/28 - loss 1.73862684 - samples/sec: 28.98 - lr: 0.100000
2021-05-16 13:47:32,634 epoch 8 - iter 14/28 - loss 1.67443235 - samples/sec: 28.88 - lr: 0.100000
2021-05-16 13:47:33,829 epoch 8 - iter 16/28 - loss 1.81713814 - samples/sec: 26.79 - lr: 0.100000
2021-05-16 13:47:34,856 epoch 8 - iter 18/28 - loss 1.81712654 - samples/sec: 31.15 - lr: 0.100000
2021-05-16 13:47:35,992 epoch 8 - iter 20/28 - loss 1.86812458 - samples/sec: 28.19 - lr: 0.100000
2021-05-16 13:47:37,004 epoch 8 - iter 22/28 - loss 1.86503367 - samples/sec: 31.65 - lr: 0.100000
2021-05-16 13:47:38,006 epoch 8 - iter 24/28 - loss 1.84744338 - samples/sec: 31.96 - lr: 0.100000
2021-05-16 13:47:39,205 epoch 8 - iter 26/28 - loss 1.84176241 - samples/sec: 26.68 - lr: 0.100000
2021-05-16 13:47:40,013 epoch 8 - iter 28/28 - loss 1.79779219 - samples/sec: 39.63 - lr: 0.100000
2021-05-16 13:47:40,014 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:40,014 EPOCH 8 done: loss 1.7978 - lr 0.1000000
2021-05-16 13:47:41,698 DEV : loss 2.3430771827697754 - score 0.2772
2021-05-16 13:47:41,717 BAD EPOCHS (no improvement): 2
2021-05-16 13:47:41,718 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:43,010 epoch 9 - iter 2/28 - loss 1.70992398 - samples/sec: 24.78 - lr: 0.100000
2021-05-16 13:47:44,220 epoch 9 - iter 4/28 - loss 2.16029501 - samples/sec: 26.45 - lr: 0.100000
2021-05-16 13:47:45,527 epoch 9 - iter 6/28 - loss 1.83657205 - samples/sec: 24.50 - lr: 0.100000
2021-05-16 13:47:46,869 epoch 9 - iter 8/28 - loss 1.56247243 - samples/sec: 23.85 - lr: 0.100000
2021-05-16 13:47:47,909 epoch 9 - iter 10/28 - loss 1.53998610 - samples/sec: 30.80 - lr: 0.100000
2021-05-16 13:47:48,905 epoch 9 - iter 12/28 - loss 1.49280130 - samples/sec: 32.16 - lr: 0.100000
2021-05-16 13:47:50,046 epoch 9 - iter 14/28 - loss 1.42657896 - samples/sec: 28.08 - lr: 0.100000
2021-05-16 13:47:51,234 epoch 9 - iter 16/28 - loss 1.60572346 - samples/sec: 27.02 - lr: 0.100000
2021-05-16 13:47:52,441 epoch 9 - iter 18/28 - loss 1.76406029 - samples/sec: 26.52 - lr: 0.100000
2021-05-16 13:47:53,661 epoch 9 - iter 20/28 - loss 1.82744029 - samples/sec: 26.24 - lr: 0.100000
2021-05-16 13:47:55,022 epoch 9 - iter 22/28 - loss 1.78807071 - samples/sec: 23.57 - lr: 0.100000
2021-05-16 13:47:56,207 epoch 9 - iter 24/28 - loss 1.77910972 - samples/sec: 27.03 - lr: 0.100000
2021-05-16 13:47:57,260 epoch 9 - iter 26/28 - loss 1.74891329 - samples/sec: 30.40 - lr: 0.100000
2021-05-16 13:47:58,125 epoch 9 - iter 28/28 - loss 1.75636144 - samples/sec: 37.02 - lr: 0.100000
2021-05-16 13:47:58,125 ----------------------------------------------------------------------------------------------------
2021-05-16 13:47:58,127 EPOCH 9 done: loss 1.7564 - lr 0.1000000
2021-05-16 13:47:59,626 DEV : loss 1.664988398551941 - score 0.5
2021-05-16 13:47:59,645 BAD EPOCHS (no improvement): 3
2021-05-16 13:47:59,645 ----------------------------------------------------------------------------------------------------
2021-05-16 13:48:00,670 epoch 10 - iter 2/28 - loss 2.65669751 - samples/sec: 31.26 - lr: 0.100000
2021-05-16 13:48:01,660 epoch 10 - iter 4/28 - loss 2.65652025 - samples/sec: 32.34 - lr: 0.100000
2021-05-16 13:48:02,912 epoch 10 - iter 6/28 - loss 2.55337465 - samples/sec: 25.57 - lr: 0.100000
2021-05-16 13:48:04,141 epoch 10 - iter 8/28 - loss 2.26693687 - samples/sec: 26.06 - lr: 0.100000
2021-05-16 13:48:05,184 epoch 10 - iter 10/28 - loss 2.28221498 - samples/sec: 30.73 - lr: 0.100000
2021-05-16 13:48:06,333 epoch 10 - iter 12/28 - loss 2.07322744 - samples/sec: 27.86 - lr: 0.100000
2021-05-16 13:48:07,423 epoch 10 - iter 14/28 - loss 2.01291005 - samples/sec: 29.37 - lr: 0.100000
2021-05-16 13:48:08,598 epoch 10 - iter 16/28 - loss 1.93770555 - samples/sec: 27.26 - lr: 0.100000
2021-05-16 13:48:09,754 epoch 10 - iter 18/28 - loss 1.86005722 - samples/sec: 27.67 - lr: 0.100000
2021-05-16 13:48:10,888 epoch 10 - iter 20/28 - loss 1.85275233 - samples/sec: 28.24 - lr: 0.100000
2021-05-16 13:48:11,989 epoch 10 - iter 22/28 - loss 1.85682068 - samples/sec: 29.07 - lr: 0.100000
2021-05-16 13:48:13,164 epoch 10 - iter 24/28 - loss 1.77429701 - samples/sec: 27.26 - lr: 0.100000
2021-05-16 13:48:14,315 epoch 10 - iter 26/28 - loss 1.78047710 - samples/sec: 27.81 - lr: 0.100000
2021-05-16 13:48:15,222 epoch 10 - iter 28/28 - loss 1.71446845 - samples/sec: 35.29 - lr: 0.100000
2021-05-16 13:48:15,223 ----------------------------------------------------------------------------------------------------
2021-05-16 13:48:15,223 EPOCH 10 done: loss 1.7145 - lr 0.1000000
2021-05-16 13:48:16,478 DEV : loss 0.9535346627235413 - score 0.8101
2021-05-16 13:48:16,497 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:48:27,485 ----------------------------------------------------------------------------------------------------
2021-05-16 13:48:29,129 epoch 11 - iter 2/28 - loss 1.95558786 - samples/sec: 19.48 - lr: 0.100000
2021-05-16 13:48:30,504 epoch 11 - iter 4/28 - loss 2.08477572 - samples/sec: 23.29 - lr: 0.100000
2021-05-16 13:48:31,650 epoch 11 - iter 6/28 - loss 1.79010203 - samples/sec: 28.05 - lr: 0.100000
2021-05-16 13:48:32,711 epoch 11 - iter 8/28 - loss 1.68462057 - samples/sec: 30.16 - lr: 0.100000
2021-05-16 13:48:33,852 epoch 11 - iter 10/28 - loss 1.55635544 - samples/sec: 28.05 - lr: 0.100000
2021-05-16 13:48:35,051 epoch 11 - iter 12/28 - loss 1.62718488 - samples/sec: 26.70 - lr: 0.100000
2021-05-16 13:48:36,227 epoch 11 - iter 14/28 - loss 1.79390505 - samples/sec: 27.22 - lr: 0.100000
2021-05-16 13:48:37,305 epoch 11 - iter 16/28 - loss 1.79189172 - samples/sec: 29.72 - lr: 0.100000
2021-05-16 13:48:38,316 epoch 11 - iter 18/28 - loss 1.71837707 - samples/sec: 31.65 - lr: 0.100000
2021-05-16 13:48:39,441 epoch 11 - iter 20/28 - loss 1.60280781 - samples/sec: 28.48 - lr: 0.100000
2021-05-16 13:48:40,488 epoch 11 - iter 22/28 - loss 1.53962268 - samples/sec: 30.57 - lr: 0.100000
2021-05-16 13:48:41,871 epoch 11 - iter 24/28 - loss 1.57080797 - samples/sec: 23.16 - lr: 0.100000
2021-05-16 13:48:43,089 epoch 11 - iter 26/28 - loss 1.65833095 - samples/sec: 26.27 - lr: 0.100000
2021-05-16 13:48:44,121 epoch 11 - iter 28/28 - loss 1.61814788 - samples/sec: 31.04 - lr: 0.100000
2021-05-16 13:48:44,122 ----------------------------------------------------------------------------------------------------
2021-05-16 13:48:44,122 EPOCH 11 done: loss 1.6181 - lr 0.1000000
2021-05-16 13:48:45,902 DEV : loss 1.9267818927764893 - score 0.562
2021-05-16 13:48:45,922 BAD EPOCHS (no improvement): 1
2021-05-16 13:48:45,922 ----------------------------------------------------------------------------------------------------
2021-05-16 13:48:47,124 epoch 12 - iter 2/28 - loss 0.99867767 - samples/sec: 26.64 - lr: 0.100000
2021-05-16 13:48:48,251 epoch 12 - iter 4/28 - loss 1.19866440 - samples/sec: 28.42 - lr: 0.100000
2021-05-16 13:48:49,336 epoch 12 - iter 6/28 - loss 1.30416201 - samples/sec: 29.51 - lr: 0.100000
2021-05-16 13:48:50,479 epoch 12 - iter 8/28 - loss 1.47771595 - samples/sec: 27.99 - lr: 0.100000
2021-05-16 13:48:51,544 epoch 12 - iter 10/28 - loss 1.41111789 - samples/sec: 30.08 - lr: 0.100000
2021-05-16 13:48:52,604 epoch 12 - iter 12/28 - loss 1.42856383 - samples/sec: 30.20 - lr: 0.100000
2021-05-16 13:48:53,896 epoch 12 - iter 14/28 - loss 1.33529858 - samples/sec: 24.79 - lr: 0.100000
2021-05-16 13:48:54,878 epoch 12 - iter 16/28 - loss 1.31193666 - samples/sec: 32.61 - lr: 0.100000
2021-05-16 13:48:55,954 epoch 12 - iter 18/28 - loss 1.30903998 - samples/sec: 29.76 - lr: 0.100000
2021-05-16 13:48:57,516 epoch 12 - iter 20/28 - loss 1.32693813 - samples/sec: 20.50 - lr: 0.100000
2021-05-16 13:48:58,894 epoch 12 - iter 22/28 - loss 1.29513286 - samples/sec: 23.27 - lr: 0.100000
2021-05-16 13:49:00,336 epoch 12 - iter 24/28 - loss 1.30268082 - samples/sec: 22.21 - lr: 0.100000
2021-05-16 13:49:01,507 epoch 12 - iter 26/28 - loss 1.34231405 - samples/sec: 27.33 - lr: 0.100000
2021-05-16 13:49:02,242 epoch 12 - iter 28/28 - loss 1.35899313 - samples/sec: 43.61 - lr: 0.100000
2021-05-16 13:49:02,243 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:02,243 EPOCH 12 done: loss 1.3590 - lr 0.1000000
2021-05-16 13:49:03,623 DEV : loss 1.6444252729415894 - score 0.6615
2021-05-16 13:49:03,639 BAD EPOCHS (no improvement): 2
2021-05-16 13:49:03,640 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:04,822 epoch 13 - iter 2/28 - loss 1.93191314 - samples/sec: 27.08 - lr: 0.100000
2021-05-16 13:49:05,967 epoch 13 - iter 4/28 - loss 2.32002175 - samples/sec: 27.96 - lr: 0.100000
2021-05-16 13:49:07,076 epoch 13 - iter 6/28 - loss 1.94391306 - samples/sec: 28.87 - lr: 0.100000
2021-05-16 13:49:08,197 epoch 13 - iter 8/28 - loss 1.79829246 - samples/sec: 28.55 - lr: 0.100000
2021-05-16 13:49:09,415 epoch 13 - iter 10/28 - loss 1.61862154 - samples/sec: 26.32 - lr: 0.100000
2021-05-16 13:49:10,552 epoch 13 - iter 12/28 - loss 1.68577767 - samples/sec: 28.15 - lr: 0.100000
2021-05-16 13:49:11,846 epoch 13 - iter 14/28 - loss 1.60397524 - samples/sec: 24.75 - lr: 0.100000
2021-05-16 13:49:12,931 epoch 13 - iter 16/28 - loss 1.56508648 - samples/sec: 29.52 - lr: 0.100000
2021-05-16 13:49:14,072 epoch 13 - iter 18/28 - loss 1.60383243 - samples/sec: 28.06 - lr: 0.100000
2021-05-16 13:49:15,229 epoch 13 - iter 20/28 - loss 1.57266213 - samples/sec: 27.67 - lr: 0.100000
2021-05-16 13:49:16,636 epoch 13 - iter 22/28 - loss 1.52176447 - samples/sec: 22.75 - lr: 0.100000
2021-05-16 13:49:17,961 epoch 13 - iter 24/28 - loss 1.51868203 - samples/sec: 24.17 - lr: 0.100000
2021-05-16 13:49:19,343 epoch 13 - iter 26/28 - loss 1.55090354 - samples/sec: 23.17 - lr: 0.100000
2021-05-16 13:49:20,238 epoch 13 - iter 28/28 - loss 1.57069666 - samples/sec: 35.78 - lr: 0.100000
2021-05-16 13:49:20,238 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:20,238 EPOCH 13 done: loss 1.5707 - lr 0.1000000
2021-05-16 13:49:21,544 DEV : loss 1.1059359312057495 - score 0.766
2021-05-16 13:49:21,564 BAD EPOCHS (no improvement): 3
2021-05-16 13:49:21,565 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:22,800 epoch 14 - iter 2/28 - loss 1.80488372 - samples/sec: 25.92 - lr: 0.100000
2021-05-16 13:49:23,839 epoch 14 - iter 4/28 - loss 1.88387126 - samples/sec: 30.82 - lr: 0.100000
2021-05-16 13:49:24,926 epoch 14 - iter 6/28 - loss 1.86618483 - samples/sec: 29.45 - lr: 0.100000
2021-05-16 13:49:25,934 epoch 14 - iter 8/28 - loss 1.67726929 - samples/sec: 31.74 - lr: 0.100000
2021-05-16 13:49:27,002 epoch 14 - iter 10/28 - loss 2.01911669 - samples/sec: 29.99 - lr: 0.100000
2021-05-16 13:49:28,122 epoch 14 - iter 12/28 - loss 1.97909372 - samples/sec: 28.58 - lr: 0.100000
2021-05-16 13:49:29,238 epoch 14 - iter 14/28 - loss 1.93090049 - samples/sec: 28.71 - lr: 0.100000
2021-05-16 13:49:30,579 epoch 14 - iter 16/28 - loss 1.85174162 - samples/sec: 23.87 - lr: 0.100000
2021-05-16 13:49:31,812 epoch 14 - iter 18/28 - loss 1.79998899 - samples/sec: 25.96 - lr: 0.100000
2021-05-16 13:49:33,003 epoch 14 - iter 20/28 - loss 1.76830331 - samples/sec: 26.88 - lr: 0.100000
2021-05-16 13:49:33,912 epoch 14 - iter 22/28 - loss 1.69672971 - samples/sec: 35.24 - lr: 0.100000
2021-05-16 13:49:34,923 epoch 14 - iter 24/28 - loss 1.74853951 - samples/sec: 31.66 - lr: 0.100000
2021-05-16 13:49:36,056 epoch 14 - iter 26/28 - loss 1.68170530 - samples/sec: 28.25 - lr: 0.100000
2021-05-16 13:49:37,071 epoch 14 - iter 28/28 - loss 1.66639601 - samples/sec: 31.68 - lr: 0.100000
2021-05-16 13:49:37,075 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:37,075 EPOCH 14 done: loss 1.6664 - lr 0.1000000
2021-05-16 13:49:38,369 DEV : loss 1.0995770692825317 - score 0.7407
Epoch    14: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 13:49:38,388 BAD EPOCHS (no improvement): 4
2021-05-16 13:49:38,389 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:39,765 epoch 15 - iter 2/28 - loss 1.15262985 - samples/sec: 23.26 - lr: 0.050000
2021-05-16 13:49:41,113 epoch 15 - iter 4/28 - loss 0.86323594 - samples/sec: 23.76 - lr: 0.050000
2021-05-16 13:49:42,368 epoch 15 - iter 6/28 - loss 0.86162964 - samples/sec: 25.50 - lr: 0.050000
2021-05-16 13:49:43,501 epoch 15 - iter 8/28 - loss 0.84420633 - samples/sec: 28.38 - lr: 0.050000
2021-05-16 13:49:44,891 epoch 15 - iter 10/28 - loss 0.95963144 - samples/sec: 23.02 - lr: 0.050000
2021-05-16 13:49:46,560 epoch 15 - iter 12/28 - loss 0.93372805 - samples/sec: 19.24 - lr: 0.050000
2021-05-16 13:49:47,757 epoch 15 - iter 14/28 - loss 0.90644076 - samples/sec: 26.75 - lr: 0.050000
2021-05-16 13:49:49,089 epoch 15 - iter 16/28 - loss 0.85231810 - samples/sec: 24.05 - lr: 0.050000
2021-05-16 13:49:50,755 epoch 15 - iter 18/28 - loss 0.91483891 - samples/sec: 19.20 - lr: 0.050000
2021-05-16 13:49:52,101 epoch 15 - iter 20/28 - loss 0.96375450 - samples/sec: 23.78 - lr: 0.050000
2021-05-16 13:49:53,274 epoch 15 - iter 22/28 - loss 1.03043329 - samples/sec: 27.30 - lr: 0.050000
2021-05-16 13:49:54,460 epoch 15 - iter 24/28 - loss 1.05940436 - samples/sec: 26.99 - lr: 0.050000
2021-05-16 13:49:55,479 epoch 15 - iter 26/28 - loss 1.01999806 - samples/sec: 31.43 - lr: 0.050000
2021-05-16 13:49:56,399 epoch 15 - iter 28/28 - loss 0.99984926 - samples/sec: 34.80 - lr: 0.050000
2021-05-16 13:49:56,401 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:56,401 EPOCH 15 done: loss 0.9998 - lr 0.0500000
2021-05-16 13:49:58,031 DEV : loss 0.8902885317802429 - score 0.8025
2021-05-16 13:49:58,050 BAD EPOCHS (no improvement): 1
2021-05-16 13:49:58,051 ----------------------------------------------------------------------------------------------------
2021-05-16 13:49:59,177 epoch 16 - iter 2/28 - loss 0.60159445 - samples/sec: 28.43 - lr: 0.050000
2021-05-16 13:50:00,387 epoch 16 - iter 4/28 - loss 0.71477876 - samples/sec: 26.46 - lr: 0.050000
2021-05-16 13:50:01,470 epoch 16 - iter 6/28 - loss 0.79414549 - samples/sec: 29.57 - lr: 0.050000
2021-05-16 13:50:02,707 epoch 16 - iter 8/28 - loss 0.90802151 - samples/sec: 25.88 - lr: 0.050000
2021-05-16 13:50:03,772 epoch 16 - iter 10/28 - loss 1.01669569 - samples/sec: 30.06 - lr: 0.050000
2021-05-16 13:50:04,948 epoch 16 - iter 12/28 - loss 0.94269936 - samples/sec: 27.24 - lr: 0.050000
2021-05-16 13:50:06,090 epoch 16 - iter 14/28 - loss 0.94765553 - samples/sec: 28.04 - lr: 0.050000
2021-05-16 13:50:07,351 epoch 16 - iter 16/28 - loss 0.95474835 - samples/sec: 25.39 - lr: 0.050000
2021-05-16 13:50:08,461 epoch 16 - iter 18/28 - loss 0.95843539 - samples/sec: 28.83 - lr: 0.050000
2021-05-16 13:50:09,602 epoch 16 - iter 20/28 - loss 0.96544590 - samples/sec: 28.09 - lr: 0.050000
2021-05-16 13:50:10,594 epoch 16 - iter 22/28 - loss 0.99891276 - samples/sec: 32.27 - lr: 0.050000
2021-05-16 13:50:11,695 epoch 16 - iter 24/28 - loss 0.97445392 - samples/sec: 29.07 - lr: 0.050000
2021-05-16 13:50:12,910 epoch 16 - iter 26/28 - loss 0.96856144 - samples/sec: 26.36 - lr: 0.050000
2021-05-16 13:50:13,869 epoch 16 - iter 28/28 - loss 1.03510029 - samples/sec: 33.39 - lr: 0.050000
2021-05-16 13:50:13,869 ----------------------------------------------------------------------------------------------------
2021-05-16 13:50:13,869 EPOCH 16 done: loss 1.0351 - lr 0.0500000
2021-05-16 13:50:15,335 DEV : loss 0.8856598734855652 - score 0.8129
2021-05-16 13:50:15,355 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:50:26,048 ----------------------------------------------------------------------------------------------------
2021-05-16 13:50:27,336 epoch 17 - iter 2/28 - loss 1.03711891 - samples/sec: 24.86 - lr: 0.050000
2021-05-16 13:50:28,361 epoch 17 - iter 4/28 - loss 1.49661791 - samples/sec: 31.25 - lr: 0.050000
2021-05-16 13:50:29,547 epoch 17 - iter 6/28 - loss 1.29980600 - samples/sec: 26.99 - lr: 0.050000
2021-05-16 13:50:30,881 epoch 17 - iter 8/28 - loss 1.17817727 - samples/sec: 24.00 - lr: 0.050000
2021-05-16 13:50:32,255 epoch 17 - iter 10/28 - loss 1.19384120 - samples/sec: 23.31 - lr: 0.050000
2021-05-16 13:50:33,358 epoch 17 - iter 12/28 - loss 1.08609297 - samples/sec: 29.03 - lr: 0.050000
2021-05-16 13:50:34,725 epoch 17 - iter 14/28 - loss 1.10149755 - samples/sec: 23.41 - lr: 0.050000
2021-05-16 13:50:35,894 epoch 17 - iter 16/28 - loss 1.01598775 - samples/sec: 27.42 - lr: 0.050000
2021-05-16 13:50:37,026 epoch 17 - iter 18/28 - loss 0.96863087 - samples/sec: 28.28 - lr: 0.050000
2021-05-16 13:50:38,347 epoch 17 - iter 20/28 - loss 0.95960374 - samples/sec: 24.24 - lr: 0.050000
2021-05-16 13:50:39,702 epoch 17 - iter 22/28 - loss 0.92083558 - samples/sec: 23.62 - lr: 0.050000
2021-05-16 13:50:40,727 epoch 17 - iter 24/28 - loss 0.91587028 - samples/sec: 31.27 - lr: 0.050000
2021-05-16 13:50:41,780 epoch 17 - iter 26/28 - loss 0.92127553 - samples/sec: 30.40 - lr: 0.050000
2021-05-16 13:50:42,684 epoch 17 - iter 28/28 - loss 0.89050231 - samples/sec: 35.43 - lr: 0.050000
2021-05-16 13:50:42,684 ----------------------------------------------------------------------------------------------------
2021-05-16 13:50:42,685 EPOCH 17 done: loss 0.8905 - lr 0.0500000
2021-05-16 13:50:43,953 DEV : loss 0.9045004844665527 - score 0.8366
2021-05-16 13:50:43,972 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:50:55,038 ----------------------------------------------------------------------------------------------------
2021-05-16 13:50:56,384 epoch 18 - iter 2/28 - loss 0.83946037 - samples/sec: 23.79 - lr: 0.050000
2021-05-16 13:50:57,514 epoch 18 - iter 4/28 - loss 1.05116558 - samples/sec: 28.35 - lr: 0.050000
2021-05-16 13:50:58,608 epoch 18 - iter 6/28 - loss 1.10556509 - samples/sec: 29.27 - lr: 0.050000
2021-05-16 13:50:59,804 epoch 18 - iter 8/28 - loss 1.00835279 - samples/sec: 26.76 - lr: 0.050000
2021-05-16 13:51:01,150 epoch 18 - iter 10/28 - loss 0.93310726 - samples/sec: 23.79 - lr: 0.050000
2021-05-16 13:51:02,179 epoch 18 - iter 12/28 - loss 0.89837960 - samples/sec: 31.13 - lr: 0.050000
2021-05-16 13:51:03,529 epoch 18 - iter 14/28 - loss 1.00891839 - samples/sec: 23.71 - lr: 0.050000
2021-05-16 13:51:04,564 epoch 18 - iter 16/28 - loss 0.94286799 - samples/sec: 30.93 - lr: 0.050000
2021-05-16 13:51:05,893 epoch 18 - iter 18/28 - loss 0.96675883 - samples/sec: 24.10 - lr: 0.050000
2021-05-16 13:51:07,050 epoch 18 - iter 20/28 - loss 0.96475227 - samples/sec: 27.67 - lr: 0.050000
2021-05-16 13:51:08,089 epoch 18 - iter 22/28 - loss 0.94432476 - samples/sec: 30.81 - lr: 0.050000
2021-05-16 13:51:09,161 epoch 18 - iter 24/28 - loss 0.95592090 - samples/sec: 29.86 - lr: 0.050000
2021-05-16 13:51:10,256 epoch 18 - iter 26/28 - loss 0.92462692 - samples/sec: 29.26 - lr: 0.050000
2021-05-16 13:51:10,963 epoch 18 - iter 28/28 - loss 0.89824519 - samples/sec: 45.26 - lr: 0.050000
2021-05-16 13:51:10,964 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:10,964 EPOCH 18 done: loss 0.8982 - lr 0.0500000
2021-05-16 13:51:12,500 DEV : loss 0.9138608574867249 - score 0.7709
2021-05-16 13:51:12,520 BAD EPOCHS (no improvement): 1
2021-05-16 13:51:12,521 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:13,792 epoch 19 - iter 2/28 - loss 0.76245247 - samples/sec: 25.20 - lr: 0.050000
2021-05-16 13:51:14,803 epoch 19 - iter 4/28 - loss 0.60218196 - samples/sec: 31.66 - lr: 0.050000
2021-05-16 13:51:16,076 epoch 19 - iter 6/28 - loss 0.70805017 - samples/sec: 25.15 - lr: 0.050000
2021-05-16 13:51:17,083 epoch 19 - iter 8/28 - loss 0.73752899 - samples/sec: 31.79 - lr: 0.050000
2021-05-16 13:51:18,187 epoch 19 - iter 10/28 - loss 0.68366420 - samples/sec: 29.00 - lr: 0.050000
2021-05-16 13:51:19,307 epoch 19 - iter 12/28 - loss 0.61530147 - samples/sec: 28.57 - lr: 0.050000
2021-05-16 13:51:20,284 epoch 19 - iter 14/28 - loss 0.74926200 - samples/sec: 32.81 - lr: 0.050000
2021-05-16 13:51:21,280 epoch 19 - iter 16/28 - loss 0.75759096 - samples/sec: 32.15 - lr: 0.050000
2021-05-16 13:51:22,457 epoch 19 - iter 18/28 - loss 0.73576047 - samples/sec: 27.19 - lr: 0.050000
2021-05-16 13:51:23,714 epoch 19 - iter 20/28 - loss 0.72063336 - samples/sec: 25.48 - lr: 0.050000
2021-05-16 13:51:24,761 epoch 19 - iter 22/28 - loss 0.78181799 - samples/sec: 30.59 - lr: 0.050000
2021-05-16 13:51:25,786 epoch 19 - iter 24/28 - loss 0.81926007 - samples/sec: 31.23 - lr: 0.050000
2021-05-16 13:51:26,855 epoch 19 - iter 26/28 - loss 0.83912516 - samples/sec: 29.95 - lr: 0.050000
2021-05-16 13:51:27,836 epoch 19 - iter 28/28 - loss 0.84804337 - samples/sec: 32.68 - lr: 0.050000
2021-05-16 13:51:27,836 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:27,836 EPOCH 19 done: loss 0.8480 - lr 0.0500000
2021-05-16 13:51:29,406 DEV : loss 1.003510594367981 - score 0.7692
2021-05-16 13:51:29,425 BAD EPOCHS (no improvement): 2
2021-05-16 13:51:29,426 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:30,747 epoch 20 - iter 2/28 - loss 0.88008052 - samples/sec: 24.23 - lr: 0.050000
2021-05-16 13:51:32,017 epoch 20 - iter 4/28 - loss 0.76447275 - samples/sec: 25.20 - lr: 0.050000
2021-05-16 13:51:33,178 epoch 20 - iter 6/28 - loss 0.78407266 - samples/sec: 27.59 - lr: 0.050000
2021-05-16 13:51:34,488 epoch 20 - iter 8/28 - loss 0.78973909 - samples/sec: 24.43 - lr: 0.050000
2021-05-16 13:51:35,764 epoch 20 - iter 10/28 - loss 0.71157850 - samples/sec: 25.09 - lr: 0.050000
2021-05-16 13:51:36,862 epoch 20 - iter 12/28 - loss 0.73362616 - samples/sec: 29.18 - lr: 0.050000
2021-05-16 13:51:37,988 epoch 20 - iter 14/28 - loss 0.69155551 - samples/sec: 28.43 - lr: 0.050000
2021-05-16 13:51:39,177 epoch 20 - iter 16/28 - loss 0.84083719 - samples/sec: 26.94 - lr: 0.050000
2021-05-16 13:51:40,231 epoch 20 - iter 18/28 - loss 0.79295361 - samples/sec: 30.38 - lr: 0.050000
2021-05-16 13:51:41,437 epoch 20 - iter 20/28 - loss 0.77143399 - samples/sec: 26.54 - lr: 0.050000
2021-05-16 13:51:42,641 epoch 20 - iter 22/28 - loss 0.74540480 - samples/sec: 26.60 - lr: 0.050000
2021-05-16 13:51:43,788 epoch 20 - iter 24/28 - loss 0.78949672 - samples/sec: 27.93 - lr: 0.050000
2021-05-16 13:51:45,050 epoch 20 - iter 26/28 - loss 0.76430794 - samples/sec: 25.38 - lr: 0.050000
2021-05-16 13:51:46,018 epoch 20 - iter 28/28 - loss 0.76563263 - samples/sec: 33.20 - lr: 0.050000
2021-05-16 13:51:46,019 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:46,019 EPOCH 20 done: loss 0.7656 - lr 0.0500000
2021-05-16 13:51:47,691 DEV : loss 1.0403107404708862 - score 0.8344
2021-05-16 13:51:47,710 BAD EPOCHS (no improvement): 3
2021-05-16 13:51:47,711 ----------------------------------------------------------------------------------------------------
2021-05-16 13:51:48,829 epoch 21 - iter 2/28 - loss 0.59857512 - samples/sec: 28.64 - lr: 0.050000
2021-05-16 13:51:50,192 epoch 21 - iter 4/28 - loss 1.01983714 - samples/sec: 23.49 - lr: 0.050000
2021-05-16 13:51:51,347 epoch 21 - iter 6/28 - loss 1.09321864 - samples/sec: 27.72 - lr: 0.050000
2021-05-16 13:51:52,698 epoch 21 - iter 8/28 - loss 1.02968457 - samples/sec: 23.69 - lr: 0.050000
2021-05-16 13:51:54,195 epoch 21 - iter 10/28 - loss 0.97497466 - samples/sec: 21.39 - lr: 0.050000
2021-05-16 13:51:55,335 epoch 21 - iter 12/28 - loss 1.03386152 - samples/sec: 28.10 - lr: 0.050000
2021-05-16 13:51:56,539 epoch 21 - iter 14/28 - loss 0.95712795 - samples/sec: 26.62 - lr: 0.050000
2021-05-16 13:51:57,671 epoch 21 - iter 16/28 - loss 0.93530753 - samples/sec: 28.28 - lr: 0.050000
2021-05-16 13:51:58,990 epoch 21 - iter 18/28 - loss 0.92183781 - samples/sec: 24.28 - lr: 0.050000
2021-05-16 13:52:00,354 epoch 21 - iter 20/28 - loss 0.91349306 - samples/sec: 23.46 - lr: 0.050000
2021-05-16 13:52:01,618 epoch 21 - iter 22/28 - loss 0.91021177 - samples/sec: 25.33 - lr: 0.050000
2021-05-16 13:52:02,998 epoch 21 - iter 24/28 - loss 0.90192011 - samples/sec: 23.20 - lr: 0.050000
2021-05-16 13:52:04,271 epoch 21 - iter 26/28 - loss 0.88832486 - samples/sec: 25.14 - lr: 0.050000
2021-05-16 13:52:05,403 epoch 21 - iter 28/28 - loss 0.88608511 - samples/sec: 28.30 - lr: 0.050000
2021-05-16 13:52:05,403 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:05,403 EPOCH 21 done: loss 0.8861 - lr 0.0500000
2021-05-16 13:52:07,239 DEV : loss 0.9778538942337036 - score 0.8163
Epoch    21: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 13:52:07,259 BAD EPOCHS (no improvement): 4
2021-05-16 13:52:07,259 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:08,483 epoch 22 - iter 2/28 - loss 0.72783577 - samples/sec: 26.17 - lr: 0.025000
2021-05-16 13:52:09,554 epoch 22 - iter 4/28 - loss 0.69212824 - samples/sec: 29.89 - lr: 0.025000
2021-05-16 13:52:10,744 epoch 22 - iter 6/28 - loss 0.77111872 - samples/sec: 26.90 - lr: 0.025000
2021-05-16 13:52:12,202 epoch 22 - iter 8/28 - loss 0.69337749 - samples/sec: 21.97 - lr: 0.025000
2021-05-16 13:52:13,532 epoch 22 - iter 10/28 - loss 0.81579986 - samples/sec: 24.06 - lr: 0.025000
2021-05-16 13:52:15,008 epoch 22 - iter 12/28 - loss 0.74294762 - samples/sec: 21.69 - lr: 0.025000
2021-05-16 13:52:16,230 epoch 22 - iter 14/28 - loss 0.76338180 - samples/sec: 26.21 - lr: 0.025000
2021-05-16 13:52:17,451 epoch 22 - iter 16/28 - loss 0.73427987 - samples/sec: 26.23 - lr: 0.025000
2021-05-16 13:52:18,747 epoch 22 - iter 18/28 - loss 0.82849316 - samples/sec: 24.70 - lr: 0.025000
2021-05-16 13:52:20,225 epoch 22 - iter 20/28 - loss 0.81955058 - samples/sec: 21.66 - lr: 0.025000
2021-05-16 13:52:21,491 epoch 22 - iter 22/28 - loss 0.77595848 - samples/sec: 25.30 - lr: 0.025000
2021-05-16 13:52:22,831 epoch 22 - iter 24/28 - loss 0.78209925 - samples/sec: 23.90 - lr: 0.025000
2021-05-16 13:52:24,444 epoch 22 - iter 26/28 - loss 0.78487626 - samples/sec: 19.85 - lr: 0.025000
2021-05-16 13:52:25,536 epoch 22 - iter 28/28 - loss 0.75916737 - samples/sec: 29.31 - lr: 0.025000
2021-05-16 13:52:25,536 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:25,537 EPOCH 22 done: loss 0.7592 - lr 0.0250000
2021-05-16 13:52:26,973 DEV : loss 0.8292887210845947 - score 0.8171
2021-05-16 13:52:26,993 BAD EPOCHS (no improvement): 1
2021-05-16 13:52:26,993 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:28,168 epoch 23 - iter 2/28 - loss 1.01556396 - samples/sec: 27.27 - lr: 0.025000
2021-05-16 13:52:29,597 epoch 23 - iter 4/28 - loss 0.86600795 - samples/sec: 22.41 - lr: 0.025000
2021-05-16 13:52:30,745 epoch 23 - iter 6/28 - loss 0.70648581 - samples/sec: 27.88 - lr: 0.025000
2021-05-16 13:52:32,188 epoch 23 - iter 8/28 - loss 0.77098028 - samples/sec: 22.18 - lr: 0.025000
2021-05-16 13:52:33,430 epoch 23 - iter 10/28 - loss 0.90278679 - samples/sec: 25.79 - lr: 0.025000
2021-05-16 13:52:34,841 epoch 23 - iter 12/28 - loss 0.84818669 - samples/sec: 22.69 - lr: 0.025000
2021-05-16 13:52:36,349 epoch 23 - iter 14/28 - loss 0.81252752 - samples/sec: 21.23 - lr: 0.025000
2021-05-16 13:52:37,936 epoch 23 - iter 16/28 - loss 0.76357463 - samples/sec: 20.17 - lr: 0.025000
2021-05-16 13:52:39,122 epoch 23 - iter 18/28 - loss 0.76387132 - samples/sec: 26.98 - lr: 0.025000
2021-05-16 13:52:40,386 epoch 23 - iter 20/28 - loss 0.75290738 - samples/sec: 25.33 - lr: 0.025000
2021-05-16 13:52:41,708 epoch 23 - iter 22/28 - loss 0.74803847 - samples/sec: 24.22 - lr: 0.025000
2021-05-16 13:52:42,902 epoch 23 - iter 24/28 - loss 0.74769510 - samples/sec: 26.81 - lr: 0.025000
2021-05-16 13:52:44,209 epoch 23 - iter 26/28 - loss 0.72246525 - samples/sec: 24.50 - lr: 0.025000
2021-05-16 13:52:45,280 epoch 23 - iter 28/28 - loss 0.71812676 - samples/sec: 29.91 - lr: 0.025000
2021-05-16 13:52:45,281 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:45,282 EPOCH 23 done: loss 0.7181 - lr 0.0250000
2021-05-16 13:52:46,802 DEV : loss 0.9646568894386292 - score 0.7979
2021-05-16 13:52:46,821 BAD EPOCHS (no improvement): 2
2021-05-16 13:52:46,821 ----------------------------------------------------------------------------------------------------
2021-05-16 13:52:48,143 epoch 24 - iter 2/28 - loss 0.53570324 - samples/sec: 24.22 - lr: 0.025000
2021-05-16 13:52:49,238 epoch 24 - iter 4/28 - loss 0.83975723 - samples/sec: 29.26 - lr: 0.025000
2021-05-16 13:52:50,343 epoch 24 - iter 6/28 - loss 0.90644703 - samples/sec: 29.00 - lr: 0.025000
2021-05-16 13:52:51,412 epoch 24 - iter 8/28 - loss 0.74427380 - samples/sec: 29.93 - lr: 0.025000
2021-05-16 13:52:52,551 epoch 24 - iter 10/28 - loss 0.81092008 - samples/sec: 28.11 - lr: 0.025000
2021-05-16 13:52:53,678 epoch 24 - iter 12/28 - loss 0.75255728 - samples/sec: 28.44 - lr: 0.025000
2021-05-16 13:52:54,960 epoch 24 - iter 14/28 - loss 0.76155203 - samples/sec: 24.96 - lr: 0.025000
2021-05-16 13:52:56,086 epoch 24 - iter 16/28 - loss 0.74796131 - samples/sec: 28.44 - lr: 0.025000
2021-05-16 13:52:57,518 epoch 24 - iter 18/28 - loss 0.73026105 - samples/sec: 22.36 - lr: 0.025000
2021-05-16 13:52:59,111 epoch 24 - iter 20/28 - loss 0.71012678 - samples/sec: 20.10 - lr: 0.025000
2021-05-16 13:53:00,305 epoch 24 - iter 22/28 - loss 0.69681901 - samples/sec: 26.82 - lr: 0.025000
2021-05-16 13:53:01,662 epoch 24 - iter 24/28 - loss 0.71630792 - samples/sec: 23.58 - lr: 0.025000
2021-05-16 13:53:02,852 epoch 24 - iter 26/28 - loss 0.70045398 - samples/sec: 26.91 - lr: 0.025000
2021-05-16 13:53:03,949 epoch 24 - iter 28/28 - loss 0.73294110 - samples/sec: 29.29 - lr: 0.025000
2021-05-16 13:53:03,950 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:03,950 EPOCH 24 done: loss 0.7329 - lr 0.0250000
2021-05-16 13:53:05,471 DEV : loss 0.9207488298416138 - score 0.7892
2021-05-16 13:53:05,509 BAD EPOCHS (no improvement): 3
2021-05-16 13:53:05,509 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:06,656 epoch 25 - iter 2/28 - loss 0.81305480 - samples/sec: 27.92 - lr: 0.025000
2021-05-16 13:53:07,885 epoch 25 - iter 4/28 - loss 0.49665916 - samples/sec: 26.06 - lr: 0.025000
2021-05-16 13:53:09,096 epoch 25 - iter 6/28 - loss 0.47255246 - samples/sec: 26.43 - lr: 0.025000
2021-05-16 13:53:10,137 epoch 25 - iter 8/28 - loss 0.49411917 - samples/sec: 30.75 - lr: 0.025000
2021-05-16 13:53:11,071 epoch 25 - iter 10/28 - loss 0.56170850 - samples/sec: 34.27 - lr: 0.025000
2021-05-16 13:53:12,344 epoch 25 - iter 12/28 - loss 0.63053469 - samples/sec: 25.16 - lr: 0.025000
2021-05-16 13:53:13,542 epoch 25 - iter 14/28 - loss 0.57206542 - samples/sec: 26.72 - lr: 0.025000
2021-05-16 13:53:14,716 epoch 25 - iter 16/28 - loss 0.59393018 - samples/sec: 27.27 - lr: 0.025000
2021-05-16 13:53:15,817 epoch 25 - iter 18/28 - loss 0.66461659 - samples/sec: 29.10 - lr: 0.025000
2021-05-16 13:53:16,853 epoch 25 - iter 20/28 - loss 0.67426568 - samples/sec: 30.91 - lr: 0.025000
2021-05-16 13:53:17,994 epoch 25 - iter 22/28 - loss 0.67273178 - samples/sec: 28.10 - lr: 0.025000
2021-05-16 13:53:19,157 epoch 25 - iter 24/28 - loss 0.68115122 - samples/sec: 27.53 - lr: 0.025000
2021-05-16 13:53:20,176 epoch 25 - iter 26/28 - loss 0.68383296 - samples/sec: 31.41 - lr: 0.025000
2021-05-16 13:53:21,501 epoch 25 - iter 28/28 - loss 0.69356046 - samples/sec: 24.24 - lr: 0.025000
2021-05-16 13:53:21,502 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:21,502 EPOCH 25 done: loss 0.6936 - lr 0.0250000
2021-05-16 13:53:23,300 DEV : loss 0.8670610189437866 - score 0.8144
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-16 13:53:23,320 BAD EPOCHS (no improvement): 4
2021-05-16 13:53:23,320 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:24,687 epoch 26 - iter 2/28 - loss 0.88448359 - samples/sec: 23.43 - lr: 0.012500
2021-05-16 13:53:25,759 epoch 26 - iter 4/28 - loss 0.66086758 - samples/sec: 29.87 - lr: 0.012500
2021-05-16 13:53:26,953 epoch 26 - iter 6/28 - loss 0.61550260 - samples/sec: 26.80 - lr: 0.012500
2021-05-16 13:53:28,315 epoch 26 - iter 8/28 - loss 0.58001691 - samples/sec: 23.52 - lr: 0.012500
2021-05-16 13:53:29,429 epoch 26 - iter 10/28 - loss 0.69177842 - samples/sec: 28.75 - lr: 0.012500
2021-05-16 13:53:30,855 epoch 26 - iter 12/28 - loss 0.75060642 - samples/sec: 22.45 - lr: 0.012500
2021-05-16 13:53:32,234 epoch 26 - iter 14/28 - loss 0.71858723 - samples/sec: 23.22 - lr: 0.012500
2021-05-16 13:53:33,504 epoch 26 - iter 16/28 - loss 0.73329752 - samples/sec: 25.20 - lr: 0.012500
2021-05-16 13:53:35,048 epoch 26 - iter 18/28 - loss 0.72487922 - samples/sec: 20.79 - lr: 0.012500
2021-05-16 13:53:36,161 epoch 26 - iter 20/28 - loss 0.76708658 - samples/sec: 28.75 - lr: 0.012500
2021-05-16 13:53:37,400 epoch 26 - iter 22/28 - loss 0.77855230 - samples/sec: 25.83 - lr: 0.012500
2021-05-16 13:53:38,406 epoch 26 - iter 24/28 - loss 0.75294520 - samples/sec: 31.85 - lr: 0.012500
2021-05-16 13:53:39,569 epoch 26 - iter 26/28 - loss 0.72575440 - samples/sec: 27.54 - lr: 0.012500
2021-05-16 13:53:40,568 epoch 26 - iter 28/28 - loss 0.71318314 - samples/sec: 32.05 - lr: 0.012500
2021-05-16 13:53:40,568 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:40,569 EPOCH 26 done: loss 0.7132 - lr 0.0125000
2021-05-16 13:53:42,151 DEV : loss 0.8781459927558899 - score 0.807
2021-05-16 13:53:42,171 BAD EPOCHS (no improvement): 1
2021-05-16 13:53:42,172 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:43,220 epoch 27 - iter 2/28 - loss 0.26754713 - samples/sec: 30.54 - lr: 0.012500
2021-05-16 13:53:44,422 epoch 27 - iter 4/28 - loss 0.71375692 - samples/sec: 26.63 - lr: 0.012500
2021-05-16 13:53:45,592 epoch 27 - iter 6/28 - loss 0.67964085 - samples/sec: 27.37 - lr: 0.012500
2021-05-16 13:53:46,533 epoch 27 - iter 8/28 - loss 0.68296462 - samples/sec: 34.03 - lr: 0.012500
2021-05-16 13:53:47,686 epoch 27 - iter 10/28 - loss 0.64765621 - samples/sec: 27.77 - lr: 0.012500
2021-05-16 13:53:48,709 epoch 27 - iter 12/28 - loss 0.60347470 - samples/sec: 31.29 - lr: 0.012500
2021-05-16 13:53:50,015 epoch 27 - iter 14/28 - loss 0.61867864 - samples/sec: 24.51 - lr: 0.012500
2021-05-16 13:53:51,244 epoch 27 - iter 16/28 - loss 0.58859933 - samples/sec: 26.04 - lr: 0.012500
2021-05-16 13:53:52,254 epoch 27 - iter 18/28 - loss 0.58676937 - samples/sec: 31.75 - lr: 0.012500
2021-05-16 13:53:53,314 epoch 27 - iter 20/28 - loss 0.59127188 - samples/sec: 30.20 - lr: 0.012500
2021-05-16 13:53:54,376 epoch 27 - iter 22/28 - loss 0.60531386 - samples/sec: 30.15 - lr: 0.012500
2021-05-16 13:53:55,583 epoch 27 - iter 24/28 - loss 0.67708590 - samples/sec: 26.52 - lr: 0.012500
2021-05-16 13:53:56,687 epoch 27 - iter 26/28 - loss 0.66511910 - samples/sec: 29.09 - lr: 0.012500
2021-05-16 13:53:57,630 epoch 27 - iter 28/28 - loss 0.65529298 - samples/sec: 33.96 - lr: 0.012500
2021-05-16 13:53:57,631 ----------------------------------------------------------------------------------------------------
2021-05-16 13:53:57,631 EPOCH 27 done: loss 0.6553 - lr 0.0125000
2021-05-16 13:53:58,973 DEV : loss 0.881950855255127 - score 0.8466
2021-05-16 13:53:58,992 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:54:10,411 ----------------------------------------------------------------------------------------------------
2021-05-16 13:54:11,509 epoch 28 - iter 2/28 - loss 0.73286605 - samples/sec: 29.16 - lr: 0.012500
2021-05-16 13:54:12,556 epoch 28 - iter 4/28 - loss 0.91927490 - samples/sec: 30.59 - lr: 0.012500
2021-05-16 13:54:13,671 epoch 28 - iter 6/28 - loss 0.75862890 - samples/sec: 28.70 - lr: 0.012500
2021-05-16 13:54:15,058 epoch 28 - iter 8/28 - loss 0.61356772 - samples/sec: 23.09 - lr: 0.012500
2021-05-16 13:54:16,550 epoch 28 - iter 10/28 - loss 0.66014808 - samples/sec: 21.49 - lr: 0.012500
2021-05-16 13:54:18,044 epoch 28 - iter 12/28 - loss 0.62199349 - samples/sec: 21.43 - lr: 0.012500
2021-05-16 13:54:19,129 epoch 28 - iter 14/28 - loss 0.60431517 - samples/sec: 29.51 - lr: 0.012500
2021-05-16 13:54:20,256 epoch 28 - iter 16/28 - loss 0.59199383 - samples/sec: 28.43 - lr: 0.012500
2021-05-16 13:54:21,412 epoch 28 - iter 18/28 - loss 0.55456873 - samples/sec: 27.70 - lr: 0.012500
2021-05-16 13:54:22,611 epoch 28 - iter 20/28 - loss 0.60880963 - samples/sec: 26.70 - lr: 0.012500
2021-05-16 13:54:23,864 epoch 28 - iter 22/28 - loss 0.58190169 - samples/sec: 25.54 - lr: 0.012500
2021-05-16 13:54:24,965 epoch 28 - iter 24/28 - loss 0.59243376 - samples/sec: 29.10 - lr: 0.012500
2021-05-16 13:54:26,056 epoch 28 - iter 26/28 - loss 0.63061268 - samples/sec: 29.33 - lr: 0.012500
2021-05-16 13:54:26,927 epoch 28 - iter 28/28 - loss 0.64814072 - samples/sec: 36.81 - lr: 0.012500
2021-05-16 13:54:26,931 ----------------------------------------------------------------------------------------------------
2021-05-16 13:54:26,931 EPOCH 28 done: loss 0.6481 - lr 0.0125000
2021-05-16 13:54:28,345 DEV : loss 0.851926863193512 - score 0.7931
2021-05-16 13:54:28,365 BAD EPOCHS (no improvement): 1
2021-05-16 13:54:28,365 ----------------------------------------------------------------------------------------------------
2021-05-16 13:54:29,624 epoch 29 - iter 2/28 - loss 0.24490607 - samples/sec: 25.44 - lr: 0.012500
2021-05-16 13:54:30,910 epoch 29 - iter 4/28 - loss 0.27740842 - samples/sec: 24.90 - lr: 0.012500
2021-05-16 13:54:32,234 epoch 29 - iter 6/28 - loss 0.50919235 - samples/sec: 24.17 - lr: 0.012500
2021-05-16 13:54:33,541 epoch 29 - iter 8/28 - loss 0.47179773 - samples/sec: 24.51 - lr: 0.012500
2021-05-16 13:54:34,814 epoch 29 - iter 10/28 - loss 0.48896892 - samples/sec: 25.14 - lr: 0.012500
2021-05-16 13:54:36,173 epoch 29 - iter 12/28 - loss 0.48056302 - samples/sec: 23.56 - lr: 0.012500
2021-05-16 13:54:37,468 epoch 29 - iter 14/28 - loss 0.49462400 - samples/sec: 24.72 - lr: 0.012500
2021-05-16 13:54:38,972 epoch 29 - iter 16/28 - loss 0.55118986 - samples/sec: 21.29 - lr: 0.012500
2021-05-16 13:54:40,175 epoch 29 - iter 18/28 - loss 0.55836904 - samples/sec: 26.61 - lr: 0.012500
2021-05-16 13:54:41,235 epoch 29 - iter 20/28 - loss 0.54152179 - samples/sec: 30.21 - lr: 0.012500
2021-05-16 13:54:42,473 epoch 29 - iter 22/28 - loss 0.52172987 - samples/sec: 25.85 - lr: 0.012500
2021-05-16 13:54:43,811 epoch 29 - iter 24/28 - loss 0.54126455 - samples/sec: 23.93 - lr: 0.012500
2021-05-16 13:54:44,887 epoch 29 - iter 26/28 - loss 0.54264579 - samples/sec: 29.77 - lr: 0.012500
2021-05-16 13:54:45,972 epoch 29 - iter 28/28 - loss 0.58152405 - samples/sec: 29.51 - lr: 0.012500
2021-05-16 13:54:45,972 ----------------------------------------------------------------------------------------------------
2021-05-16 13:54:45,972 EPOCH 29 done: loss 0.5815 - lr 0.0125000
2021-05-16 13:54:47,499 DEV : loss 0.8656717538833618 - score 0.7977
2021-05-16 13:54:47,519 BAD EPOCHS (no improvement): 2
2021-05-16 13:54:47,519 ----------------------------------------------------------------------------------------------------
2021-05-16 13:54:48,819 epoch 30 - iter 2/28 - loss 0.61909294 - samples/sec: 24.63 - lr: 0.012500
2021-05-16 13:54:49,955 epoch 30 - iter 4/28 - loss 0.61277315 - samples/sec: 28.18 - lr: 0.012500
2021-05-16 13:54:51,127 epoch 30 - iter 6/28 - loss 0.56412401 - samples/sec: 27.32 - lr: 0.012500
2021-05-16 13:54:52,211 epoch 30 - iter 8/28 - loss 0.51871864 - samples/sec: 29.55 - lr: 0.012500
2021-05-16 13:54:53,238 epoch 30 - iter 10/28 - loss 0.51715935 - samples/sec: 31.16 - lr: 0.012500
2021-05-16 13:54:54,195 epoch 30 - iter 12/28 - loss 0.54633063 - samples/sec: 33.45 - lr: 0.012500
2021-05-16 13:54:55,419 epoch 30 - iter 14/28 - loss 0.59619339 - samples/sec: 26.17 - lr: 0.012500
2021-05-16 13:54:56,790 epoch 30 - iter 16/28 - loss 0.62043257 - samples/sec: 23.36 - lr: 0.012500
2021-05-16 13:54:58,046 epoch 30 - iter 18/28 - loss 0.62227929 - samples/sec: 25.49 - lr: 0.012500
2021-05-16 13:54:59,329 epoch 30 - iter 20/28 - loss 0.61148919 - samples/sec: 24.96 - lr: 0.012500
2021-05-16 13:55:00,638 epoch 30 - iter 22/28 - loss 0.61136386 - samples/sec: 24.64 - lr: 0.012500
2021-05-16 13:55:01,922 epoch 30 - iter 24/28 - loss 0.59361769 - samples/sec: 24.93 - lr: 0.012500
2021-05-16 13:55:03,164 epoch 30 - iter 26/28 - loss 0.59031133 - samples/sec: 25.78 - lr: 0.012500
2021-05-16 13:55:04,208 epoch 30 - iter 28/28 - loss 0.59203117 - samples/sec: 30.66 - lr: 0.012500
2021-05-16 13:55:04,208 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:04,209 EPOCH 30 done: loss 0.5920 - lr 0.0125000
2021-05-16 13:55:05,628 DEV : loss 0.8384714722633362 - score 0.807
2021-05-16 13:55:05,647 BAD EPOCHS (no improvement): 3
2021-05-16 13:55:16,524 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:16,524 Testing using best model ...
2021-05-16 13:55:16,525 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/zho.rst.sctb/best-model.pt
2021-05-16 13:55:30,305 0.9381	0.8030	0.8653
2021-05-16 13:55:30,305 
Results:
- F1-score (micro) 0.8653
- F1-score (macro) 0.8653

By class:
SENT       tp: 106 - fp: 7 - fn: 26 - precision: 0.9381 - recall: 0.8030 - f1-score: 0.8653
2021-05-16 13:55:30,305 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis/
2021-05-16 13:55:30,342 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis
2021-05-16 13:55:30,343 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis/sent_train.txt
2021-05-16 13:55:30,345 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis/sent_dev.txt
2021-05-16 13:55:30,345 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis/sent_test.txt
Corpus: 1092 train + 243 dev + 249 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 13:55:46,629 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,638 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 13:55:46,643 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,643 Corpus: "Corpus: 1092 train + 243 dev + 249 test sentences"
2021-05-16 13:55:46,643 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,643 Parameters:
2021-05-16 13:55:46,644  - learning_rate: "0.1"
2021-05-16 13:55:46,644  - mini_batch_size: "16"
2021-05-16 13:55:46,644  - patience: "3"
2021-05-16 13:55:46,644  - anneal_factor: "0.5"
2021-05-16 13:55:46,644  - max_epochs: "30"
2021-05-16 13:55:46,644  - shuffle: "True"
2021-05-16 13:55:46,644  - train_with_dev: "False"
2021-05-16 13:55:46,645  - batch_growth_annealing: "False"
2021-05-16 13:55:46,645 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,645 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis"
2021-05-16 13:55:46,645 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,645 Device: cuda:0
2021-05-16 13:55:46,645 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:46,646 Embeddings storage mode: cpu
2021-05-16 13:55:46,651 ----------------------------------------------------------------------------------------------------
2021-05-16 13:55:56,499 epoch 1 - iter 6/69 - loss 13.33486303 - samples/sec: 9.75 - lr: 0.100000
2021-05-16 13:56:06,518 epoch 1 - iter 12/69 - loss 9.94108665 - samples/sec: 9.58 - lr: 0.100000
2021-05-16 13:56:16,375 epoch 1 - iter 18/69 - loss 9.08400616 - samples/sec: 9.74 - lr: 0.100000
2021-05-16 13:56:26,227 epoch 1 - iter 24/69 - loss 8.32537485 - samples/sec: 9.75 - lr: 0.100000
2021-05-16 13:56:36,039 epoch 1 - iter 30/69 - loss 7.72067095 - samples/sec: 9.78 - lr: 0.100000
2021-05-16 13:56:46,262 epoch 1 - iter 36/69 - loss 7.27138774 - samples/sec: 9.39 - lr: 0.100000
2021-05-16 13:56:56,263 epoch 1 - iter 42/69 - loss 6.92117369 - samples/sec: 9.60 - lr: 0.100000
2021-05-16 13:57:05,844 epoch 1 - iter 48/69 - loss 6.48589804 - samples/sec: 10.02 - lr: 0.100000
2021-05-16 13:57:16,301 epoch 1 - iter 54/69 - loss 6.15053349 - samples/sec: 9.18 - lr: 0.100000
2021-05-16 13:57:27,116 epoch 1 - iter 60/69 - loss 5.81564269 - samples/sec: 8.88 - lr: 0.100000
2021-05-16 13:57:37,982 epoch 1 - iter 66/69 - loss 5.55350855 - samples/sec: 8.84 - lr: 0.100000
2021-05-16 13:57:42,319 ----------------------------------------------------------------------------------------------------
2021-05-16 13:57:42,320 EPOCH 1 done: loss 5.4691 - lr 0.1000000
2021-05-16 13:58:00,646 DEV : loss 1.8216198682785034 - score 0.0
2021-05-16 13:58:00,682 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:58:11,285 ----------------------------------------------------------------------------------------------------
2021-05-16 13:58:15,436 epoch 2 - iter 6/69 - loss 2.78494600 - samples/sec: 23.13 - lr: 0.100000
2021-05-16 13:58:19,418 epoch 2 - iter 12/69 - loss 2.42821326 - samples/sec: 24.11 - lr: 0.100000
2021-05-16 13:58:23,541 epoch 2 - iter 18/69 - loss 2.41656946 - samples/sec: 23.29 - lr: 0.100000
2021-05-16 13:58:27,726 epoch 2 - iter 24/69 - loss 2.38234161 - samples/sec: 22.94 - lr: 0.100000
2021-05-16 13:58:31,421 epoch 2 - iter 30/69 - loss 2.37179361 - samples/sec: 25.99 - lr: 0.100000
2021-05-16 13:58:35,595 epoch 2 - iter 36/69 - loss 2.15796300 - samples/sec: 23.00 - lr: 0.100000
2021-05-16 13:58:39,814 epoch 2 - iter 42/69 - loss 2.06634945 - samples/sec: 22.76 - lr: 0.100000
2021-05-16 13:58:43,689 epoch 2 - iter 48/69 - loss 2.04136055 - samples/sec: 24.78 - lr: 0.100000
2021-05-16 13:58:48,174 epoch 2 - iter 54/69 - loss 1.97453708 - samples/sec: 21.43 - lr: 0.100000
2021-05-16 13:58:52,195 epoch 2 - iter 60/69 - loss 1.85979533 - samples/sec: 23.88 - lr: 0.100000
2021-05-16 13:58:56,276 epoch 2 - iter 66/69 - loss 1.79301580 - samples/sec: 23.53 - lr: 0.100000
2021-05-16 13:58:57,896 ----------------------------------------------------------------------------------------------------
2021-05-16 13:58:57,896 EPOCH 2 done: loss 1.7611 - lr 0.1000000
2021-05-16 13:59:02,676 DEV : loss 0.49354979395866394 - score 0.8947
2021-05-16 13:59:02,721 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 13:59:13,450 ----------------------------------------------------------------------------------------------------
2021-05-16 13:59:17,844 epoch 3 - iter 6/69 - loss 1.24709590 - samples/sec: 21.86 - lr: 0.100000
2021-05-16 13:59:22,555 epoch 3 - iter 12/69 - loss 1.01713030 - samples/sec: 20.39 - lr: 0.100000
2021-05-16 13:59:26,836 epoch 3 - iter 18/69 - loss 1.10330454 - samples/sec: 22.43 - lr: 0.100000
2021-05-16 13:59:31,547 epoch 3 - iter 24/69 - loss 1.03277567 - samples/sec: 20.39 - lr: 0.100000
2021-05-16 13:59:35,531 epoch 3 - iter 30/69 - loss 1.02741789 - samples/sec: 24.10 - lr: 0.100000
2021-05-16 13:59:39,478 epoch 3 - iter 36/69 - loss 1.00335497 - samples/sec: 24.33 - lr: 0.100000
2021-05-16 13:59:43,920 epoch 3 - iter 42/69 - loss 1.05495940 - samples/sec: 21.62 - lr: 0.100000
2021-05-16 13:59:48,356 epoch 3 - iter 48/69 - loss 1.05871848 - samples/sec: 21.65 - lr: 0.100000
2021-05-16 13:59:52,776 epoch 3 - iter 54/69 - loss 1.08335098 - samples/sec: 21.72 - lr: 0.100000
2021-05-16 13:59:56,561 epoch 3 - iter 60/69 - loss 1.06732511 - samples/sec: 25.36 - lr: 0.100000
2021-05-16 14:00:00,011 epoch 3 - iter 66/69 - loss 1.03199894 - samples/sec: 27.84 - lr: 0.100000
2021-05-16 14:00:01,679 ----------------------------------------------------------------------------------------------------
2021-05-16 14:00:01,680 EPOCH 3 done: loss 1.0165 - lr 0.1000000
2021-05-16 14:00:06,036 DEV : loss 0.23899151384830475 - score 0.9577
2021-05-16 14:00:06,097 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:00:17,202 ----------------------------------------------------------------------------------------------------
2021-05-16 14:00:21,942 epoch 4 - iter 6/69 - loss 1.39073579 - samples/sec: 20.26 - lr: 0.100000
2021-05-16 14:00:26,929 epoch 4 - iter 12/69 - loss 1.12602658 - samples/sec: 19.27 - lr: 0.100000
2021-05-16 14:00:31,122 epoch 4 - iter 18/69 - loss 1.17333938 - samples/sec: 22.90 - lr: 0.100000
2021-05-16 14:00:35,021 epoch 4 - iter 24/69 - loss 1.28610772 - samples/sec: 24.63 - lr: 0.100000
2021-05-16 14:00:38,833 epoch 4 - iter 30/69 - loss 1.19924066 - samples/sec: 25.19 - lr: 0.100000
2021-05-16 14:00:42,952 epoch 4 - iter 36/69 - loss 1.08990555 - samples/sec: 23.31 - lr: 0.100000
2021-05-16 14:00:47,636 epoch 4 - iter 42/69 - loss 1.01211680 - samples/sec: 20.50 - lr: 0.100000
2021-05-16 14:00:51,874 epoch 4 - iter 48/69 - loss 0.94747127 - samples/sec: 22.66 - lr: 0.100000
2021-05-16 14:00:55,919 epoch 4 - iter 54/69 - loss 0.94639093 - samples/sec: 23.74 - lr: 0.100000
2021-05-16 14:01:00,344 epoch 4 - iter 60/69 - loss 0.91131963 - samples/sec: 21.69 - lr: 0.100000
2021-05-16 14:01:04,231 epoch 4 - iter 66/69 - loss 0.89615085 - samples/sec: 24.71 - lr: 0.100000
2021-05-16 14:01:05,782 ----------------------------------------------------------------------------------------------------
2021-05-16 14:01:05,782 EPOCH 4 done: loss 0.8711 - lr 0.1000000
2021-05-16 14:01:09,737 DEV : loss 0.29818013310432434 - score 0.9478
2021-05-16 14:01:09,783 BAD EPOCHS (no improvement): 1
2021-05-16 14:01:09,784 ----------------------------------------------------------------------------------------------------
2021-05-16 14:01:13,420 epoch 5 - iter 6/69 - loss 0.40866353 - samples/sec: 26.41 - lr: 0.100000
2021-05-16 14:01:17,099 epoch 5 - iter 12/69 - loss 0.53769840 - samples/sec: 26.10 - lr: 0.100000
2021-05-16 14:01:21,168 epoch 5 - iter 18/69 - loss 0.63420983 - samples/sec: 23.60 - lr: 0.100000
2021-05-16 14:01:26,213 epoch 5 - iter 24/69 - loss 0.66503478 - samples/sec: 19.03 - lr: 0.100000
2021-05-16 14:01:30,001 epoch 5 - iter 30/69 - loss 0.62596415 - samples/sec: 25.36 - lr: 0.100000
2021-05-16 14:01:33,655 epoch 5 - iter 36/69 - loss 0.62550530 - samples/sec: 26.28 - lr: 0.100000
2021-05-16 14:01:37,302 epoch 5 - iter 42/69 - loss 0.62475847 - samples/sec: 26.33 - lr: 0.100000
2021-05-16 14:01:41,347 epoch 5 - iter 48/69 - loss 0.63902445 - samples/sec: 23.74 - lr: 0.100000
2021-05-16 14:01:45,648 epoch 5 - iter 54/69 - loss 0.64483365 - samples/sec: 22.32 - lr: 0.100000
2021-05-16 14:01:49,375 epoch 5 - iter 60/69 - loss 0.67574169 - samples/sec: 25.77 - lr: 0.100000
2021-05-16 14:01:52,901 epoch 5 - iter 66/69 - loss 0.68672982 - samples/sec: 27.23 - lr: 0.100000
2021-05-16 14:01:54,402 ----------------------------------------------------------------------------------------------------
2021-05-16 14:01:54,403 EPOCH 5 done: loss 0.6743 - lr 0.1000000
2021-05-16 14:01:58,762 DEV : loss 0.33715417981147766 - score 0.9476
2021-05-16 14:01:58,807 BAD EPOCHS (no improvement): 2
2021-05-16 14:01:58,807 ----------------------------------------------------------------------------------------------------
2021-05-16 14:02:03,073 epoch 6 - iter 6/69 - loss 0.37625751 - samples/sec: 22.51 - lr: 0.100000
2021-05-16 14:02:07,354 epoch 6 - iter 12/69 - loss 0.72654867 - samples/sec: 22.43 - lr: 0.100000
2021-05-16 14:02:11,055 epoch 6 - iter 18/69 - loss 0.75573884 - samples/sec: 25.95 - lr: 0.100000
2021-05-16 14:02:15,575 epoch 6 - iter 24/69 - loss 0.81735670 - samples/sec: 21.24 - lr: 0.100000
2021-05-16 14:02:19,798 epoch 6 - iter 30/69 - loss 0.72756543 - samples/sec: 22.73 - lr: 0.100000
2021-05-16 14:02:23,624 epoch 6 - iter 36/69 - loss 0.70946145 - samples/sec: 25.10 - lr: 0.100000
2021-05-16 14:02:28,063 epoch 6 - iter 42/69 - loss 0.70876116 - samples/sec: 21.63 - lr: 0.100000
2021-05-16 14:02:32,545 epoch 6 - iter 48/69 - loss 0.78449209 - samples/sec: 21.42 - lr: 0.100000
2021-05-16 14:02:36,560 epoch 6 - iter 54/69 - loss 0.79339344 - samples/sec: 23.92 - lr: 0.100000
2021-05-16 14:02:40,054 epoch 6 - iter 60/69 - loss 0.79035713 - samples/sec: 27.48 - lr: 0.100000
2021-05-16 14:02:43,770 epoch 6 - iter 66/69 - loss 0.77760551 - samples/sec: 25.84 - lr: 0.100000
2021-05-16 14:02:45,291 ----------------------------------------------------------------------------------------------------
2021-05-16 14:02:45,292 EPOCH 6 done: loss 0.7971 - lr 0.1000000
2021-05-16 14:02:49,235 DEV : loss 0.887228786945343 - score 0.7059
2021-05-16 14:02:49,280 BAD EPOCHS (no improvement): 3
2021-05-16 14:02:49,280 ----------------------------------------------------------------------------------------------------
2021-05-16 14:02:53,086 epoch 7 - iter 6/69 - loss 0.61169159 - samples/sec: 25.23 - lr: 0.100000
2021-05-16 14:02:57,011 epoch 7 - iter 12/69 - loss 0.57590001 - samples/sec: 24.49 - lr: 0.100000
2021-05-16 14:03:00,791 epoch 7 - iter 18/69 - loss 0.58194356 - samples/sec: 25.43 - lr: 0.100000
2021-05-16 14:03:04,349 epoch 7 - iter 24/69 - loss 0.56056180 - samples/sec: 26.99 - lr: 0.100000
2021-05-16 14:03:08,471 epoch 7 - iter 30/69 - loss 0.58899720 - samples/sec: 23.29 - lr: 0.100000
2021-05-16 14:03:12,498 epoch 7 - iter 36/69 - loss 0.59433176 - samples/sec: 23.84 - lr: 0.100000
2021-05-16 14:03:16,492 epoch 7 - iter 42/69 - loss 0.60353181 - samples/sec: 24.04 - lr: 0.100000
2021-05-16 14:03:20,564 epoch 7 - iter 48/69 - loss 0.64667474 - samples/sec: 23.58 - lr: 0.100000
2021-05-16 14:03:24,285 epoch 7 - iter 54/69 - loss 0.64627558 - samples/sec: 25.82 - lr: 0.100000
2021-05-16 14:03:28,255 epoch 7 - iter 60/69 - loss 0.64094668 - samples/sec: 24.19 - lr: 0.100000
2021-05-16 14:03:32,638 epoch 7 - iter 66/69 - loss 0.64994062 - samples/sec: 21.91 - lr: 0.100000
2021-05-16 14:03:34,236 ----------------------------------------------------------------------------------------------------
2021-05-16 14:03:34,236 EPOCH 7 done: loss 0.6595 - lr 0.1000000
2021-05-16 14:03:38,290 DEV : loss 0.18600410223007202 - score 0.9719
2021-05-16 14:03:38,335 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:03:48,528 ----------------------------------------------------------------------------------------------------
2021-05-16 14:03:52,903 epoch 8 - iter 6/69 - loss 0.89769228 - samples/sec: 21.95 - lr: 0.100000
2021-05-16 14:03:57,215 epoch 8 - iter 12/69 - loss 0.71462810 - samples/sec: 22.27 - lr: 0.100000
2021-05-16 14:04:01,006 epoch 8 - iter 18/69 - loss 0.74661122 - samples/sec: 25.33 - lr: 0.100000
2021-05-16 14:04:05,512 epoch 8 - iter 24/69 - loss 0.70182000 - samples/sec: 21.31 - lr: 0.100000
2021-05-16 14:04:09,301 epoch 8 - iter 30/69 - loss 0.64351070 - samples/sec: 25.34 - lr: 0.100000
2021-05-16 14:04:13,380 epoch 8 - iter 36/69 - loss 0.64038398 - samples/sec: 23.54 - lr: 0.100000
2021-05-16 14:04:16,925 epoch 8 - iter 42/69 - loss 0.63171002 - samples/sec: 27.12 - lr: 0.100000
2021-05-16 14:04:20,680 epoch 8 - iter 48/69 - loss 0.59729244 - samples/sec: 25.57 - lr: 0.100000
2021-05-16 14:04:24,493 epoch 8 - iter 54/69 - loss 0.60983635 - samples/sec: 25.19 - lr: 0.100000
2021-05-16 14:04:28,624 epoch 8 - iter 60/69 - loss 0.60815244 - samples/sec: 23.24 - lr: 0.100000
2021-05-16 14:04:32,743 epoch 8 - iter 66/69 - loss 0.60941191 - samples/sec: 23.31 - lr: 0.100000
2021-05-16 14:04:34,241 ----------------------------------------------------------------------------------------------------
2021-05-16 14:04:34,241 EPOCH 8 done: loss 0.6584 - lr 0.1000000
2021-05-16 14:04:37,679 DEV : loss 0.2941337823867798 - score 0.9452
2021-05-16 14:04:37,724 BAD EPOCHS (no improvement): 1
2021-05-16 14:04:37,724 ----------------------------------------------------------------------------------------------------
2021-05-16 14:04:41,915 epoch 9 - iter 6/69 - loss 0.75160925 - samples/sec: 22.91 - lr: 0.100000
2021-05-16 14:04:45,559 epoch 9 - iter 12/69 - loss 0.57289832 - samples/sec: 26.36 - lr: 0.100000
2021-05-16 14:04:49,421 epoch 9 - iter 18/69 - loss 0.50648425 - samples/sec: 24.86 - lr: 0.100000
2021-05-16 14:04:54,112 epoch 9 - iter 24/69 - loss 0.52160543 - samples/sec: 20.47 - lr: 0.100000
2021-05-16 14:04:58,598 epoch 9 - iter 30/69 - loss 0.52635621 - samples/sec: 21.40 - lr: 0.100000
2021-05-16 14:05:02,835 epoch 9 - iter 36/69 - loss 0.56687383 - samples/sec: 22.66 - lr: 0.100000
2021-05-16 14:05:07,257 epoch 9 - iter 42/69 - loss 0.57233249 - samples/sec: 21.71 - lr: 0.100000
2021-05-16 14:05:11,638 epoch 9 - iter 48/69 - loss 0.59948163 - samples/sec: 21.92 - lr: 0.100000
2021-05-16 14:05:15,672 epoch 9 - iter 54/69 - loss 0.60882812 - samples/sec: 23.80 - lr: 0.100000
2021-05-16 14:05:19,767 epoch 9 - iter 60/69 - loss 0.60492493 - samples/sec: 23.45 - lr: 0.100000
2021-05-16 14:05:23,425 epoch 9 - iter 66/69 - loss 0.62003135 - samples/sec: 26.25 - lr: 0.100000
2021-05-16 14:05:24,729 ----------------------------------------------------------------------------------------------------
2021-05-16 14:05:24,729 EPOCH 9 done: loss 0.6053 - lr 0.1000000
2021-05-16 14:05:28,769 DEV : loss 0.34215641021728516 - score 0.9379
2021-05-16 14:05:28,814 BAD EPOCHS (no improvement): 2
2021-05-16 14:05:28,814 ----------------------------------------------------------------------------------------------------
2021-05-16 14:05:32,554 epoch 10 - iter 6/69 - loss 0.79509890 - samples/sec: 25.68 - lr: 0.100000
2021-05-16 14:05:36,567 epoch 10 - iter 12/69 - loss 0.74877946 - samples/sec: 23.93 - lr: 0.100000
2021-05-16 14:05:40,537 epoch 10 - iter 18/69 - loss 0.71155719 - samples/sec: 24.19 - lr: 0.100000
2021-05-16 14:05:44,326 epoch 10 - iter 24/69 - loss 0.65360157 - samples/sec: 25.34 - lr: 0.100000
2021-05-16 14:05:48,287 epoch 10 - iter 30/69 - loss 0.62404344 - samples/sec: 24.24 - lr: 0.100000
2021-05-16 14:05:52,045 epoch 10 - iter 36/69 - loss 0.59027405 - samples/sec: 25.55 - lr: 0.100000
2021-05-16 14:05:56,197 epoch 10 - iter 42/69 - loss 0.61210628 - samples/sec: 23.13 - lr: 0.100000
2021-05-16 14:06:00,116 epoch 10 - iter 48/69 - loss 0.61808087 - samples/sec: 24.50 - lr: 0.100000
2021-05-16 14:06:03,670 epoch 10 - iter 54/69 - loss 0.62114598 - samples/sec: 27.02 - lr: 0.100000
2021-05-16 14:06:07,919 epoch 10 - iter 60/69 - loss 0.59714409 - samples/sec: 22.60 - lr: 0.100000
2021-05-16 14:06:11,507 epoch 10 - iter 66/69 - loss 0.58957861 - samples/sec: 26.77 - lr: 0.100000
2021-05-16 14:06:12,884 ----------------------------------------------------------------------------------------------------
2021-05-16 14:06:12,885 EPOCH 10 done: loss 0.5785 - lr 0.1000000
2021-05-16 14:06:17,003 DEV : loss 0.2804217040538788 - score 0.95
2021-05-16 14:06:17,048 BAD EPOCHS (no improvement): 3
2021-05-16 14:06:17,049 ----------------------------------------------------------------------------------------------------
2021-05-16 14:06:20,584 epoch 11 - iter 6/69 - loss 0.63984074 - samples/sec: 27.16 - lr: 0.100000
2021-05-16 14:06:24,839 epoch 11 - iter 12/69 - loss 0.51026984 - samples/sec: 22.57 - lr: 0.100000
2021-05-16 14:06:28,786 epoch 11 - iter 18/69 - loss 0.51732761 - samples/sec: 24.33 - lr: 0.100000
2021-05-16 14:06:32,443 epoch 11 - iter 24/69 - loss 0.52343478 - samples/sec: 26.26 - lr: 0.100000
2021-05-16 14:06:36,159 epoch 11 - iter 30/69 - loss 0.56963103 - samples/sec: 25.84 - lr: 0.100000
2021-05-16 14:06:39,939 epoch 11 - iter 36/69 - loss 0.52389066 - samples/sec: 25.40 - lr: 0.100000
2021-05-16 14:06:43,770 epoch 11 - iter 42/69 - loss 0.55650984 - samples/sec: 25.07 - lr: 0.100000
2021-05-16 14:06:47,537 epoch 11 - iter 48/69 - loss 0.57568109 - samples/sec: 25.48 - lr: 0.100000
2021-05-16 14:06:51,352 epoch 11 - iter 54/69 - loss 0.54675005 - samples/sec: 25.17 - lr: 0.100000
2021-05-16 14:06:55,507 epoch 11 - iter 60/69 - loss 0.53721344 - samples/sec: 23.12 - lr: 0.100000
2021-05-16 14:06:59,816 epoch 11 - iter 66/69 - loss 0.52144869 - samples/sec: 22.28 - lr: 0.100000
2021-05-16 14:07:01,225 ----------------------------------------------------------------------------------------------------
2021-05-16 14:07:01,226 EPOCH 11 done: loss 0.5118 - lr 0.1000000
2021-05-16 14:07:05,310 DEV : loss 0.19005659222602844 - score 0.9761
2021-05-16 14:07:05,356 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:07:16,036 ----------------------------------------------------------------------------------------------------
2021-05-16 14:07:19,998 epoch 12 - iter 6/69 - loss 0.39122608 - samples/sec: 24.24 - lr: 0.100000
2021-05-16 14:07:23,972 epoch 12 - iter 12/69 - loss 0.44579301 - samples/sec: 24.16 - lr: 0.100000
2021-05-16 14:07:28,104 epoch 12 - iter 18/69 - loss 0.41520784 - samples/sec: 23.24 - lr: 0.100000
2021-05-16 14:07:32,201 epoch 12 - iter 24/69 - loss 0.46103208 - samples/sec: 23.43 - lr: 0.100000
2021-05-16 14:07:35,903 epoch 12 - iter 30/69 - loss 0.45462569 - samples/sec: 25.93 - lr: 0.100000
2021-05-16 14:07:40,077 epoch 12 - iter 36/69 - loss 0.45680178 - samples/sec: 23.01 - lr: 0.100000
2021-05-16 14:07:44,177 epoch 12 - iter 42/69 - loss 0.47779497 - samples/sec: 23.42 - lr: 0.100000
2021-05-16 14:07:48,261 epoch 12 - iter 48/69 - loss 0.53591850 - samples/sec: 23.51 - lr: 0.100000
2021-05-16 14:07:52,066 epoch 12 - iter 54/69 - loss 0.55808764 - samples/sec: 25.23 - lr: 0.100000
2021-05-16 14:07:55,592 epoch 12 - iter 60/69 - loss 0.58371848 - samples/sec: 27.23 - lr: 0.100000
2021-05-16 14:07:59,722 epoch 12 - iter 66/69 - loss 0.56929381 - samples/sec: 23.25 - lr: 0.100000
2021-05-16 14:08:01,222 ----------------------------------------------------------------------------------------------------
2021-05-16 14:08:01,222 EPOCH 12 done: loss 0.5788 - lr 0.1000000
2021-05-16 14:08:04,862 DEV : loss 0.18952548503875732 - score 0.9644
2021-05-16 14:08:04,908 BAD EPOCHS (no improvement): 1
2021-05-16 14:08:04,908 ----------------------------------------------------------------------------------------------------
2021-05-16 14:08:08,963 epoch 13 - iter 6/69 - loss 0.58610529 - samples/sec: 23.68 - lr: 0.100000
2021-05-16 14:08:12,523 epoch 13 - iter 12/69 - loss 0.46547919 - samples/sec: 26.97 - lr: 0.100000
2021-05-16 14:08:16,325 epoch 13 - iter 18/69 - loss 0.52065333 - samples/sec: 25.25 - lr: 0.100000
2021-05-16 14:08:20,503 epoch 13 - iter 24/69 - loss 0.49748839 - samples/sec: 22.99 - lr: 0.100000
2021-05-16 14:08:24,741 epoch 13 - iter 30/69 - loss 0.55660440 - samples/sec: 22.65 - lr: 0.100000
2021-05-16 14:08:28,467 epoch 13 - iter 36/69 - loss 0.52043856 - samples/sec: 25.77 - lr: 0.100000
2021-05-16 14:08:32,409 epoch 13 - iter 42/69 - loss 0.50757619 - samples/sec: 24.36 - lr: 0.100000
2021-05-16 14:08:36,264 epoch 13 - iter 48/69 - loss 0.51877035 - samples/sec: 24.91 - lr: 0.100000
2021-05-16 14:08:40,029 epoch 13 - iter 54/69 - loss 0.49035962 - samples/sec: 25.50 - lr: 0.100000
2021-05-16 14:08:43,687 epoch 13 - iter 60/69 - loss 0.51167195 - samples/sec: 26.25 - lr: 0.100000
2021-05-16 14:08:47,783 epoch 13 - iter 66/69 - loss 0.52513921 - samples/sec: 23.44 - lr: 0.100000
2021-05-16 14:08:49,216 ----------------------------------------------------------------------------------------------------
2021-05-16 14:08:49,216 EPOCH 13 done: loss 0.5094 - lr 0.1000000
2021-05-16 14:08:53,120 DEV : loss 0.21053332090377808 - score 0.955
2021-05-16 14:08:53,166 BAD EPOCHS (no improvement): 2
2021-05-16 14:08:53,166 ----------------------------------------------------------------------------------------------------
2021-05-16 14:08:56,973 epoch 14 - iter 6/69 - loss 0.44636794 - samples/sec: 25.22 - lr: 0.100000
2021-05-16 14:09:00,664 epoch 14 - iter 12/69 - loss 0.51265122 - samples/sec: 26.02 - lr: 0.100000
2021-05-16 14:09:05,098 epoch 14 - iter 18/69 - loss 0.54201588 - samples/sec: 21.65 - lr: 0.100000
2021-05-16 14:09:09,266 epoch 14 - iter 24/69 - loss 0.53553077 - samples/sec: 23.07 - lr: 0.100000
2021-05-16 14:09:13,452 epoch 14 - iter 30/69 - loss 0.55602639 - samples/sec: 22.94 - lr: 0.100000
2021-05-16 14:09:17,510 epoch 14 - iter 36/69 - loss 0.53470392 - samples/sec: 23.66 - lr: 0.100000
2021-05-16 14:09:21,600 epoch 14 - iter 42/69 - loss 0.54352704 - samples/sec: 23.48 - lr: 0.100000
2021-05-16 14:09:26,145 epoch 14 - iter 48/69 - loss 0.60259125 - samples/sec: 21.12 - lr: 0.100000
2021-05-16 14:09:29,840 epoch 14 - iter 54/69 - loss 0.57514392 - samples/sec: 25.98 - lr: 0.100000
2021-05-16 14:09:34,117 epoch 14 - iter 60/69 - loss 0.55881319 - samples/sec: 22.45 - lr: 0.100000
2021-05-16 14:09:37,966 epoch 14 - iter 66/69 - loss 0.54908980 - samples/sec: 24.95 - lr: 0.100000
2021-05-16 14:09:39,451 ----------------------------------------------------------------------------------------------------
2021-05-16 14:09:39,451 EPOCH 14 done: loss 0.5493 - lr 0.1000000
2021-05-16 14:09:43,795 DEV : loss 0.19936347007751465 - score 0.9713
2021-05-16 14:09:43,840 BAD EPOCHS (no improvement): 3
2021-05-16 14:09:43,840 ----------------------------------------------------------------------------------------------------
2021-05-16 14:09:47,814 epoch 15 - iter 6/69 - loss 0.50129124 - samples/sec: 24.16 - lr: 0.100000
2021-05-16 14:09:51,606 epoch 15 - iter 12/69 - loss 0.69372886 - samples/sec: 25.32 - lr: 0.100000
2021-05-16 14:09:55,353 epoch 15 - iter 18/69 - loss 0.74286939 - samples/sec: 25.62 - lr: 0.100000
2021-05-16 14:09:59,085 epoch 15 - iter 24/69 - loss 0.64673662 - samples/sec: 25.73 - lr: 0.100000
2021-05-16 14:10:02,902 epoch 15 - iter 30/69 - loss 0.61923411 - samples/sec: 25.16 - lr: 0.100000
2021-05-16 14:10:06,377 epoch 15 - iter 36/69 - loss 0.58443579 - samples/sec: 27.63 - lr: 0.100000
2021-05-16 14:10:10,541 epoch 15 - iter 42/69 - loss 0.56887009 - samples/sec: 23.06 - lr: 0.100000
2021-05-16 14:10:14,515 epoch 15 - iter 48/69 - loss 0.55449105 - samples/sec: 24.16 - lr: 0.100000
2021-05-16 14:10:18,881 epoch 15 - iter 54/69 - loss 0.53426932 - samples/sec: 21.99 - lr: 0.100000
2021-05-16 14:10:22,936 epoch 15 - iter 60/69 - loss 0.52058912 - samples/sec: 23.68 - lr: 0.100000
2021-05-16 14:10:27,122 epoch 15 - iter 66/69 - loss 0.52456833 - samples/sec: 22.94 - lr: 0.100000
2021-05-16 14:10:28,571 ----------------------------------------------------------------------------------------------------
2021-05-16 14:10:28,572 EPOCH 15 done: loss 0.5145 - lr 0.1000000
2021-05-16 14:10:32,365 DEV : loss 0.2683568000793457 - score 0.9618
Epoch    15: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 14:10:32,410 BAD EPOCHS (no improvement): 4
2021-05-16 14:10:32,411 ----------------------------------------------------------------------------------------------------
2021-05-16 14:10:36,623 epoch 16 - iter 6/69 - loss 0.24785386 - samples/sec: 22.79 - lr: 0.050000
2021-05-16 14:10:40,134 epoch 16 - iter 12/69 - loss 0.27714192 - samples/sec: 27.35 - lr: 0.050000
2021-05-16 14:10:43,637 epoch 16 - iter 18/69 - loss 0.32037231 - samples/sec: 27.41 - lr: 0.050000
2021-05-16 14:10:47,354 epoch 16 - iter 24/69 - loss 0.38056401 - samples/sec: 25.84 - lr: 0.050000
2021-05-16 14:10:50,897 epoch 16 - iter 30/69 - loss 0.36924405 - samples/sec: 27.10 - lr: 0.050000
2021-05-16 14:10:54,628 epoch 16 - iter 36/69 - loss 0.38911980 - samples/sec: 25.73 - lr: 0.050000
2021-05-16 14:10:58,160 epoch 16 - iter 42/69 - loss 0.41691259 - samples/sec: 27.19 - lr: 0.050000
2021-05-16 14:11:02,766 epoch 16 - iter 48/69 - loss 0.40038709 - samples/sec: 20.85 - lr: 0.050000
2021-05-16 14:11:07,064 epoch 16 - iter 54/69 - loss 0.43230191 - samples/sec: 22.34 - lr: 0.050000
2021-05-16 14:11:10,859 epoch 16 - iter 60/69 - loss 0.42425592 - samples/sec: 25.30 - lr: 0.050000
2021-05-16 14:11:14,771 epoch 16 - iter 66/69 - loss 0.42838917 - samples/sec: 24.55 - lr: 0.050000
2021-05-16 14:11:16,453 ----------------------------------------------------------------------------------------------------
2021-05-16 14:11:16,453 EPOCH 16 done: loss 0.4239 - lr 0.0500000
2021-05-16 14:11:20,058 DEV : loss 0.4130524694919586 - score 0.9052
2021-05-16 14:11:20,103 BAD EPOCHS (no improvement): 1
2021-05-16 14:11:20,104 ----------------------------------------------------------------------------------------------------
2021-05-16 14:11:23,635 epoch 17 - iter 6/69 - loss 0.25382261 - samples/sec: 27.20 - lr: 0.050000
2021-05-16 14:11:27,481 epoch 17 - iter 12/69 - loss 0.36888191 - samples/sec: 24.96 - lr: 0.050000
2021-05-16 14:11:32,085 epoch 17 - iter 18/69 - loss 0.35821593 - samples/sec: 20.86 - lr: 0.050000
2021-05-16 14:11:36,435 epoch 17 - iter 24/69 - loss 0.34184394 - samples/sec: 22.07 - lr: 0.050000
2021-05-16 14:11:40,927 epoch 17 - iter 30/69 - loss 0.32607832 - samples/sec: 21.38 - lr: 0.050000
2021-05-16 14:11:44,554 epoch 17 - iter 36/69 - loss 0.35171362 - samples/sec: 26.47 - lr: 0.050000
2021-05-16 14:11:48,089 epoch 17 - iter 42/69 - loss 0.38323456 - samples/sec: 27.16 - lr: 0.050000
2021-05-16 14:11:52,761 epoch 17 - iter 48/69 - loss 0.37490784 - samples/sec: 20.55 - lr: 0.050000
2021-05-16 14:11:56,370 epoch 17 - iter 54/69 - loss 0.37799748 - samples/sec: 26.60 - lr: 0.050000
2021-05-16 14:11:59,685 epoch 17 - iter 60/69 - loss 0.38702752 - samples/sec: 28.97 - lr: 0.050000
2021-05-16 14:12:03,705 epoch 17 - iter 66/69 - loss 0.37358409 - samples/sec: 23.88 - lr: 0.050000
2021-05-16 14:12:05,158 ----------------------------------------------------------------------------------------------------
2021-05-16 14:12:05,158 EPOCH 17 done: loss 0.3752 - lr 0.0500000
2021-05-16 14:12:09,296 DEV : loss 0.1634334772825241 - score 0.9713
2021-05-16 14:12:09,390 BAD EPOCHS (no improvement): 2
2021-05-16 14:12:09,395 ----------------------------------------------------------------------------------------------------
2021-05-16 14:12:13,457 epoch 18 - iter 6/69 - loss 0.37531686 - samples/sec: 23.64 - lr: 0.050000
2021-05-16 14:12:17,914 epoch 18 - iter 12/69 - loss 0.31927538 - samples/sec: 21.54 - lr: 0.050000
2021-05-16 14:12:22,231 epoch 18 - iter 18/69 - loss 0.33405503 - samples/sec: 22.24 - lr: 0.050000
2021-05-16 14:12:26,466 epoch 18 - iter 24/69 - loss 0.31222820 - samples/sec: 22.67 - lr: 0.050000
2021-05-16 14:12:30,892 epoch 18 - iter 30/69 - loss 0.32619938 - samples/sec: 21.70 - lr: 0.050000
2021-05-16 14:12:35,321 epoch 18 - iter 36/69 - loss 0.35444358 - samples/sec: 21.68 - lr: 0.050000
2021-05-16 14:12:39,152 epoch 18 - iter 42/69 - loss 0.36327726 - samples/sec: 25.06 - lr: 0.050000
2021-05-16 14:12:42,850 epoch 18 - iter 48/69 - loss 0.34500040 - samples/sec: 25.96 - lr: 0.050000
2021-05-16 14:12:47,004 epoch 18 - iter 54/69 - loss 0.36865140 - samples/sec: 23.12 - lr: 0.050000
2021-05-16 14:12:52,012 epoch 18 - iter 60/69 - loss 0.37829378 - samples/sec: 19.17 - lr: 0.050000
2021-05-16 14:12:56,595 epoch 18 - iter 66/69 - loss 0.37536644 - samples/sec: 20.95 - lr: 0.050000
2021-05-16 14:12:58,268 ----------------------------------------------------------------------------------------------------
2021-05-16 14:12:58,268 EPOCH 18 done: loss 0.3682 - lr 0.0500000
2021-05-16 14:13:02,223 DEV : loss 0.1659364253282547 - score 0.9736
2021-05-16 14:13:02,268 BAD EPOCHS (no improvement): 3
2021-05-16 14:13:02,269 ----------------------------------------------------------------------------------------------------
2021-05-16 14:13:06,171 epoch 19 - iter 6/69 - loss 0.15738519 - samples/sec: 24.61 - lr: 0.050000
2021-05-16 14:13:10,367 epoch 19 - iter 12/69 - loss 0.21831119 - samples/sec: 22.88 - lr: 0.050000
2021-05-16 14:13:13,954 epoch 19 - iter 18/69 - loss 0.24483909 - samples/sec: 26.77 - lr: 0.050000
2021-05-16 14:13:17,735 epoch 19 - iter 24/69 - loss 0.25608109 - samples/sec: 25.39 - lr: 0.050000
2021-05-16 14:13:22,344 epoch 19 - iter 30/69 - loss 0.27793378 - samples/sec: 20.84 - lr: 0.050000
2021-05-16 14:13:26,191 epoch 19 - iter 36/69 - loss 0.30161713 - samples/sec: 24.96 - lr: 0.050000
2021-05-16 14:13:29,947 epoch 19 - iter 42/69 - loss 0.32409187 - samples/sec: 25.56 - lr: 0.050000
2021-05-16 14:13:34,077 epoch 19 - iter 48/69 - loss 0.32393430 - samples/sec: 23.25 - lr: 0.050000
2021-05-16 14:13:38,254 epoch 19 - iter 54/69 - loss 0.32053143 - samples/sec: 22.99 - lr: 0.050000
2021-05-16 14:13:42,101 epoch 19 - iter 60/69 - loss 0.32442184 - samples/sec: 24.99 - lr: 0.050000
2021-05-16 14:13:47,282 epoch 19 - iter 66/69 - loss 0.34860855 - samples/sec: 18.53 - lr: 0.050000
2021-05-16 14:13:48,701 ----------------------------------------------------------------------------------------------------
2021-05-16 14:13:48,701 EPOCH 19 done: loss 0.3381 - lr 0.0500000
2021-05-16 14:13:53,278 DEV : loss 0.18455356359481812 - score 0.9698
Epoch    19: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 14:13:53,330 BAD EPOCHS (no improvement): 4
2021-05-16 14:13:53,331 ----------------------------------------------------------------------------------------------------
2021-05-16 14:13:57,118 epoch 20 - iter 6/69 - loss 0.25097465 - samples/sec: 25.36 - lr: 0.025000
2021-05-16 14:14:01,469 epoch 20 - iter 12/69 - loss 0.27243002 - samples/sec: 22.07 - lr: 0.025000
2021-05-16 14:14:05,615 epoch 20 - iter 18/69 - loss 0.26224331 - samples/sec: 23.16 - lr: 0.025000
2021-05-16 14:14:09,410 epoch 20 - iter 24/69 - loss 0.25653955 - samples/sec: 25.30 - lr: 0.025000
2021-05-16 14:14:13,450 epoch 20 - iter 30/69 - loss 0.29422163 - samples/sec: 23.77 - lr: 0.025000
2021-05-16 14:14:17,468 epoch 20 - iter 36/69 - loss 0.34722944 - samples/sec: 23.89 - lr: 0.025000
2021-05-16 14:14:21,211 epoch 20 - iter 42/69 - loss 0.33295328 - samples/sec: 25.65 - lr: 0.025000
2021-05-16 14:14:24,982 epoch 20 - iter 48/69 - loss 0.34758034 - samples/sec: 25.47 - lr: 0.025000
2021-05-16 14:14:28,555 epoch 20 - iter 54/69 - loss 0.35132688 - samples/sec: 26.87 - lr: 0.025000
2021-05-16 14:14:32,118 epoch 20 - iter 60/69 - loss 0.34309955 - samples/sec: 26.95 - lr: 0.025000
2021-05-16 14:14:36,169 epoch 20 - iter 66/69 - loss 0.33494620 - samples/sec: 23.70 - lr: 0.025000
2021-05-16 14:14:37,709 ----------------------------------------------------------------------------------------------------
2021-05-16 14:14:37,709 EPOCH 20 done: loss 0.3239 - lr 0.0250000
2021-05-16 14:14:41,701 DEV : loss 0.1707383245229721 - score 0.9714
2021-05-16 14:14:41,757 BAD EPOCHS (no improvement): 1
2021-05-16 14:14:41,758 ----------------------------------------------------------------------------------------------------
2021-05-16 14:14:45,949 epoch 21 - iter 6/69 - loss 0.39490084 - samples/sec: 22.91 - lr: 0.025000
2021-05-16 14:14:49,662 epoch 21 - iter 12/69 - loss 0.26998009 - samples/sec: 25.86 - lr: 0.025000
2021-05-16 14:14:53,137 epoch 21 - iter 18/69 - loss 0.26719522 - samples/sec: 27.63 - lr: 0.025000
2021-05-16 14:14:57,046 epoch 21 - iter 24/69 - loss 0.29018747 - samples/sec: 24.56 - lr: 0.025000
2021-05-16 14:15:01,131 epoch 21 - iter 30/69 - loss 0.27328513 - samples/sec: 23.51 - lr: 0.025000
2021-05-16 14:15:05,315 epoch 21 - iter 36/69 - loss 0.29044681 - samples/sec: 22.94 - lr: 0.025000
2021-05-16 14:15:08,981 epoch 21 - iter 42/69 - loss 0.30052435 - samples/sec: 26.20 - lr: 0.025000
2021-05-16 14:15:12,424 epoch 21 - iter 48/69 - loss 0.30327877 - samples/sec: 27.89 - lr: 0.025000
2021-05-16 14:15:16,301 epoch 21 - iter 54/69 - loss 0.29179803 - samples/sec: 24.77 - lr: 0.025000
2021-05-16 14:15:20,244 epoch 21 - iter 60/69 - loss 0.29800977 - samples/sec: 24.35 - lr: 0.025000
2021-05-16 14:15:23,966 epoch 21 - iter 66/69 - loss 0.28635056 - samples/sec: 25.80 - lr: 0.025000
2021-05-16 14:15:25,382 ----------------------------------------------------------------------------------------------------
2021-05-16 14:15:25,383 EPOCH 21 done: loss 0.2936 - lr 0.0250000
2021-05-16 14:15:29,020 DEV : loss 0.16216744482517242 - score 0.9737
2021-05-16 14:15:29,069 BAD EPOCHS (no improvement): 2
2021-05-16 14:15:29,070 ----------------------------------------------------------------------------------------------------
2021-05-16 14:15:33,252 epoch 22 - iter 6/69 - loss 0.23324620 - samples/sec: 22.96 - lr: 0.025000
2021-05-16 14:15:37,088 epoch 22 - iter 12/69 - loss 0.29184526 - samples/sec: 25.03 - lr: 0.025000
2021-05-16 14:15:41,621 epoch 22 - iter 18/69 - loss 0.27608853 - samples/sec: 21.18 - lr: 0.025000
2021-05-16 14:15:45,690 epoch 22 - iter 24/69 - loss 0.30874322 - samples/sec: 23.60 - lr: 0.025000
2021-05-16 14:15:49,406 epoch 22 - iter 30/69 - loss 0.29017017 - samples/sec: 25.84 - lr: 0.025000
2021-05-16 14:15:53,086 epoch 22 - iter 36/69 - loss 0.30272002 - samples/sec: 26.09 - lr: 0.025000
2021-05-16 14:15:56,817 epoch 22 - iter 42/69 - loss 0.29357895 - samples/sec: 25.73 - lr: 0.025000
2021-05-16 14:16:00,797 epoch 22 - iter 48/69 - loss 0.30060070 - samples/sec: 24.12 - lr: 0.025000
2021-05-16 14:16:04,445 epoch 22 - iter 54/69 - loss 0.29393482 - samples/sec: 26.32 - lr: 0.025000
2021-05-16 14:16:08,615 epoch 22 - iter 60/69 - loss 0.28971870 - samples/sec: 23.03 - lr: 0.025000
2021-05-16 14:16:12,388 epoch 22 - iter 66/69 - loss 0.28642078 - samples/sec: 25.45 - lr: 0.025000
2021-05-16 14:16:13,847 ----------------------------------------------------------------------------------------------------
2021-05-16 14:16:13,847 EPOCH 22 done: loss 0.3089 - lr 0.0250000
2021-05-16 14:16:17,933 DEV : loss 0.15887804329395294 - score 0.9739
2021-05-16 14:16:17,978 BAD EPOCHS (no improvement): 3
2021-05-16 14:16:17,979 ----------------------------------------------------------------------------------------------------
2021-05-16 14:16:22,029 epoch 23 - iter 6/69 - loss 0.22235836 - samples/sec: 23.71 - lr: 0.025000
2021-05-16 14:16:26,013 epoch 23 - iter 12/69 - loss 0.23769824 - samples/sec: 24.10 - lr: 0.025000
2021-05-16 14:16:29,941 epoch 23 - iter 18/69 - loss 0.24706310 - samples/sec: 24.45 - lr: 0.025000
2021-05-16 14:16:33,715 epoch 23 - iter 24/69 - loss 0.24494512 - samples/sec: 25.44 - lr: 0.025000
2021-05-16 14:16:38,064 epoch 23 - iter 30/69 - loss 0.23178841 - samples/sec: 22.08 - lr: 0.025000
2021-05-16 14:16:42,476 epoch 23 - iter 36/69 - loss 0.24849185 - samples/sec: 21.77 - lr: 0.025000
2021-05-16 14:16:46,195 epoch 23 - iter 42/69 - loss 0.26727264 - samples/sec: 25.81 - lr: 0.025000
2021-05-16 14:16:50,458 epoch 23 - iter 48/69 - loss 0.26718906 - samples/sec: 22.53 - lr: 0.025000
2021-05-16 14:16:54,231 epoch 23 - iter 54/69 - loss 0.29019284 - samples/sec: 25.45 - lr: 0.025000
2021-05-16 14:16:57,704 epoch 23 - iter 60/69 - loss 0.30077817 - samples/sec: 27.65 - lr: 0.025000
2021-05-16 14:17:01,461 epoch 23 - iter 66/69 - loss 0.29040062 - samples/sec: 25.55 - lr: 0.025000
2021-05-16 14:17:03,107 ----------------------------------------------------------------------------------------------------
2021-05-16 14:17:03,107 EPOCH 23 done: loss 0.2907 - lr 0.0250000
2021-05-16 14:17:07,310 DEV : loss 0.21342159807682037 - score 0.9658
Epoch    23: reducing learning rate of group 0 to 1.2500e-02.
2021-05-16 14:17:07,355 BAD EPOCHS (no improvement): 4
2021-05-16 14:17:07,356 ----------------------------------------------------------------------------------------------------
2021-05-16 14:17:11,738 epoch 24 - iter 6/69 - loss 0.25076774 - samples/sec: 21.91 - lr: 0.012500
2021-05-16 14:17:15,589 epoch 24 - iter 12/69 - loss 0.24643456 - samples/sec: 24.94 - lr: 0.012500
2021-05-16 14:17:18,832 epoch 24 - iter 18/69 - loss 0.24376552 - samples/sec: 29.61 - lr: 0.012500
2021-05-16 14:17:22,234 epoch 24 - iter 24/69 - loss 0.22971186 - samples/sec: 28.22 - lr: 0.012500
2021-05-16 14:17:25,891 epoch 24 - iter 30/69 - loss 0.21912435 - samples/sec: 26.26 - lr: 0.012500
2021-05-16 14:17:29,631 epoch 24 - iter 36/69 - loss 0.21322126 - samples/sec: 25.67 - lr: 0.012500
2021-05-16 14:17:33,437 epoch 24 - iter 42/69 - loss 0.20670847 - samples/sec: 25.23 - lr: 0.012500
2021-05-16 14:17:37,396 epoch 24 - iter 48/69 - loss 0.22446915 - samples/sec: 24.25 - lr: 0.012500
2021-05-16 14:17:41,063 epoch 24 - iter 54/69 - loss 0.23252926 - samples/sec: 26.19 - lr: 0.012500
2021-05-16 14:17:44,953 epoch 24 - iter 60/69 - loss 0.25536411 - samples/sec: 24.68 - lr: 0.012500
2021-05-16 14:17:48,153 epoch 24 - iter 66/69 - loss 0.25831192 - samples/sec: 30.01 - lr: 0.012500
2021-05-16 14:17:49,614 ----------------------------------------------------------------------------------------------------
2021-05-16 14:17:49,614 EPOCH 24 done: loss 0.2596 - lr 0.0125000
2021-05-16 14:17:54,595 DEV : loss 0.16683249175548553 - score 0.9737
2021-05-16 14:17:54,641 BAD EPOCHS (no improvement): 1
2021-05-16 14:17:54,641 ----------------------------------------------------------------------------------------------------
2021-05-16 14:17:59,221 epoch 25 - iter 6/69 - loss 0.27577347 - samples/sec: 20.97 - lr: 0.012500
2021-05-16 14:18:03,242 epoch 25 - iter 12/69 - loss 0.25064145 - samples/sec: 23.89 - lr: 0.012500
2021-05-16 14:18:07,256 epoch 25 - iter 18/69 - loss 0.28327302 - samples/sec: 23.93 - lr: 0.012500
2021-05-16 14:18:10,892 epoch 25 - iter 24/69 - loss 0.25815079 - samples/sec: 26.40 - lr: 0.012500
2021-05-16 14:18:14,714 epoch 25 - iter 30/69 - loss 0.25289564 - samples/sec: 25.13 - lr: 0.012500
2021-05-16 14:18:18,664 epoch 25 - iter 36/69 - loss 0.27924306 - samples/sec: 24.31 - lr: 0.012500
2021-05-16 14:18:22,789 epoch 25 - iter 42/69 - loss 0.29803931 - samples/sec: 23.28 - lr: 0.012500
2021-05-16 14:18:27,013 epoch 25 - iter 48/69 - loss 0.27817115 - samples/sec: 22.73 - lr: 0.012500
2021-05-16 14:18:30,836 epoch 25 - iter 54/69 - loss 0.26926995 - samples/sec: 25.11 - lr: 0.012500
2021-05-16 14:18:34,290 epoch 25 - iter 60/69 - loss 0.25832314 - samples/sec: 27.80 - lr: 0.012500
2021-05-16 14:18:38,593 epoch 25 - iter 66/69 - loss 0.26653620 - samples/sec: 22.32 - lr: 0.012500
2021-05-16 14:18:40,276 ----------------------------------------------------------------------------------------------------
2021-05-16 14:18:40,279 EPOCH 25 done: loss 0.2577 - lr 0.0125000
2021-05-16 14:18:44,206 DEV : loss 0.1720701903104782 - score 0.9716
2021-05-16 14:18:44,288 BAD EPOCHS (no improvement): 2
2021-05-16 14:18:44,289 ----------------------------------------------------------------------------------------------------
2021-05-16 14:18:48,107 epoch 26 - iter 6/69 - loss 0.16293695 - samples/sec: 25.15 - lr: 0.012500
2021-05-16 14:18:52,124 epoch 26 - iter 12/69 - loss 0.24063258 - samples/sec: 23.90 - lr: 0.012500
2021-05-16 14:18:56,037 epoch 26 - iter 18/69 - loss 0.27002329 - samples/sec: 24.55 - lr: 0.012500
2021-05-16 14:19:00,438 epoch 26 - iter 24/69 - loss 0.28825891 - samples/sec: 21.82 - lr: 0.012500
2021-05-16 14:19:04,679 epoch 26 - iter 30/69 - loss 0.29604944 - samples/sec: 22.64 - lr: 0.012500
2021-05-16 14:19:08,116 epoch 26 - iter 36/69 - loss 0.29114287 - samples/sec: 27.96 - lr: 0.012500
2021-05-16 14:19:12,311 epoch 26 - iter 42/69 - loss 0.27605280 - samples/sec: 22.89 - lr: 0.012500
2021-05-16 14:19:16,884 epoch 26 - iter 48/69 - loss 0.26282942 - samples/sec: 21.00 - lr: 0.012500
2021-05-16 14:19:20,618 epoch 26 - iter 54/69 - loss 0.27202399 - samples/sec: 25.72 - lr: 0.012500
2021-05-16 14:19:24,458 epoch 26 - iter 60/69 - loss 0.25816806 - samples/sec: 25.01 - lr: 0.012500
2021-05-16 14:19:27,638 epoch 26 - iter 66/69 - loss 0.26167130 - samples/sec: 30.20 - lr: 0.012500
2021-05-16 14:19:29,069 ----------------------------------------------------------------------------------------------------
2021-05-16 14:19:29,070 EPOCH 26 done: loss 0.2632 - lr 0.0125000
2021-05-16 14:19:32,825 DEV : loss 0.16504190862178802 - score 0.9697
2021-05-16 14:19:32,870 BAD EPOCHS (no improvement): 3
2021-05-16 14:19:32,870 ----------------------------------------------------------------------------------------------------
2021-05-16 14:19:36,758 epoch 27 - iter 6/69 - loss 0.21485504 - samples/sec: 24.70 - lr: 0.012500
2021-05-16 14:19:40,498 epoch 27 - iter 12/69 - loss 0.25761864 - samples/sec: 25.68 - lr: 0.012500
2021-05-16 14:19:44,064 epoch 27 - iter 18/69 - loss 0.29287413 - samples/sec: 26.93 - lr: 0.012500
2021-05-16 14:19:48,284 epoch 27 - iter 24/69 - loss 0.26056937 - samples/sec: 22.76 - lr: 0.012500
2021-05-16 14:19:52,196 epoch 27 - iter 30/69 - loss 0.28476777 - samples/sec: 24.55 - lr: 0.012500
2021-05-16 14:19:55,995 epoch 27 - iter 36/69 - loss 0.27066378 - samples/sec: 25.27 - lr: 0.012500
2021-05-16 14:20:00,080 epoch 27 - iter 42/69 - loss 0.24112690 - samples/sec: 23.50 - lr: 0.012500
2021-05-16 14:20:04,012 epoch 27 - iter 48/69 - loss 0.25031800 - samples/sec: 24.44 - lr: 0.012500
2021-05-16 14:20:07,851 epoch 27 - iter 54/69 - loss 0.24814453 - samples/sec: 25.01 - lr: 0.012500
2021-05-16 14:20:12,000 epoch 27 - iter 60/69 - loss 0.24894871 - samples/sec: 23.14 - lr: 0.012500
2021-05-16 14:20:15,900 epoch 27 - iter 66/69 - loss 0.25321927 - samples/sec: 24.62 - lr: 0.012500
2021-05-16 14:20:17,505 ----------------------------------------------------------------------------------------------------
2021-05-16 14:20:17,505 EPOCH 27 done: loss 0.2491 - lr 0.0125000
2021-05-16 14:20:21,662 DEV : loss 0.15518687665462494 - score 0.9738
Epoch    27: reducing learning rate of group 0 to 6.2500e-03.
2021-05-16 14:20:21,707 BAD EPOCHS (no improvement): 4
2021-05-16 14:20:21,707 ----------------------------------------------------------------------------------------------------
2021-05-16 14:20:25,885 epoch 28 - iter 6/69 - loss 0.11716119 - samples/sec: 22.99 - lr: 0.006250
2021-05-16 14:20:29,665 epoch 28 - iter 12/69 - loss 0.17728254 - samples/sec: 25.40 - lr: 0.006250
2021-05-16 14:20:33,341 epoch 28 - iter 18/69 - loss 0.20970692 - samples/sec: 26.12 - lr: 0.006250
2021-05-16 14:20:37,151 epoch 28 - iter 24/69 - loss 0.21910685 - samples/sec: 25.20 - lr: 0.006250
2021-05-16 14:20:41,361 epoch 28 - iter 30/69 - loss 0.23100846 - samples/sec: 22.81 - lr: 0.006250
2021-05-16 14:20:45,444 epoch 28 - iter 36/69 - loss 0.23328178 - samples/sec: 23.52 - lr: 0.006250
2021-05-16 14:20:49,472 epoch 28 - iter 42/69 - loss 0.23580924 - samples/sec: 23.83 - lr: 0.006250
2021-05-16 14:20:53,066 epoch 28 - iter 48/69 - loss 0.24085068 - samples/sec: 26.72 - lr: 0.006250
2021-05-16 14:20:57,246 epoch 28 - iter 54/69 - loss 0.24494659 - samples/sec: 22.97 - lr: 0.006250
2021-05-16 14:21:00,961 epoch 28 - iter 60/69 - loss 0.25178401 - samples/sec: 25.85 - lr: 0.006250
2021-05-16 14:21:05,572 epoch 28 - iter 66/69 - loss 0.23881854 - samples/sec: 20.82 - lr: 0.006250
2021-05-16 14:21:07,363 ----------------------------------------------------------------------------------------------------
2021-05-16 14:21:07,364 EPOCH 28 done: loss 0.2375 - lr 0.0062500
2021-05-16 14:21:12,069 DEV : loss 0.15984997153282166 - score 0.9717
2021-05-16 14:21:12,114 BAD EPOCHS (no improvement): 1
2021-05-16 14:21:12,114 ----------------------------------------------------------------------------------------------------
2021-05-16 14:21:16,171 epoch 29 - iter 6/69 - loss 0.34149962 - samples/sec: 23.67 - lr: 0.006250
2021-05-16 14:21:20,302 epoch 29 - iter 12/69 - loss 0.30852881 - samples/sec: 23.25 - lr: 0.006250
2021-05-16 14:21:23,911 epoch 29 - iter 18/69 - loss 0.27008556 - samples/sec: 26.60 - lr: 0.006250
2021-05-16 14:21:27,874 epoch 29 - iter 24/69 - loss 0.27790258 - samples/sec: 24.24 - lr: 0.006250
2021-05-16 14:21:32,023 epoch 29 - iter 30/69 - loss 0.25636759 - samples/sec: 23.14 - lr: 0.006250
2021-05-16 14:21:35,746 epoch 29 - iter 36/69 - loss 0.23930547 - samples/sec: 25.79 - lr: 0.006250
2021-05-16 14:21:39,735 epoch 29 - iter 42/69 - loss 0.22803359 - samples/sec: 24.07 - lr: 0.006250
2021-05-16 14:21:43,493 epoch 29 - iter 48/69 - loss 0.22319509 - samples/sec: 25.55 - lr: 0.006250
2021-05-16 14:21:47,199 epoch 29 - iter 54/69 - loss 0.23718648 - samples/sec: 25.91 - lr: 0.006250
2021-05-16 14:21:50,841 epoch 29 - iter 60/69 - loss 0.24127396 - samples/sec: 26.36 - lr: 0.006250
2021-05-16 14:21:54,526 epoch 29 - iter 66/69 - loss 0.23664614 - samples/sec: 26.06 - lr: 0.006250
2021-05-16 14:21:56,005 ----------------------------------------------------------------------------------------------------
2021-05-16 14:21:56,006 EPOCH 29 done: loss 0.2406 - lr 0.0062500
2021-05-16 14:22:00,217 DEV : loss 0.1651136726140976 - score 0.9738
2021-05-16 14:22:00,263 BAD EPOCHS (no improvement): 2
2021-05-16 14:22:00,263 ----------------------------------------------------------------------------------------------------
2021-05-16 14:22:04,637 epoch 30 - iter 6/69 - loss 0.30960310 - samples/sec: 21.96 - lr: 0.006250
2021-05-16 14:22:09,194 epoch 30 - iter 12/69 - loss 0.30816756 - samples/sec: 21.07 - lr: 0.006250
2021-05-16 14:22:13,031 epoch 30 - iter 18/69 - loss 0.26430771 - samples/sec: 25.03 - lr: 0.006250
2021-05-16 14:22:16,879 epoch 30 - iter 24/69 - loss 0.25578600 - samples/sec: 24.95 - lr: 0.006250
2021-05-16 14:22:21,308 epoch 30 - iter 30/69 - loss 0.22949254 - samples/sec: 21.68 - lr: 0.006250
2021-05-16 14:22:25,754 epoch 30 - iter 36/69 - loss 0.22123566 - samples/sec: 21.60 - lr: 0.006250
2021-05-16 14:22:29,551 epoch 30 - iter 42/69 - loss 0.20767167 - samples/sec: 25.28 - lr: 0.006250
2021-05-16 14:22:33,054 epoch 30 - iter 48/69 - loss 0.21907529 - samples/sec: 27.41 - lr: 0.006250
2021-05-16 14:22:36,719 epoch 30 - iter 54/69 - loss 0.22231031 - samples/sec: 26.20 - lr: 0.006250
2021-05-16 14:22:41,196 epoch 30 - iter 60/69 - loss 0.23723767 - samples/sec: 21.45 - lr: 0.006250
2021-05-16 14:22:46,191 epoch 30 - iter 66/69 - loss 0.22504058 - samples/sec: 19.22 - lr: 0.006250
2021-05-16 14:22:47,841 ----------------------------------------------------------------------------------------------------
2021-05-16 14:22:47,841 EPOCH 30 done: loss 0.2268 - lr 0.0062500
2021-05-16 14:22:52,184 DEV : loss 0.1633140593767166 - score 0.9738
2021-05-16 14:22:52,235 BAD EPOCHS (no improvement): 3
2021-05-16 14:23:02,350 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:02,351 Testing using best model ...
2021-05-16 14:23:02,351 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/fra.sdrt.annodis/best-model.pt
2021-05-16 14:23:22,096 0.8333	0.9953	0.9071
2021-05-16 14:23:22,096 
Results:
- F1-score (micro) 0.9071
- F1-score (macro) 0.9071

By class:
SENT       tp: 210 - fp: 42 - fn: 1 - precision: 0.8333 - recall: 0.9953 - f1-score: 0.9071
2021-05-16 14:23:22,097 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert/
2021-05-16 14:23:22,134 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert
2021-05-16 14:23:22,137 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert/sent_train.txt
2021-05-16 14:23:22,137 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert/sent_dev.txt
2021-05-16 14:23:22,137 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert/sent_test.txt
Corpus: 1046 train + 371 dev + 330 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 14:23:37,052 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,057 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 14:23:37,057 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,057 Corpus: "Corpus: 1046 train + 371 dev + 330 test sentences"
2021-05-16 14:23:37,058 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,058 Parameters:
2021-05-16 14:23:37,058  - learning_rate: "0.1"
2021-05-16 14:23:37,058  - mini_batch_size: "16"
2021-05-16 14:23:37,058  - patience: "3"
2021-05-16 14:23:37,058  - anneal_factor: "0.5"
2021-05-16 14:23:37,058  - max_epochs: "30"
2021-05-16 14:23:37,058  - shuffle: "True"
2021-05-16 14:23:37,058  - train_with_dev: "False"
2021-05-16 14:23:37,058  - batch_growth_annealing: "False"
2021-05-16 14:23:37,058 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,059 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert"
2021-05-16 14:23:37,059 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,059 Device: cuda:0
2021-05-16 14:23:37,059 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:37,059 Embeddings storage mode: cpu
2021-05-16 14:23:37,062 ----------------------------------------------------------------------------------------------------
2021-05-16 14:23:46,962 epoch 1 - iter 6/66 - loss 5.99164438 - samples/sec: 9.70 - lr: 0.100000
2021-05-16 14:23:57,273 epoch 1 - iter 12/66 - loss 5.87218177 - samples/sec: 9.31 - lr: 0.100000
2021-05-16 14:24:08,209 epoch 1 - iter 18/66 - loss 5.61090485 - samples/sec: 8.78 - lr: 0.100000
2021-05-16 14:24:17,712 epoch 1 - iter 24/66 - loss 5.87402153 - samples/sec: 10.10 - lr: 0.100000
2021-05-16 14:24:27,804 epoch 1 - iter 30/66 - loss 5.57995029 - samples/sec: 9.51 - lr: 0.100000
2021-05-16 14:24:37,375 epoch 1 - iter 36/66 - loss 5.32304474 - samples/sec: 10.03 - lr: 0.100000
2021-05-16 14:24:48,265 epoch 1 - iter 42/66 - loss 5.10392557 - samples/sec: 8.82 - lr: 0.100000
2021-05-16 14:24:59,610 epoch 1 - iter 48/66 - loss 4.87816324 - samples/sec: 8.46 - lr: 0.100000
2021-05-16 14:25:09,846 epoch 1 - iter 54/66 - loss 4.65125864 - samples/sec: 9.38 - lr: 0.100000
2021-05-16 14:25:20,606 epoch 1 - iter 60/66 - loss 4.44170041 - samples/sec: 8.92 - lr: 0.100000
2021-05-16 14:25:30,768 epoch 1 - iter 66/66 - loss 4.24792865 - samples/sec: 9.45 - lr: 0.100000
2021-05-16 14:25:30,769 ----------------------------------------------------------------------------------------------------
2021-05-16 14:25:30,769 EPOCH 1 done: loss 4.2479 - lr 0.1000000
2021-05-16 14:25:57,843 DEV : loss 1.6986339092254639 - score 0.7706
2021-05-16 14:25:57,911 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:26:08,664 ----------------------------------------------------------------------------------------------------
2021-05-16 14:26:12,766 epoch 2 - iter 6/66 - loss 2.39501913 - samples/sec: 23.40 - lr: 0.100000
2021-05-16 14:26:16,895 epoch 2 - iter 12/66 - loss 2.04180492 - samples/sec: 23.26 - lr: 0.100000
2021-05-16 14:26:21,201 epoch 2 - iter 18/66 - loss 1.99700274 - samples/sec: 22.30 - lr: 0.100000
2021-05-16 14:26:25,495 epoch 2 - iter 24/66 - loss 2.03576149 - samples/sec: 22.36 - lr: 0.100000
2021-05-16 14:26:29,264 epoch 2 - iter 30/66 - loss 1.93032274 - samples/sec: 25.48 - lr: 0.100000
2021-05-16 14:26:32,743 epoch 2 - iter 36/66 - loss 1.82834355 - samples/sec: 27.61 - lr: 0.100000
2021-05-16 14:26:36,812 epoch 2 - iter 42/66 - loss 1.77262785 - samples/sec: 23.60 - lr: 0.100000
2021-05-16 14:26:40,848 epoch 2 - iter 48/66 - loss 1.76541663 - samples/sec: 23.79 - lr: 0.100000
2021-05-16 14:26:44,880 epoch 2 - iter 54/66 - loss 1.69725625 - samples/sec: 23.81 - lr: 0.100000
2021-05-16 14:26:48,631 epoch 2 - iter 60/66 - loss 1.64961204 - samples/sec: 25.60 - lr: 0.100000
2021-05-16 14:26:52,553 epoch 2 - iter 66/66 - loss 1.62145665 - samples/sec: 24.48 - lr: 0.100000
2021-05-16 14:26:52,553 ----------------------------------------------------------------------------------------------------
2021-05-16 14:26:52,554 EPOCH 2 done: loss 1.6215 - lr 0.1000000
2021-05-16 14:26:58,527 DEV : loss 0.6207006573677063 - score 0.8331
2021-05-16 14:26:58,595 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:27:09,619 ----------------------------------------------------------------------------------------------------
2021-05-16 14:27:13,768 epoch 3 - iter 6/66 - loss 0.96954483 - samples/sec: 23.15 - lr: 0.100000
2021-05-16 14:27:17,936 epoch 3 - iter 12/66 - loss 1.06104993 - samples/sec: 23.03 - lr: 0.100000
2021-05-16 14:27:21,857 epoch 3 - iter 18/66 - loss 1.08443829 - samples/sec: 24.49 - lr: 0.100000
2021-05-16 14:27:26,352 epoch 3 - iter 24/66 - loss 1.01548885 - samples/sec: 21.36 - lr: 0.100000
2021-05-16 14:27:30,803 epoch 3 - iter 30/66 - loss 1.23978730 - samples/sec: 21.57 - lr: 0.100000
2021-05-16 14:27:35,074 epoch 3 - iter 36/66 - loss 1.23836405 - samples/sec: 22.48 - lr: 0.100000
2021-05-16 14:27:39,204 epoch 3 - iter 42/66 - loss 1.24566479 - samples/sec: 23.25 - lr: 0.100000
2021-05-16 14:27:43,308 epoch 3 - iter 48/66 - loss 1.19885166 - samples/sec: 23.40 - lr: 0.100000
2021-05-16 14:27:48,221 epoch 3 - iter 54/66 - loss 1.21643635 - samples/sec: 19.54 - lr: 0.100000
2021-05-16 14:27:52,760 epoch 3 - iter 60/66 - loss 1.23647738 - samples/sec: 21.15 - lr: 0.100000
2021-05-16 14:27:56,733 epoch 3 - iter 66/66 - loss 1.21000033 - samples/sec: 24.17 - lr: 0.100000
2021-05-16 14:27:56,734 ----------------------------------------------------------------------------------------------------
2021-05-16 14:27:56,734 EPOCH 3 done: loss 1.2100 - lr 0.1000000
2021-05-16 14:28:03,536 DEV : loss 0.4468902349472046 - score 0.9088
2021-05-16 14:28:03,623 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:28:14,905 ----------------------------------------------------------------------------------------------------
2021-05-16 14:28:19,048 epoch 4 - iter 6/66 - loss 1.39307519 - samples/sec: 23.18 - lr: 0.100000
2021-05-16 14:28:23,224 epoch 4 - iter 12/66 - loss 1.12638825 - samples/sec: 23.00 - lr: 0.100000
2021-05-16 14:28:27,715 epoch 4 - iter 18/66 - loss 1.03945597 - samples/sec: 21.38 - lr: 0.100000
2021-05-16 14:28:32,058 epoch 4 - iter 24/66 - loss 1.01135692 - samples/sec: 22.11 - lr: 0.100000
2021-05-16 14:28:36,638 epoch 4 - iter 30/66 - loss 1.04496640 - samples/sec: 20.96 - lr: 0.100000
2021-05-16 14:28:40,705 epoch 4 - iter 36/66 - loss 0.99473138 - samples/sec: 23.61 - lr: 0.100000
2021-05-16 14:28:45,013 epoch 4 - iter 42/66 - loss 0.94782900 - samples/sec: 22.29 - lr: 0.100000
2021-05-16 14:28:48,823 epoch 4 - iter 48/66 - loss 0.96309693 - samples/sec: 25.20 - lr: 0.100000
2021-05-16 14:28:52,895 epoch 4 - iter 54/66 - loss 0.98939050 - samples/sec: 23.58 - lr: 0.100000
2021-05-16 14:28:56,556 epoch 4 - iter 60/66 - loss 0.96374796 - samples/sec: 26.22 - lr: 0.100000
2021-05-16 14:29:00,445 epoch 4 - iter 66/66 - loss 0.97583018 - samples/sec: 24.69 - lr: 0.100000
2021-05-16 14:29:00,446 ----------------------------------------------------------------------------------------------------
2021-05-16 14:29:00,446 EPOCH 4 done: loss 0.9758 - lr 0.1000000
2021-05-16 14:29:06,764 DEV : loss 0.6420896053314209 - score 0.8589
2021-05-16 14:29:06,828 BAD EPOCHS (no improvement): 1
2021-05-16 14:29:06,829 ----------------------------------------------------------------------------------------------------
2021-05-16 14:29:10,189 epoch 5 - iter 6/66 - loss 1.17159730 - samples/sec: 28.57 - lr: 0.100000
2021-05-16 14:29:14,255 epoch 5 - iter 12/66 - loss 0.94991679 - samples/sec: 23.62 - lr: 0.100000
2021-05-16 14:29:18,185 epoch 5 - iter 18/66 - loss 0.90582671 - samples/sec: 24.43 - lr: 0.100000
2021-05-16 14:29:22,108 epoch 5 - iter 24/66 - loss 0.93882996 - samples/sec: 24.47 - lr: 0.100000
2021-05-16 14:29:25,898 epoch 5 - iter 30/66 - loss 0.86621248 - samples/sec: 25.34 - lr: 0.100000
2021-05-16 14:29:29,525 epoch 5 - iter 36/66 - loss 0.83702047 - samples/sec: 26.47 - lr: 0.100000
2021-05-16 14:29:33,617 epoch 5 - iter 42/66 - loss 0.83420947 - samples/sec: 23.47 - lr: 0.100000
2021-05-16 14:29:37,702 epoch 5 - iter 48/66 - loss 0.81353537 - samples/sec: 23.50 - lr: 0.100000
2021-05-16 14:29:42,572 epoch 5 - iter 54/66 - loss 0.81745354 - samples/sec: 19.72 - lr: 0.100000
2021-05-16 14:29:46,237 epoch 5 - iter 60/66 - loss 0.81119910 - samples/sec: 26.20 - lr: 0.100000
2021-05-16 14:29:49,694 epoch 5 - iter 66/66 - loss 0.83855779 - samples/sec: 27.77 - lr: 0.100000
2021-05-16 14:29:49,695 ----------------------------------------------------------------------------------------------------
2021-05-16 14:29:49,695 EPOCH 5 done: loss 0.8386 - lr 0.1000000
2021-05-16 14:29:55,588 DEV : loss 0.40083909034729004 - score 0.93
2021-05-16 14:29:55,655 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:30:06,632 ----------------------------------------------------------------------------------------------------
2021-05-16 14:30:10,756 epoch 6 - iter 6/66 - loss 0.25689875 - samples/sec: 23.29 - lr: 0.100000
2021-05-16 14:30:14,924 epoch 6 - iter 12/66 - loss 0.46741314 - samples/sec: 23.04 - lr: 0.100000
2021-05-16 14:30:18,930 epoch 6 - iter 18/66 - loss 0.67505430 - samples/sec: 23.97 - lr: 0.100000
2021-05-16 14:30:23,336 epoch 6 - iter 24/66 - loss 0.69162747 - samples/sec: 21.79 - lr: 0.100000
2021-05-16 14:30:27,614 epoch 6 - iter 30/66 - loss 0.69518484 - samples/sec: 22.45 - lr: 0.100000
2021-05-16 14:30:31,248 epoch 6 - iter 36/66 - loss 0.80217634 - samples/sec: 26.42 - lr: 0.100000
2021-05-16 14:30:35,171 epoch 6 - iter 42/66 - loss 0.84211400 - samples/sec: 24.48 - lr: 0.100000
2021-05-16 14:30:38,850 epoch 6 - iter 48/66 - loss 0.83446524 - samples/sec: 26.12 - lr: 0.100000
2021-05-16 14:30:43,322 epoch 6 - iter 54/66 - loss 0.82096455 - samples/sec: 21.47 - lr: 0.100000
2021-05-16 14:30:47,267 epoch 6 - iter 60/66 - loss 0.81154096 - samples/sec: 24.35 - lr: 0.100000
2021-05-16 14:30:50,424 epoch 6 - iter 66/66 - loss 0.81955578 - samples/sec: 30.42 - lr: 0.100000
2021-05-16 14:30:50,425 ----------------------------------------------------------------------------------------------------
2021-05-16 14:30:50,425 EPOCH 6 done: loss 0.8196 - lr 0.1000000
2021-05-16 14:30:56,408 DEV : loss 0.4528138041496277 - score 0.9055
2021-05-16 14:30:56,477 BAD EPOCHS (no improvement): 1
2021-05-16 14:30:56,478 ----------------------------------------------------------------------------------------------------
2021-05-16 14:31:00,577 epoch 7 - iter 6/66 - loss 1.07130563 - samples/sec: 23.42 - lr: 0.100000
2021-05-16 14:31:05,776 epoch 7 - iter 12/66 - loss 0.95115110 - samples/sec: 18.47 - lr: 0.100000
2021-05-16 14:31:09,656 epoch 7 - iter 18/66 - loss 0.95723059 - samples/sec: 24.75 - lr: 0.100000
2021-05-16 14:31:14,000 epoch 7 - iter 24/66 - loss 0.80268603 - samples/sec: 22.12 - lr: 0.100000
2021-05-16 14:31:17,873 epoch 7 - iter 30/66 - loss 0.78367753 - samples/sec: 24.79 - lr: 0.100000
2021-05-16 14:31:22,129 epoch 7 - iter 36/66 - loss 0.73719964 - samples/sec: 22.56 - lr: 0.100000
2021-05-16 14:31:26,219 epoch 7 - iter 42/66 - loss 0.74432954 - samples/sec: 23.48 - lr: 0.100000
2021-05-16 14:31:30,260 epoch 7 - iter 48/66 - loss 0.76709333 - samples/sec: 23.77 - lr: 0.100000
2021-05-16 14:31:34,083 epoch 7 - iter 54/66 - loss 0.74499397 - samples/sec: 25.11 - lr: 0.100000
2021-05-16 14:31:38,129 epoch 7 - iter 60/66 - loss 0.73284206 - samples/sec: 23.73 - lr: 0.100000
2021-05-16 14:31:41,690 epoch 7 - iter 66/66 - loss 0.70217268 - samples/sec: 26.99 - lr: 0.100000
2021-05-16 14:31:41,690 ----------------------------------------------------------------------------------------------------
2021-05-16 14:31:41,690 EPOCH 7 done: loss 0.7022 - lr 0.1000000
2021-05-16 14:31:48,634 DEV : loss 0.4491078555583954 - score 0.8946
2021-05-16 14:31:48,703 BAD EPOCHS (no improvement): 2
2021-05-16 14:31:48,703 ----------------------------------------------------------------------------------------------------
2021-05-16 14:31:52,535 epoch 8 - iter 6/66 - loss 0.69749049 - samples/sec: 25.06 - lr: 0.100000
2021-05-16 14:31:56,512 epoch 8 - iter 12/66 - loss 0.49887240 - samples/sec: 24.14 - lr: 0.100000
2021-05-16 14:32:01,405 epoch 8 - iter 18/66 - loss 0.53549191 - samples/sec: 19.62 - lr: 0.100000
2021-05-16 14:32:05,221 epoch 8 - iter 24/66 - loss 0.50797515 - samples/sec: 25.16 - lr: 0.100000
2021-05-16 14:32:08,783 epoch 8 - iter 30/66 - loss 0.47913772 - samples/sec: 26.96 - lr: 0.100000
2021-05-16 14:32:12,242 epoch 8 - iter 36/66 - loss 0.50422307 - samples/sec: 27.76 - lr: 0.100000
2021-05-16 14:32:16,454 epoch 8 - iter 42/66 - loss 0.54936603 - samples/sec: 22.80 - lr: 0.100000
2021-05-16 14:32:20,678 epoch 8 - iter 48/66 - loss 0.63526046 - samples/sec: 22.73 - lr: 0.100000
2021-05-16 14:32:24,443 epoch 8 - iter 54/66 - loss 0.64985469 - samples/sec: 25.50 - lr: 0.100000
2021-05-16 14:32:28,337 epoch 8 - iter 60/66 - loss 0.66913085 - samples/sec: 24.66 - lr: 0.100000
2021-05-16 14:32:32,264 epoch 8 - iter 66/66 - loss 0.65958316 - samples/sec: 24.45 - lr: 0.100000
2021-05-16 14:32:32,264 ----------------------------------------------------------------------------------------------------
2021-05-16 14:32:32,264 EPOCH 8 done: loss 0.6596 - lr 0.1000000
2021-05-16 14:32:38,884 DEV : loss 0.47623300552368164 - score 0.9212
2021-05-16 14:32:38,952 BAD EPOCHS (no improvement): 3
2021-05-16 14:32:38,952 ----------------------------------------------------------------------------------------------------
2021-05-16 14:32:43,198 epoch 9 - iter 6/66 - loss 0.32050913 - samples/sec: 22.61 - lr: 0.100000
2021-05-16 14:32:47,410 epoch 9 - iter 12/66 - loss 0.41890290 - samples/sec: 22.80 - lr: 0.100000
2021-05-16 14:32:51,376 epoch 9 - iter 18/66 - loss 0.48994613 - samples/sec: 24.21 - lr: 0.100000
2021-05-16 14:32:55,472 epoch 9 - iter 24/66 - loss 0.47796915 - samples/sec: 23.44 - lr: 0.100000
2021-05-16 14:32:59,481 epoch 9 - iter 30/66 - loss 0.47833779 - samples/sec: 24.02 - lr: 0.100000
2021-05-16 14:33:03,264 epoch 9 - iter 36/66 - loss 0.52038977 - samples/sec: 25.39 - lr: 0.100000
2021-05-16 14:33:07,259 epoch 9 - iter 42/66 - loss 0.51184645 - samples/sec: 24.04 - lr: 0.100000
2021-05-16 14:33:11,456 epoch 9 - iter 48/66 - loss 0.53640352 - samples/sec: 22.87 - lr: 0.100000
2021-05-16 14:33:15,270 epoch 9 - iter 54/66 - loss 0.55516019 - samples/sec: 25.18 - lr: 0.100000
2021-05-16 14:33:19,046 epoch 9 - iter 60/66 - loss 0.57951324 - samples/sec: 25.43 - lr: 0.100000
2021-05-16 14:33:22,032 epoch 9 - iter 66/66 - loss 0.59868683 - samples/sec: 32.16 - lr: 0.100000
2021-05-16 14:33:22,033 ----------------------------------------------------------------------------------------------------
2021-05-16 14:33:22,033 EPOCH 9 done: loss 0.5987 - lr 0.1000000
2021-05-16 14:33:28,334 DEV : loss 1.0035067796707153 - score 0.86
Epoch     9: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 14:33:28,403 BAD EPOCHS (no improvement): 4
2021-05-16 14:33:28,403 ----------------------------------------------------------------------------------------------------
2021-05-16 14:33:32,094 epoch 10 - iter 6/66 - loss 0.36295923 - samples/sec: 26.02 - lr: 0.050000
2021-05-16 14:33:35,958 epoch 10 - iter 12/66 - loss 0.44982226 - samples/sec: 24.85 - lr: 0.050000
2021-05-16 14:33:40,049 epoch 10 - iter 18/66 - loss 0.44823358 - samples/sec: 23.48 - lr: 0.050000
2021-05-16 14:33:43,630 epoch 10 - iter 24/66 - loss 0.44300912 - samples/sec: 26.81 - lr: 0.050000
2021-05-16 14:33:47,731 epoch 10 - iter 30/66 - loss 0.44284529 - samples/sec: 23.41 - lr: 0.050000
2021-05-16 14:33:51,577 epoch 10 - iter 36/66 - loss 0.46154121 - samples/sec: 24.97 - lr: 0.050000
2021-05-16 14:33:55,704 epoch 10 - iter 42/66 - loss 0.49188607 - samples/sec: 23.26 - lr: 0.050000
2021-05-16 14:33:59,849 epoch 10 - iter 48/66 - loss 0.47815222 - samples/sec: 23.17 - lr: 0.050000
2021-05-16 14:34:03,929 epoch 10 - iter 54/66 - loss 0.47228375 - samples/sec: 23.54 - lr: 0.050000
2021-05-16 14:34:08,264 epoch 10 - iter 60/66 - loss 0.45741445 - samples/sec: 22.14 - lr: 0.050000
2021-05-16 14:34:12,487 epoch 10 - iter 66/66 - loss 0.45119278 - samples/sec: 22.74 - lr: 0.050000
2021-05-16 14:34:12,488 ----------------------------------------------------------------------------------------------------
2021-05-16 14:34:12,488 EPOCH 10 done: loss 0.4512 - lr 0.0500000
2021-05-16 14:34:21,627 DEV : loss 0.31341367959976196 - score 0.9418
2021-05-16 14:34:21,711 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:34:32,801 ----------------------------------------------------------------------------------------------------
2021-05-16 14:34:36,827 epoch 11 - iter 6/66 - loss 0.28036845 - samples/sec: 23.86 - lr: 0.050000
2021-05-16 14:34:40,544 epoch 11 - iter 12/66 - loss 0.32542457 - samples/sec: 25.83 - lr: 0.050000
2021-05-16 14:34:43,966 epoch 11 - iter 18/66 - loss 0.34902265 - samples/sec: 28.06 - lr: 0.050000
2021-05-16 14:34:48,516 epoch 11 - iter 24/66 - loss 0.38167472 - samples/sec: 21.10 - lr: 0.050000
2021-05-16 14:34:52,628 epoch 11 - iter 30/66 - loss 0.37461178 - samples/sec: 23.35 - lr: 0.050000
2021-05-16 14:34:56,393 epoch 11 - iter 36/66 - loss 0.35856862 - samples/sec: 25.53 - lr: 0.050000
2021-05-16 14:35:01,012 epoch 11 - iter 42/66 - loss 0.38282279 - samples/sec: 20.79 - lr: 0.050000
2021-05-16 14:35:05,278 epoch 11 - iter 48/66 - loss 0.37387846 - samples/sec: 22.51 - lr: 0.050000
2021-05-16 14:35:09,078 epoch 11 - iter 54/66 - loss 0.36577412 - samples/sec: 25.27 - lr: 0.050000
2021-05-16 14:35:13,092 epoch 11 - iter 60/66 - loss 0.37145859 - samples/sec: 23.92 - lr: 0.050000
2021-05-16 14:35:17,047 epoch 11 - iter 66/66 - loss 0.37479069 - samples/sec: 24.28 - lr: 0.050000
2021-05-16 14:35:17,047 ----------------------------------------------------------------------------------------------------
2021-05-16 14:35:17,048 EPOCH 11 done: loss 0.3748 - lr 0.0500000
2021-05-16 14:35:23,758 DEV : loss 0.43092793226242065 - score 0.9373
2021-05-16 14:35:23,828 BAD EPOCHS (no improvement): 1
2021-05-16 14:35:23,829 ----------------------------------------------------------------------------------------------------
2021-05-16 14:35:28,096 epoch 12 - iter 6/66 - loss 0.39502873 - samples/sec: 22.50 - lr: 0.050000
2021-05-16 14:35:32,337 epoch 12 - iter 12/66 - loss 0.37930448 - samples/sec: 22.64 - lr: 0.050000
2021-05-16 14:35:36,570 epoch 12 - iter 18/66 - loss 0.35246207 - samples/sec: 22.69 - lr: 0.050000
2021-05-16 14:35:40,695 epoch 12 - iter 24/66 - loss 0.38570635 - samples/sec: 23.28 - lr: 0.050000
2021-05-16 14:35:45,187 epoch 12 - iter 30/66 - loss 0.37331746 - samples/sec: 21.38 - lr: 0.050000
2021-05-16 14:35:49,297 epoch 12 - iter 36/66 - loss 0.35036369 - samples/sec: 23.38 - lr: 0.050000
2021-05-16 14:35:53,069 epoch 12 - iter 42/66 - loss 0.35353232 - samples/sec: 25.46 - lr: 0.050000
2021-05-16 14:35:56,680 epoch 12 - iter 48/66 - loss 0.36622741 - samples/sec: 26.59 - lr: 0.050000
2021-05-16 14:36:00,342 epoch 12 - iter 54/66 - loss 0.36006296 - samples/sec: 26.22 - lr: 0.050000
2021-05-16 14:36:03,978 epoch 12 - iter 60/66 - loss 0.33990723 - samples/sec: 26.41 - lr: 0.050000
2021-05-16 14:36:07,881 epoch 12 - iter 66/66 - loss 0.34015234 - samples/sec: 24.60 - lr: 0.050000
2021-05-16 14:36:07,882 ----------------------------------------------------------------------------------------------------
2021-05-16 14:36:07,882 EPOCH 12 done: loss 0.3402 - lr 0.0500000
2021-05-16 14:36:13,687 DEV : loss 0.27642059326171875 - score 0.9509
2021-05-16 14:36:13,756 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:36:24,818 ----------------------------------------------------------------------------------------------------
2021-05-16 14:36:28,995 epoch 13 - iter 6/66 - loss 0.34457889 - samples/sec: 22.99 - lr: 0.050000
2021-05-16 14:36:33,653 epoch 13 - iter 12/66 - loss 0.41842065 - samples/sec: 20.61 - lr: 0.050000
2021-05-16 14:36:38,080 epoch 13 - iter 18/66 - loss 0.40853904 - samples/sec: 21.69 - lr: 0.050000
2021-05-16 14:36:42,689 epoch 13 - iter 24/66 - loss 0.36661880 - samples/sec: 20.83 - lr: 0.050000
2021-05-16 14:36:46,683 epoch 13 - iter 30/66 - loss 0.36129715 - samples/sec: 24.04 - lr: 0.050000
2021-05-16 14:36:50,510 epoch 13 - iter 36/66 - loss 0.35597367 - samples/sec: 25.09 - lr: 0.050000
2021-05-16 14:36:54,554 epoch 13 - iter 42/66 - loss 0.34971898 - samples/sec: 23.75 - lr: 0.050000
2021-05-16 14:36:59,185 epoch 13 - iter 48/66 - loss 0.35532167 - samples/sec: 20.73 - lr: 0.050000
2021-05-16 14:37:03,183 epoch 13 - iter 54/66 - loss 0.35023695 - samples/sec: 24.02 - lr: 0.050000
2021-05-16 14:37:06,998 epoch 13 - iter 60/66 - loss 0.35202567 - samples/sec: 25.18 - lr: 0.050000
2021-05-16 14:37:10,815 epoch 13 - iter 66/66 - loss 0.35274760 - samples/sec: 25.15 - lr: 0.050000
2021-05-16 14:37:10,816 ----------------------------------------------------------------------------------------------------
2021-05-16 14:37:10,816 EPOCH 13 done: loss 0.3527 - lr 0.0500000
2021-05-16 14:37:17,473 DEV : loss 0.40238818526268005 - score 0.935
2021-05-16 14:37:17,541 BAD EPOCHS (no improvement): 1
2021-05-16 14:37:17,541 ----------------------------------------------------------------------------------------------------
2021-05-16 14:37:21,603 epoch 14 - iter 6/66 - loss 0.37431029 - samples/sec: 23.64 - lr: 0.050000
2021-05-16 14:37:25,433 epoch 14 - iter 12/66 - loss 0.36844412 - samples/sec: 25.07 - lr: 0.050000
2021-05-16 14:37:30,295 epoch 14 - iter 18/66 - loss 0.34085694 - samples/sec: 19.76 - lr: 0.050000
2021-05-16 14:37:34,431 epoch 14 - iter 24/66 - loss 0.32706623 - samples/sec: 23.22 - lr: 0.050000
2021-05-16 14:37:37,915 epoch 14 - iter 30/66 - loss 0.33330993 - samples/sec: 27.56 - lr: 0.050000
2021-05-16 14:37:41,648 epoch 14 - iter 36/66 - loss 0.34779231 - samples/sec: 25.72 - lr: 0.050000
2021-05-16 14:37:45,562 epoch 14 - iter 42/66 - loss 0.32258964 - samples/sec: 24.53 - lr: 0.050000
2021-05-16 14:37:49,463 epoch 14 - iter 48/66 - loss 0.33197286 - samples/sec: 24.62 - lr: 0.050000
2021-05-16 14:37:53,907 epoch 14 - iter 54/66 - loss 0.31891715 - samples/sec: 21.61 - lr: 0.050000
2021-05-16 14:37:58,065 epoch 14 - iter 60/66 - loss 0.31876634 - samples/sec: 23.09 - lr: 0.050000
2021-05-16 14:38:02,097 epoch 14 - iter 66/66 - loss 0.33172047 - samples/sec: 23.81 - lr: 0.050000
2021-05-16 14:38:02,098 ----------------------------------------------------------------------------------------------------
2021-05-16 14:38:02,098 EPOCH 14 done: loss 0.3317 - lr 0.0500000
2021-05-16 14:38:08,104 DEV : loss 0.30172353982925415 - score 0.9492
2021-05-16 14:38:08,174 BAD EPOCHS (no improvement): 2
2021-05-16 14:38:08,174 ----------------------------------------------------------------------------------------------------
2021-05-16 14:38:12,023 epoch 15 - iter 6/66 - loss 0.43995812 - samples/sec: 24.95 - lr: 0.050000
2021-05-16 14:38:16,357 epoch 15 - iter 12/66 - loss 0.42846883 - samples/sec: 22.16 - lr: 0.050000
2021-05-16 14:38:19,959 epoch 15 - iter 18/66 - loss 0.34757681 - samples/sec: 26.66 - lr: 0.050000
2021-05-16 14:38:23,932 epoch 15 - iter 24/66 - loss 0.40031507 - samples/sec: 24.19 - lr: 0.050000
2021-05-16 14:38:28,679 epoch 15 - iter 30/66 - loss 0.37182862 - samples/sec: 20.23 - lr: 0.050000
2021-05-16 14:38:33,072 epoch 15 - iter 36/66 - loss 0.34869391 - samples/sec: 21.86 - lr: 0.050000
2021-05-16 14:38:36,539 epoch 15 - iter 42/66 - loss 0.33292025 - samples/sec: 27.69 - lr: 0.050000
2021-05-16 14:38:40,366 epoch 15 - iter 48/66 - loss 0.32212754 - samples/sec: 25.09 - lr: 0.050000
2021-05-16 14:38:44,391 epoch 15 - iter 54/66 - loss 0.31244269 - samples/sec: 23.85 - lr: 0.050000
2021-05-16 14:38:48,831 epoch 15 - iter 60/66 - loss 0.31040018 - samples/sec: 21.62 - lr: 0.050000
2021-05-16 14:38:52,497 epoch 15 - iter 66/66 - loss 0.31003597 - samples/sec: 26.19 - lr: 0.050000
2021-05-16 14:38:52,498 ----------------------------------------------------------------------------------------------------
2021-05-16 14:38:52,498 EPOCH 15 done: loss 0.3100 - lr 0.0500000
2021-05-16 14:38:58,432 DEV : loss 0.37827765941619873 - score 0.9402
2021-05-16 14:38:58,516 BAD EPOCHS (no improvement): 3
2021-05-16 14:38:58,516 ----------------------------------------------------------------------------------------------------
2021-05-16 14:39:02,261 epoch 16 - iter 6/66 - loss 0.18925171 - samples/sec: 25.64 - lr: 0.050000
2021-05-16 14:39:06,245 epoch 16 - iter 12/66 - loss 0.19774111 - samples/sec: 24.10 - lr: 0.050000
2021-05-16 14:39:09,888 epoch 16 - iter 18/66 - loss 0.24944221 - samples/sec: 26.36 - lr: 0.050000
2021-05-16 14:39:14,101 epoch 16 - iter 24/66 - loss 0.25142284 - samples/sec: 22.79 - lr: 0.050000
2021-05-16 14:39:17,854 epoch 16 - iter 30/66 - loss 0.23435271 - samples/sec: 25.58 - lr: 0.050000
2021-05-16 14:39:21,627 epoch 16 - iter 36/66 - loss 0.24414799 - samples/sec: 25.45 - lr: 0.050000
2021-05-16 14:39:25,419 epoch 16 - iter 42/66 - loss 0.24431108 - samples/sec: 25.32 - lr: 0.050000
2021-05-16 14:39:29,529 epoch 16 - iter 48/66 - loss 0.25690630 - samples/sec: 23.36 - lr: 0.050000
2021-05-16 14:39:33,202 epoch 16 - iter 54/66 - loss 0.27667935 - samples/sec: 26.14 - lr: 0.050000
2021-05-16 14:39:36,630 epoch 16 - iter 60/66 - loss 0.28765353 - samples/sec: 28.01 - lr: 0.050000
2021-05-16 14:39:40,363 epoch 16 - iter 66/66 - loss 0.29762245 - samples/sec: 25.73 - lr: 0.050000
2021-05-16 14:39:40,363 ----------------------------------------------------------------------------------------------------
2021-05-16 14:39:40,363 EPOCH 16 done: loss 0.2976 - lr 0.0500000
2021-05-16 14:39:46,622 DEV : loss 0.3154597878456116 - score 0.9353
Epoch    16: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 14:39:46,693 BAD EPOCHS (no improvement): 4
2021-05-16 14:39:46,693 ----------------------------------------------------------------------------------------------------
2021-05-16 14:39:50,762 epoch 17 - iter 6/66 - loss 0.29860696 - samples/sec: 23.60 - lr: 0.025000
2021-05-16 14:39:55,128 epoch 17 - iter 12/66 - loss 0.33173691 - samples/sec: 21.99 - lr: 0.025000
2021-05-16 14:39:58,791 epoch 17 - iter 18/66 - loss 0.29230954 - samples/sec: 26.21 - lr: 0.025000
2021-05-16 14:40:02,562 epoch 17 - iter 24/66 - loss 0.30213557 - samples/sec: 25.47 - lr: 0.025000
2021-05-16 14:40:07,148 epoch 17 - iter 30/66 - loss 0.29559178 - samples/sec: 20.94 - lr: 0.025000
2021-05-16 14:40:11,588 epoch 17 - iter 36/66 - loss 0.28196179 - samples/sec: 21.62 - lr: 0.025000
2021-05-16 14:40:16,049 epoch 17 - iter 42/66 - loss 0.27035874 - samples/sec: 21.52 - lr: 0.025000
2021-05-16 14:40:19,944 epoch 17 - iter 48/66 - loss 0.26925743 - samples/sec: 24.65 - lr: 0.025000
2021-05-16 14:40:23,708 epoch 17 - iter 54/66 - loss 0.26853335 - samples/sec: 25.51 - lr: 0.025000
2021-05-16 14:40:27,905 epoch 17 - iter 60/66 - loss 0.26455348 - samples/sec: 22.88 - lr: 0.025000
2021-05-16 14:40:31,538 epoch 17 - iter 66/66 - loss 0.26732299 - samples/sec: 26.43 - lr: 0.025000
2021-05-16 14:40:31,539 ----------------------------------------------------------------------------------------------------
2021-05-16 14:40:31,539 EPOCH 17 done: loss 0.2673 - lr 0.0250000
2021-05-16 14:40:37,933 DEV : loss 0.30097368359565735 - score 0.9499
2021-05-16 14:40:38,020 BAD EPOCHS (no improvement): 1
2021-05-16 14:40:38,020 ----------------------------------------------------------------------------------------------------
2021-05-16 14:40:42,331 epoch 18 - iter 6/66 - loss 0.20210135 - samples/sec: 22.28 - lr: 0.025000
2021-05-16 14:40:46,211 epoch 18 - iter 12/66 - loss 0.24071795 - samples/sec: 24.75 - lr: 0.025000
2021-05-16 14:40:49,934 epoch 18 - iter 18/66 - loss 0.24482983 - samples/sec: 25.80 - lr: 0.025000
2021-05-16 14:40:53,927 epoch 18 - iter 24/66 - loss 0.23427384 - samples/sec: 24.05 - lr: 0.025000
2021-05-16 14:40:57,627 epoch 18 - iter 30/66 - loss 0.22671674 - samples/sec: 25.95 - lr: 0.025000
2021-05-16 14:41:02,380 epoch 18 - iter 36/66 - loss 0.21525065 - samples/sec: 20.20 - lr: 0.025000
2021-05-16 14:41:06,522 epoch 18 - iter 42/66 - loss 0.21178150 - samples/sec: 23.19 - lr: 0.025000
2021-05-16 14:41:10,493 epoch 18 - iter 48/66 - loss 0.21364500 - samples/sec: 24.18 - lr: 0.025000
2021-05-16 14:41:14,414 epoch 18 - iter 54/66 - loss 0.21330420 - samples/sec: 24.49 - lr: 0.025000
2021-05-16 14:41:17,875 epoch 18 - iter 60/66 - loss 0.22775853 - samples/sec: 27.74 - lr: 0.025000
2021-05-16 14:41:21,631 epoch 18 - iter 66/66 - loss 0.22439464 - samples/sec: 25.57 - lr: 0.025000
2021-05-16 14:41:21,632 ----------------------------------------------------------------------------------------------------
2021-05-16 14:41:21,632 EPOCH 18 done: loss 0.2244 - lr 0.0250000
2021-05-16 14:41:28,340 DEV : loss 0.30391016602516174 - score 0.9472
2021-05-16 14:41:28,408 BAD EPOCHS (no improvement): 2
2021-05-16 14:41:28,409 ----------------------------------------------------------------------------------------------------
2021-05-16 14:41:32,609 epoch 19 - iter 6/66 - loss 0.18216085 - samples/sec: 22.86 - lr: 0.025000
2021-05-16 14:41:36,762 epoch 19 - iter 12/66 - loss 0.18991129 - samples/sec: 23.12 - lr: 0.025000
2021-05-16 14:41:41,377 epoch 19 - iter 18/66 - loss 0.18840894 - samples/sec: 20.81 - lr: 0.025000
2021-05-16 14:41:45,411 epoch 19 - iter 24/66 - loss 0.20581362 - samples/sec: 23.80 - lr: 0.025000
2021-05-16 14:41:49,715 epoch 19 - iter 30/66 - loss 0.20141609 - samples/sec: 22.31 - lr: 0.025000
2021-05-16 14:41:54,066 epoch 19 - iter 36/66 - loss 0.20498307 - samples/sec: 22.07 - lr: 0.025000
2021-05-16 14:41:58,805 epoch 19 - iter 42/66 - loss 0.21254833 - samples/sec: 20.26 - lr: 0.025000
2021-05-16 14:42:03,672 epoch 19 - iter 48/66 - loss 0.22806970 - samples/sec: 19.73 - lr: 0.025000
2021-05-16 14:42:08,519 epoch 19 - iter 54/66 - loss 0.22464470 - samples/sec: 19.81 - lr: 0.025000
2021-05-16 14:42:12,933 epoch 19 - iter 60/66 - loss 0.23644620 - samples/sec: 21.75 - lr: 0.025000
2021-05-16 14:42:17,054 epoch 19 - iter 66/66 - loss 0.24192388 - samples/sec: 23.30 - lr: 0.025000
2021-05-16 14:42:17,054 ----------------------------------------------------------------------------------------------------
2021-05-16 14:42:17,054 EPOCH 19 done: loss 0.2419 - lr 0.0250000
2021-05-16 14:42:24,463 DEV : loss 0.25577425956726074 - score 0.9543
2021-05-16 14:42:24,532 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:42:35,768 ----------------------------------------------------------------------------------------------------
2021-05-16 14:42:40,018 epoch 20 - iter 6/66 - loss 0.26045957 - samples/sec: 22.59 - lr: 0.025000
2021-05-16 14:42:43,895 epoch 20 - iter 12/66 - loss 0.25214455 - samples/sec: 24.77 - lr: 0.025000
2021-05-16 14:42:47,815 epoch 20 - iter 18/66 - loss 0.22555550 - samples/sec: 24.51 - lr: 0.025000
2021-05-16 14:42:51,639 epoch 20 - iter 24/66 - loss 0.23186888 - samples/sec: 25.11 - lr: 0.025000
2021-05-16 14:42:55,473 epoch 20 - iter 30/66 - loss 0.20297285 - samples/sec: 25.04 - lr: 0.025000
2021-05-16 14:42:59,702 epoch 20 - iter 36/66 - loss 0.20391535 - samples/sec: 22.70 - lr: 0.025000
2021-05-16 14:43:04,477 epoch 20 - iter 42/66 - loss 0.21206692 - samples/sec: 20.11 - lr: 0.025000
2021-05-16 14:43:08,709 epoch 20 - iter 48/66 - loss 0.21834166 - samples/sec: 22.69 - lr: 0.025000
2021-05-16 14:43:12,992 epoch 20 - iter 54/66 - loss 0.21088162 - samples/sec: 22.42 - lr: 0.025000
2021-05-16 14:43:16,972 epoch 20 - iter 60/66 - loss 0.20990577 - samples/sec: 24.13 - lr: 0.025000
2021-05-16 14:43:20,719 epoch 20 - iter 66/66 - loss 0.20777895 - samples/sec: 25.63 - lr: 0.025000
2021-05-16 14:43:20,720 ----------------------------------------------------------------------------------------------------
2021-05-16 14:43:20,720 EPOCH 20 done: loss 0.2078 - lr 0.0250000
2021-05-16 14:43:26,417 DEV : loss 0.29127615690231323 - score 0.9496
2021-05-16 14:43:26,490 BAD EPOCHS (no improvement): 1
2021-05-16 14:43:26,490 ----------------------------------------------------------------------------------------------------
2021-05-16 14:43:30,724 epoch 21 - iter 6/66 - loss 0.26634034 - samples/sec: 22.68 - lr: 0.025000
2021-05-16 14:43:35,028 epoch 21 - iter 12/66 - loss 0.26700009 - samples/sec: 22.31 - lr: 0.025000
2021-05-16 14:43:38,866 epoch 21 - iter 18/66 - loss 0.28246915 - samples/sec: 25.03 - lr: 0.025000
2021-05-16 14:43:42,929 epoch 21 - iter 24/66 - loss 0.28092824 - samples/sec: 23.63 - lr: 0.025000
2021-05-16 14:43:46,877 epoch 21 - iter 30/66 - loss 0.26845875 - samples/sec: 24.32 - lr: 0.025000
2021-05-16 14:43:50,629 epoch 21 - iter 36/66 - loss 0.27678484 - samples/sec: 25.59 - lr: 0.025000
2021-05-16 14:43:55,129 epoch 21 - iter 42/66 - loss 0.27871252 - samples/sec: 21.34 - lr: 0.025000
2021-05-16 14:43:59,141 epoch 21 - iter 48/66 - loss 0.26482691 - samples/sec: 23.94 - lr: 0.025000
2021-05-16 14:44:03,619 epoch 21 - iter 54/66 - loss 0.27107718 - samples/sec: 21.46 - lr: 0.025000
2021-05-16 14:44:08,077 epoch 21 - iter 60/66 - loss 0.28031921 - samples/sec: 21.54 - lr: 0.025000
2021-05-16 14:44:11,829 epoch 21 - iter 66/66 - loss 0.27210375 - samples/sec: 25.59 - lr: 0.025000
2021-05-16 14:44:11,829 ----------------------------------------------------------------------------------------------------
2021-05-16 14:44:11,830 EPOCH 21 done: loss 0.2721 - lr 0.0250000
2021-05-16 14:44:19,086 DEV : loss 0.2659970223903656 - score 0.956
2021-05-16 14:44:19,157 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 14:44:30,578 ----------------------------------------------------------------------------------------------------
2021-05-16 14:44:34,864 epoch 22 - iter 6/66 - loss 0.28948092 - samples/sec: 22.41 - lr: 0.025000
2021-05-16 14:44:38,891 epoch 22 - iter 12/66 - loss 0.17137221 - samples/sec: 23.85 - lr: 0.025000
2021-05-16 14:44:43,251 epoch 22 - iter 18/66 - loss 0.19220336 - samples/sec: 22.02 - lr: 0.025000
2021-05-16 14:44:47,624 epoch 22 - iter 24/66 - loss 0.20717323 - samples/sec: 21.96 - lr: 0.025000
2021-05-16 14:44:52,065 epoch 22 - iter 30/66 - loss 0.19399753 - samples/sec: 21.62 - lr: 0.025000
2021-05-16 14:44:56,258 epoch 22 - iter 36/66 - loss 0.18826244 - samples/sec: 22.90 - lr: 0.025000
2021-05-16 14:45:00,391 epoch 22 - iter 42/66 - loss 0.20241614 - samples/sec: 23.24 - lr: 0.025000
2021-05-16 14:45:04,866 epoch 22 - iter 48/66 - loss 0.20332164 - samples/sec: 21.47 - lr: 0.025000
2021-05-16 14:45:09,696 epoch 22 - iter 54/66 - loss 0.19523072 - samples/sec: 19.88 - lr: 0.025000
2021-05-16 14:45:14,202 epoch 22 - iter 60/66 - loss 0.19813405 - samples/sec: 21.31 - lr: 0.025000
2021-05-16 14:45:17,631 epoch 22 - iter 66/66 - loss 0.20041828 - samples/sec: 28.01 - lr: 0.025000
2021-05-16 14:45:17,632 ----------------------------------------------------------------------------------------------------
2021-05-16 14:45:17,632 EPOCH 22 done: loss 0.2004 - lr 0.0250000
2021-05-16 14:45:24,731 DEV : loss 0.3022928237915039 - score 0.9509
2021-05-16 14:45:24,801 BAD EPOCHS (no improvement): 1
2021-05-16 14:45:24,801 ----------------------------------------------------------------------------------------------------
2021-05-16 14:45:28,683 epoch 23 - iter 6/66 - loss 0.10512982 - samples/sec: 24.74 - lr: 0.025000
2021-05-16 14:45:32,912 epoch 23 - iter 12/66 - loss 0.22486859 - samples/sec: 22.70 - lr: 0.025000
2021-05-16 14:45:36,689 epoch 23 - iter 18/66 - loss 0.18659034 - samples/sec: 25.43 - lr: 0.025000
2021-05-16 14:45:41,101 epoch 23 - iter 24/66 - loss 0.19657938 - samples/sec: 21.76 - lr: 0.025000
2021-05-16 14:45:44,870 epoch 23 - iter 30/66 - loss 0.19243933 - samples/sec: 25.47 - lr: 0.025000
2021-05-16 14:45:48,749 epoch 23 - iter 36/66 - loss 0.20852534 - samples/sec: 24.76 - lr: 0.025000
2021-05-16 14:45:52,740 epoch 23 - iter 42/66 - loss 0.19949388 - samples/sec: 24.06 - lr: 0.025000
2021-05-16 14:45:56,759 epoch 23 - iter 48/66 - loss 0.19417732 - samples/sec: 23.89 - lr: 0.025000
2021-05-16 14:46:00,997 epoch 23 - iter 54/66 - loss 0.19655878 - samples/sec: 22.66 - lr: 0.025000
2021-05-16 14:46:05,161 epoch 23 - iter 60/66 - loss 0.19072289 - samples/sec: 23.06 - lr: 0.025000
2021-05-16 14:46:09,375 epoch 23 - iter 66/66 - loss 0.19572626 - samples/sec: 22.79 - lr: 0.025000
2021-05-16 14:46:09,375 ----------------------------------------------------------------------------------------------------
2021-05-16 14:46:09,375 EPOCH 23 done: loss 0.1957 - lr 0.0250000
2021-05-16 14:46:16,165 DEV : loss 0.39627277851104736 - score 0.9402
2021-05-16 14:46:16,266 BAD EPOCHS (no improvement): 2
2021-05-16 14:46:16,273 ----------------------------------------------------------------------------------------------------
2021-05-16 14:46:20,286 epoch 24 - iter 6/66 - loss 0.14157893 - samples/sec: 23.93 - lr: 0.025000
2021-05-16 14:46:24,595 epoch 24 - iter 12/66 - loss 0.18983211 - samples/sec: 22.28 - lr: 0.025000
2021-05-16 14:46:28,646 epoch 24 - iter 18/66 - loss 0.21412170 - samples/sec: 23.70 - lr: 0.025000
2021-05-16 14:46:32,726 epoch 24 - iter 24/66 - loss 0.22448617 - samples/sec: 23.53 - lr: 0.025000
2021-05-16 14:46:36,529 epoch 24 - iter 30/66 - loss 0.20362150 - samples/sec: 25.25 - lr: 0.025000
2021-05-16 14:46:40,283 epoch 24 - iter 36/66 - loss 0.20092020 - samples/sec: 25.58 - lr: 0.025000
2021-05-16 14:46:44,486 epoch 24 - iter 42/66 - loss 0.20377644 - samples/sec: 22.84 - lr: 0.025000
2021-05-16 14:46:48,309 epoch 24 - iter 48/66 - loss 0.21516789 - samples/sec: 25.12 - lr: 0.025000
2021-05-16 14:46:52,507 epoch 24 - iter 54/66 - loss 0.20919207 - samples/sec: 22.87 - lr: 0.025000
2021-05-16 14:46:56,769 epoch 24 - iter 60/66 - loss 0.21069536 - samples/sec: 22.53 - lr: 0.025000
2021-05-16 14:47:00,730 epoch 24 - iter 66/66 - loss 0.21990808 - samples/sec: 24.25 - lr: 0.025000
2021-05-16 14:47:00,730 ----------------------------------------------------------------------------------------------------
2021-05-16 14:47:00,730 EPOCH 24 done: loss 0.2199 - lr 0.0250000
2021-05-16 14:47:06,751 DEV : loss 0.41569650173187256 - score 0.9378
2021-05-16 14:47:06,832 BAD EPOCHS (no improvement): 3
2021-05-16 14:47:06,832 ----------------------------------------------------------------------------------------------------
2021-05-16 14:47:10,846 epoch 25 - iter 6/66 - loss 0.16386878 - samples/sec: 23.93 - lr: 0.025000
2021-05-16 14:47:14,860 epoch 25 - iter 12/66 - loss 0.20271639 - samples/sec: 23.92 - lr: 0.025000
2021-05-16 14:47:18,575 epoch 25 - iter 18/66 - loss 0.19108340 - samples/sec: 25.85 - lr: 0.025000
2021-05-16 14:47:22,280 epoch 25 - iter 24/66 - loss 0.18849830 - samples/sec: 25.91 - lr: 0.025000
2021-05-16 14:47:26,332 epoch 25 - iter 30/66 - loss 0.19075862 - samples/sec: 23.70 - lr: 0.025000
2021-05-16 14:47:30,903 epoch 25 - iter 36/66 - loss 0.19139948 - samples/sec: 21.00 - lr: 0.025000
2021-05-16 14:47:35,072 epoch 25 - iter 42/66 - loss 0.19999843 - samples/sec: 23.04 - lr: 0.025000
2021-05-16 14:47:38,608 epoch 25 - iter 48/66 - loss 0.18978080 - samples/sec: 27.16 - lr: 0.025000
2021-05-16 14:47:42,342 epoch 25 - iter 54/66 - loss 0.18752027 - samples/sec: 25.72 - lr: 0.025000
2021-05-16 14:47:46,641 epoch 25 - iter 60/66 - loss 0.19428761 - samples/sec: 22.34 - lr: 0.025000
2021-05-16 14:47:50,066 epoch 25 - iter 66/66 - loss 0.19663153 - samples/sec: 28.03 - lr: 0.025000
2021-05-16 14:47:50,067 ----------------------------------------------------------------------------------------------------
2021-05-16 14:47:50,067 EPOCH 25 done: loss 0.1966 - lr 0.0250000
2021-05-16 14:47:56,774 DEV : loss 0.4760029911994934 - score 0.9314
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-16 14:47:56,843 BAD EPOCHS (no improvement): 4
2021-05-16 14:47:56,843 ----------------------------------------------------------------------------------------------------
2021-05-16 14:48:01,392 epoch 26 - iter 6/66 - loss 0.22693531 - samples/sec: 21.11 - lr: 0.012500
2021-05-16 14:48:05,382 epoch 26 - iter 12/66 - loss 0.20019802 - samples/sec: 24.07 - lr: 0.012500
2021-05-16 14:48:09,872 epoch 26 - iter 18/66 - loss 0.21663629 - samples/sec: 21.38 - lr: 0.012500
2021-05-16 14:48:13,872 epoch 26 - iter 24/66 - loss 0.24120621 - samples/sec: 24.00 - lr: 0.012500
2021-05-16 14:48:17,979 epoch 26 - iter 30/66 - loss 0.20834397 - samples/sec: 23.40 - lr: 0.012500
2021-05-16 14:48:21,925 epoch 26 - iter 36/66 - loss 0.20752001 - samples/sec: 24.33 - lr: 0.012500
2021-05-16 14:48:26,192 epoch 26 - iter 42/66 - loss 0.20691787 - samples/sec: 22.50 - lr: 0.012500
2021-05-16 14:48:30,465 epoch 26 - iter 48/66 - loss 0.20127424 - samples/sec: 22.47 - lr: 0.012500
2021-05-16 14:48:34,933 epoch 26 - iter 54/66 - loss 0.18666581 - samples/sec: 21.51 - lr: 0.012500
2021-05-16 14:48:38,996 epoch 26 - iter 60/66 - loss 0.18155987 - samples/sec: 23.63 - lr: 0.012500
2021-05-16 14:48:42,468 epoch 26 - iter 66/66 - loss 0.18304238 - samples/sec: 27.66 - lr: 0.012500
2021-05-16 14:48:42,469 ----------------------------------------------------------------------------------------------------
2021-05-16 14:48:42,469 EPOCH 26 done: loss 0.1830 - lr 0.0125000
2021-05-16 14:48:48,756 DEV : loss 0.34901270270347595 - score 0.9492
2021-05-16 14:48:48,824 BAD EPOCHS (no improvement): 1
2021-05-16 14:48:48,824 ----------------------------------------------------------------------------------------------------
2021-05-16 14:48:52,706 epoch 27 - iter 6/66 - loss 0.09633964 - samples/sec: 24.74 - lr: 0.012500
2021-05-16 14:48:58,045 epoch 27 - iter 12/66 - loss 0.20254549 - samples/sec: 17.98 - lr: 0.012500
2021-05-16 14:49:02,574 epoch 27 - iter 18/66 - loss 0.20182485 - samples/sec: 21.20 - lr: 0.012500
2021-05-16 14:49:06,993 epoch 27 - iter 24/66 - loss 0.19903660 - samples/sec: 21.73 - lr: 0.012500
2021-05-16 14:49:10,866 epoch 27 - iter 30/66 - loss 0.19848744 - samples/sec: 24.80 - lr: 0.012500
2021-05-16 14:49:14,712 epoch 27 - iter 36/66 - loss 0.18515352 - samples/sec: 25.00 - lr: 0.012500
2021-05-16 14:49:18,728 epoch 27 - iter 42/66 - loss 0.18949528 - samples/sec: 23.91 - lr: 0.012500
2021-05-16 14:49:22,702 epoch 27 - iter 48/66 - loss 0.17515348 - samples/sec: 24.16 - lr: 0.012500
2021-05-16 14:49:26,254 epoch 27 - iter 54/66 - loss 0.17901132 - samples/sec: 27.03 - lr: 0.012500
2021-05-16 14:49:30,158 epoch 27 - iter 60/66 - loss 0.18388539 - samples/sec: 24.60 - lr: 0.012500
2021-05-16 14:49:33,921 epoch 27 - iter 66/66 - loss 0.17750884 - samples/sec: 25.52 - lr: 0.012500
2021-05-16 14:49:33,922 ----------------------------------------------------------------------------------------------------
2021-05-16 14:49:33,922 EPOCH 27 done: loss 0.1775 - lr 0.0125000
2021-05-16 14:49:40,508 DEV : loss 0.32439959049224854 - score 0.9556
2021-05-16 14:49:40,576 BAD EPOCHS (no improvement): 2
2021-05-16 14:49:40,576 ----------------------------------------------------------------------------------------------------
2021-05-16 14:49:44,468 epoch 28 - iter 6/66 - loss 0.25154240 - samples/sec: 24.67 - lr: 0.012500
2021-05-16 14:49:48,064 epoch 28 - iter 12/66 - loss 0.21289304 - samples/sec: 26.70 - lr: 0.012500
2021-05-16 14:49:51,830 epoch 28 - iter 18/66 - loss 0.23062351 - samples/sec: 25.50 - lr: 0.012500
2021-05-16 14:49:55,652 epoch 28 - iter 24/66 - loss 0.20103379 - samples/sec: 25.12 - lr: 0.012500
2021-05-16 14:49:59,576 epoch 28 - iter 30/66 - loss 0.20231762 - samples/sec: 24.47 - lr: 0.012500
2021-05-16 14:50:03,792 epoch 28 - iter 36/66 - loss 0.21384385 - samples/sec: 22.78 - lr: 0.012500
2021-05-16 14:50:07,686 epoch 28 - iter 42/66 - loss 0.20663674 - samples/sec: 24.66 - lr: 0.012500
2021-05-16 14:50:11,318 epoch 28 - iter 48/66 - loss 0.19023339 - samples/sec: 26.43 - lr: 0.012500
2021-05-16 14:50:15,240 epoch 28 - iter 54/66 - loss 0.19473209 - samples/sec: 24.48 - lr: 0.012500
2021-05-16 14:50:19,801 epoch 28 - iter 60/66 - loss 0.19459397 - samples/sec: 21.05 - lr: 0.012500
2021-05-16 14:50:23,230 epoch 28 - iter 66/66 - loss 0.18533621 - samples/sec: 28.00 - lr: 0.012500
2021-05-16 14:50:23,230 ----------------------------------------------------------------------------------------------------
2021-05-16 14:50:23,231 EPOCH 28 done: loss 0.1853 - lr 0.0125000
2021-05-16 14:50:29,941 DEV : loss 0.36800628900527954 - score 0.9478
2021-05-16 14:50:30,010 BAD EPOCHS (no improvement): 3
2021-05-16 14:50:30,010 ----------------------------------------------------------------------------------------------------
2021-05-16 14:50:33,823 epoch 29 - iter 6/66 - loss 0.11841580 - samples/sec: 25.19 - lr: 0.012500
2021-05-16 14:50:37,694 epoch 29 - iter 12/66 - loss 0.12954124 - samples/sec: 24.80 - lr: 0.012500
2021-05-16 14:50:41,578 epoch 29 - iter 18/66 - loss 0.13363690 - samples/sec: 24.72 - lr: 0.012500
2021-05-16 14:50:46,013 epoch 29 - iter 24/66 - loss 0.12790521 - samples/sec: 21.65 - lr: 0.012500
2021-05-16 14:50:49,981 epoch 29 - iter 30/66 - loss 0.18543498 - samples/sec: 24.19 - lr: 0.012500
2021-05-16 14:50:53,916 epoch 29 - iter 36/66 - loss 0.19031586 - samples/sec: 24.40 - lr: 0.012500
2021-05-16 14:50:57,673 epoch 29 - iter 42/66 - loss 0.17935258 - samples/sec: 25.56 - lr: 0.012500
2021-05-16 14:51:02,912 epoch 29 - iter 48/66 - loss 0.17771670 - samples/sec: 18.33 - lr: 0.012500
2021-05-16 14:51:06,524 epoch 29 - iter 54/66 - loss 0.16638793 - samples/sec: 26.59 - lr: 0.012500
2021-05-16 14:51:10,386 epoch 29 - iter 60/66 - loss 0.17174169 - samples/sec: 24.86 - lr: 0.012500
2021-05-16 14:51:14,272 epoch 29 - iter 66/66 - loss 0.17830297 - samples/sec: 24.71 - lr: 0.012500
2021-05-16 14:51:14,273 ----------------------------------------------------------------------------------------------------
2021-05-16 14:51:14,273 EPOCH 29 done: loss 0.1783 - lr 0.0125000
2021-05-16 14:51:21,464 DEV : loss 0.3346039950847626 - score 0.9475
Epoch    29: reducing learning rate of group 0 to 6.2500e-03.
2021-05-16 14:51:21,533 BAD EPOCHS (no improvement): 4
2021-05-16 14:51:21,533 ----------------------------------------------------------------------------------------------------
2021-05-16 14:51:26,293 epoch 30 - iter 6/66 - loss 0.20233438 - samples/sec: 20.17 - lr: 0.006250
2021-05-16 14:51:31,330 epoch 30 - iter 12/66 - loss 0.21090205 - samples/sec: 19.06 - lr: 0.006250
2021-05-16 14:51:35,866 epoch 30 - iter 18/66 - loss 0.17138286 - samples/sec: 21.17 - lr: 0.006250
2021-05-16 14:51:41,574 epoch 30 - iter 24/66 - loss 0.19726983 - samples/sec: 16.82 - lr: 0.006250
2021-05-16 14:51:45,745 epoch 30 - iter 30/66 - loss 0.19212647 - samples/sec: 23.02 - lr: 0.006250
2021-05-16 14:51:50,117 epoch 30 - iter 36/66 - loss 0.18492679 - samples/sec: 21.96 - lr: 0.006250
2021-05-16 14:51:54,715 epoch 30 - iter 42/66 - loss 0.17793869 - samples/sec: 20.89 - lr: 0.006250
2021-05-16 14:51:59,048 epoch 30 - iter 48/66 - loss 0.17148635 - samples/sec: 22.16 - lr: 0.006250
2021-05-16 14:52:02,844 epoch 30 - iter 54/66 - loss 0.16855760 - samples/sec: 25.30 - lr: 0.006250
2021-05-16 14:52:07,365 epoch 30 - iter 60/66 - loss 0.16856894 - samples/sec: 21.24 - lr: 0.006250
2021-05-16 14:52:11,206 epoch 30 - iter 66/66 - loss 0.15967065 - samples/sec: 25.00 - lr: 0.006250
2021-05-16 14:52:11,206 ----------------------------------------------------------------------------------------------------
2021-05-16 14:52:11,206 EPOCH 30 done: loss 0.1597 - lr 0.0062500
2021-05-16 14:52:17,728 DEV : loss 0.3581927716732025 - score 0.9504
2021-05-16 14:52:17,797 BAD EPOCHS (no improvement): 1
2021-05-16 14:52:28,532 ----------------------------------------------------------------------------------------------------
2021-05-16 14:52:28,532 Testing using best model ...
2021-05-16 14:52:28,533 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/eus.rst.ert/best-model.pt
2021-05-16 14:52:53,983 0.9654	0.9594	0.9624
2021-05-16 14:52:53,984 
Results:
- F1-score (micro) 0.9624
- F1-score (macro) 0.9624

By class:
SENT       tp: 307 - fp: 11 - fn: 13 - precision: 0.9654 - recall: 0.9594 - f1-score: 0.9624
2021-05-16 14:52:53,984 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb/
2021-05-16 14:52:54,018 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb
2021-05-16 14:52:54,021 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb/sent_train.txt
2021-05-16 14:52:54,021 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb/sent_dev.txt
2021-05-16 14:52:54,021 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb/sent_test.txt
Corpus: 51481 train + 1929 dev + 2695 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-16 14:53:45,459 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,467 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-16 14:53:45,470 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,470 Corpus: "Corpus: 51481 train + 1929 dev + 2695 test sentences"
2021-05-16 14:53:45,470 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,470 Parameters:
2021-05-16 14:53:45,470  - learning_rate: "0.1"
2021-05-16 14:53:45,470  - mini_batch_size: "16"
2021-05-16 14:53:45,471  - patience: "3"
2021-05-16 14:53:45,471  - anneal_factor: "0.5"
2021-05-16 14:53:45,471  - max_epochs: "30"
2021-05-16 14:53:45,471  - shuffle: "True"
2021-05-16 14:53:45,471  - train_with_dev: "False"
2021-05-16 14:53:45,471  - batch_growth_annealing: "False"
2021-05-16 14:53:45,471 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,472 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb"
2021-05-16 14:53:45,472 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,472 Device: cuda:0
2021-05-16 14:53:45,472 ----------------------------------------------------------------------------------------------------
2021-05-16 14:53:45,472 Embeddings storage mode: cpu
2021-05-16 14:53:45,476 ----------------------------------------------------------------------------------------------------
2021-05-16 15:03:34,863 epoch 1 - iter 321/3218 - loss 4.04372110 - samples/sec: 8.71 - lr: 0.100000
2021-05-16 15:12:41,068 epoch 1 - iter 642/3218 - loss 3.85496107 - samples/sec: 9.40 - lr: 0.100000
2021-05-16 15:21:55,142 epoch 1 - iter 963/3218 - loss 3.80540447 - samples/sec: 9.27 - lr: 0.100000
2021-05-16 15:31:03,095 epoch 1 - iter 1284/3218 - loss 3.77574109 - samples/sec: 9.37 - lr: 0.100000
2021-05-16 15:40:31,683 epoch 1 - iter 1605/3218 - loss 3.74895495 - samples/sec: 9.03 - lr: 0.100000
2021-05-16 15:50:26,771 epoch 1 - iter 1926/3218 - loss 3.73456063 - samples/sec: 8.63 - lr: 0.100000
2021-05-16 16:00:50,587 epoch 1 - iter 2247/3218 - loss 3.71728399 - samples/sec: 8.23 - lr: 0.100000
2021-05-16 16:11:32,152 epoch 1 - iter 2568/3218 - loss 3.71758421 - samples/sec: 8.01 - lr: 0.100000
2021-05-16 16:21:39,567 epoch 1 - iter 2889/3218 - loss 3.71472698 - samples/sec: 8.46 - lr: 0.100000
2021-05-16 16:32:33,121 epoch 1 - iter 3210/3218 - loss 3.70714503 - samples/sec: 7.86 - lr: 0.100000
2021-05-16 16:32:49,886 ----------------------------------------------------------------------------------------------------
2021-05-16 16:32:49,887 EPOCH 1 done: loss 3.7071 - lr 0.1000000
2021-05-16 16:36:06,750 DEV : loss 3.6217682361602783 - score 0.0
2021-05-16 16:36:07,462 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 16:36:17,188 ----------------------------------------------------------------------------------------------------
2021-05-16 16:39:58,342 epoch 2 - iter 321/3218 - loss 3.64807086 - samples/sec: 23.23 - lr: 0.100000
2021-05-16 16:43:34,172 epoch 2 - iter 642/3218 - loss 3.66145541 - samples/sec: 23.80 - lr: 0.100000
2021-05-16 16:47:04,551 epoch 2 - iter 963/3218 - loss 3.66727518 - samples/sec: 24.42 - lr: 0.100000
2021-05-16 16:50:41,185 epoch 2 - iter 1284/3218 - loss 3.64783629 - samples/sec: 23.71 - lr: 0.100000
2021-05-16 16:54:13,634 epoch 2 - iter 1605/3218 - loss 3.63852396 - samples/sec: 24.18 - lr: 0.100000
2021-05-16 16:57:50,336 epoch 2 - iter 1926/3218 - loss 3.62324173 - samples/sec: 23.70 - lr: 0.100000
2021-05-16 17:01:20,503 epoch 2 - iter 2247/3218 - loss 3.62074748 - samples/sec: 24.44 - lr: 0.100000
2021-05-16 17:04:50,626 epoch 2 - iter 2568/3218 - loss 3.61043840 - samples/sec: 24.45 - lr: 0.100000
2021-05-16 17:08:21,178 epoch 2 - iter 2889/3218 - loss 3.60884896 - samples/sec: 24.40 - lr: 0.100000
2021-05-16 17:11:49,748 epoch 2 - iter 3210/3218 - loss 3.61548129 - samples/sec: 24.63 - lr: 0.100000
2021-05-16 17:11:54,455 ----------------------------------------------------------------------------------------------------
2021-05-16 17:11:54,455 EPOCH 2 done: loss 3.6158 - lr 0.1000000
2021-05-16 17:12:27,789 DEV : loss 3.6601455211639404 - score 0.0
2021-05-16 17:12:28,205 BAD EPOCHS (no improvement): 1
2021-05-16 17:12:28,212 ----------------------------------------------------------------------------------------------------
2021-05-16 17:16:04,732 epoch 3 - iter 321/3218 - loss 3.56843796 - samples/sec: 23.72 - lr: 0.100000
2021-05-16 17:19:38,477 epoch 3 - iter 642/3218 - loss 3.59370803 - samples/sec: 24.03 - lr: 0.100000
2021-05-16 17:23:16,378 epoch 3 - iter 963/3218 - loss 3.60130391 - samples/sec: 23.57 - lr: 0.100000
2021-05-16 17:27:00,263 epoch 3 - iter 1284/3218 - loss 3.60904775 - samples/sec: 22.94 - lr: 0.100000
2021-05-16 17:30:46,639 epoch 3 - iter 1605/3218 - loss 3.60861855 - samples/sec: 22.69 - lr: 0.100000
2021-05-16 17:34:32,594 epoch 3 - iter 1926/3218 - loss 3.60487342 - samples/sec: 22.73 - lr: 0.100000
2021-05-16 17:38:16,597 epoch 3 - iter 2247/3218 - loss 3.60387348 - samples/sec: 22.93 - lr: 0.100000
2021-05-16 17:42:01,598 epoch 3 - iter 2568/3218 - loss 3.60226297 - samples/sec: 22.83 - lr: 0.100000
2021-05-16 17:45:42,877 epoch 3 - iter 2889/3218 - loss 3.60609079 - samples/sec: 23.21 - lr: 0.100000
2021-05-16 17:49:26,050 epoch 3 - iter 3210/3218 - loss 3.60688669 - samples/sec: 23.02 - lr: 0.100000
2021-05-16 17:49:31,392 ----------------------------------------------------------------------------------------------------
2021-05-16 17:49:31,393 EPOCH 3 done: loss 3.6075 - lr 0.1000000
2021-05-16 17:50:03,437 DEV : loss 3.779505968093872 - score 0.0
2021-05-16 17:50:03,788 BAD EPOCHS (no improvement): 2
2021-05-16 17:50:03,788 ----------------------------------------------------------------------------------------------------
2021-05-16 17:53:47,858 epoch 4 - iter 321/3218 - loss 3.60726118 - samples/sec: 22.92 - lr: 0.100000
2021-05-16 17:57:33,033 epoch 4 - iter 642/3218 - loss 3.58426594 - samples/sec: 22.81 - lr: 0.100000
2021-05-16 18:01:14,753 epoch 4 - iter 963/3218 - loss 3.59348814 - samples/sec: 23.17 - lr: 0.100000
2021-05-16 18:04:53,306 epoch 4 - iter 1284/3218 - loss 3.59686535 - samples/sec: 23.50 - lr: 0.100000
2021-05-16 18:08:36,986 epoch 4 - iter 1605/3218 - loss 3.60029333 - samples/sec: 22.96 - lr: 0.100000
2021-05-16 18:12:19,875 epoch 4 - iter 1926/3218 - loss 3.60260944 - samples/sec: 23.04 - lr: 0.100000
2021-05-16 18:15:54,906 epoch 4 - iter 2247/3218 - loss 3.59923119 - samples/sec: 23.89 - lr: 0.100000
2021-05-16 18:19:28,485 epoch 4 - iter 2568/3218 - loss 3.61181042 - samples/sec: 24.05 - lr: 0.100000
2021-05-16 18:23:03,604 epoch 4 - iter 2889/3218 - loss 3.61504477 - samples/sec: 23.88 - lr: 0.100000
2021-05-16 18:26:41,689 epoch 4 - iter 3210/3218 - loss 3.61407712 - samples/sec: 23.55 - lr: 0.100000
2021-05-16 18:26:46,781 ----------------------------------------------------------------------------------------------------
2021-05-16 18:26:46,788 EPOCH 4 done: loss 3.6143 - lr 0.1000000
2021-05-16 18:27:20,846 DEV : loss 3.6324024200439453 - score 0.0
2021-05-16 18:27:21,254 BAD EPOCHS (no improvement): 3
2021-05-16 18:27:21,254 ----------------------------------------------------------------------------------------------------
2021-05-16 18:31:01,893 epoch 5 - iter 321/3218 - loss 3.70345927 - samples/sec: 23.28 - lr: 0.100000
2021-05-16 18:34:45,134 epoch 5 - iter 642/3218 - loss 3.67882270 - samples/sec: 23.01 - lr: 0.100000
2021-05-16 18:38:23,849 epoch 5 - iter 963/3218 - loss 3.66237010 - samples/sec: 23.49 - lr: 0.100000
2021-05-16 18:42:01,245 epoch 5 - iter 1284/3218 - loss 3.64913032 - samples/sec: 23.63 - lr: 0.100000
2021-05-16 18:45:44,834 epoch 5 - iter 1605/3218 - loss 3.65117520 - samples/sec: 22.97 - lr: 0.100000
2021-05-16 18:49:20,622 epoch 5 - iter 1926/3218 - loss 3.64353279 - samples/sec: 23.80 - lr: 0.100000
2021-05-16 18:52:53,587 epoch 5 - iter 2247/3218 - loss 3.64033076 - samples/sec: 24.12 - lr: 0.100000
2021-05-16 18:56:22,430 epoch 5 - iter 2568/3218 - loss 3.64218155 - samples/sec: 24.60 - lr: 0.100000
2021-05-16 18:59:53,202 epoch 5 - iter 2889/3218 - loss 3.64683908 - samples/sec: 24.37 - lr: 0.100000
2021-05-16 19:03:29,218 epoch 5 - iter 3210/3218 - loss 3.64497304 - samples/sec: 23.78 - lr: 0.100000
2021-05-16 19:03:34,461 ----------------------------------------------------------------------------------------------------
2021-05-16 19:03:34,461 EPOCH 5 done: loss 3.6454 - lr 0.1000000
2021-05-16 19:04:09,267 DEV : loss 3.6992316246032715 - score 0.0
Epoch     5: reducing learning rate of group 0 to 5.0000e-02.
2021-05-16 19:04:09,621 BAD EPOCHS (no improvement): 4
2021-05-16 19:04:09,621 ----------------------------------------------------------------------------------------------------
2021-05-16 19:07:57,563 epoch 6 - iter 321/3218 - loss 3.56923978 - samples/sec: 22.53 - lr: 0.050000
2021-05-16 19:11:50,731 epoch 6 - iter 642/3218 - loss 3.59417825 - samples/sec: 22.03 - lr: 0.050000
2021-05-16 19:15:27,404 epoch 6 - iter 963/3218 - loss 3.58868916 - samples/sec: 23.71 - lr: 0.050000
2021-05-16 19:19:09,089 epoch 6 - iter 1284/3218 - loss 3.58458099 - samples/sec: 23.17 - lr: 0.050000
2021-05-16 19:22:54,622 epoch 6 - iter 1605/3218 - loss 3.58369873 - samples/sec: 22.77 - lr: 0.050000
2021-05-16 19:26:44,275 epoch 6 - iter 1926/3218 - loss 3.58157135 - samples/sec: 22.37 - lr: 0.050000
2021-05-16 19:30:17,931 epoch 6 - iter 2247/3218 - loss 3.58784137 - samples/sec: 24.04 - lr: 0.050000
2021-05-16 19:33:54,173 epoch 6 - iter 2568/3218 - loss 3.58539318 - samples/sec: 23.75 - lr: 0.050000
2021-05-16 19:37:29,520 epoch 6 - iter 2889/3218 - loss 3.58046179 - samples/sec: 23.85 - lr: 0.050000
2021-05-16 19:41:01,987 epoch 6 - iter 3210/3218 - loss 3.57872847 - samples/sec: 24.18 - lr: 0.050000
2021-05-16 19:41:07,165 ----------------------------------------------------------------------------------------------------
2021-05-16 19:41:07,165 EPOCH 6 done: loss 3.5787 - lr 0.0500000
2021-05-16 19:41:40,158 DEV : loss 3.6159722805023193 - score 0.0
2021-05-16 19:41:40,552 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 19:41:51,581 ----------------------------------------------------------------------------------------------------
2021-05-16 19:45:30,535 epoch 7 - iter 321/3218 - loss 3.64180282 - samples/sec: 23.46 - lr: 0.050000
2021-05-16 19:49:06,085 epoch 7 - iter 642/3218 - loss 3.60968641 - samples/sec: 23.83 - lr: 0.050000
2021-05-16 19:52:34,962 epoch 7 - iter 963/3218 - loss 3.57767778 - samples/sec: 24.59 - lr: 0.050000
2021-05-16 19:56:08,457 epoch 7 - iter 1284/3218 - loss 3.57707087 - samples/sec: 24.06 - lr: 0.050000
2021-05-16 19:59:41,614 epoch 7 - iter 1605/3218 - loss 3.58445951 - samples/sec: 24.10 - lr: 0.050000
2021-05-16 20:03:18,475 epoch 7 - iter 1926/3218 - loss 3.57946763 - samples/sec: 23.69 - lr: 0.050000
2021-05-16 20:06:48,097 epoch 7 - iter 2247/3218 - loss 3.57582437 - samples/sec: 24.50 - lr: 0.050000
2021-05-16 20:10:26,320 epoch 7 - iter 2568/3218 - loss 3.57488251 - samples/sec: 23.54 - lr: 0.050000
2021-05-16 20:14:06,412 epoch 7 - iter 2889/3218 - loss 3.57754017 - samples/sec: 23.34 - lr: 0.050000
2021-05-16 20:17:40,810 epoch 7 - iter 3210/3218 - loss 3.57777603 - samples/sec: 23.96 - lr: 0.050000
2021-05-16 20:17:45,378 ----------------------------------------------------------------------------------------------------
2021-05-16 20:17:45,378 EPOCH 7 done: loss 3.5781 - lr 0.0500000
2021-05-16 20:18:15,709 DEV : loss 3.6519219875335693 - score 0.0
2021-05-16 20:18:16,181 BAD EPOCHS (no improvement): 1
2021-05-16 20:18:16,181 ----------------------------------------------------------------------------------------------------
2021-05-16 20:21:50,449 epoch 8 - iter 321/3218 - loss 3.54362487 - samples/sec: 23.98 - lr: 0.050000
2021-05-16 20:25:32,266 epoch 8 - iter 642/3218 - loss 3.56715330 - samples/sec: 23.16 - lr: 0.050000
2021-05-16 20:29:07,898 epoch 8 - iter 963/3218 - loss 3.56614175 - samples/sec: 23.82 - lr: 0.050000
2021-05-16 20:32:42,635 epoch 8 - iter 1284/3218 - loss 3.57476982 - samples/sec: 23.92 - lr: 0.050000
2021-05-16 20:36:19,405 epoch 8 - iter 1605/3218 - loss 3.58138416 - samples/sec: 23.70 - lr: 0.050000
2021-05-16 20:39:57,526 epoch 8 - iter 1926/3218 - loss 3.58498485 - samples/sec: 23.55 - lr: 0.050000
2021-05-16 20:43:36,715 epoch 8 - iter 2247/3218 - loss 3.58384486 - samples/sec: 23.43 - lr: 0.050000
2021-05-16 20:47:17,062 epoch 8 - iter 2568/3218 - loss 3.57812572 - samples/sec: 23.31 - lr: 0.050000
2021-05-16 20:50:55,710 epoch 8 - iter 2889/3218 - loss 3.58195544 - samples/sec: 23.49 - lr: 0.050000
2021-05-16 20:54:28,391 epoch 8 - iter 3210/3218 - loss 3.58152714 - samples/sec: 24.15 - lr: 0.050000
2021-05-16 20:54:32,975 ----------------------------------------------------------------------------------------------------
2021-05-16 20:54:32,975 EPOCH 8 done: loss 3.5814 - lr 0.0500000
2021-05-16 20:55:08,970 DEV : loss 3.6176598072052 - score 0.0
2021-05-16 20:55:09,327 BAD EPOCHS (no improvement): 2
2021-05-16 20:55:09,328 ----------------------------------------------------------------------------------------------------
2021-05-16 20:58:44,514 epoch 9 - iter 321/3218 - loss 3.55124807 - samples/sec: 23.87 - lr: 0.050000
2021-05-16 21:02:17,984 epoch 9 - iter 642/3218 - loss 3.56042765 - samples/sec: 24.06 - lr: 0.050000
2021-05-16 21:06:03,707 epoch 9 - iter 963/3218 - loss 3.57391394 - samples/sec: 22.76 - lr: 0.050000
2021-05-16 21:09:47,989 epoch 9 - iter 1284/3218 - loss 3.58497133 - samples/sec: 22.90 - lr: 0.050000
2021-05-16 21:13:21,744 epoch 9 - iter 1605/3218 - loss 3.58493797 - samples/sec: 24.03 - lr: 0.050000
2021-05-16 21:16:59,229 epoch 9 - iter 1926/3218 - loss 3.58257139 - samples/sec: 23.62 - lr: 0.050000
2021-05-16 21:20:31,007 epoch 9 - iter 2247/3218 - loss 3.58455532 - samples/sec: 24.25 - lr: 0.050000
2021-05-16 21:24:26,954 epoch 9 - iter 2568/3218 - loss 3.58864620 - samples/sec: 21.77 - lr: 0.050000
2021-05-16 21:28:20,790 epoch 9 - iter 2889/3218 - loss 3.58821461 - samples/sec: 21.97 - lr: 0.050000
2021-05-16 21:32:11,522 epoch 9 - iter 3210/3218 - loss 3.58956019 - samples/sec: 22.26 - lr: 0.050000
2021-05-16 21:32:16,515 ----------------------------------------------------------------------------------------------------
2021-05-16 21:32:16,516 EPOCH 9 done: loss 3.5903 - lr 0.0500000
2021-05-16 21:32:47,876 DEV : loss 3.639127016067505 - score 0.0
2021-05-16 21:32:48,322 BAD EPOCHS (no improvement): 3
2021-05-16 21:32:48,323 ----------------------------------------------------------------------------------------------------
2021-05-16 21:36:44,402 epoch 10 - iter 321/3218 - loss 3.61145821 - samples/sec: 21.76 - lr: 0.050000
2021-05-16 21:40:25,283 epoch 10 - iter 642/3218 - loss 3.61143926 - samples/sec: 23.25 - lr: 0.050000
2021-05-16 21:44:40,900 epoch 10 - iter 963/3218 - loss 3.59583382 - samples/sec: 20.09 - lr: 0.050000
2021-05-16 21:48:54,308 epoch 10 - iter 1284/3218 - loss 3.59217717 - samples/sec: 20.27 - lr: 0.050000
2021-05-16 21:52:55,678 epoch 10 - iter 1605/3218 - loss 3.59115992 - samples/sec: 21.28 - lr: 0.050000
2021-05-16 21:57:03,157 epoch 10 - iter 1926/3218 - loss 3.58908032 - samples/sec: 20.75 - lr: 0.050000
2021-05-16 22:01:05,465 epoch 10 - iter 2247/3218 - loss 3.58189479 - samples/sec: 21.20 - lr: 0.050000
2021-05-16 22:05:25,662 epoch 10 - iter 2568/3218 - loss 3.58123582 - samples/sec: 19.74 - lr: 0.050000
2021-05-16 22:09:16,101 epoch 10 - iter 2889/3218 - loss 3.58058351 - samples/sec: 22.29 - lr: 0.050000
2021-05-16 22:13:05,107 epoch 10 - iter 3210/3218 - loss 3.58212950 - samples/sec: 22.43 - lr: 0.050000
2021-05-16 22:13:09,659 ----------------------------------------------------------------------------------------------------
2021-05-16 22:13:09,659 EPOCH 10 done: loss 3.5813 - lr 0.0500000
2021-05-16 22:13:44,437 DEV : loss 3.673933744430542 - score 0.0
Epoch    10: reducing learning rate of group 0 to 2.5000e-02.
2021-05-16 22:13:44,665 BAD EPOCHS (no improvement): 4
2021-05-16 22:13:44,666 ----------------------------------------------------------------------------------------------------
2021-05-16 22:17:04,018 epoch 11 - iter 321/3218 - loss 3.56500053 - samples/sec: 25.77 - lr: 0.025000
2021-05-16 22:20:18,638 epoch 11 - iter 642/3218 - loss 3.56732049 - samples/sec: 26.39 - lr: 0.025000
2021-05-16 22:23:30,263 epoch 11 - iter 963/3218 - loss 3.55905427 - samples/sec: 26.80 - lr: 0.025000
2021-05-16 22:26:52,435 epoch 11 - iter 1284/3218 - loss 3.55602230 - samples/sec: 25.41 - lr: 0.025000
2021-05-16 22:30:12,277 epoch 11 - iter 1605/3218 - loss 3.55725106 - samples/sec: 25.70 - lr: 0.025000
2021-05-16 22:33:24,557 epoch 11 - iter 1926/3218 - loss 3.56383333 - samples/sec: 26.71 - lr: 0.025000
2021-05-16 22:36:53,082 epoch 11 - iter 2247/3218 - loss 3.56102910 - samples/sec: 24.63 - lr: 0.025000
2021-05-16 22:40:05,823 epoch 11 - iter 2568/3218 - loss 3.56513369 - samples/sec: 26.65 - lr: 0.025000
2021-05-16 22:43:55,742 epoch 11 - iter 2889/3218 - loss 3.56661581 - samples/sec: 22.34 - lr: 0.025000
2021-05-16 22:47:33,764 epoch 11 - iter 3210/3218 - loss 3.56766821 - samples/sec: 23.56 - lr: 0.025000
2021-05-16 22:47:39,696 ----------------------------------------------------------------------------------------------------
2021-05-16 22:47:39,697 EPOCH 11 done: loss 3.5672 - lr 0.0250000
2021-05-16 22:48:26,213 DEV : loss 3.639958143234253 - score 0.0
2021-05-16 22:48:26,551 BAD EPOCHS (no improvement): 1
2021-05-16 22:48:26,552 ----------------------------------------------------------------------------------------------------
2021-05-16 22:52:02,973 epoch 12 - iter 321/3218 - loss 3.57990018 - samples/sec: 23.73 - lr: 0.025000
2021-05-16 22:55:38,597 epoch 12 - iter 642/3218 - loss 3.57095400 - samples/sec: 23.82 - lr: 0.025000
2021-05-16 22:59:19,970 epoch 12 - iter 963/3218 - loss 3.56942375 - samples/sec: 23.20 - lr: 0.025000
2021-05-16 23:02:57,000 epoch 12 - iter 1284/3218 - loss 3.56530219 - samples/sec: 23.67 - lr: 0.025000
2021-05-16 23:06:31,718 epoch 12 - iter 1605/3218 - loss 3.56075189 - samples/sec: 23.92 - lr: 0.025000
2021-05-16 23:10:05,930 epoch 12 - iter 1926/3218 - loss 3.56510110 - samples/sec: 23.98 - lr: 0.025000
2021-05-16 23:13:38,402 epoch 12 - iter 2247/3218 - loss 3.56688693 - samples/sec: 24.17 - lr: 0.025000
2021-05-16 23:17:15,083 epoch 12 - iter 2568/3218 - loss 3.56911465 - samples/sec: 23.70 - lr: 0.025000
2021-05-16 23:20:52,630 epoch 12 - iter 2889/3218 - loss 3.56350120 - samples/sec: 23.61 - lr: 0.025000
2021-05-16 23:24:25,536 epoch 12 - iter 3210/3218 - loss 3.56657111 - samples/sec: 24.12 - lr: 0.025000
2021-05-16 23:24:31,163 ----------------------------------------------------------------------------------------------------
2021-05-16 23:24:31,164 EPOCH 12 done: loss 3.5661 - lr 0.0250000
2021-05-16 23:25:12,283 DEV : loss 3.609809637069702 - score 0.0
2021-05-16 23:25:12,605 BAD EPOCHS (no improvement): 0
saving best model
2021-05-16 23:25:23,670 ----------------------------------------------------------------------------------------------------
2021-05-16 23:29:02,631 epoch 13 - iter 321/3218 - loss 3.54815360 - samples/sec: 23.46 - lr: 0.025000
2021-05-16 23:32:36,859 epoch 13 - iter 642/3218 - loss 3.56592886 - samples/sec: 23.98 - lr: 0.025000
2021-05-16 23:36:16,961 epoch 13 - iter 963/3218 - loss 3.55510917 - samples/sec: 23.34 - lr: 0.025000
2021-05-16 23:39:32,291 epoch 13 - iter 1284/3218 - loss 3.55667317 - samples/sec: 26.30 - lr: 0.025000
2021-05-16 23:42:35,768 epoch 13 - iter 1605/3218 - loss 3.55317120 - samples/sec: 27.99 - lr: 0.025000
2021-05-16 23:45:45,148 epoch 13 - iter 1926/3218 - loss 3.55586173 - samples/sec: 27.12 - lr: 0.025000
2021-05-16 23:48:48,955 epoch 13 - iter 2247/3218 - loss 3.56265203 - samples/sec: 27.94 - lr: 0.025000
2021-05-16 23:51:56,389 epoch 13 - iter 2568/3218 - loss 3.57159592 - samples/sec: 27.40 - lr: 0.025000
2021-05-16 23:55:00,500 epoch 13 - iter 2889/3218 - loss 3.56620342 - samples/sec: 27.90 - lr: 0.025000
2021-05-16 23:58:09,366 epoch 13 - iter 3210/3218 - loss 3.56354219 - samples/sec: 27.20 - lr: 0.025000
2021-05-16 23:58:13,367 ----------------------------------------------------------------------------------------------------
2021-05-16 23:58:13,367 EPOCH 13 done: loss 3.5638 - lr 0.0250000
2021-05-16 23:58:45,806 DEV : loss 3.611846685409546 - score 0.0
2021-05-16 23:58:46,127 BAD EPOCHS (no improvement): 1
2021-05-16 23:58:46,128 ----------------------------------------------------------------------------------------------------
2021-05-17 00:01:50,537 epoch 14 - iter 321/3218 - loss 3.55653931 - samples/sec: 27.85 - lr: 0.025000
2021-05-17 00:04:59,245 epoch 14 - iter 642/3218 - loss 3.54705946 - samples/sec: 27.22 - lr: 0.025000
2021-05-17 00:08:48,747 epoch 14 - iter 963/3218 - loss 3.55247364 - samples/sec: 22.38 - lr: 0.025000
2021-05-17 00:12:38,525 epoch 14 - iter 1284/3218 - loss 3.55388135 - samples/sec: 22.35 - lr: 0.025000
2021-05-17 00:16:09,379 epoch 14 - iter 1605/3218 - loss 3.55363515 - samples/sec: 24.36 - lr: 0.025000
2021-05-17 00:19:29,862 epoch 14 - iter 1926/3218 - loss 3.55223572 - samples/sec: 25.62 - lr: 0.025000
2021-05-17 00:22:46,176 epoch 14 - iter 2247/3218 - loss 3.55655126 - samples/sec: 26.16 - lr: 0.025000
2021-05-17 00:25:55,631 epoch 14 - iter 2568/3218 - loss 3.56566107 - samples/sec: 27.11 - lr: 0.025000
2021-05-17 00:29:01,856 epoch 14 - iter 2889/3218 - loss 3.56720632 - samples/sec: 27.58 - lr: 0.025000
2021-05-17 00:32:15,313 epoch 14 - iter 3210/3218 - loss 3.56585117 - samples/sec: 26.55 - lr: 0.025000
2021-05-17 00:32:19,815 ----------------------------------------------------------------------------------------------------
2021-05-17 00:32:19,815 EPOCH 14 done: loss 3.5660 - lr 0.0250000
2021-05-17 00:32:55,360 DEV : loss 3.605605363845825 - score 0.0
2021-05-17 00:32:55,591 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 00:33:06,601 ----------------------------------------------------------------------------------------------------
2021-05-17 00:36:26,751 epoch 15 - iter 321/3218 - loss 3.54997268 - samples/sec: 25.66 - lr: 0.025000
2021-05-17 00:39:46,974 epoch 15 - iter 642/3218 - loss 3.55452035 - samples/sec: 25.65 - lr: 0.025000
2021-05-17 00:43:04,503 epoch 15 - iter 963/3218 - loss 3.55265319 - samples/sec: 26.00 - lr: 0.025000
2021-05-17 00:46:09,350 epoch 15 - iter 1284/3218 - loss 3.54896613 - samples/sec: 27.79 - lr: 0.025000
2021-05-17 00:49:23,038 epoch 15 - iter 1605/3218 - loss 3.56106639 - samples/sec: 26.52 - lr: 0.025000
2021-05-17 00:52:39,548 epoch 15 - iter 1926/3218 - loss 3.56094607 - samples/sec: 26.14 - lr: 0.025000
2021-05-17 00:56:02,736 epoch 15 - iter 2247/3218 - loss 3.56222889 - samples/sec: 25.28 - lr: 0.025000
2021-05-17 00:59:18,261 epoch 15 - iter 2568/3218 - loss 3.56385717 - samples/sec: 26.27 - lr: 0.025000
2021-05-17 01:02:48,169 epoch 15 - iter 2889/3218 - loss 3.56516746 - samples/sec: 24.47 - lr: 0.025000
2021-05-17 01:06:18,549 epoch 15 - iter 3210/3218 - loss 3.56342766 - samples/sec: 24.41 - lr: 0.025000
2021-05-17 01:06:22,932 ----------------------------------------------------------------------------------------------------
2021-05-17 01:06:22,932 EPOCH 15 done: loss 3.5639 - lr 0.0250000
2021-05-17 01:07:00,893 DEV : loss 3.6059913635253906 - score 0.0
2021-05-17 01:07:01,122 BAD EPOCHS (no improvement): 1
2021-05-17 01:07:01,122 ----------------------------------------------------------------------------------------------------
2021-05-17 01:10:23,686 epoch 16 - iter 321/3218 - loss 3.55105286 - samples/sec: 25.36 - lr: 0.025000
2021-05-17 01:13:43,273 epoch 16 - iter 642/3218 - loss 3.56610071 - samples/sec: 25.74 - lr: 0.025000
2021-05-17 01:16:53,497 epoch 16 - iter 963/3218 - loss 3.56074135 - samples/sec: 27.00 - lr: 0.025000
2021-05-17 01:19:31,045 epoch 16 - iter 1284/3218 - loss 3.56431240 - samples/sec: 32.60 - lr: 0.025000
2021-05-17 01:21:00,714 epoch 16 - iter 1605/3218 - loss 3.55825232 - samples/sec: 57.28 - lr: 0.025000
2021-05-17 01:22:29,923 epoch 16 - iter 1926/3218 - loss 3.55540253 - samples/sec: 57.58 - lr: 0.025000
2021-05-17 01:23:59,588 epoch 16 - iter 2247/3218 - loss 3.55886133 - samples/sec: 57.29 - lr: 0.025000
2021-05-17 01:25:28,904 epoch 16 - iter 2568/3218 - loss 3.55667548 - samples/sec: 57.51 - lr: 0.025000
2021-05-17 01:26:58,350 epoch 16 - iter 2889/3218 - loss 3.55875128 - samples/sec: 57.43 - lr: 0.025000
2021-05-17 01:28:27,585 epoch 16 - iter 3210/3218 - loss 3.55692516 - samples/sec: 57.56 - lr: 0.025000
2021-05-17 01:28:29,738 ----------------------------------------------------------------------------------------------------
2021-05-17 01:28:29,738 EPOCH 16 done: loss 3.5568 - lr 0.0250000
2021-05-17 01:28:44,526 DEV : loss 3.608982563018799 - score 0.0
2021-05-17 01:28:44,709 BAD EPOCHS (no improvement): 2
2021-05-17 01:28:44,710 ----------------------------------------------------------------------------------------------------
2021-05-17 01:30:14,025 epoch 17 - iter 321/3218 - loss 3.52641293 - samples/sec: 57.51 - lr: 0.025000
2021-05-17 01:31:42,891 epoch 17 - iter 642/3218 - loss 3.52955288 - samples/sec: 57.80 - lr: 0.025000
2021-05-17 01:33:11,841 epoch 17 - iter 963/3218 - loss 3.54333306 - samples/sec: 57.75 - lr: 0.025000
2021-05-17 01:34:41,051 epoch 17 - iter 1284/3218 - loss 3.54877219 - samples/sec: 57.58 - lr: 0.025000
2021-05-17 01:36:10,399 epoch 17 - iter 1605/3218 - loss 3.54550738 - samples/sec: 57.49 - lr: 0.025000
2021-05-17 01:37:39,840 epoch 17 - iter 1926/3218 - loss 3.54621613 - samples/sec: 57.43 - lr: 0.025000
2021-05-17 01:39:08,808 epoch 17 - iter 2247/3218 - loss 3.54723690 - samples/sec: 57.74 - lr: 0.025000
2021-05-17 01:40:38,350 epoch 17 - iter 2568/3218 - loss 3.55162432 - samples/sec: 57.37 - lr: 0.025000
2021-05-17 01:42:07,419 epoch 17 - iter 2889/3218 - loss 3.55032104 - samples/sec: 57.67 - lr: 0.025000
2021-05-17 01:43:36,640 epoch 17 - iter 3210/3218 - loss 3.55217276 - samples/sec: 57.57 - lr: 0.025000
2021-05-17 01:43:38,722 ----------------------------------------------------------------------------------------------------
2021-05-17 01:43:38,722 EPOCH 17 done: loss 3.5519 - lr 0.0250000
2021-05-17 01:43:50,356 DEV : loss 3.572843551635742 - score 0.0
2021-05-17 01:43:50,542 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 01:44:00,921 ----------------------------------------------------------------------------------------------------
2021-05-17 01:45:30,383 epoch 18 - iter 321/3218 - loss 3.54551047 - samples/sec: 57.42 - lr: 0.025000
2021-05-17 01:46:59,836 epoch 18 - iter 642/3218 - loss 3.54716175 - samples/sec: 57.42 - lr: 0.025000
2021-05-17 01:48:29,040 epoch 18 - iter 963/3218 - loss 3.54075047 - samples/sec: 57.58 - lr: 0.025000
2021-05-17 01:49:58,185 epoch 18 - iter 1284/3218 - loss 3.54283734 - samples/sec: 57.62 - lr: 0.025000
2021-05-17 01:51:27,021 epoch 18 - iter 1605/3218 - loss 3.54279311 - samples/sec: 57.82 - lr: 0.025000
2021-05-17 01:52:56,339 epoch 18 - iter 1926/3218 - loss 3.54072652 - samples/sec: 57.51 - lr: 0.025000
2021-05-17 01:54:25,740 epoch 18 - iter 2247/3218 - loss 3.54216986 - samples/sec: 57.46 - lr: 0.025000
2021-05-17 01:55:55,041 epoch 18 - iter 2568/3218 - loss 3.54686192 - samples/sec: 57.52 - lr: 0.025000
2021-05-17 01:57:24,242 epoch 18 - iter 2889/3218 - loss 3.54654829 - samples/sec: 57.59 - lr: 0.025000
2021-05-17 01:58:53,581 epoch 18 - iter 3210/3218 - loss 3.55181044 - samples/sec: 57.50 - lr: 0.025000
2021-05-17 01:58:55,663 ----------------------------------------------------------------------------------------------------
2021-05-17 01:58:55,663 EPOCH 18 done: loss 3.5516 - lr 0.0250000
2021-05-17 01:59:07,292 DEV : loss 3.5928752422332764 - score 0.0
2021-05-17 01:59:07,478 BAD EPOCHS (no improvement): 1
2021-05-17 01:59:07,478 ----------------------------------------------------------------------------------------------------
2021-05-17 02:00:36,721 epoch 19 - iter 321/3218 - loss 3.54209356 - samples/sec: 57.56 - lr: 0.025000
2021-05-17 02:02:05,665 epoch 19 - iter 642/3218 - loss 3.53386166 - samples/sec: 57.75 - lr: 0.025000
2021-05-17 02:03:35,080 epoch 19 - iter 963/3218 - loss 3.51977237 - samples/sec: 57.45 - lr: 0.025000
2021-05-17 02:05:04,077 epoch 19 - iter 1284/3218 - loss 3.53092098 - samples/sec: 57.72 - lr: 0.025000
2021-05-17 02:06:33,083 epoch 19 - iter 1605/3218 - loss 3.52774462 - samples/sec: 57.71 - lr: 0.025000
2021-05-17 02:08:02,689 epoch 19 - iter 1926/3218 - loss 3.53471577 - samples/sec: 57.32 - lr: 0.025000
2021-05-17 02:09:32,069 epoch 19 - iter 2247/3218 - loss 3.54316538 - samples/sec: 57.47 - lr: 0.025000
2021-05-17 02:11:01,344 epoch 19 - iter 2568/3218 - loss 3.54445130 - samples/sec: 57.54 - lr: 0.025000
2021-05-17 02:12:30,817 epoch 19 - iter 2889/3218 - loss 3.54571586 - samples/sec: 57.41 - lr: 0.025000
2021-05-17 02:14:00,075 epoch 19 - iter 3210/3218 - loss 3.54873033 - samples/sec: 57.55 - lr: 0.025000
2021-05-17 02:14:02,223 ----------------------------------------------------------------------------------------------------
2021-05-17 02:14:02,224 EPOCH 19 done: loss 3.5487 - lr 0.0250000
2021-05-17 02:14:13,819 DEV : loss 3.572237968444824 - score 0.0
2021-05-17 02:14:14,002 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 02:14:24,622 ----------------------------------------------------------------------------------------------------
2021-05-17 02:15:53,952 epoch 20 - iter 321/3218 - loss 3.52033234 - samples/sec: 57.51 - lr: 0.025000
2021-05-17 02:17:23,091 epoch 20 - iter 642/3218 - loss 3.51514607 - samples/sec: 57.63 - lr: 0.025000
2021-05-17 02:18:52,435 epoch 20 - iter 963/3218 - loss 3.51031859 - samples/sec: 57.49 - lr: 0.025000
2021-05-17 02:20:21,982 epoch 20 - iter 1284/3218 - loss 3.53541231 - samples/sec: 57.36 - lr: 0.025000
2021-05-17 02:21:50,641 epoch 20 - iter 1605/3218 - loss 3.53027043 - samples/sec: 57.94 - lr: 0.025000
2021-05-17 02:23:20,068 epoch 20 - iter 1926/3218 - loss 3.53953440 - samples/sec: 57.44 - lr: 0.025000
2021-05-17 02:24:49,664 epoch 20 - iter 2247/3218 - loss 3.53936257 - samples/sec: 57.33 - lr: 0.025000
2021-05-17 02:26:19,183 epoch 20 - iter 2568/3218 - loss 3.54233383 - samples/sec: 57.38 - lr: 0.025000
2021-05-17 02:27:48,539 epoch 20 - iter 2889/3218 - loss 3.54170068 - samples/sec: 57.49 - lr: 0.025000
2021-05-17 02:29:18,104 epoch 20 - iter 3210/3218 - loss 3.54703109 - samples/sec: 57.35 - lr: 0.025000
2021-05-17 02:29:20,205 ----------------------------------------------------------------------------------------------------
2021-05-17 02:29:20,205 EPOCH 20 done: loss 3.5473 - lr 0.0250000
2021-05-17 02:29:31,892 DEV : loss 3.578782081604004 - score 0.0
2021-05-17 02:29:32,077 BAD EPOCHS (no improvement): 1
2021-05-17 02:29:32,077 ----------------------------------------------------------------------------------------------------
2021-05-17 02:31:01,215 epoch 21 - iter 321/3218 - loss 3.53620027 - samples/sec: 57.63 - lr: 0.025000
2021-05-17 02:32:30,705 epoch 21 - iter 642/3218 - loss 3.53938675 - samples/sec: 57.40 - lr: 0.025000
2021-05-17 02:34:00,085 epoch 21 - iter 963/3218 - loss 3.53706408 - samples/sec: 57.47 - lr: 0.025000
2021-05-17 02:35:29,388 epoch 21 - iter 1284/3218 - loss 3.53436060 - samples/sec: 57.52 - lr: 0.025000
2021-05-17 02:36:58,441 epoch 21 - iter 1605/3218 - loss 3.53514149 - samples/sec: 57.68 - lr: 0.025000
2021-05-17 02:38:27,610 epoch 21 - iter 1926/3218 - loss 3.54275698 - samples/sec: 57.61 - lr: 0.025000
2021-05-17 02:39:56,718 epoch 21 - iter 2247/3218 - loss 3.54801737 - samples/sec: 57.65 - lr: 0.025000
2021-05-17 02:41:26,306 epoch 21 - iter 2568/3218 - loss 3.54564614 - samples/sec: 57.34 - lr: 0.025000
2021-05-17 02:42:55,714 epoch 21 - iter 2889/3218 - loss 3.54618948 - samples/sec: 57.45 - lr: 0.025000
2021-05-17 02:44:24,686 epoch 21 - iter 3210/3218 - loss 3.54362269 - samples/sec: 57.73 - lr: 0.025000
2021-05-17 02:44:26,806 ----------------------------------------------------------------------------------------------------
2021-05-17 02:44:26,807 EPOCH 21 done: loss 3.5439 - lr 0.0250000
2021-05-17 02:44:38,441 DEV : loss 3.566328525543213 - score 0.0
2021-05-17 02:44:38,625 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 02:44:49,137 ----------------------------------------------------------------------------------------------------
2021-05-17 02:46:18,717 epoch 22 - iter 321/3218 - loss 3.53539692 - samples/sec: 57.34 - lr: 0.025000
2021-05-17 02:47:48,317 epoch 22 - iter 642/3218 - loss 3.52946004 - samples/sec: 57.33 - lr: 0.025000
2021-05-17 02:49:17,647 epoch 22 - iter 963/3218 - loss 3.53523340 - samples/sec: 57.50 - lr: 0.025000
2021-05-17 02:50:47,301 epoch 22 - iter 1284/3218 - loss 3.53324499 - samples/sec: 57.29 - lr: 0.025000
2021-05-17 02:52:16,933 epoch 22 - iter 1605/3218 - loss 3.54293885 - samples/sec: 57.31 - lr: 0.025000
2021-05-17 02:53:45,857 epoch 22 - iter 1926/3218 - loss 3.54186312 - samples/sec: 57.77 - lr: 0.025000
2021-05-17 02:55:15,431 epoch 22 - iter 2247/3218 - loss 3.54354187 - samples/sec: 57.35 - lr: 0.025000
2021-05-17 02:56:44,796 epoch 22 - iter 2568/3218 - loss 3.54370316 - samples/sec: 57.48 - lr: 0.025000
2021-05-17 02:58:14,220 epoch 22 - iter 2889/3218 - loss 3.54407325 - samples/sec: 57.44 - lr: 0.025000
2021-05-17 02:59:43,448 epoch 22 - iter 3210/3218 - loss 3.54474452 - samples/sec: 57.57 - lr: 0.025000
2021-05-17 02:59:45,580 ----------------------------------------------------------------------------------------------------
2021-05-17 02:59:45,580 EPOCH 22 done: loss 3.5444 - lr 0.0250000
2021-05-17 02:59:57,247 DEV : loss 3.593935966491699 - score 0.0
2021-05-17 02:59:57,434 BAD EPOCHS (no improvement): 1
2021-05-17 02:59:57,434 ----------------------------------------------------------------------------------------------------
2021-05-17 03:01:26,731 epoch 23 - iter 321/3218 - loss 3.61164844 - samples/sec: 57.52 - lr: 0.025000
2021-05-17 03:02:56,094 epoch 23 - iter 642/3218 - loss 3.56516427 - samples/sec: 57.48 - lr: 0.025000
2021-05-17 03:04:25,222 epoch 23 - iter 963/3218 - loss 3.54292859 - samples/sec: 57.63 - lr: 0.025000
2021-05-17 03:05:54,583 epoch 23 - iter 1284/3218 - loss 3.53973603 - samples/sec: 57.48 - lr: 0.025000
2021-05-17 03:07:23,847 epoch 23 - iter 1605/3218 - loss 3.54683257 - samples/sec: 57.55 - lr: 0.025000
2021-05-17 03:08:53,558 epoch 23 - iter 1926/3218 - loss 3.54500629 - samples/sec: 57.26 - lr: 0.025000
2021-05-17 03:10:22,784 epoch 23 - iter 2247/3218 - loss 3.55225027 - samples/sec: 57.57 - lr: 0.025000
2021-05-17 03:11:52,371 epoch 23 - iter 2568/3218 - loss 3.55617244 - samples/sec: 57.34 - lr: 0.025000
2021-05-17 03:13:21,582 epoch 23 - iter 2889/3218 - loss 3.55053427 - samples/sec: 57.58 - lr: 0.025000
2021-05-17 03:14:50,656 epoch 23 - iter 3210/3218 - loss 3.54578177 - samples/sec: 57.67 - lr: 0.025000
2021-05-17 03:14:52,764 ----------------------------------------------------------------------------------------------------
2021-05-17 03:14:52,765 EPOCH 23 done: loss 3.5447 - lr 0.0250000
2021-05-17 03:15:04,423 DEV : loss 3.6308274269104004 - score 0.0
2021-05-17 03:15:04,607 BAD EPOCHS (no improvement): 2
2021-05-17 03:15:04,607 ----------------------------------------------------------------------------------------------------
2021-05-17 03:16:34,245 epoch 24 - iter 321/3218 - loss 3.57997259 - samples/sec: 57.31 - lr: 0.025000
2021-05-17 03:18:06,755 epoch 24 - iter 642/3218 - loss 3.57160338 - samples/sec: 55.53 - lr: 0.025000
2021-05-17 03:19:36,117 epoch 24 - iter 963/3218 - loss 3.55963577 - samples/sec: 57.48 - lr: 0.025000
2021-05-17 03:21:05,764 epoch 24 - iter 1284/3218 - loss 3.54779570 - samples/sec: 57.30 - lr: 0.025000
2021-05-17 03:22:34,986 epoch 24 - iter 1605/3218 - loss 3.55016628 - samples/sec: 57.57 - lr: 0.025000
2021-05-17 03:24:03,994 epoch 24 - iter 1926/3218 - loss 3.54810830 - samples/sec: 57.71 - lr: 0.025000
2021-05-17 03:25:33,398 epoch 24 - iter 2247/3218 - loss 3.54426503 - samples/sec: 57.45 - lr: 0.025000
2021-05-17 03:27:02,504 epoch 24 - iter 2568/3218 - loss 3.54308005 - samples/sec: 57.65 - lr: 0.025000
2021-05-17 03:28:31,279 epoch 24 - iter 2889/3218 - loss 3.54008254 - samples/sec: 57.86 - lr: 0.025000
2021-05-17 03:30:00,768 epoch 24 - iter 3210/3218 - loss 3.54264307 - samples/sec: 57.40 - lr: 0.025000
2021-05-17 03:30:02,887 ----------------------------------------------------------------------------------------------------
2021-05-17 03:30:02,888 EPOCH 24 done: loss 3.5429 - lr 0.0250000
2021-05-17 03:30:14,501 DEV : loss 3.573448419570923 - score 0.0
2021-05-17 03:30:14,689 BAD EPOCHS (no improvement): 3
2021-05-17 03:30:14,689 ----------------------------------------------------------------------------------------------------
2021-05-17 03:31:43,934 epoch 25 - iter 321/3218 - loss 3.56193005 - samples/sec: 57.56 - lr: 0.025000
2021-05-17 03:33:13,188 epoch 25 - iter 642/3218 - loss 3.55482455 - samples/sec: 57.55 - lr: 0.025000
2021-05-17 03:34:42,622 epoch 25 - iter 963/3218 - loss 3.55149308 - samples/sec: 57.44 - lr: 0.025000
2021-05-17 03:36:12,199 epoch 25 - iter 1284/3218 - loss 3.55403394 - samples/sec: 57.34 - lr: 0.025000
2021-05-17 03:37:41,095 epoch 25 - iter 1605/3218 - loss 3.54774909 - samples/sec: 57.78 - lr: 0.025000
2021-05-17 03:39:10,160 epoch 25 - iter 1926/3218 - loss 3.55317662 - samples/sec: 57.67 - lr: 0.025000
2021-05-17 03:40:39,368 epoch 25 - iter 2247/3218 - loss 3.55020010 - samples/sec: 57.58 - lr: 0.025000
2021-05-17 03:42:08,893 epoch 25 - iter 2568/3218 - loss 3.54871008 - samples/sec: 57.38 - lr: 0.025000
2021-05-17 03:43:38,660 epoch 25 - iter 2889/3218 - loss 3.54151191 - samples/sec: 57.22 - lr: 0.025000
2021-05-17 03:45:07,981 epoch 25 - iter 3210/3218 - loss 3.54162278 - samples/sec: 57.51 - lr: 0.025000
2021-05-17 03:45:10,112 ----------------------------------------------------------------------------------------------------
2021-05-17 03:45:10,112 EPOCH 25 done: loss 3.5413 - lr 0.0250000
2021-05-17 03:45:21,751 DEV : loss 3.574632167816162 - score 0.0
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 03:45:21,935 BAD EPOCHS (no improvement): 4
2021-05-17 03:45:21,935 ----------------------------------------------------------------------------------------------------
2021-05-17 03:46:51,224 epoch 26 - iter 321/3218 - loss 3.55441077 - samples/sec: 57.53 - lr: 0.012500
2021-05-17 03:48:20,518 epoch 26 - iter 642/3218 - loss 3.55800815 - samples/sec: 57.53 - lr: 0.012500
2021-05-17 03:49:49,930 epoch 26 - iter 963/3218 - loss 3.54807576 - samples/sec: 57.45 - lr: 0.012500
2021-05-17 03:51:19,600 epoch 26 - iter 1284/3218 - loss 3.55212273 - samples/sec: 57.28 - lr: 0.012500
2021-05-17 03:52:49,141 epoch 26 - iter 1605/3218 - loss 3.54215351 - samples/sec: 57.37 - lr: 0.012500
2021-05-17 03:54:18,530 epoch 26 - iter 1926/3218 - loss 3.53972473 - samples/sec: 57.46 - lr: 0.012500
2021-05-17 03:55:47,833 epoch 26 - iter 2247/3218 - loss 3.54044195 - samples/sec: 57.52 - lr: 0.012500
2021-05-17 03:57:17,090 epoch 26 - iter 2568/3218 - loss 3.53428826 - samples/sec: 57.55 - lr: 0.012500
2021-05-17 03:58:46,525 epoch 26 - iter 2889/3218 - loss 3.53500683 - samples/sec: 57.43 - lr: 0.012500
2021-05-17 04:00:15,968 epoch 26 - iter 3210/3218 - loss 3.53522893 - samples/sec: 57.43 - lr: 0.012500
2021-05-17 04:00:18,072 ----------------------------------------------------------------------------------------------------
2021-05-17 04:00:18,073 EPOCH 26 done: loss 3.5354 - lr 0.0125000
2021-05-17 04:00:29,760 DEV : loss 3.568199396133423 - score 0.0
2021-05-17 04:00:29,945 BAD EPOCHS (no improvement): 1
2021-05-17 04:00:29,945 ----------------------------------------------------------------------------------------------------
2021-05-17 04:01:59,188 epoch 27 - iter 321/3218 - loss 3.56233885 - samples/sec: 57.56 - lr: 0.012500
2021-05-17 04:03:33,577 epoch 27 - iter 642/3218 - loss 3.53942004 - samples/sec: 54.42 - lr: 0.012500
2021-05-17 04:06:51,893 epoch 27 - iter 963/3218 - loss 3.53128642 - samples/sec: 25.90 - lr: 0.012500
2021-05-17 04:11:20,265 epoch 27 - iter 1284/3218 - loss 3.53597485 - samples/sec: 19.14 - lr: 0.012500
2021-05-17 04:16:08,134 epoch 27 - iter 1605/3218 - loss 3.54085126 - samples/sec: 17.84 - lr: 0.012500
2021-05-17 04:20:49,394 epoch 27 - iter 1926/3218 - loss 3.54018348 - samples/sec: 18.26 - lr: 0.012500
2021-05-17 04:25:27,617 epoch 27 - iter 2247/3218 - loss 3.53976086 - samples/sec: 18.46 - lr: 0.012500
2021-05-17 04:30:03,765 epoch 27 - iter 2568/3218 - loss 3.53303299 - samples/sec: 18.60 - lr: 0.012500
2021-05-17 04:34:44,266 epoch 27 - iter 2889/3218 - loss 3.53383005 - samples/sec: 18.31 - lr: 0.012500
2021-05-17 04:39:25,863 epoch 27 - iter 3210/3218 - loss 3.53452973 - samples/sec: 18.24 - lr: 0.012500
2021-05-17 04:39:32,184 ----------------------------------------------------------------------------------------------------
2021-05-17 04:39:32,185 EPOCH 27 done: loss 3.5345 - lr 0.0125000
2021-05-17 04:40:15,550 DEV : loss 3.5666542053222656 - score 0.0
2021-05-17 04:40:15,987 BAD EPOCHS (no improvement): 2
2021-05-17 04:40:16,024 ----------------------------------------------------------------------------------------------------
2021-05-17 04:45:00,376 epoch 28 - iter 321/3218 - loss 3.57059123 - samples/sec: 18.07 - lr: 0.012500
2021-05-17 04:49:30,291 epoch 28 - iter 642/3218 - loss 3.53022753 - samples/sec: 19.03 - lr: 0.012500
2021-05-17 04:53:48,935 epoch 28 - iter 963/3218 - loss 3.51714182 - samples/sec: 19.86 - lr: 0.012500
2021-05-17 04:58:19,160 epoch 28 - iter 1284/3218 - loss 3.52446616 - samples/sec: 19.01 - lr: 0.012500
2021-05-17 05:02:41,168 epoch 28 - iter 1605/3218 - loss 3.52180818 - samples/sec: 19.60 - lr: 0.012500
2021-05-17 05:07:00,257 epoch 28 - iter 1926/3218 - loss 3.52886573 - samples/sec: 19.82 - lr: 0.012500
2021-05-17 05:11:27,980 epoch 28 - iter 2247/3218 - loss 3.53440655 - samples/sec: 19.19 - lr: 0.012500
2021-05-17 05:15:49,536 epoch 28 - iter 2568/3218 - loss 3.53462561 - samples/sec: 19.64 - lr: 0.012500
2021-05-17 05:20:13,436 epoch 28 - iter 2889/3218 - loss 3.53486987 - samples/sec: 19.46 - lr: 0.012500
2021-05-17 05:24:34,293 epoch 28 - iter 3210/3218 - loss 3.53218711 - samples/sec: 19.69 - lr: 0.012500
2021-05-17 05:24:40,433 ----------------------------------------------------------------------------------------------------
2021-05-17 05:24:40,433 EPOCH 28 done: loss 3.5321 - lr 0.0125000
2021-05-17 05:25:19,954 DEV : loss 3.5587620735168457 - score 0.0
2021-05-17 05:25:20,309 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 05:25:30,350 ----------------------------------------------------------------------------------------------------
2021-05-17 05:29:38,146 epoch 29 - iter 321/3218 - loss 3.52801398 - samples/sec: 20.73 - lr: 0.012500
2021-05-17 05:33:52,425 epoch 29 - iter 642/3218 - loss 3.53455233 - samples/sec: 20.20 - lr: 0.012500
2021-05-17 05:38:01,371 epoch 29 - iter 963/3218 - loss 3.53484558 - samples/sec: 20.63 - lr: 0.012500
2021-05-17 05:42:23,996 epoch 29 - iter 1284/3218 - loss 3.54252006 - samples/sec: 19.56 - lr: 0.012500
2021-05-17 05:46:50,304 epoch 29 - iter 1605/3218 - loss 3.53793694 - samples/sec: 19.29 - lr: 0.012500
2021-05-17 05:51:03,158 epoch 29 - iter 1926/3218 - loss 3.53734167 - samples/sec: 20.31 - lr: 0.012500
2021-05-17 05:55:20,397 epoch 29 - iter 2247/3218 - loss 3.52584081 - samples/sec: 19.97 - lr: 0.012500
2021-05-17 05:59:32,208 epoch 29 - iter 2568/3218 - loss 3.52466152 - samples/sec: 20.40 - lr: 0.012500
2021-05-17 06:03:57,514 epoch 29 - iter 2889/3218 - loss 3.53128778 - samples/sec: 19.36 - lr: 0.012500
2021-05-17 06:08:15,685 epoch 29 - iter 3210/3218 - loss 3.53069230 - samples/sec: 19.90 - lr: 0.012500
2021-05-17 06:08:20,909 ----------------------------------------------------------------------------------------------------
2021-05-17 06:08:20,909 EPOCH 29 done: loss 3.5311 - lr 0.0125000
2021-05-17 06:08:58,180 DEV : loss 3.5561206340789795 - score 0.0
2021-05-17 06:08:58,533 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 06:09:09,046 ----------------------------------------------------------------------------------------------------
2021-05-17 06:14:10,807 epoch 30 - iter 321/3218 - loss 3.54678571 - samples/sec: 17.02 - lr: 0.012500
2021-05-17 06:18:59,292 epoch 30 - iter 642/3218 - loss 3.53459085 - samples/sec: 17.80 - lr: 0.012500
2021-05-17 06:23:26,650 epoch 30 - iter 963/3218 - loss 3.51798680 - samples/sec: 19.21 - lr: 0.012500
2021-05-17 06:28:12,116 epoch 30 - iter 1284/3218 - loss 3.51692826 - samples/sec: 17.99 - lr: 0.012500
2021-05-17 06:32:46,146 epoch 30 - iter 1605/3218 - loss 3.52810737 - samples/sec: 18.74 - lr: 0.012500
2021-05-17 06:37:17,751 epoch 30 - iter 1926/3218 - loss 3.53368927 - samples/sec: 18.91 - lr: 0.012500
2021-05-17 06:41:42,780 epoch 30 - iter 2247/3218 - loss 3.53895661 - samples/sec: 19.38 - lr: 0.012500
2021-05-17 06:45:56,418 epoch 30 - iter 2568/3218 - loss 3.53550635 - samples/sec: 20.25 - lr: 0.012500
2021-05-17 06:50:14,650 epoch 30 - iter 2889/3218 - loss 3.53411713 - samples/sec: 19.89 - lr: 0.012500
2021-05-17 06:54:33,266 epoch 30 - iter 3210/3218 - loss 3.53079789 - samples/sec: 19.86 - lr: 0.012500
2021-05-17 06:54:38,625 ----------------------------------------------------------------------------------------------------
2021-05-17 06:54:38,626 EPOCH 30 done: loss 3.5311 - lr 0.0125000
2021-05-17 06:55:13,959 DEV : loss 3.564023733139038 - score 0.0
2021-05-17 06:55:14,313 BAD EPOCHS (no improvement): 1
2021-05-17 06:55:24,424 ----------------------------------------------------------------------------------------------------
2021-05-17 06:55:24,424 Testing using best model ...
2021-05-17 06:55:24,425 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.pdtb.pdtb/best-model.pt
2021-05-17 06:58:48,587 0.0000	0.0000	0.0000
2021-05-17 06:58:48,587 
Results:
- F1-score (micro) 0.0000
- F1-score (macro) 0.0000

By class:
SENT       tp: 0 - fp: 0 - fn: 2364 - precision: 0.0000 - recall: 0.0000 - f1-score: 0.0000
2021-05-17 06:58:48,588 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum/
2021-05-17 06:58:48,704 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum
2021-05-17 06:58:48,705 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum/sent_train.txt
2021-05-17 06:58:48,708 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum/sent_dev.txt
2021-05-17 06:58:48,709 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum/sent_test.txt
Corpus: 3228 train + 752 dev + 767 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 06:59:07,210 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,216 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 06:59:07,216 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,216 Corpus: "Corpus: 3228 train + 752 dev + 767 test sentences"
2021-05-17 06:59:07,216 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,217 Parameters:
2021-05-17 06:59:07,217  - learning_rate: "0.1"
2021-05-17 06:59:07,217  - mini_batch_size: "16"
2021-05-17 06:59:07,217  - patience: "3"
2021-05-17 06:59:07,217  - anneal_factor: "0.5"
2021-05-17 06:59:07,217  - max_epochs: "30"
2021-05-17 06:59:07,217  - shuffle: "True"
2021-05-17 06:59:07,217  - train_with_dev: "False"
2021-05-17 06:59:07,217  - batch_growth_annealing: "False"
2021-05-17 06:59:07,217 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,217 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum"
2021-05-17 06:59:07,217 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,217 Device: cuda:0
2021-05-17 06:59:07,217 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:07,218 Embeddings storage mode: cpu
2021-05-17 06:59:07,221 ----------------------------------------------------------------------------------------------------
2021-05-17 06:59:43,185 epoch 1 - iter 20/202 - loss 12.81177726 - samples/sec: 8.90 - lr: 0.100000
2021-05-17 07:00:18,267 epoch 1 - iter 40/202 - loss 8.95279121 - samples/sec: 9.12 - lr: 0.100000
2021-05-17 07:00:59,097 epoch 1 - iter 60/202 - loss 7.46040665 - samples/sec: 7.84 - lr: 0.100000
2021-05-17 07:01:37,483 epoch 1 - iter 80/202 - loss 6.46893711 - samples/sec: 8.34 - lr: 0.100000
2021-05-17 07:02:12,803 epoch 1 - iter 100/202 - loss 5.69811792 - samples/sec: 9.06 - lr: 0.100000
2021-05-17 07:02:47,822 epoch 1 - iter 120/202 - loss 5.17697597 - samples/sec: 9.14 - lr: 0.100000
2021-05-17 07:03:21,242 epoch 1 - iter 140/202 - loss 4.71363093 - samples/sec: 9.58 - lr: 0.100000
2021-05-17 07:03:54,661 epoch 1 - iter 160/202 - loss 4.36003911 - samples/sec: 9.58 - lr: 0.100000
2021-05-17 07:04:33,573 epoch 1 - iter 180/202 - loss 4.07228804 - samples/sec: 8.22 - lr: 0.100000
2021-05-17 07:05:08,246 epoch 1 - iter 200/202 - loss 3.85046921 - samples/sec: 9.23 - lr: 0.100000
2021-05-17 07:05:11,306 ----------------------------------------------------------------------------------------------------
2021-05-17 07:05:11,306 EPOCH 1 done: loss 3.8351 - lr 0.1000000
2021-05-17 07:06:15,602 DEV : loss 0.5798121094703674 - score 0.9059
2021-05-17 07:06:15,744 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:06:25,747 ----------------------------------------------------------------------------------------------------
2021-05-17 07:06:42,480 epoch 2 - iter 20/202 - loss 1.50151716 - samples/sec: 19.17 - lr: 0.100000
2021-05-17 07:06:57,175 epoch 2 - iter 40/202 - loss 1.48583820 - samples/sec: 21.78 - lr: 0.100000
2021-05-17 07:07:14,392 epoch 2 - iter 60/202 - loss 1.51773880 - samples/sec: 18.59 - lr: 0.100000
2021-05-17 07:07:29,554 epoch 2 - iter 80/202 - loss 1.50665214 - samples/sec: 21.11 - lr: 0.100000
2021-05-17 07:07:43,960 epoch 2 - iter 100/202 - loss 1.51228807 - samples/sec: 22.21 - lr: 0.100000
2021-05-17 07:07:58,663 epoch 2 - iter 120/202 - loss 1.53442111 - samples/sec: 21.77 - lr: 0.100000
2021-05-17 07:08:15,984 epoch 2 - iter 140/202 - loss 1.51308653 - samples/sec: 18.48 - lr: 0.100000
2021-05-17 07:08:30,593 epoch 2 - iter 160/202 - loss 1.51526686 - samples/sec: 21.91 - lr: 0.100000
2021-05-17 07:08:44,144 epoch 2 - iter 180/202 - loss 1.48693804 - samples/sec: 23.62 - lr: 0.100000
2021-05-17 07:08:59,142 epoch 2 - iter 200/202 - loss 1.47855899 - samples/sec: 21.34 - lr: 0.100000
2021-05-17 07:09:00,831 ----------------------------------------------------------------------------------------------------
2021-05-17 07:09:00,832 EPOCH 2 done: loss 1.4717 - lr 0.1000000
2021-05-17 07:09:14,149 DEV : loss 0.5079368352890015 - score 0.9247
2021-05-17 07:09:14,286 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:09:24,580 ----------------------------------------------------------------------------------------------------
2021-05-17 07:09:37,883 epoch 3 - iter 20/202 - loss 1.23213584 - samples/sec: 24.07 - lr: 0.100000
2021-05-17 07:09:54,716 epoch 3 - iter 40/202 - loss 1.32567313 - samples/sec: 19.01 - lr: 0.100000
2021-05-17 07:10:08,972 epoch 3 - iter 60/202 - loss 1.30871267 - samples/sec: 22.45 - lr: 0.100000
2021-05-17 07:10:22,675 epoch 3 - iter 80/202 - loss 1.24684693 - samples/sec: 23.36 - lr: 0.100000
2021-05-17 07:10:36,543 epoch 3 - iter 100/202 - loss 1.24644954 - samples/sec: 23.08 - lr: 0.100000
2021-05-17 07:10:51,044 epoch 3 - iter 120/202 - loss 1.23921274 - samples/sec: 22.07 - lr: 0.100000
2021-05-17 07:11:07,927 epoch 3 - iter 140/202 - loss 1.22575432 - samples/sec: 18.96 - lr: 0.100000
2021-05-17 07:11:23,924 epoch 3 - iter 160/202 - loss 1.21281246 - samples/sec: 20.01 - lr: 0.100000
2021-05-17 07:11:39,104 epoch 3 - iter 180/202 - loss 1.19749989 - samples/sec: 21.08 - lr: 0.100000
2021-05-17 07:11:55,954 epoch 3 - iter 200/202 - loss 1.18676993 - samples/sec: 19.01 - lr: 0.100000
2021-05-17 07:11:57,566 ----------------------------------------------------------------------------------------------------
2021-05-17 07:11:57,567 EPOCH 3 done: loss 1.1835 - lr 0.1000000
2021-05-17 07:12:10,735 DEV : loss 0.7163540124893188 - score 0.8977
2021-05-17 07:12:10,906 BAD EPOCHS (no improvement): 1
2021-05-17 07:12:10,906 ----------------------------------------------------------------------------------------------------
2021-05-17 07:12:26,719 epoch 4 - iter 20/202 - loss 0.97867380 - samples/sec: 20.24 - lr: 0.100000
2021-05-17 07:12:42,915 epoch 4 - iter 40/202 - loss 1.06205347 - samples/sec: 19.76 - lr: 0.100000
2021-05-17 07:12:57,079 epoch 4 - iter 60/202 - loss 1.02405517 - samples/sec: 22.60 - lr: 0.100000
2021-05-17 07:13:13,320 epoch 4 - iter 80/202 - loss 1.09934259 - samples/sec: 19.70 - lr: 0.100000
2021-05-17 07:13:29,599 epoch 4 - iter 100/202 - loss 1.10503476 - samples/sec: 19.67 - lr: 0.100000
2021-05-17 07:13:44,776 epoch 4 - iter 120/202 - loss 1.09408613 - samples/sec: 21.09 - lr: 0.100000
2021-05-17 07:13:59,353 epoch 4 - iter 140/202 - loss 1.12722299 - samples/sec: 21.96 - lr: 0.100000
2021-05-17 07:14:14,309 epoch 4 - iter 160/202 - loss 1.09300224 - samples/sec: 21.40 - lr: 0.100000
2021-05-17 07:14:29,085 epoch 4 - iter 180/202 - loss 1.06605729 - samples/sec: 21.66 - lr: 0.100000
2021-05-17 07:14:45,671 epoch 4 - iter 200/202 - loss 1.05089533 - samples/sec: 19.30 - lr: 0.100000
2021-05-17 07:14:46,851 ----------------------------------------------------------------------------------------------------
2021-05-17 07:14:46,851 EPOCH 4 done: loss 1.0587 - lr 0.1000000
2021-05-17 07:14:59,571 DEV : loss 0.5724183320999146 - score 0.9261
2021-05-17 07:14:59,711 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:15:09,784 ----------------------------------------------------------------------------------------------------
2021-05-17 07:15:24,173 epoch 5 - iter 20/202 - loss 0.87747567 - samples/sec: 22.29 - lr: 0.100000
2021-05-17 07:15:39,539 epoch 5 - iter 40/202 - loss 0.88563002 - samples/sec: 20.83 - lr: 0.100000
2021-05-17 07:15:55,661 epoch 5 - iter 60/202 - loss 0.88974143 - samples/sec: 19.85 - lr: 0.100000
2021-05-17 07:16:10,931 epoch 5 - iter 80/202 - loss 0.86917626 - samples/sec: 20.96 - lr: 0.100000
2021-05-17 07:16:24,885 epoch 5 - iter 100/202 - loss 0.84395060 - samples/sec: 22.93 - lr: 0.100000
2021-05-17 07:16:40,771 epoch 5 - iter 120/202 - loss 0.84741494 - samples/sec: 20.15 - lr: 0.100000
2021-05-17 07:16:54,672 epoch 5 - iter 140/202 - loss 0.87162753 - samples/sec: 23.02 - lr: 0.100000
2021-05-17 07:17:11,504 epoch 5 - iter 160/202 - loss 0.87207193 - samples/sec: 19.01 - lr: 0.100000
2021-05-17 07:17:25,692 epoch 5 - iter 180/202 - loss 0.87255182 - samples/sec: 22.56 - lr: 0.100000
2021-05-17 07:17:40,203 epoch 5 - iter 200/202 - loss 0.87468711 - samples/sec: 22.05 - lr: 0.100000
2021-05-17 07:17:41,677 ----------------------------------------------------------------------------------------------------
2021-05-17 07:17:41,677 EPOCH 5 done: loss 0.8729 - lr 0.1000000
2021-05-17 07:17:55,701 DEV : loss 0.5224027037620544 - score 0.9262
2021-05-17 07:17:55,953 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:18:06,004 ----------------------------------------------------------------------------------------------------
2021-05-17 07:18:22,944 epoch 6 - iter 20/202 - loss 0.89921324 - samples/sec: 18.93 - lr: 0.100000
2021-05-17 07:18:36,980 epoch 6 - iter 40/202 - loss 0.89913691 - samples/sec: 22.80 - lr: 0.100000
2021-05-17 07:18:51,642 epoch 6 - iter 60/202 - loss 0.97159955 - samples/sec: 21.83 - lr: 0.100000
2021-05-17 07:19:08,855 epoch 6 - iter 80/202 - loss 0.94494115 - samples/sec: 18.59 - lr: 0.100000
2021-05-17 07:19:25,338 epoch 6 - iter 100/202 - loss 0.90473585 - samples/sec: 19.42 - lr: 0.100000
2021-05-17 07:19:43,353 epoch 6 - iter 120/202 - loss 0.89556592 - samples/sec: 17.77 - lr: 0.100000
2021-05-17 07:20:00,230 epoch 6 - iter 140/202 - loss 0.87131966 - samples/sec: 18.97 - lr: 0.100000
2021-05-17 07:20:16,614 epoch 6 - iter 160/202 - loss 0.86662347 - samples/sec: 19.53 - lr: 0.100000
2021-05-17 07:20:33,641 epoch 6 - iter 180/202 - loss 0.88503581 - samples/sec: 18.80 - lr: 0.100000
2021-05-17 07:20:48,987 epoch 6 - iter 200/202 - loss 0.89023719 - samples/sec: 20.85 - lr: 0.100000
2021-05-17 07:20:50,238 ----------------------------------------------------------------------------------------------------
2021-05-17 07:20:50,238 EPOCH 6 done: loss 0.8924 - lr 0.1000000
2021-05-17 07:21:06,132 DEV : loss 0.45442497730255127 - score 0.9399
2021-05-17 07:21:06,270 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:21:16,392 ----------------------------------------------------------------------------------------------------
2021-05-17 07:21:32,162 epoch 7 - iter 20/202 - loss 0.96706440 - samples/sec: 20.32 - lr: 0.100000
2021-05-17 07:21:48,143 epoch 7 - iter 40/202 - loss 0.87608488 - samples/sec: 20.03 - lr: 0.100000
2021-05-17 07:22:03,613 epoch 7 - iter 60/202 - loss 0.90153661 - samples/sec: 20.69 - lr: 0.100000
2021-05-17 07:22:21,798 epoch 7 - iter 80/202 - loss 0.87430786 - samples/sec: 17.60 - lr: 0.100000
2021-05-17 07:22:38,128 epoch 7 - iter 100/202 - loss 0.87887195 - samples/sec: 19.60 - lr: 0.100000
2021-05-17 07:22:53,156 epoch 7 - iter 120/202 - loss 0.83493271 - samples/sec: 21.30 - lr: 0.100000
2021-05-17 07:23:08,418 epoch 7 - iter 140/202 - loss 0.82513423 - samples/sec: 20.97 - lr: 0.100000
2021-05-17 07:23:25,099 epoch 7 - iter 160/202 - loss 0.82722258 - samples/sec: 19.19 - lr: 0.100000
2021-05-17 07:23:40,416 epoch 7 - iter 180/202 - loss 0.83282629 - samples/sec: 20.89 - lr: 0.100000
2021-05-17 07:23:57,095 epoch 7 - iter 200/202 - loss 0.83987272 - samples/sec: 19.19 - lr: 0.100000
2021-05-17 07:23:58,279 ----------------------------------------------------------------------------------------------------
2021-05-17 07:23:58,280 EPOCH 7 done: loss 0.8406 - lr 0.1000000
2021-05-17 07:24:12,868 DEV : loss 0.6386083364486694 - score 0.9196
2021-05-17 07:24:13,018 BAD EPOCHS (no improvement): 1
2021-05-17 07:24:13,019 ----------------------------------------------------------------------------------------------------
2021-05-17 07:24:29,526 epoch 8 - iter 20/202 - loss 0.78274042 - samples/sec: 19.39 - lr: 0.100000
2021-05-17 07:24:44,509 epoch 8 - iter 40/202 - loss 0.96217944 - samples/sec: 21.36 - lr: 0.100000
2021-05-17 07:25:03,134 epoch 8 - iter 60/202 - loss 0.90047394 - samples/sec: 17.18 - lr: 0.100000
2021-05-17 07:25:21,893 epoch 8 - iter 80/202 - loss 0.93832863 - samples/sec: 17.06 - lr: 0.100000
2021-05-17 07:25:38,173 epoch 8 - iter 100/202 - loss 0.90733592 - samples/sec: 19.66 - lr: 0.100000
2021-05-17 07:25:54,507 epoch 8 - iter 120/202 - loss 0.88759132 - samples/sec: 19.59 - lr: 0.100000
2021-05-17 07:26:13,014 epoch 8 - iter 140/202 - loss 0.88151819 - samples/sec: 17.29 - lr: 0.100000
2021-05-17 07:26:27,505 epoch 8 - iter 160/202 - loss 0.85374168 - samples/sec: 22.09 - lr: 0.100000
2021-05-17 07:26:45,338 epoch 8 - iter 180/202 - loss 0.83662859 - samples/sec: 17.95 - lr: 0.100000
2021-05-17 07:27:01,436 epoch 8 - iter 200/202 - loss 0.84920976 - samples/sec: 19.88 - lr: 0.100000
2021-05-17 07:27:03,268 ----------------------------------------------------------------------------------------------------
2021-05-17 07:27:03,269 EPOCH 8 done: loss 0.8435 - lr 0.1000000
2021-05-17 07:27:17,326 DEV : loss 0.4524911642074585 - score 0.9385
2021-05-17 07:27:17,469 BAD EPOCHS (no improvement): 2
2021-05-17 07:27:17,469 ----------------------------------------------------------------------------------------------------
2021-05-17 07:27:33,038 epoch 9 - iter 20/202 - loss 0.77089994 - samples/sec: 20.56 - lr: 0.100000
2021-05-17 07:27:50,344 epoch 9 - iter 40/202 - loss 0.76023588 - samples/sec: 18.49 - lr: 0.100000
2021-05-17 07:28:08,083 epoch 9 - iter 60/202 - loss 0.74759986 - samples/sec: 18.04 - lr: 0.100000
2021-05-17 07:28:26,277 epoch 9 - iter 80/202 - loss 0.74165200 - samples/sec: 17.59 - lr: 0.100000
2021-05-17 07:28:41,302 epoch 9 - iter 100/202 - loss 0.76450216 - samples/sec: 21.30 - lr: 0.100000
2021-05-17 07:28:57,520 epoch 9 - iter 120/202 - loss 0.76923854 - samples/sec: 19.73 - lr: 0.100000
2021-05-17 07:29:13,953 epoch 9 - iter 140/202 - loss 0.77620176 - samples/sec: 19.48 - lr: 0.100000
2021-05-17 07:29:31,401 epoch 9 - iter 160/202 - loss 0.78764584 - samples/sec: 18.34 - lr: 0.100000
2021-05-17 07:29:50,627 epoch 9 - iter 180/202 - loss 0.78364965 - samples/sec: 16.65 - lr: 0.100000
2021-05-17 07:30:07,942 epoch 9 - iter 200/202 - loss 0.81102434 - samples/sec: 18.48 - lr: 0.100000
2021-05-17 07:30:09,352 ----------------------------------------------------------------------------------------------------
2021-05-17 07:30:09,352 EPOCH 9 done: loss 0.8092 - lr 0.1000000
2021-05-17 07:30:25,271 DEV : loss 0.46080976724624634 - score 0.9388
2021-05-17 07:30:25,409 BAD EPOCHS (no improvement): 3
2021-05-17 07:30:25,410 ----------------------------------------------------------------------------------------------------
2021-05-17 07:30:42,466 epoch 10 - iter 20/202 - loss 0.91813847 - samples/sec: 18.76 - lr: 0.100000
2021-05-17 07:30:59,369 epoch 10 - iter 40/202 - loss 0.92076268 - samples/sec: 18.93 - lr: 0.100000
2021-05-17 07:31:17,142 epoch 10 - iter 60/202 - loss 0.86998827 - samples/sec: 18.01 - lr: 0.100000
2021-05-17 07:31:33,495 epoch 10 - iter 80/202 - loss 0.85964805 - samples/sec: 19.57 - lr: 0.100000
2021-05-17 07:31:49,535 epoch 10 - iter 100/202 - loss 0.84310873 - samples/sec: 19.95 - lr: 0.100000
2021-05-17 07:32:04,946 epoch 10 - iter 120/202 - loss 0.84804460 - samples/sec: 20.77 - lr: 0.100000
2021-05-17 07:32:23,668 epoch 10 - iter 140/202 - loss 0.82920572 - samples/sec: 17.09 - lr: 0.100000
2021-05-17 07:32:40,383 epoch 10 - iter 160/202 - loss 0.80326871 - samples/sec: 19.15 - lr: 0.100000
2021-05-17 07:32:56,030 epoch 10 - iter 180/202 - loss 0.81048356 - samples/sec: 20.45 - lr: 0.100000
2021-05-17 07:33:11,221 epoch 10 - iter 200/202 - loss 0.79574718 - samples/sec: 21.07 - lr: 0.100000
2021-05-17 07:33:12,807 ----------------------------------------------------------------------------------------------------
2021-05-17 07:33:12,808 EPOCH 10 done: loss 0.7951 - lr 0.1000000
2021-05-17 07:33:32,403 DEV : loss 0.5314973592758179 - score 0.9341
Epoch    10: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 07:33:32,542 BAD EPOCHS (no improvement): 4
2021-05-17 07:33:32,542 ----------------------------------------------------------------------------------------------------
2021-05-17 07:33:50,920 epoch 11 - iter 20/202 - loss 0.73811382 - samples/sec: 17.41 - lr: 0.050000
2021-05-17 07:34:06,570 epoch 11 - iter 40/202 - loss 0.75994024 - samples/sec: 20.45 - lr: 0.050000
2021-05-17 07:34:23,923 epoch 11 - iter 60/202 - loss 0.77259643 - samples/sec: 18.44 - lr: 0.050000
2021-05-17 07:34:41,442 epoch 11 - iter 80/202 - loss 0.72554983 - samples/sec: 18.27 - lr: 0.050000
2021-05-17 07:34:57,573 epoch 11 - iter 100/202 - loss 0.70198032 - samples/sec: 19.84 - lr: 0.050000
2021-05-17 07:35:12,034 epoch 11 - iter 120/202 - loss 0.68390861 - samples/sec: 22.13 - lr: 0.050000
2021-05-17 07:35:27,590 epoch 11 - iter 140/202 - loss 0.68150552 - samples/sec: 20.57 - lr: 0.050000
2021-05-17 07:35:43,657 epoch 11 - iter 160/202 - loss 0.67292076 - samples/sec: 19.92 - lr: 0.050000
2021-05-17 07:36:00,721 epoch 11 - iter 180/202 - loss 0.66560882 - samples/sec: 18.75 - lr: 0.050000
2021-05-17 07:36:16,639 epoch 11 - iter 200/202 - loss 0.65947113 - samples/sec: 20.10 - lr: 0.050000
2021-05-17 07:36:17,802 ----------------------------------------------------------------------------------------------------
2021-05-17 07:36:17,802 EPOCH 11 done: loss 0.6649 - lr 0.0500000
2021-05-17 07:36:33,504 DEV : loss 0.5320676565170288 - score 0.9297
2021-05-17 07:36:33,643 BAD EPOCHS (no improvement): 1
2021-05-17 07:36:33,644 ----------------------------------------------------------------------------------------------------
2021-05-17 07:36:49,582 epoch 12 - iter 20/202 - loss 0.61721669 - samples/sec: 20.08 - lr: 0.050000
2021-05-17 07:37:05,450 epoch 12 - iter 40/202 - loss 0.63319533 - samples/sec: 20.17 - lr: 0.050000
2021-05-17 07:37:22,828 epoch 12 - iter 60/202 - loss 0.65366487 - samples/sec: 18.42 - lr: 0.050000
2021-05-17 07:37:38,855 epoch 12 - iter 80/202 - loss 0.66281657 - samples/sec: 19.97 - lr: 0.050000
2021-05-17 07:37:52,328 epoch 12 - iter 100/202 - loss 0.65731661 - samples/sec: 23.75 - lr: 0.050000
2021-05-17 07:38:08,236 epoch 12 - iter 120/202 - loss 0.64432124 - samples/sec: 20.12 - lr: 0.050000
2021-05-17 07:38:23,830 epoch 12 - iter 140/202 - loss 0.65944445 - samples/sec: 20.52 - lr: 0.050000
2021-05-17 07:38:39,694 epoch 12 - iter 160/202 - loss 0.65094736 - samples/sec: 20.17 - lr: 0.050000
2021-05-17 07:38:55,957 epoch 12 - iter 180/202 - loss 0.64873549 - samples/sec: 19.68 - lr: 0.050000
2021-05-17 07:39:11,557 epoch 12 - iter 200/202 - loss 0.64882764 - samples/sec: 20.51 - lr: 0.050000
2021-05-17 07:39:13,239 ----------------------------------------------------------------------------------------------------
2021-05-17 07:39:13,240 EPOCH 12 done: loss 0.6490 - lr 0.0500000
2021-05-17 07:39:28,098 DEV : loss 0.45967185497283936 - score 0.927
2021-05-17 07:39:28,236 BAD EPOCHS (no improvement): 2
2021-05-17 07:39:28,237 ----------------------------------------------------------------------------------------------------
2021-05-17 07:39:45,223 epoch 13 - iter 20/202 - loss 0.67636911 - samples/sec: 18.84 - lr: 0.050000
2021-05-17 07:40:00,500 epoch 13 - iter 40/202 - loss 0.63012417 - samples/sec: 20.95 - lr: 0.050000
2021-05-17 07:40:14,617 epoch 13 - iter 60/202 - loss 0.65351281 - samples/sec: 22.67 - lr: 0.050000
2021-05-17 07:40:29,105 epoch 13 - iter 80/202 - loss 0.63906750 - samples/sec: 22.09 - lr: 0.050000
2021-05-17 07:40:45,338 epoch 13 - iter 100/202 - loss 0.62774689 - samples/sec: 19.71 - lr: 0.050000
2021-05-17 07:41:01,245 epoch 13 - iter 120/202 - loss 0.63380729 - samples/sec: 20.12 - lr: 0.050000
2021-05-17 07:41:17,185 epoch 13 - iter 140/202 - loss 0.61872734 - samples/sec: 20.08 - lr: 0.050000
2021-05-17 07:41:33,409 epoch 13 - iter 160/202 - loss 0.61300059 - samples/sec: 19.73 - lr: 0.050000
2021-05-17 07:41:48,680 epoch 13 - iter 180/202 - loss 0.61731442 - samples/sec: 20.96 - lr: 0.050000
2021-05-17 07:42:04,567 epoch 13 - iter 200/202 - loss 0.61753251 - samples/sec: 20.15 - lr: 0.050000
2021-05-17 07:42:05,604 ----------------------------------------------------------------------------------------------------
2021-05-17 07:42:05,605 EPOCH 13 done: loss 0.6182 - lr 0.0500000
2021-05-17 07:42:22,275 DEV : loss 0.4683821201324463 - score 0.9367
2021-05-17 07:42:22,491 BAD EPOCHS (no improvement): 3
2021-05-17 07:42:22,491 ----------------------------------------------------------------------------------------------------
2021-05-17 07:42:39,556 epoch 14 - iter 20/202 - loss 0.52275020 - samples/sec: 18.75 - lr: 0.050000
2021-05-17 07:42:53,330 epoch 14 - iter 40/202 - loss 0.53035422 - samples/sec: 23.24 - lr: 0.050000
2021-05-17 07:43:08,356 epoch 14 - iter 60/202 - loss 0.55784151 - samples/sec: 21.31 - lr: 0.050000
2021-05-17 07:43:22,928 epoch 14 - iter 80/202 - loss 0.60250733 - samples/sec: 21.96 - lr: 0.050000
2021-05-17 07:43:38,132 epoch 14 - iter 100/202 - loss 0.59197036 - samples/sec: 21.05 - lr: 0.050000
2021-05-17 07:43:53,689 epoch 14 - iter 120/202 - loss 0.57838231 - samples/sec: 20.57 - lr: 0.050000
2021-05-17 07:44:09,030 epoch 14 - iter 140/202 - loss 0.58973171 - samples/sec: 20.86 - lr: 0.050000
2021-05-17 07:44:23,440 epoch 14 - iter 160/202 - loss 0.60474518 - samples/sec: 22.21 - lr: 0.050000
2021-05-17 07:44:40,887 epoch 14 - iter 180/202 - loss 0.59818713 - samples/sec: 18.34 - lr: 0.050000
2021-05-17 07:44:56,996 epoch 14 - iter 200/202 - loss 0.59792800 - samples/sec: 19.87 - lr: 0.050000
2021-05-17 07:44:58,371 ----------------------------------------------------------------------------------------------------
2021-05-17 07:44:58,371 EPOCH 14 done: loss 0.6002 - lr 0.0500000
2021-05-17 07:45:13,264 DEV : loss 0.47555142641067505 - score 0.94
2021-05-17 07:45:13,402 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:45:23,411 ----------------------------------------------------------------------------------------------------
2021-05-17 07:45:38,110 epoch 15 - iter 20/202 - loss 0.61556106 - samples/sec: 21.78 - lr: 0.050000
2021-05-17 07:45:53,094 epoch 15 - iter 40/202 - loss 0.60055553 - samples/sec: 21.36 - lr: 0.050000
2021-05-17 07:46:09,114 epoch 15 - iter 60/202 - loss 0.58542136 - samples/sec: 19.98 - lr: 0.050000
2021-05-17 07:46:26,155 epoch 15 - iter 80/202 - loss 0.63939942 - samples/sec: 18.78 - lr: 0.050000
2021-05-17 07:46:42,768 epoch 15 - iter 100/202 - loss 0.62769458 - samples/sec: 19.26 - lr: 0.050000
2021-05-17 07:46:58,842 epoch 15 - iter 120/202 - loss 0.61648870 - samples/sec: 19.91 - lr: 0.050000
2021-05-17 07:47:15,931 epoch 15 - iter 140/202 - loss 0.61109409 - samples/sec: 18.73 - lr: 0.050000
2021-05-17 07:47:29,790 epoch 15 - iter 160/202 - loss 0.61172770 - samples/sec: 23.09 - lr: 0.050000
2021-05-17 07:47:43,916 epoch 15 - iter 180/202 - loss 0.60821351 - samples/sec: 22.65 - lr: 0.050000
2021-05-17 07:47:59,672 epoch 15 - iter 200/202 - loss 0.60307741 - samples/sec: 20.31 - lr: 0.050000
2021-05-17 07:48:00,979 ----------------------------------------------------------------------------------------------------
2021-05-17 07:48:00,979 EPOCH 15 done: loss 0.6075 - lr 0.0500000
2021-05-17 07:48:17,222 DEV : loss 0.4373179078102112 - score 0.9383
2021-05-17 07:48:17,362 BAD EPOCHS (no improvement): 1
2021-05-17 07:48:17,362 ----------------------------------------------------------------------------------------------------
2021-05-17 07:48:32,504 epoch 16 - iter 20/202 - loss 0.64451972 - samples/sec: 21.14 - lr: 0.050000
2021-05-17 07:48:48,040 epoch 16 - iter 40/202 - loss 0.66209425 - samples/sec: 20.60 - lr: 0.050000
2021-05-17 07:49:05,577 epoch 16 - iter 60/202 - loss 0.62510224 - samples/sec: 18.25 - lr: 0.050000
2021-05-17 07:49:20,375 epoch 16 - iter 80/202 - loss 0.63396630 - samples/sec: 21.63 - lr: 0.050000
2021-05-17 07:49:34,776 epoch 16 - iter 100/202 - loss 0.61470700 - samples/sec: 22.22 - lr: 0.050000
2021-05-17 07:49:50,219 epoch 16 - iter 120/202 - loss 0.59345337 - samples/sec: 20.72 - lr: 0.050000
2021-05-17 07:50:08,696 epoch 16 - iter 140/202 - loss 0.58687534 - samples/sec: 17.32 - lr: 0.050000
2021-05-17 07:50:29,701 epoch 16 - iter 160/202 - loss 0.59579312 - samples/sec: 15.24 - lr: 0.050000
2021-05-17 07:50:49,312 epoch 16 - iter 180/202 - loss 0.59382421 - samples/sec: 16.32 - lr: 0.050000
2021-05-17 07:51:07,360 epoch 16 - iter 200/202 - loss 0.59961445 - samples/sec: 17.73 - lr: 0.050000
2021-05-17 07:51:09,333 ----------------------------------------------------------------------------------------------------
2021-05-17 07:51:09,333 EPOCH 16 done: loss 0.6026 - lr 0.0500000
2021-05-17 07:51:28,899 DEV : loss 0.4536380171775818 - score 0.9401
2021-05-17 07:51:29,038 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 07:51:38,934 ----------------------------------------------------------------------------------------------------
2021-05-17 07:51:57,513 epoch 17 - iter 20/202 - loss 0.68539415 - samples/sec: 17.29 - lr: 0.050000
2021-05-17 07:52:14,774 epoch 17 - iter 40/202 - loss 0.58214689 - samples/sec: 18.54 - lr: 0.050000
2021-05-17 07:52:35,370 epoch 17 - iter 60/202 - loss 0.59938094 - samples/sec: 15.54 - lr: 0.050000
2021-05-17 07:52:52,885 epoch 17 - iter 80/202 - loss 0.55444865 - samples/sec: 18.27 - lr: 0.050000
2021-05-17 07:53:10,359 epoch 17 - iter 100/202 - loss 0.57367848 - samples/sec: 18.32 - lr: 0.050000
2021-05-17 07:53:27,516 epoch 17 - iter 120/202 - loss 0.56987785 - samples/sec: 18.65 - lr: 0.050000
2021-05-17 07:53:42,963 epoch 17 - iter 140/202 - loss 0.57594554 - samples/sec: 20.72 - lr: 0.050000
2021-05-17 07:53:58,527 epoch 17 - iter 160/202 - loss 0.57532598 - samples/sec: 20.56 - lr: 0.050000
2021-05-17 07:54:17,330 epoch 17 - iter 180/202 - loss 0.56560284 - samples/sec: 17.03 - lr: 0.050000
2021-05-17 07:54:33,409 epoch 17 - iter 200/202 - loss 0.57273445 - samples/sec: 19.91 - lr: 0.050000
2021-05-17 07:54:34,698 ----------------------------------------------------------------------------------------------------
2021-05-17 07:54:34,699 EPOCH 17 done: loss 0.5717 - lr 0.0500000
2021-05-17 07:54:48,050 DEV : loss 0.44566139578819275 - score 0.9395
2021-05-17 07:54:48,189 BAD EPOCHS (no improvement): 1
2021-05-17 07:54:48,195 ----------------------------------------------------------------------------------------------------
2021-05-17 07:55:02,381 epoch 18 - iter 20/202 - loss 0.60010901 - samples/sec: 22.56 - lr: 0.050000
2021-05-17 07:55:17,944 epoch 18 - iter 40/202 - loss 0.60642437 - samples/sec: 20.56 - lr: 0.050000
2021-05-17 07:55:37,317 epoch 18 - iter 60/202 - loss 0.60598476 - samples/sec: 16.52 - lr: 0.050000
2021-05-17 07:55:56,165 epoch 18 - iter 80/202 - loss 0.56497493 - samples/sec: 16.98 - lr: 0.050000
2021-05-17 07:56:13,894 epoch 18 - iter 100/202 - loss 0.56642765 - samples/sec: 18.05 - lr: 0.050000
2021-05-17 07:56:30,857 epoch 18 - iter 120/202 - loss 0.57056729 - samples/sec: 18.87 - lr: 0.050000
2021-05-17 07:56:49,244 epoch 18 - iter 140/202 - loss 0.55429120 - samples/sec: 17.41 - lr: 0.050000
2021-05-17 07:57:07,061 epoch 18 - iter 160/202 - loss 0.55921211 - samples/sec: 17.96 - lr: 0.050000
2021-05-17 07:57:22,598 epoch 18 - iter 180/202 - loss 0.56286089 - samples/sec: 20.60 - lr: 0.050000
2021-05-17 07:57:40,598 epoch 18 - iter 200/202 - loss 0.55428717 - samples/sec: 17.78 - lr: 0.050000
2021-05-17 07:57:42,525 ----------------------------------------------------------------------------------------------------
2021-05-17 07:57:42,525 EPOCH 18 done: loss 0.5513 - lr 0.0500000
2021-05-17 07:57:58,789 DEV : loss 0.4837401211261749 - score 0.9364
2021-05-17 07:57:58,929 BAD EPOCHS (no improvement): 2
2021-05-17 07:57:58,929 ----------------------------------------------------------------------------------------------------
2021-05-17 07:58:16,126 epoch 19 - iter 20/202 - loss 0.52844657 - samples/sec: 18.61 - lr: 0.050000
2021-05-17 07:58:34,275 epoch 19 - iter 40/202 - loss 0.59152834 - samples/sec: 17.63 - lr: 0.050000
2021-05-17 07:58:54,221 epoch 19 - iter 60/202 - loss 0.53531011 - samples/sec: 16.04 - lr: 0.050000
2021-05-17 07:59:12,212 epoch 19 - iter 80/202 - loss 0.53969274 - samples/sec: 17.79 - lr: 0.050000
2021-05-17 07:59:31,785 epoch 19 - iter 100/202 - loss 0.54665113 - samples/sec: 16.35 - lr: 0.050000
2021-05-17 07:59:48,967 epoch 19 - iter 120/202 - loss 0.56229723 - samples/sec: 18.63 - lr: 0.050000
2021-05-17 08:00:03,738 epoch 19 - iter 140/202 - loss 0.56052726 - samples/sec: 21.67 - lr: 0.050000
2021-05-17 08:00:18,315 epoch 19 - iter 160/202 - loss 0.56146017 - samples/sec: 21.96 - lr: 0.050000
2021-05-17 08:00:32,493 epoch 19 - iter 180/202 - loss 0.56996409 - samples/sec: 22.57 - lr: 0.050000
2021-05-17 08:00:48,301 epoch 19 - iter 200/202 - loss 0.57990154 - samples/sec: 20.25 - lr: 0.050000
2021-05-17 08:00:50,075 ----------------------------------------------------------------------------------------------------
2021-05-17 08:00:50,076 EPOCH 19 done: loss 0.5774 - lr 0.0500000
2021-05-17 08:01:04,262 DEV : loss 0.44493913650512695 - score 0.943
2021-05-17 08:01:04,400 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:01:14,492 ----------------------------------------------------------------------------------------------------
2021-05-17 08:01:33,068 epoch 20 - iter 20/202 - loss 0.45186125 - samples/sec: 17.23 - lr: 0.050000
2021-05-17 08:01:49,705 epoch 20 - iter 40/202 - loss 0.51752952 - samples/sec: 19.24 - lr: 0.050000
2021-05-17 08:02:07,154 epoch 20 - iter 60/202 - loss 0.50851301 - samples/sec: 18.34 - lr: 0.050000
2021-05-17 08:02:24,164 epoch 20 - iter 80/202 - loss 0.49153895 - samples/sec: 18.81 - lr: 0.050000
2021-05-17 08:02:42,261 epoch 20 - iter 100/202 - loss 0.51034790 - samples/sec: 17.68 - lr: 0.050000
2021-05-17 08:02:59,366 epoch 20 - iter 120/202 - loss 0.51254347 - samples/sec: 18.71 - lr: 0.050000
2021-05-17 08:03:15,642 epoch 20 - iter 140/202 - loss 0.52858117 - samples/sec: 19.66 - lr: 0.050000
2021-05-17 08:03:32,171 epoch 20 - iter 160/202 - loss 0.52972168 - samples/sec: 19.36 - lr: 0.050000
2021-05-17 08:03:49,145 epoch 20 - iter 180/202 - loss 0.53433243 - samples/sec: 18.85 - lr: 0.050000
2021-05-17 08:04:07,450 epoch 20 - iter 200/202 - loss 0.52870298 - samples/sec: 17.48 - lr: 0.050000
2021-05-17 08:04:09,237 ----------------------------------------------------------------------------------------------------
2021-05-17 08:04:09,238 EPOCH 20 done: loss 0.5293 - lr 0.0500000
2021-05-17 08:04:27,797 DEV : loss 0.5080016851425171 - score 0.9314
2021-05-17 08:04:27,937 BAD EPOCHS (no improvement): 1
2021-05-17 08:04:27,937 ----------------------------------------------------------------------------------------------------
2021-05-17 08:04:44,454 epoch 21 - iter 20/202 - loss 0.46786234 - samples/sec: 19.38 - lr: 0.050000
2021-05-17 08:05:02,216 epoch 21 - iter 40/202 - loss 0.52460553 - samples/sec: 18.02 - lr: 0.050000
2021-05-17 08:05:21,531 epoch 21 - iter 60/202 - loss 0.57533259 - samples/sec: 16.57 - lr: 0.050000
2021-05-17 08:05:38,765 epoch 21 - iter 80/202 - loss 0.57082260 - samples/sec: 18.57 - lr: 0.050000
2021-05-17 08:05:56,194 epoch 21 - iter 100/202 - loss 0.56239977 - samples/sec: 18.36 - lr: 0.050000
2021-05-17 08:06:14,356 epoch 21 - iter 120/202 - loss 0.55752919 - samples/sec: 17.62 - lr: 0.050000
2021-05-17 08:06:30,208 epoch 21 - iter 140/202 - loss 0.55255539 - samples/sec: 20.19 - lr: 0.050000
2021-05-17 08:06:45,999 epoch 21 - iter 160/202 - loss 0.54310945 - samples/sec: 20.27 - lr: 0.050000
2021-05-17 08:07:03,403 epoch 21 - iter 180/202 - loss 0.55106065 - samples/sec: 18.39 - lr: 0.050000
2021-05-17 08:07:21,842 epoch 21 - iter 200/202 - loss 0.54560462 - samples/sec: 17.36 - lr: 0.050000
2021-05-17 08:07:23,806 ----------------------------------------------------------------------------------------------------
2021-05-17 08:07:23,807 EPOCH 21 done: loss 0.5453 - lr 0.0500000
2021-05-17 08:07:41,366 DEV : loss 0.43945011496543884 - score 0.9442
2021-05-17 08:07:41,540 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:07:51,756 ----------------------------------------------------------------------------------------------------
2021-05-17 08:08:10,847 epoch 22 - iter 20/202 - loss 0.50933427 - samples/sec: 16.81 - lr: 0.050000
2021-05-17 08:08:30,345 epoch 22 - iter 40/202 - loss 0.53204578 - samples/sec: 16.41 - lr: 0.050000
2021-05-17 08:08:49,242 epoch 22 - iter 60/202 - loss 0.52291239 - samples/sec: 16.93 - lr: 0.050000
2021-05-17 08:09:06,797 epoch 22 - iter 80/202 - loss 0.51875902 - samples/sec: 18.24 - lr: 0.050000
2021-05-17 08:09:25,157 epoch 22 - iter 100/202 - loss 0.51483964 - samples/sec: 17.43 - lr: 0.050000
2021-05-17 08:09:42,179 epoch 22 - iter 120/202 - loss 0.49234822 - samples/sec: 18.80 - lr: 0.050000
2021-05-17 08:10:00,142 epoch 22 - iter 140/202 - loss 0.50836140 - samples/sec: 17.82 - lr: 0.050000
2021-05-17 08:10:18,342 epoch 22 - iter 160/202 - loss 0.51113899 - samples/sec: 17.58 - lr: 0.050000
2021-05-17 08:10:34,723 epoch 22 - iter 180/202 - loss 0.51049545 - samples/sec: 19.54 - lr: 0.050000
2021-05-17 08:10:52,946 epoch 22 - iter 200/202 - loss 0.50648422 - samples/sec: 17.56 - lr: 0.050000
2021-05-17 08:10:54,436 ----------------------------------------------------------------------------------------------------
2021-05-17 08:10:54,436 EPOCH 22 done: loss 0.5056 - lr 0.0500000
2021-05-17 08:11:10,674 DEV : loss 0.46784311532974243 - score 0.9383
2021-05-17 08:11:10,825 BAD EPOCHS (no improvement): 1
2021-05-17 08:11:10,826 ----------------------------------------------------------------------------------------------------
2021-05-17 08:11:26,406 epoch 23 - iter 20/202 - loss 0.45847545 - samples/sec: 20.54 - lr: 0.050000
2021-05-17 08:11:46,056 epoch 23 - iter 40/202 - loss 0.51070598 - samples/sec: 16.29 - lr: 0.050000
2021-05-17 08:12:00,837 epoch 23 - iter 60/202 - loss 0.52689691 - samples/sec: 21.65 - lr: 0.050000
2021-05-17 08:12:17,636 epoch 23 - iter 80/202 - loss 0.53883910 - samples/sec: 19.05 - lr: 0.050000
2021-05-17 08:12:34,341 epoch 23 - iter 100/202 - loss 0.55120517 - samples/sec: 19.16 - lr: 0.050000
2021-05-17 08:12:49,490 epoch 23 - iter 120/202 - loss 0.53352636 - samples/sec: 21.13 - lr: 0.050000
2021-05-17 08:13:05,003 epoch 23 - iter 140/202 - loss 0.53296907 - samples/sec: 20.63 - lr: 0.050000
2021-05-17 08:13:20,900 epoch 23 - iter 160/202 - loss 0.53946452 - samples/sec: 20.13 - lr: 0.050000
2021-05-17 08:13:36,822 epoch 23 - iter 180/202 - loss 0.52308461 - samples/sec: 20.10 - lr: 0.050000
2021-05-17 08:13:52,957 epoch 23 - iter 200/202 - loss 0.50702512 - samples/sec: 19.83 - lr: 0.050000
2021-05-17 08:13:54,746 ----------------------------------------------------------------------------------------------------
2021-05-17 08:13:54,755 EPOCH 23 done: loss 0.5099 - lr 0.0500000
2021-05-17 08:14:09,648 DEV : loss 0.48724469542503357 - score 0.9425
2021-05-17 08:14:09,787 BAD EPOCHS (no improvement): 2
2021-05-17 08:14:09,788 ----------------------------------------------------------------------------------------------------
2021-05-17 08:14:25,827 epoch 24 - iter 20/202 - loss 0.52404748 - samples/sec: 19.95 - lr: 0.050000
2021-05-17 08:14:40,985 epoch 24 - iter 40/202 - loss 0.52160150 - samples/sec: 21.11 - lr: 0.050000
2021-05-17 08:14:58,449 epoch 24 - iter 60/202 - loss 0.52752789 - samples/sec: 18.33 - lr: 0.050000
2021-05-17 08:15:14,935 epoch 24 - iter 80/202 - loss 0.51196819 - samples/sec: 19.41 - lr: 0.050000
2021-05-17 08:15:33,600 epoch 24 - iter 100/202 - loss 0.53345146 - samples/sec: 17.15 - lr: 0.050000
2021-05-17 08:15:51,224 epoch 24 - iter 120/202 - loss 0.54393208 - samples/sec: 18.17 - lr: 0.050000
2021-05-17 08:16:06,649 epoch 24 - iter 140/202 - loss 0.53856355 - samples/sec: 20.75 - lr: 0.050000
2021-05-17 08:16:22,834 epoch 24 - iter 160/202 - loss 0.54306535 - samples/sec: 19.77 - lr: 0.050000
2021-05-17 08:16:39,497 epoch 24 - iter 180/202 - loss 0.53417298 - samples/sec: 19.21 - lr: 0.050000
2021-05-17 08:16:57,282 epoch 24 - iter 200/202 - loss 0.53264410 - samples/sec: 17.99 - lr: 0.050000
2021-05-17 08:16:58,463 ----------------------------------------------------------------------------------------------------
2021-05-17 08:16:58,468 EPOCH 24 done: loss 0.5320 - lr 0.0500000
2021-05-17 08:17:15,889 DEV : loss 0.43189987540245056 - score 0.9408
2021-05-17 08:17:16,028 BAD EPOCHS (no improvement): 3
2021-05-17 08:17:16,029 ----------------------------------------------------------------------------------------------------
2021-05-17 08:17:33,842 epoch 25 - iter 20/202 - loss 0.46557674 - samples/sec: 17.97 - lr: 0.050000
2021-05-17 08:17:49,032 epoch 25 - iter 40/202 - loss 0.51334136 - samples/sec: 21.07 - lr: 0.050000
2021-05-17 08:18:06,572 epoch 25 - iter 60/202 - loss 0.51264376 - samples/sec: 18.25 - lr: 0.050000
2021-05-17 08:18:23,725 epoch 25 - iter 80/202 - loss 0.52423492 - samples/sec: 18.66 - lr: 0.050000
2021-05-17 08:18:39,653 epoch 25 - iter 100/202 - loss 0.52102719 - samples/sec: 20.09 - lr: 0.050000
2021-05-17 08:18:55,816 epoch 25 - iter 120/202 - loss 0.52190293 - samples/sec: 19.80 - lr: 0.050000
2021-05-17 08:19:09,933 epoch 25 - iter 140/202 - loss 0.50720273 - samples/sec: 22.67 - lr: 0.050000
2021-05-17 08:19:26,879 epoch 25 - iter 160/202 - loss 0.50229992 - samples/sec: 18.89 - lr: 0.050000
2021-05-17 08:19:44,281 epoch 25 - iter 180/202 - loss 0.50891009 - samples/sec: 18.39 - lr: 0.050000
2021-05-17 08:20:04,522 epoch 25 - iter 200/202 - loss 0.50979790 - samples/sec: 15.81 - lr: 0.050000
2021-05-17 08:20:06,215 ----------------------------------------------------------------------------------------------------
2021-05-17 08:20:06,216 EPOCH 25 done: loss 0.5129 - lr 0.0500000
2021-05-17 08:20:21,805 DEV : loss 0.40649813413619995 - score 0.946
2021-05-17 08:20:21,945 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:20:50,808 ----------------------------------------------------------------------------------------------------
2021-05-17 08:21:07,958 epoch 26 - iter 20/202 - loss 0.46885270 - samples/sec: 18.66 - lr: 0.050000
2021-05-17 08:21:28,371 epoch 26 - iter 40/202 - loss 0.53414567 - samples/sec: 15.68 - lr: 0.050000
2021-05-17 08:21:47,213 epoch 26 - iter 60/202 - loss 0.57565812 - samples/sec: 16.99 - lr: 0.050000
2021-05-17 08:22:07,216 epoch 26 - iter 80/202 - loss 0.52749438 - samples/sec: 16.00 - lr: 0.050000
2021-05-17 08:22:24,475 epoch 26 - iter 100/202 - loss 0.51958540 - samples/sec: 18.55 - lr: 0.050000
2021-05-17 08:22:39,646 epoch 26 - iter 120/202 - loss 0.50393654 - samples/sec: 21.10 - lr: 0.050000
2021-05-17 08:22:55,766 epoch 26 - iter 140/202 - loss 0.49963282 - samples/sec: 19.85 - lr: 0.050000
2021-05-17 08:23:10,517 epoch 26 - iter 160/202 - loss 0.49365344 - samples/sec: 21.70 - lr: 0.050000
2021-05-17 08:23:27,089 epoch 26 - iter 180/202 - loss 0.49799912 - samples/sec: 19.31 - lr: 0.050000
2021-05-17 08:23:43,921 epoch 26 - iter 200/202 - loss 0.50451109 - samples/sec: 19.01 - lr: 0.050000
2021-05-17 08:23:45,380 ----------------------------------------------------------------------------------------------------
2021-05-17 08:23:45,381 EPOCH 26 done: loss 0.5082 - lr 0.0500000
2021-05-17 08:24:01,300 DEV : loss 0.41960716247558594 - score 0.9434
2021-05-17 08:24:01,441 BAD EPOCHS (no improvement): 1
2021-05-17 08:24:01,441 ----------------------------------------------------------------------------------------------------
2021-05-17 08:24:18,674 epoch 27 - iter 20/202 - loss 0.44951742 - samples/sec: 18.57 - lr: 0.050000
2021-05-17 08:24:35,587 epoch 27 - iter 40/202 - loss 0.47727279 - samples/sec: 18.92 - lr: 0.050000
2021-05-17 08:24:53,687 epoch 27 - iter 60/202 - loss 0.48676835 - samples/sec: 17.68 - lr: 0.050000
2021-05-17 08:25:11,737 epoch 27 - iter 80/202 - loss 0.46953376 - samples/sec: 17.73 - lr: 0.050000
2021-05-17 08:25:29,794 epoch 27 - iter 100/202 - loss 0.47182808 - samples/sec: 17.72 - lr: 0.050000
2021-05-17 08:25:45,674 epoch 27 - iter 120/202 - loss 0.48604640 - samples/sec: 20.15 - lr: 0.050000
2021-05-17 08:26:04,347 epoch 27 - iter 140/202 - loss 0.49078049 - samples/sec: 17.14 - lr: 0.050000
2021-05-17 08:26:20,936 epoch 27 - iter 160/202 - loss 0.47829303 - samples/sec: 19.29 - lr: 0.050000
2021-05-17 08:26:38,131 epoch 27 - iter 180/202 - loss 0.49183713 - samples/sec: 18.61 - lr: 0.050000
2021-05-17 08:26:55,457 epoch 27 - iter 200/202 - loss 0.49122205 - samples/sec: 18.48 - lr: 0.050000
2021-05-17 08:26:56,770 ----------------------------------------------------------------------------------------------------
2021-05-17 08:26:56,771 EPOCH 27 done: loss 0.4910 - lr 0.0500000
2021-05-17 08:27:12,538 DEV : loss 0.4514802396297455 - score 0.9436
2021-05-17 08:27:12,679 BAD EPOCHS (no improvement): 2
2021-05-17 08:27:12,679 ----------------------------------------------------------------------------------------------------
2021-05-17 08:27:30,416 epoch 28 - iter 20/202 - loss 0.47320818 - samples/sec: 18.04 - lr: 0.050000
2021-05-17 08:27:44,536 epoch 28 - iter 40/202 - loss 0.43449842 - samples/sec: 22.67 - lr: 0.050000
2021-05-17 08:28:00,846 epoch 28 - iter 60/202 - loss 0.50414871 - samples/sec: 19.63 - lr: 0.050000
2021-05-17 08:28:16,236 epoch 28 - iter 80/202 - loss 0.50072823 - samples/sec: 20.79 - lr: 0.050000
2021-05-17 08:28:33,511 epoch 28 - iter 100/202 - loss 0.48923912 - samples/sec: 18.53 - lr: 0.050000
2021-05-17 08:28:50,610 epoch 28 - iter 120/202 - loss 0.50230133 - samples/sec: 18.72 - lr: 0.050000
2021-05-17 08:29:10,369 epoch 28 - iter 140/202 - loss 0.49731660 - samples/sec: 16.20 - lr: 0.050000
2021-05-17 08:29:26,491 epoch 28 - iter 160/202 - loss 0.51174793 - samples/sec: 19.85 - lr: 0.050000
2021-05-17 08:29:41,398 epoch 28 - iter 180/202 - loss 0.51147841 - samples/sec: 21.47 - lr: 0.050000
2021-05-17 08:29:56,837 epoch 28 - iter 200/202 - loss 0.50577051 - samples/sec: 20.73 - lr: 0.050000
2021-05-17 08:29:58,012 ----------------------------------------------------------------------------------------------------
2021-05-17 08:29:58,013 EPOCH 28 done: loss 0.5030 - lr 0.0500000
2021-05-17 08:30:16,727 DEV : loss 0.4637364447116852 - score 0.9468
2021-05-17 08:30:16,867 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:30:26,964 ----------------------------------------------------------------------------------------------------
2021-05-17 08:30:44,606 epoch 29 - iter 20/202 - loss 0.45574893 - samples/sec: 18.14 - lr: 0.050000
2021-05-17 08:31:02,026 epoch 29 - iter 40/202 - loss 0.44030548 - samples/sec: 18.37 - lr: 0.050000
2021-05-17 08:31:19,215 epoch 29 - iter 60/202 - loss 0.46496591 - samples/sec: 18.62 - lr: 0.050000
2021-05-17 08:31:37,994 epoch 29 - iter 80/202 - loss 0.50335318 - samples/sec: 17.05 - lr: 0.050000
2021-05-17 08:31:53,510 epoch 29 - iter 100/202 - loss 0.47974183 - samples/sec: 20.63 - lr: 0.050000
2021-05-17 08:32:10,719 epoch 29 - iter 120/202 - loss 0.48182440 - samples/sec: 18.60 - lr: 0.050000
2021-05-17 08:32:28,648 epoch 29 - iter 140/202 - loss 0.47731642 - samples/sec: 17.85 - lr: 0.050000
2021-05-17 08:32:44,903 epoch 29 - iter 160/202 - loss 0.47111950 - samples/sec: 19.69 - lr: 0.050000
2021-05-17 08:33:03,054 epoch 29 - iter 180/202 - loss 0.48502869 - samples/sec: 17.63 - lr: 0.050000
2021-05-17 08:33:19,002 epoch 29 - iter 200/202 - loss 0.49028924 - samples/sec: 20.09 - lr: 0.050000
2021-05-17 08:33:20,337 ----------------------------------------------------------------------------------------------------
2021-05-17 08:33:20,338 EPOCH 29 done: loss 0.4867 - lr 0.0500000
2021-05-17 08:33:36,886 DEV : loss 0.4059242904186249 - score 0.9449
2021-05-17 08:33:37,028 BAD EPOCHS (no improvement): 1
2021-05-17 08:33:37,029 ----------------------------------------------------------------------------------------------------
2021-05-17 08:33:53,602 epoch 30 - iter 20/202 - loss 0.40747280 - samples/sec: 19.31 - lr: 0.050000
2021-05-17 08:34:08,652 epoch 30 - iter 40/202 - loss 0.42786597 - samples/sec: 21.26 - lr: 0.050000
2021-05-17 08:34:27,256 epoch 30 - iter 60/202 - loss 0.46528116 - samples/sec: 17.20 - lr: 0.050000
2021-05-17 08:34:44,478 epoch 30 - iter 80/202 - loss 0.49909290 - samples/sec: 18.58 - lr: 0.050000
2021-05-17 08:35:02,841 epoch 30 - iter 100/202 - loss 0.49323475 - samples/sec: 17.43 - lr: 0.050000
2021-05-17 08:35:20,467 epoch 30 - iter 120/202 - loss 0.48422912 - samples/sec: 18.16 - lr: 0.050000
2021-05-17 08:35:37,779 epoch 30 - iter 140/202 - loss 0.50052687 - samples/sec: 18.49 - lr: 0.050000
2021-05-17 08:35:56,837 epoch 30 - iter 160/202 - loss 0.50659059 - samples/sec: 16.79 - lr: 0.050000
2021-05-17 08:36:14,586 epoch 30 - iter 180/202 - loss 0.50185220 - samples/sec: 18.03 - lr: 0.050000
2021-05-17 08:36:30,705 epoch 30 - iter 200/202 - loss 0.48988618 - samples/sec: 19.86 - lr: 0.050000
2021-05-17 08:36:32,149 ----------------------------------------------------------------------------------------------------
2021-05-17 08:36:32,149 EPOCH 30 done: loss 0.4918 - lr 0.0500000
2021-05-17 08:36:46,932 DEV : loss 0.5088082551956177 - score 0.9321
2021-05-17 08:36:47,073 BAD EPOCHS (no improvement): 2
2021-05-17 08:36:57,192 ----------------------------------------------------------------------------------------------------
2021-05-17 08:36:57,192 Testing using best model ...
2021-05-17 08:36:57,193 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.gum/best-model.pt
2021-05-17 08:38:01,782 0.9148	0.9045	0.9096
2021-05-17 08:38:01,782 
Results:
- F1-score (micro) 0.9096
- F1-score (macro) 0.9096

By class:
SENT       tp: 805 - fp: 75 - fn: 85 - precision: 0.9148 - recall: 0.9045 - f1-score: 0.9096
2021-05-17 08:38:01,783 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt/
2021-05-17 08:38:01,824 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt
2021-05-17 08:38:01,825 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt/sent_train.txt
2021-05-17 08:38:01,827 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt/sent_dev.txt
2021-05-17 08:38:01,829 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt/sent_test.txt
Corpus: 865 train + 186 dev + 176 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 08:38:19,364 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,370 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 08:38:19,377 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,377 Corpus: "Corpus: 865 train + 186 dev + 176 test sentences"
2021-05-17 08:38:19,377 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,377 Parameters:
2021-05-17 08:38:19,377  - learning_rate: "0.1"
2021-05-17 08:38:19,378  - mini_batch_size: "16"
2021-05-17 08:38:19,378  - patience: "3"
2021-05-17 08:38:19,378  - anneal_factor: "0.5"
2021-05-17 08:38:19,378  - max_epochs: "30"
2021-05-17 08:38:19,378  - shuffle: "True"
2021-05-17 08:38:19,378  - train_with_dev: "False"
2021-05-17 08:38:19,379  - batch_growth_annealing: "False"
2021-05-17 08:38:19,379 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,379 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt"
2021-05-17 08:38:19,379 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,379 Device: cuda:0
2021-05-17 08:38:19,379 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:19,380 Embeddings storage mode: cpu
2021-05-17 08:38:19,383 ----------------------------------------------------------------------------------------------------
2021-05-17 08:38:29,813 epoch 1 - iter 5/55 - loss 22.26820507 - samples/sec: 7.67 - lr: 0.100000
2021-05-17 08:38:40,609 epoch 1 - iter 10/55 - loss 15.44765668 - samples/sec: 7.41 - lr: 0.100000
2021-05-17 08:38:51,871 epoch 1 - iter 15/55 - loss 13.23116013 - samples/sec: 7.10 - lr: 0.100000
2021-05-17 08:39:00,721 epoch 1 - iter 20/55 - loss 11.68106499 - samples/sec: 9.04 - lr: 0.100000
2021-05-17 08:39:10,062 epoch 1 - iter 25/55 - loss 10.49251637 - samples/sec: 8.57 - lr: 0.100000
2021-05-17 08:39:19,615 epoch 1 - iter 30/55 - loss 9.63285054 - samples/sec: 8.38 - lr: 0.100000
2021-05-17 08:39:28,871 epoch 1 - iter 35/55 - loss 9.00123729 - samples/sec: 8.64 - lr: 0.100000
2021-05-17 08:39:39,177 epoch 1 - iter 40/55 - loss 8.39296347 - samples/sec: 7.76 - lr: 0.100000
2021-05-17 08:39:48,354 epoch 1 - iter 45/55 - loss 7.85494817 - samples/sec: 8.72 - lr: 0.100000
2021-05-17 08:39:58,046 epoch 1 - iter 50/55 - loss 7.41570397 - samples/sec: 8.25 - lr: 0.100000
2021-05-17 08:40:05,678 epoch 1 - iter 55/55 - loss 7.06398521 - samples/sec: 10.48 - lr: 0.100000
2021-05-17 08:40:05,678 ----------------------------------------------------------------------------------------------------
2021-05-17 08:40:05,678 EPOCH 1 done: loss 7.0640 - lr 0.1000000
2021-05-17 08:40:19,169 DEV : loss 1.3991812467575073 - score 0.753
2021-05-17 08:40:19,205 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:40:29,610 ----------------------------------------------------------------------------------------------------
2021-05-17 08:40:33,527 epoch 2 - iter 5/55 - loss 3.58901422 - samples/sec: 20.43 - lr: 0.100000
2021-05-17 08:40:38,058 epoch 2 - iter 10/55 - loss 3.21706260 - samples/sec: 17.66 - lr: 0.100000
2021-05-17 08:40:42,314 epoch 2 - iter 15/55 - loss 2.95326947 - samples/sec: 18.80 - lr: 0.100000
2021-05-17 08:40:46,453 epoch 2 - iter 20/55 - loss 2.67718585 - samples/sec: 19.33 - lr: 0.100000
2021-05-17 08:40:50,126 epoch 2 - iter 25/55 - loss 2.63986032 - samples/sec: 21.78 - lr: 0.100000
2021-05-17 08:40:54,307 epoch 2 - iter 30/55 - loss 2.54063619 - samples/sec: 19.14 - lr: 0.100000
2021-05-17 08:40:58,668 epoch 2 - iter 35/55 - loss 2.36275374 - samples/sec: 18.35 - lr: 0.100000
2021-05-17 08:41:02,475 epoch 2 - iter 40/55 - loss 2.28098606 - samples/sec: 21.02 - lr: 0.100000
2021-05-17 08:41:06,445 epoch 2 - iter 45/55 - loss 2.23584361 - samples/sec: 20.16 - lr: 0.100000
2021-05-17 08:41:10,811 epoch 2 - iter 50/55 - loss 2.15811413 - samples/sec: 18.33 - lr: 0.100000
2021-05-17 08:41:14,652 epoch 2 - iter 55/55 - loss 2.09453108 - samples/sec: 20.83 - lr: 0.100000
2021-05-17 08:41:14,653 ----------------------------------------------------------------------------------------------------
2021-05-17 08:41:14,653 EPOCH 2 done: loss 2.0945 - lr 0.1000000
2021-05-17 08:41:18,295 DEV : loss 0.7243688106536865 - score 0.895
2021-05-17 08:41:18,331 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:41:28,716 ----------------------------------------------------------------------------------------------------
2021-05-17 08:41:32,983 epoch 3 - iter 5/55 - loss 1.55379827 - samples/sec: 18.75 - lr: 0.100000
2021-05-17 08:41:37,129 epoch 3 - iter 10/55 - loss 1.61823878 - samples/sec: 19.32 - lr: 0.100000
2021-05-17 08:41:40,983 epoch 3 - iter 15/55 - loss 1.58636748 - samples/sec: 20.76 - lr: 0.100000
2021-05-17 08:41:45,872 epoch 3 - iter 20/55 - loss 1.56368743 - samples/sec: 16.37 - lr: 0.100000
2021-05-17 08:41:50,956 epoch 3 - iter 25/55 - loss 1.63208223 - samples/sec: 15.74 - lr: 0.100000
2021-05-17 08:41:55,887 epoch 3 - iter 30/55 - loss 1.53429506 - samples/sec: 16.23 - lr: 0.100000
2021-05-17 08:42:00,058 epoch 3 - iter 35/55 - loss 1.53918188 - samples/sec: 19.19 - lr: 0.100000
2021-05-17 08:42:04,671 epoch 3 - iter 40/55 - loss 1.45917950 - samples/sec: 17.34 - lr: 0.100000
2021-05-17 08:42:08,884 epoch 3 - iter 45/55 - loss 1.45293478 - samples/sec: 18.99 - lr: 0.100000
2021-05-17 08:42:12,595 epoch 3 - iter 50/55 - loss 1.43639743 - samples/sec: 21.56 - lr: 0.100000
2021-05-17 08:42:16,051 epoch 3 - iter 55/55 - loss 1.39051838 - samples/sec: 23.15 - lr: 0.100000
2021-05-17 08:42:16,052 ----------------------------------------------------------------------------------------------------
2021-05-17 08:42:16,052 EPOCH 3 done: loss 1.3905 - lr 0.1000000
2021-05-17 08:42:19,506 DEV : loss 0.6807627081871033 - score 0.898
2021-05-17 08:42:19,542 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:42:30,077 ----------------------------------------------------------------------------------------------------
2021-05-17 08:42:34,324 epoch 4 - iter 5/55 - loss 1.52682199 - samples/sec: 18.85 - lr: 0.100000
2021-05-17 08:42:38,675 epoch 4 - iter 10/55 - loss 1.21573379 - samples/sec: 18.39 - lr: 0.100000
2021-05-17 08:42:42,695 epoch 4 - iter 15/55 - loss 1.42982748 - samples/sec: 19.91 - lr: 0.100000
2021-05-17 08:42:46,413 epoch 4 - iter 20/55 - loss 1.38964719 - samples/sec: 21.52 - lr: 0.100000
2021-05-17 08:42:50,656 epoch 4 - iter 25/55 - loss 1.34903399 - samples/sec: 18.86 - lr: 0.100000
2021-05-17 08:42:55,773 epoch 4 - iter 30/55 - loss 1.29648841 - samples/sec: 15.64 - lr: 0.100000
2021-05-17 08:43:00,094 epoch 4 - iter 35/55 - loss 1.24738545 - samples/sec: 18.52 - lr: 0.100000
2021-05-17 08:43:04,770 epoch 4 - iter 40/55 - loss 1.27174700 - samples/sec: 17.11 - lr: 0.100000
2021-05-17 08:43:09,260 epoch 4 - iter 45/55 - loss 1.23912370 - samples/sec: 17.82 - lr: 0.100000
2021-05-17 08:43:14,417 epoch 4 - iter 50/55 - loss 1.17369184 - samples/sec: 15.51 - lr: 0.100000
2021-05-17 08:43:17,709 epoch 4 - iter 55/55 - loss 1.13128471 - samples/sec: 24.31 - lr: 0.100000
2021-05-17 08:43:17,710 ----------------------------------------------------------------------------------------------------
2021-05-17 08:43:17,710 EPOCH 4 done: loss 1.1313 - lr 0.1000000
2021-05-17 08:43:21,553 DEV : loss 0.7650623321533203 - score 0.9076
2021-05-17 08:43:21,590 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:43:32,136 ----------------------------------------------------------------------------------------------------
2021-05-17 08:43:36,185 epoch 5 - iter 5/55 - loss 1.05541916 - samples/sec: 19.76 - lr: 0.100000
2021-05-17 08:43:39,962 epoch 5 - iter 10/55 - loss 0.89657698 - samples/sec: 21.19 - lr: 0.100000
2021-05-17 08:43:44,472 epoch 5 - iter 15/55 - loss 0.91496356 - samples/sec: 17.74 - lr: 0.100000
2021-05-17 08:43:47,897 epoch 5 - iter 20/55 - loss 1.11283304 - samples/sec: 23.36 - lr: 0.100000
2021-05-17 08:43:51,420 epoch 5 - iter 25/55 - loss 1.06647223 - samples/sec: 22.72 - lr: 0.100000
2021-05-17 08:43:56,099 epoch 5 - iter 30/55 - loss 1.01489348 - samples/sec: 17.10 - lr: 0.100000
2021-05-17 08:44:01,595 epoch 5 - iter 35/55 - loss 1.00655015 - samples/sec: 14.56 - lr: 0.100000
2021-05-17 08:44:05,970 epoch 5 - iter 40/55 - loss 1.00091497 - samples/sec: 18.29 - lr: 0.100000
2021-05-17 08:44:10,498 epoch 5 - iter 45/55 - loss 1.02097183 - samples/sec: 17.69 - lr: 0.100000
2021-05-17 08:44:14,601 epoch 5 - iter 50/55 - loss 1.02982006 - samples/sec: 19.50 - lr: 0.100000
2021-05-17 08:44:17,634 epoch 5 - iter 55/55 - loss 1.01607777 - samples/sec: 26.38 - lr: 0.100000
2021-05-17 08:44:17,635 ----------------------------------------------------------------------------------------------------
2021-05-17 08:44:17,635 EPOCH 5 done: loss 1.0161 - lr 0.1000000
2021-05-17 08:44:21,566 DEV : loss 0.6743883490562439 - score 0.908
2021-05-17 08:44:21,602 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:44:32,047 ----------------------------------------------------------------------------------------------------
2021-05-17 08:44:35,452 epoch 6 - iter 5/55 - loss 1.02355943 - samples/sec: 23.50 - lr: 0.100000
2021-05-17 08:44:39,785 epoch 6 - iter 10/55 - loss 0.96296861 - samples/sec: 18.47 - lr: 0.100000
2021-05-17 08:44:43,376 epoch 6 - iter 15/55 - loss 0.97064887 - samples/sec: 22.28 - lr: 0.100000
2021-05-17 08:44:47,745 epoch 6 - iter 20/55 - loss 1.06221122 - samples/sec: 18.31 - lr: 0.100000
2021-05-17 08:44:53,039 epoch 6 - iter 25/55 - loss 1.01002516 - samples/sec: 15.11 - lr: 0.100000
2021-05-17 08:44:57,145 epoch 6 - iter 30/55 - loss 0.96368846 - samples/sec: 19.49 - lr: 0.100000
2021-05-17 08:45:01,576 epoch 6 - iter 35/55 - loss 0.95671884 - samples/sec: 18.06 - lr: 0.100000
2021-05-17 08:45:05,860 epoch 6 - iter 40/55 - loss 0.98462054 - samples/sec: 18.68 - lr: 0.100000
2021-05-17 08:45:09,964 epoch 6 - iter 45/55 - loss 0.98462027 - samples/sec: 19.50 - lr: 0.100000
2021-05-17 08:45:13,959 epoch 6 - iter 50/55 - loss 0.98594740 - samples/sec: 20.03 - lr: 0.100000
2021-05-17 08:45:17,050 epoch 6 - iter 55/55 - loss 1.05454138 - samples/sec: 25.89 - lr: 0.100000
2021-05-17 08:45:17,051 ----------------------------------------------------------------------------------------------------
2021-05-17 08:45:17,059 EPOCH 6 done: loss 1.0545 - lr 0.1000000
2021-05-17 08:45:20,611 DEV : loss 0.6194931268692017 - score 0.9167
2021-05-17 08:45:20,647 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:45:31,109 ----------------------------------------------------------------------------------------------------
2021-05-17 08:45:36,096 epoch 7 - iter 5/55 - loss 1.58368502 - samples/sec: 16.05 - lr: 0.100000
2021-05-17 08:45:40,684 epoch 7 - iter 10/55 - loss 1.19495947 - samples/sec: 17.44 - lr: 0.100000
2021-05-17 08:45:44,560 epoch 7 - iter 15/55 - loss 0.92531757 - samples/sec: 20.64 - lr: 0.100000
2021-05-17 08:45:48,701 epoch 7 - iter 20/55 - loss 0.90063236 - samples/sec: 19.32 - lr: 0.100000
2021-05-17 08:45:53,011 epoch 7 - iter 25/55 - loss 0.98069908 - samples/sec: 18.57 - lr: 0.100000
2021-05-17 08:45:57,709 epoch 7 - iter 30/55 - loss 0.93083986 - samples/sec: 17.03 - lr: 0.100000
2021-05-17 08:46:02,306 epoch 7 - iter 35/55 - loss 0.93740457 - samples/sec: 17.41 - lr: 0.100000
2021-05-17 08:46:05,918 epoch 7 - iter 40/55 - loss 1.01328706 - samples/sec: 22.15 - lr: 0.100000
2021-05-17 08:46:09,737 epoch 7 - iter 45/55 - loss 0.98967453 - samples/sec: 20.95 - lr: 0.100000
2021-05-17 08:46:13,537 epoch 7 - iter 50/55 - loss 1.00268008 - samples/sec: 21.06 - lr: 0.100000
2021-05-17 08:46:16,623 epoch 7 - iter 55/55 - loss 0.98066439 - samples/sec: 25.93 - lr: 0.100000
2021-05-17 08:46:16,623 ----------------------------------------------------------------------------------------------------
2021-05-17 08:46:16,623 EPOCH 7 done: loss 0.9807 - lr 0.1000000
2021-05-17 08:46:21,698 DEV : loss 0.6621484160423279 - score 0.9248
2021-05-17 08:46:21,738 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 08:46:31,988 ----------------------------------------------------------------------------------------------------
2021-05-17 08:46:37,209 epoch 8 - iter 5/55 - loss 0.74108996 - samples/sec: 15.33 - lr: 0.100000
2021-05-17 08:46:41,413 epoch 8 - iter 10/55 - loss 0.63556834 - samples/sec: 19.03 - lr: 0.100000
2021-05-17 08:46:45,138 epoch 8 - iter 15/55 - loss 0.72089423 - samples/sec: 21.48 - lr: 0.100000
2021-05-17 08:46:48,764 epoch 8 - iter 20/55 - loss 0.75560773 - samples/sec: 22.06 - lr: 0.100000
2021-05-17 08:46:52,642 epoch 8 - iter 25/55 - loss 0.73887402 - samples/sec: 20.64 - lr: 0.100000
2021-05-17 08:46:56,921 epoch 8 - iter 30/55 - loss 0.72435227 - samples/sec: 18.70 - lr: 0.100000
2021-05-17 08:47:01,183 epoch 8 - iter 35/55 - loss 0.72365135 - samples/sec: 18.78 - lr: 0.100000
2021-05-17 08:47:06,202 epoch 8 - iter 40/55 - loss 0.72377631 - samples/sec: 15.94 - lr: 0.100000
2021-05-17 08:47:11,353 epoch 8 - iter 45/55 - loss 0.79231364 - samples/sec: 15.53 - lr: 0.100000
2021-05-17 08:47:17,019 epoch 8 - iter 50/55 - loss 0.77277198 - samples/sec: 14.12 - lr: 0.100000
2021-05-17 08:47:20,859 epoch 8 - iter 55/55 - loss 0.84002701 - samples/sec: 20.85 - lr: 0.100000
2021-05-17 08:47:20,860 ----------------------------------------------------------------------------------------------------
2021-05-17 08:47:20,860 EPOCH 8 done: loss 0.8400 - lr 0.1000000
2021-05-17 08:47:25,894 DEV : loss 0.7103621363639832 - score 0.8851
2021-05-17 08:47:25,935 BAD EPOCHS (no improvement): 1
2021-05-17 08:47:25,938 ----------------------------------------------------------------------------------------------------
2021-05-17 08:47:30,343 epoch 9 - iter 5/55 - loss 0.46638513 - samples/sec: 18.16 - lr: 0.100000
2021-05-17 08:47:34,853 epoch 9 - iter 10/55 - loss 0.68019187 - samples/sec: 17.75 - lr: 0.100000
2021-05-17 08:47:40,359 epoch 9 - iter 15/55 - loss 0.73535461 - samples/sec: 14.53 - lr: 0.100000
2021-05-17 08:47:44,190 epoch 9 - iter 20/55 - loss 0.75933006 - samples/sec: 20.88 - lr: 0.100000
2021-05-17 08:47:47,752 epoch 9 - iter 25/55 - loss 0.74797389 - samples/sec: 22.47 - lr: 0.100000
2021-05-17 08:47:51,332 epoch 9 - iter 30/55 - loss 0.70238562 - samples/sec: 22.35 - lr: 0.100000
2021-05-17 08:47:55,002 epoch 9 - iter 35/55 - loss 0.70383858 - samples/sec: 21.80 - lr: 0.100000
2021-05-17 08:47:58,594 epoch 9 - iter 40/55 - loss 0.78650225 - samples/sec: 22.28 - lr: 0.100000
2021-05-17 08:48:02,300 epoch 9 - iter 45/55 - loss 0.80377061 - samples/sec: 21.62 - lr: 0.100000
2021-05-17 08:48:06,698 epoch 9 - iter 50/55 - loss 0.76801487 - samples/sec: 18.19 - lr: 0.100000
2021-05-17 08:48:10,119 epoch 9 - iter 55/55 - loss 0.76199995 - samples/sec: 23.39 - lr: 0.100000
2021-05-17 08:48:10,120 ----------------------------------------------------------------------------------------------------
2021-05-17 08:48:10,120 EPOCH 9 done: loss 0.7620 - lr 0.1000000
2021-05-17 08:48:14,695 DEV : loss 0.6777138710021973 - score 0.9216
2021-05-17 08:48:14,762 BAD EPOCHS (no improvement): 2
2021-05-17 08:48:14,762 ----------------------------------------------------------------------------------------------------
2021-05-17 08:48:18,521 epoch 10 - iter 5/55 - loss 0.66417065 - samples/sec: 21.29 - lr: 0.100000
2021-05-17 08:48:22,456 epoch 10 - iter 10/55 - loss 0.64284563 - samples/sec: 20.33 - lr: 0.100000
2021-05-17 08:48:26,798 epoch 10 - iter 15/55 - loss 0.59657879 - samples/sec: 18.43 - lr: 0.100000
2021-05-17 08:48:30,533 epoch 10 - iter 20/55 - loss 0.68084081 - samples/sec: 21.46 - lr: 0.100000
2021-05-17 08:48:34,325 epoch 10 - iter 25/55 - loss 0.65261165 - samples/sec: 21.10 - lr: 0.100000
2021-05-17 08:48:37,951 epoch 10 - iter 30/55 - loss 0.66683101 - samples/sec: 22.07 - lr: 0.100000
2021-05-17 08:48:41,378 epoch 10 - iter 35/55 - loss 0.67856674 - samples/sec: 23.35 - lr: 0.100000
2021-05-17 08:48:45,476 epoch 10 - iter 40/55 - loss 0.71564107 - samples/sec: 19.53 - lr: 0.100000
2021-05-17 08:48:49,751 epoch 10 - iter 45/55 - loss 0.76595698 - samples/sec: 18.74 - lr: 0.100000
2021-05-17 08:48:53,791 epoch 10 - iter 50/55 - loss 0.73487970 - samples/sec: 19.86 - lr: 0.100000
2021-05-17 08:48:56,994 epoch 10 - iter 55/55 - loss 0.82109417 - samples/sec: 24.98 - lr: 0.100000
2021-05-17 08:48:56,994 ----------------------------------------------------------------------------------------------------
2021-05-17 08:48:56,994 EPOCH 10 done: loss 0.8211 - lr 0.1000000
2021-05-17 08:49:00,840 DEV : loss 0.8173543810844421 - score 0.8913
2021-05-17 08:49:00,876 BAD EPOCHS (no improvement): 3
2021-05-17 08:49:00,877 ----------------------------------------------------------------------------------------------------
2021-05-17 08:49:05,555 epoch 11 - iter 5/55 - loss 0.80441875 - samples/sec: 17.13 - lr: 0.100000
2021-05-17 08:49:09,760 epoch 11 - iter 10/55 - loss 1.20615946 - samples/sec: 19.03 - lr: 0.100000
2021-05-17 08:49:14,678 epoch 11 - iter 15/55 - loss 1.05238334 - samples/sec: 16.27 - lr: 0.100000
2021-05-17 08:49:19,278 epoch 11 - iter 20/55 - loss 1.03043482 - samples/sec: 17.39 - lr: 0.100000
2021-05-17 08:49:23,266 epoch 11 - iter 25/55 - loss 0.95523905 - samples/sec: 20.08 - lr: 0.100000
2021-05-17 08:49:27,211 epoch 11 - iter 30/55 - loss 0.92137677 - samples/sec: 20.28 - lr: 0.100000
2021-05-17 08:49:31,603 epoch 11 - iter 35/55 - loss 0.88892605 - samples/sec: 18.22 - lr: 0.100000
2021-05-17 08:49:36,502 epoch 11 - iter 40/55 - loss 0.88482180 - samples/sec: 16.33 - lr: 0.100000
2021-05-17 08:49:40,967 epoch 11 - iter 45/55 - loss 0.85799063 - samples/sec: 17.92 - lr: 0.100000
2021-05-17 08:49:44,855 epoch 11 - iter 50/55 - loss 0.83379169 - samples/sec: 20.59 - lr: 0.100000
2021-05-17 08:49:47,961 epoch 11 - iter 55/55 - loss 0.82131527 - samples/sec: 25.76 - lr: 0.100000
2021-05-17 08:49:47,962 ----------------------------------------------------------------------------------------------------
2021-05-17 08:49:47,962 EPOCH 11 done: loss 0.8213 - lr 0.1000000
2021-05-17 08:49:52,715 DEV : loss 1.4535927772521973 - score 0.8384
Epoch    11: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 08:49:52,760 BAD EPOCHS (no improvement): 4
2021-05-17 08:49:52,760 ----------------------------------------------------------------------------------------------------
2021-05-17 08:49:57,291 epoch 12 - iter 5/55 - loss 0.63932061 - samples/sec: 17.66 - lr: 0.050000
2021-05-17 08:50:01,862 epoch 12 - iter 10/55 - loss 0.73901880 - samples/sec: 17.50 - lr: 0.050000
2021-05-17 08:50:05,587 epoch 12 - iter 15/55 - loss 0.61244118 - samples/sec: 21.48 - lr: 0.050000
2021-05-17 08:50:09,908 epoch 12 - iter 20/55 - loss 0.64232201 - samples/sec: 18.57 - lr: 0.050000
2021-05-17 08:50:14,600 epoch 12 - iter 25/55 - loss 0.58704746 - samples/sec: 17.05 - lr: 0.050000
2021-05-17 08:50:18,603 epoch 12 - iter 30/55 - loss 0.59292971 - samples/sec: 19.99 - lr: 0.050000
2021-05-17 08:50:22,431 epoch 12 - iter 35/55 - loss 0.60260678 - samples/sec: 20.90 - lr: 0.050000
2021-05-17 08:50:26,254 epoch 12 - iter 40/55 - loss 0.57767657 - samples/sec: 20.94 - lr: 0.050000
2021-05-17 08:50:30,297 epoch 12 - iter 45/55 - loss 0.56773339 - samples/sec: 19.79 - lr: 0.050000
2021-05-17 08:50:34,501 epoch 12 - iter 50/55 - loss 0.56105468 - samples/sec: 19.05 - lr: 0.050000
2021-05-17 08:50:37,529 epoch 12 - iter 55/55 - loss 0.55111972 - samples/sec: 26.43 - lr: 0.050000
2021-05-17 08:50:37,529 ----------------------------------------------------------------------------------------------------
2021-05-17 08:50:37,530 EPOCH 12 done: loss 0.5511 - lr 0.0500000
2021-05-17 08:50:41,234 DEV : loss 0.96271151304245 - score 0.8849
2021-05-17 08:50:41,271 BAD EPOCHS (no improvement): 1
2021-05-17 08:50:41,272 ----------------------------------------------------------------------------------------------------
2021-05-17 08:50:44,468 epoch 13 - iter 5/55 - loss 0.98523694 - samples/sec: 25.04 - lr: 0.050000
2021-05-17 08:50:48,460 epoch 13 - iter 10/55 - loss 0.67438516 - samples/sec: 20.05 - lr: 0.050000
2021-05-17 08:50:52,529 epoch 13 - iter 15/55 - loss 0.60373800 - samples/sec: 19.67 - lr: 0.050000
2021-05-17 08:50:56,393 epoch 13 - iter 20/55 - loss 0.52913147 - samples/sec: 20.71 - lr: 0.050000
2021-05-17 08:51:00,861 epoch 13 - iter 25/55 - loss 0.59619663 - samples/sec: 17.96 - lr: 0.050000
2021-05-17 08:51:05,690 epoch 13 - iter 30/55 - loss 0.61034249 - samples/sec: 16.57 - lr: 0.050000
2021-05-17 08:51:08,928 epoch 13 - iter 35/55 - loss 0.61010972 - samples/sec: 24.72 - lr: 0.050000
2021-05-17 08:51:12,383 epoch 13 - iter 40/55 - loss 0.57721223 - samples/sec: 23.16 - lr: 0.050000
2021-05-17 08:51:16,082 epoch 13 - iter 45/55 - loss 0.54940882 - samples/sec: 21.63 - lr: 0.050000
2021-05-17 08:51:19,835 epoch 13 - iter 50/55 - loss 0.55641474 - samples/sec: 21.32 - lr: 0.050000
2021-05-17 08:51:22,904 epoch 13 - iter 55/55 - loss 0.53247548 - samples/sec: 26.08 - lr: 0.050000
2021-05-17 08:51:22,904 ----------------------------------------------------------------------------------------------------
2021-05-17 08:51:22,904 EPOCH 13 done: loss 0.5325 - lr 0.0500000
2021-05-17 08:51:27,214 DEV : loss 0.6851941347122192 - score 0.9167
2021-05-17 08:51:27,251 BAD EPOCHS (no improvement): 2
2021-05-17 08:51:27,251 ----------------------------------------------------------------------------------------------------
2021-05-17 08:51:31,105 epoch 14 - iter 5/55 - loss 0.83492203 - samples/sec: 20.76 - lr: 0.050000
2021-05-17 08:51:35,211 epoch 14 - iter 10/55 - loss 0.82609251 - samples/sec: 19.51 - lr: 0.050000
2021-05-17 08:51:39,482 epoch 14 - iter 15/55 - loss 0.70014415 - samples/sec: 18.73 - lr: 0.050000
2021-05-17 08:51:43,462 epoch 14 - iter 20/55 - loss 0.66263371 - samples/sec: 20.13 - lr: 0.050000
2021-05-17 08:51:47,459 epoch 14 - iter 25/55 - loss 0.66113653 - samples/sec: 20.02 - lr: 0.050000
2021-05-17 08:51:52,448 epoch 14 - iter 30/55 - loss 0.60398846 - samples/sec: 16.04 - lr: 0.050000
2021-05-17 08:51:57,619 epoch 14 - iter 35/55 - loss 0.59861189 - samples/sec: 15.48 - lr: 0.050000
2021-05-17 08:52:01,770 epoch 14 - iter 40/55 - loss 0.57402595 - samples/sec: 19.28 - lr: 0.050000
2021-05-17 08:52:05,872 epoch 14 - iter 45/55 - loss 0.55948848 - samples/sec: 19.51 - lr: 0.050000
2021-05-17 08:52:10,215 epoch 14 - iter 50/55 - loss 0.53841634 - samples/sec: 18.43 - lr: 0.050000
2021-05-17 08:52:13,409 epoch 14 - iter 55/55 - loss 0.50783772 - samples/sec: 25.09 - lr: 0.050000
2021-05-17 08:52:13,410 ----------------------------------------------------------------------------------------------------
2021-05-17 08:52:13,410 EPOCH 14 done: loss 0.5078 - lr 0.0500000
2021-05-17 08:52:17,142 DEV : loss 0.8747152090072632 - score 0.8885
2021-05-17 08:52:17,178 BAD EPOCHS (no improvement): 3
2021-05-17 08:52:17,178 ----------------------------------------------------------------------------------------------------
2021-05-17 08:52:21,282 epoch 15 - iter 5/55 - loss 0.46143148 - samples/sec: 19.50 - lr: 0.050000
2021-05-17 08:52:25,669 epoch 15 - iter 10/55 - loss 0.44299735 - samples/sec: 18.25 - lr: 0.050000
2021-05-17 08:52:29,996 epoch 15 - iter 15/55 - loss 0.45382068 - samples/sec: 18.49 - lr: 0.050000
2021-05-17 08:52:34,742 epoch 15 - iter 20/55 - loss 0.41799235 - samples/sec: 16.86 - lr: 0.050000
2021-05-17 08:52:40,158 epoch 15 - iter 25/55 - loss 0.44707397 - samples/sec: 14.77 - lr: 0.050000
2021-05-17 08:52:45,029 epoch 15 - iter 30/55 - loss 0.46842560 - samples/sec: 16.43 - lr: 0.050000
2021-05-17 08:52:49,892 epoch 15 - iter 35/55 - loss 0.51020003 - samples/sec: 16.45 - lr: 0.050000
2021-05-17 08:52:54,650 epoch 15 - iter 40/55 - loss 0.47765736 - samples/sec: 16.81 - lr: 0.050000
2021-05-17 08:53:00,151 epoch 15 - iter 45/55 - loss 0.47810743 - samples/sec: 14.55 - lr: 0.050000
2021-05-17 08:53:06,682 epoch 15 - iter 50/55 - loss 0.48088057 - samples/sec: 12.25 - lr: 0.050000
2021-05-17 08:53:11,825 epoch 15 - iter 55/55 - loss 0.49674287 - samples/sec: 15.56 - lr: 0.050000
2021-05-17 08:53:11,825 ----------------------------------------------------------------------------------------------------
2021-05-17 08:53:11,826 EPOCH 15 done: loss 0.4967 - lr 0.0500000
2021-05-17 08:53:17,530 DEV : loss 0.7208985090255737 - score 0.9222
Epoch    15: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 08:53:17,566 BAD EPOCHS (no improvement): 4
2021-05-17 08:53:17,567 ----------------------------------------------------------------------------------------------------
2021-05-17 08:53:21,638 epoch 16 - iter 5/55 - loss 0.26683801 - samples/sec: 19.65 - lr: 0.025000
2021-05-17 08:53:25,606 epoch 16 - iter 10/55 - loss 0.51082818 - samples/sec: 20.17 - lr: 0.025000
2021-05-17 08:53:29,629 epoch 16 - iter 15/55 - loss 0.43768418 - samples/sec: 19.89 - lr: 0.025000
2021-05-17 08:53:33,586 epoch 16 - iter 20/55 - loss 0.42211227 - samples/sec: 20.23 - lr: 0.025000
2021-05-17 08:53:37,830 epoch 16 - iter 25/55 - loss 0.42749077 - samples/sec: 18.85 - lr: 0.025000
2021-05-17 08:53:42,236 epoch 16 - iter 30/55 - loss 0.43576387 - samples/sec: 18.16 - lr: 0.025000
2021-05-17 08:53:46,653 epoch 16 - iter 35/55 - loss 0.42819548 - samples/sec: 18.11 - lr: 0.025000
2021-05-17 08:53:51,602 epoch 16 - iter 40/55 - loss 0.42822832 - samples/sec: 16.17 - lr: 0.025000
2021-05-17 08:53:56,590 epoch 16 - iter 45/55 - loss 0.49202025 - samples/sec: 16.04 - lr: 0.025000
2021-05-17 08:54:01,248 epoch 16 - iter 50/55 - loss 0.48253688 - samples/sec: 17.19 - lr: 0.025000
2021-05-17 08:54:05,513 epoch 16 - iter 55/55 - loss 0.47213525 - samples/sec: 18.77 - lr: 0.025000
2021-05-17 08:54:05,514 ----------------------------------------------------------------------------------------------------
2021-05-17 08:54:05,514 EPOCH 16 done: loss 0.4721 - lr 0.0250000
2021-05-17 08:54:09,683 DEV : loss 0.7690750956535339 - score 0.9213
2021-05-17 08:54:09,720 BAD EPOCHS (no improvement): 1
2021-05-17 08:54:09,721 ----------------------------------------------------------------------------------------------------
2021-05-17 08:54:14,059 epoch 17 - iter 5/55 - loss 0.39959221 - samples/sec: 18.45 - lr: 0.025000
2021-05-17 08:54:17,941 epoch 17 - iter 10/55 - loss 0.38331419 - samples/sec: 20.63 - lr: 0.025000
2021-05-17 08:54:21,864 epoch 17 - iter 15/55 - loss 0.45870289 - samples/sec: 20.40 - lr: 0.025000
2021-05-17 08:54:26,247 epoch 17 - iter 20/55 - loss 0.42228321 - samples/sec: 18.26 - lr: 0.025000
2021-05-17 08:54:30,837 epoch 17 - iter 25/55 - loss 0.38840230 - samples/sec: 17.44 - lr: 0.025000
2021-05-17 08:54:36,542 epoch 17 - iter 30/55 - loss 0.39395759 - samples/sec: 14.02 - lr: 0.025000
2021-05-17 08:54:41,408 epoch 17 - iter 35/55 - loss 0.43722007 - samples/sec: 16.44 - lr: 0.025000
2021-05-17 08:54:45,288 epoch 17 - iter 40/55 - loss 0.46641733 - samples/sec: 20.62 - lr: 0.025000
2021-05-17 08:54:50,061 epoch 17 - iter 45/55 - loss 0.47147112 - samples/sec: 16.76 - lr: 0.025000
2021-05-17 08:54:55,579 epoch 17 - iter 50/55 - loss 0.45047239 - samples/sec: 14.50 - lr: 0.025000
2021-05-17 08:54:59,519 epoch 17 - iter 55/55 - loss 0.43285789 - samples/sec: 20.31 - lr: 0.025000
2021-05-17 08:54:59,520 ----------------------------------------------------------------------------------------------------
2021-05-17 08:54:59,520 EPOCH 17 done: loss 0.4329 - lr 0.0250000
2021-05-17 08:55:05,014 DEV : loss 0.7611985206604004 - score 0.9159
2021-05-17 08:55:05,109 BAD EPOCHS (no improvement): 2
2021-05-17 08:55:05,112 ----------------------------------------------------------------------------------------------------
2021-05-17 08:55:09,641 epoch 18 - iter 5/55 - loss 0.21118221 - samples/sec: 17.67 - lr: 0.025000
2021-05-17 08:55:14,283 epoch 18 - iter 10/55 - loss 0.38826685 - samples/sec: 17.25 - lr: 0.025000
2021-05-17 08:55:18,891 epoch 18 - iter 15/55 - loss 0.32443859 - samples/sec: 17.41 - lr: 0.025000
2021-05-17 08:55:23,550 epoch 18 - iter 20/55 - loss 0.40831221 - samples/sec: 17.17 - lr: 0.025000
2021-05-17 08:55:28,178 epoch 18 - iter 25/55 - loss 0.39526336 - samples/sec: 17.29 - lr: 0.025000
2021-05-17 08:55:32,120 epoch 18 - iter 30/55 - loss 0.40954518 - samples/sec: 20.31 - lr: 0.025000
2021-05-17 08:55:36,215 epoch 18 - iter 35/55 - loss 0.39036088 - samples/sec: 19.54 - lr: 0.025000
2021-05-17 08:55:39,781 epoch 18 - iter 40/55 - loss 0.37482687 - samples/sec: 22.46 - lr: 0.025000
2021-05-17 08:55:44,208 epoch 18 - iter 45/55 - loss 0.36976560 - samples/sec: 18.07 - lr: 0.025000
2021-05-17 08:55:48,734 epoch 18 - iter 50/55 - loss 0.38956860 - samples/sec: 17.67 - lr: 0.025000
2021-05-17 08:55:52,090 epoch 18 - iter 55/55 - loss 0.41297707 - samples/sec: 23.85 - lr: 0.025000
2021-05-17 08:55:52,090 ----------------------------------------------------------------------------------------------------
2021-05-17 08:55:52,090 EPOCH 18 done: loss 0.4130 - lr 0.0250000
2021-05-17 08:55:56,494 DEV : loss 1.070802927017212 - score 0.8702
2021-05-17 08:55:56,572 BAD EPOCHS (no improvement): 3
2021-05-17 08:55:56,573 ----------------------------------------------------------------------------------------------------
2021-05-17 08:56:00,732 epoch 19 - iter 5/55 - loss 0.68818707 - samples/sec: 19.24 - lr: 0.025000
2021-05-17 08:56:05,142 epoch 19 - iter 10/55 - loss 0.52675790 - samples/sec: 18.14 - lr: 0.025000
2021-05-17 08:56:09,131 epoch 19 - iter 15/55 - loss 0.40196937 - samples/sec: 20.06 - lr: 0.025000
2021-05-17 08:56:13,688 epoch 19 - iter 20/55 - loss 0.37832772 - samples/sec: 17.56 - lr: 0.025000
2021-05-17 08:56:17,791 epoch 19 - iter 25/55 - loss 0.39730054 - samples/sec: 19.50 - lr: 0.025000
2021-05-17 08:56:22,306 epoch 19 - iter 30/55 - loss 0.38793492 - samples/sec: 17.72 - lr: 0.025000
2021-05-17 08:56:27,653 epoch 19 - iter 35/55 - loss 0.36396730 - samples/sec: 14.96 - lr: 0.025000
2021-05-17 08:56:32,883 epoch 19 - iter 40/55 - loss 0.38394365 - samples/sec: 15.30 - lr: 0.025000
2021-05-17 08:56:37,412 epoch 19 - iter 45/55 - loss 0.37895162 - samples/sec: 17.67 - lr: 0.025000
2021-05-17 08:56:41,199 epoch 19 - iter 50/55 - loss 0.38290587 - samples/sec: 21.13 - lr: 0.025000
2021-05-17 08:56:44,196 epoch 19 - iter 55/55 - loss 0.38259209 - samples/sec: 26.70 - lr: 0.025000
2021-05-17 08:56:44,197 ----------------------------------------------------------------------------------------------------
2021-05-17 08:56:44,197 EPOCH 19 done: loss 0.3826 - lr 0.0250000
2021-05-17 08:56:48,802 DEV : loss 0.8765848875045776 - score 0.9091
Epoch    19: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 08:56:48,839 BAD EPOCHS (no improvement): 4
2021-05-17 08:56:48,839 ----------------------------------------------------------------------------------------------------
2021-05-17 08:56:53,613 epoch 20 - iter 5/55 - loss 0.57408669 - samples/sec: 16.76 - lr: 0.012500
2021-05-17 08:56:57,815 epoch 20 - iter 10/55 - loss 0.64374782 - samples/sec: 19.04 - lr: 0.012500
2021-05-17 08:57:02,192 epoch 20 - iter 15/55 - loss 0.56590321 - samples/sec: 18.28 - lr: 0.012500
2021-05-17 08:57:05,984 epoch 20 - iter 20/55 - loss 0.53816232 - samples/sec: 21.10 - lr: 0.012500
2021-05-17 08:57:09,615 epoch 20 - iter 25/55 - loss 0.50028749 - samples/sec: 22.04 - lr: 0.012500
2021-05-17 08:57:13,528 epoch 20 - iter 30/55 - loss 0.49435449 - samples/sec: 20.45 - lr: 0.012500
2021-05-17 08:57:18,272 epoch 20 - iter 35/55 - loss 0.46788308 - samples/sec: 16.87 - lr: 0.012500
2021-05-17 08:57:22,614 epoch 20 - iter 40/55 - loss 0.47413408 - samples/sec: 18.43 - lr: 0.012500
2021-05-17 08:57:26,494 epoch 20 - iter 45/55 - loss 0.44514638 - samples/sec: 20.62 - lr: 0.012500
2021-05-17 08:57:30,680 epoch 20 - iter 50/55 - loss 0.44391448 - samples/sec: 19.12 - lr: 0.012500
2021-05-17 08:57:34,096 epoch 20 - iter 55/55 - loss 0.43147450 - samples/sec: 23.43 - lr: 0.012500
2021-05-17 08:57:34,096 ----------------------------------------------------------------------------------------------------
2021-05-17 08:57:34,096 EPOCH 20 done: loss 0.4315 - lr 0.0125000
2021-05-17 08:57:37,820 DEV : loss 0.8031554222106934 - score 0.9135
2021-05-17 08:57:37,856 BAD EPOCHS (no improvement): 1
2021-05-17 08:57:37,857 ----------------------------------------------------------------------------------------------------
2021-05-17 08:57:41,541 epoch 21 - iter 5/55 - loss 0.36451890 - samples/sec: 21.72 - lr: 0.012500
2021-05-17 08:57:45,795 epoch 21 - iter 10/55 - loss 0.48821145 - samples/sec: 18.81 - lr: 0.012500
2021-05-17 08:57:50,531 epoch 21 - iter 15/55 - loss 0.57792267 - samples/sec: 16.89 - lr: 0.012500
2021-05-17 08:57:54,590 epoch 21 - iter 20/55 - loss 0.51459959 - samples/sec: 19.71 - lr: 0.012500
2021-05-17 08:57:58,282 epoch 21 - iter 25/55 - loss 0.49798533 - samples/sec: 21.67 - lr: 0.012500
2021-05-17 08:58:02,840 epoch 21 - iter 30/55 - loss 0.46934277 - samples/sec: 17.55 - lr: 0.012500
2021-05-17 08:58:06,928 epoch 21 - iter 35/55 - loss 0.46104426 - samples/sec: 19.58 - lr: 0.012500
2021-05-17 08:58:11,064 epoch 21 - iter 40/55 - loss 0.44134760 - samples/sec: 19.35 - lr: 0.012500
2021-05-17 08:58:15,202 epoch 21 - iter 45/55 - loss 0.43227848 - samples/sec: 19.34 - lr: 0.012500
2021-05-17 08:58:19,455 epoch 21 - iter 50/55 - loss 0.41693155 - samples/sec: 18.81 - lr: 0.012500
2021-05-17 08:58:22,504 epoch 21 - iter 55/55 - loss 0.41633949 - samples/sec: 26.25 - lr: 0.012500
2021-05-17 08:58:22,504 ----------------------------------------------------------------------------------------------------
2021-05-17 08:58:22,504 EPOCH 21 done: loss 0.4163 - lr 0.0125000
2021-05-17 08:58:26,176 DEV : loss 0.9144809246063232 - score 0.8933
2021-05-17 08:58:26,213 BAD EPOCHS (no improvement): 2
2021-05-17 08:58:26,213 ----------------------------------------------------------------------------------------------------
2021-05-17 08:58:29,592 epoch 22 - iter 5/55 - loss 0.30912113 - samples/sec: 23.69 - lr: 0.012500
2021-05-17 08:58:33,360 epoch 22 - iter 10/55 - loss 0.28520572 - samples/sec: 21.23 - lr: 0.012500
2021-05-17 08:58:37,515 epoch 22 - iter 15/55 - loss 0.29323233 - samples/sec: 19.26 - lr: 0.012500
2021-05-17 08:58:41,676 epoch 22 - iter 20/55 - loss 0.32096049 - samples/sec: 19.25 - lr: 0.012500
2021-05-17 08:58:45,328 epoch 22 - iter 25/55 - loss 0.28980600 - samples/sec: 21.91 - lr: 0.012500
2021-05-17 08:58:49,155 epoch 22 - iter 30/55 - loss 0.29943101 - samples/sec: 20.91 - lr: 0.012500
2021-05-17 08:58:52,770 epoch 22 - iter 35/55 - loss 0.32454959 - samples/sec: 22.14 - lr: 0.012500
2021-05-17 08:58:56,961 epoch 22 - iter 40/55 - loss 0.38061389 - samples/sec: 19.10 - lr: 0.012500
2021-05-17 08:59:01,751 epoch 22 - iter 45/55 - loss 0.37271597 - samples/sec: 16.70 - lr: 0.012500
2021-05-17 08:59:06,743 epoch 22 - iter 50/55 - loss 0.38001048 - samples/sec: 16.04 - lr: 0.012500
2021-05-17 08:59:11,160 epoch 22 - iter 55/55 - loss 0.37559876 - samples/sec: 18.12 - lr: 0.012500
2021-05-17 08:59:11,161 ----------------------------------------------------------------------------------------------------
2021-05-17 08:59:11,161 EPOCH 22 done: loss 0.3756 - lr 0.0125000
2021-05-17 08:59:16,639 DEV : loss 0.7607161402702332 - score 0.919
2021-05-17 08:59:16,677 BAD EPOCHS (no improvement): 3
2021-05-17 08:59:16,677 ----------------------------------------------------------------------------------------------------
2021-05-17 08:59:21,307 epoch 23 - iter 5/55 - loss 0.33780308 - samples/sec: 17.28 - lr: 0.012500
2021-05-17 08:59:26,711 epoch 23 - iter 10/55 - loss 0.31260872 - samples/sec: 14.81 - lr: 0.012500
2021-05-17 08:59:31,101 epoch 23 - iter 15/55 - loss 0.33115629 - samples/sec: 18.23 - lr: 0.012500
2021-05-17 08:59:35,256 epoch 23 - iter 20/55 - loss 0.30900772 - samples/sec: 19.26 - lr: 0.012500
2021-05-17 08:59:39,334 epoch 23 - iter 25/55 - loss 0.32512789 - samples/sec: 19.62 - lr: 0.012500
2021-05-17 08:59:43,962 epoch 23 - iter 30/55 - loss 0.34480815 - samples/sec: 17.29 - lr: 0.012500
2021-05-17 08:59:48,132 epoch 23 - iter 35/55 - loss 0.33623255 - samples/sec: 19.19 - lr: 0.012500
2021-05-17 08:59:51,994 epoch 23 - iter 40/55 - loss 0.32416316 - samples/sec: 20.72 - lr: 0.012500
2021-05-17 08:59:55,434 epoch 23 - iter 45/55 - loss 0.30216413 - samples/sec: 23.26 - lr: 0.012500
2021-05-17 09:00:00,578 epoch 23 - iter 50/55 - loss 0.33507886 - samples/sec: 15.56 - lr: 0.012500
2021-05-17 09:00:05,114 epoch 23 - iter 55/55 - loss 0.32650179 - samples/sec: 17.66 - lr: 0.012500
2021-05-17 09:00:05,114 ----------------------------------------------------------------------------------------------------
2021-05-17 09:00:05,114 EPOCH 23 done: loss 0.3265 - lr 0.0125000
2021-05-17 09:00:10,189 DEV : loss 0.8109532594680786 - score 0.9125
Epoch    23: reducing learning rate of group 0 to 6.2500e-03.
2021-05-17 09:00:10,254 BAD EPOCHS (no improvement): 4
2021-05-17 09:00:10,254 ----------------------------------------------------------------------------------------------------
2021-05-17 09:00:13,997 epoch 24 - iter 5/55 - loss 0.22817693 - samples/sec: 21.39 - lr: 0.006250
2021-05-17 09:00:17,670 epoch 24 - iter 10/55 - loss 0.27752237 - samples/sec: 21.78 - lr: 0.006250
2021-05-17 09:00:21,812 epoch 24 - iter 15/55 - loss 0.31205784 - samples/sec: 19.32 - lr: 0.006250
2021-05-17 09:00:25,684 epoch 24 - iter 20/55 - loss 0.34247441 - samples/sec: 20.66 - lr: 0.006250
2021-05-17 09:00:30,154 epoch 24 - iter 25/55 - loss 0.45836073 - samples/sec: 17.90 - lr: 0.006250
2021-05-17 09:00:34,465 epoch 24 - iter 30/55 - loss 0.41684086 - samples/sec: 18.56 - lr: 0.006250
2021-05-17 09:00:39,272 epoch 24 - iter 35/55 - loss 0.37830612 - samples/sec: 16.64 - lr: 0.006250
2021-05-17 09:00:42,625 epoch 24 - iter 40/55 - loss 0.37219859 - samples/sec: 23.86 - lr: 0.006250
2021-05-17 09:00:45,906 epoch 24 - iter 45/55 - loss 0.35723680 - samples/sec: 24.38 - lr: 0.006250
2021-05-17 09:00:49,596 epoch 24 - iter 50/55 - loss 0.33704986 - samples/sec: 21.69 - lr: 0.006250
2021-05-17 09:00:53,015 epoch 24 - iter 55/55 - loss 0.33998170 - samples/sec: 23.41 - lr: 0.006250
2021-05-17 09:00:53,015 ----------------------------------------------------------------------------------------------------
2021-05-17 09:00:53,015 EPOCH 24 done: loss 0.3400 - lr 0.0062500
2021-05-17 09:00:57,263 DEV : loss 0.790529191493988 - score 0.9121
2021-05-17 09:00:57,298 BAD EPOCHS (no improvement): 1
2021-05-17 09:00:57,299 ----------------------------------------------------------------------------------------------------
2021-05-17 09:01:02,034 epoch 25 - iter 5/55 - loss 0.28751428 - samples/sec: 16.90 - lr: 0.006250
2021-05-17 09:01:07,396 epoch 25 - iter 10/55 - loss 0.24892222 - samples/sec: 14.92 - lr: 0.006250
2021-05-17 09:01:12,275 epoch 25 - iter 15/55 - loss 0.22768962 - samples/sec: 16.40 - lr: 0.006250
2021-05-17 09:01:16,886 epoch 25 - iter 20/55 - loss 0.25263242 - samples/sec: 17.35 - lr: 0.006250
2021-05-17 09:01:21,366 epoch 25 - iter 25/55 - loss 0.25645633 - samples/sec: 17.86 - lr: 0.006250
2021-05-17 09:01:26,686 epoch 25 - iter 30/55 - loss 0.31557502 - samples/sec: 15.04 - lr: 0.006250
2021-05-17 09:01:31,808 epoch 25 - iter 35/55 - loss 0.32764057 - samples/sec: 15.62 - lr: 0.006250
2021-05-17 09:01:36,800 epoch 25 - iter 40/55 - loss 0.33740462 - samples/sec: 16.04 - lr: 0.006250
2021-05-17 09:01:41,360 epoch 25 - iter 45/55 - loss 0.36508004 - samples/sec: 17.55 - lr: 0.006250
2021-05-17 09:01:45,232 epoch 25 - iter 50/55 - loss 0.36070161 - samples/sec: 20.68 - lr: 0.006250
2021-05-17 09:01:48,388 epoch 25 - iter 55/55 - loss 0.34625680 - samples/sec: 25.35 - lr: 0.006250
2021-05-17 09:01:48,388 ----------------------------------------------------------------------------------------------------
2021-05-17 09:01:48,389 EPOCH 25 done: loss 0.3463 - lr 0.0062500
2021-05-17 09:01:51,908 DEV : loss 0.74553382396698 - score 0.917
2021-05-17 09:01:51,944 BAD EPOCHS (no improvement): 2
2021-05-17 09:01:51,944 ----------------------------------------------------------------------------------------------------
2021-05-17 09:01:55,488 epoch 26 - iter 5/55 - loss 0.51241760 - samples/sec: 22.58 - lr: 0.006250
2021-05-17 09:01:59,601 epoch 26 - iter 10/55 - loss 0.51664121 - samples/sec: 19.45 - lr: 0.006250
2021-05-17 09:02:05,027 epoch 26 - iter 15/55 - loss 0.46444850 - samples/sec: 14.75 - lr: 0.006250
2021-05-17 09:02:10,168 epoch 26 - iter 20/55 - loss 0.40856315 - samples/sec: 15.56 - lr: 0.006250
2021-05-17 09:02:14,121 epoch 26 - iter 25/55 - loss 0.38748232 - samples/sec: 20.25 - lr: 0.006250
2021-05-17 09:02:18,253 epoch 26 - iter 30/55 - loss 0.36387143 - samples/sec: 19.37 - lr: 0.006250
2021-05-17 09:02:22,577 epoch 26 - iter 35/55 - loss 0.35420788 - samples/sec: 18.51 - lr: 0.006250
2021-05-17 09:02:26,686 epoch 26 - iter 40/55 - loss 0.32805593 - samples/sec: 19.47 - lr: 0.006250
2021-05-17 09:02:31,012 epoch 26 - iter 45/55 - loss 0.32395153 - samples/sec: 18.50 - lr: 0.006250
2021-05-17 09:02:35,046 epoch 26 - iter 50/55 - loss 0.31018099 - samples/sec: 19.84 - lr: 0.006250
2021-05-17 09:02:38,688 epoch 26 - iter 55/55 - loss 0.31384215 - samples/sec: 21.97 - lr: 0.006250
2021-05-17 09:02:38,689 ----------------------------------------------------------------------------------------------------
2021-05-17 09:02:38,689 EPOCH 26 done: loss 0.3138 - lr 0.0062500
2021-05-17 09:02:42,780 DEV : loss 0.7719961404800415 - score 0.9135
2021-05-17 09:02:42,821 BAD EPOCHS (no improvement): 3
2021-05-17 09:02:42,821 ----------------------------------------------------------------------------------------------------
2021-05-17 09:02:46,320 epoch 27 - iter 5/55 - loss 0.26506495 - samples/sec: 22.87 - lr: 0.006250
2021-05-17 09:02:49,781 epoch 27 - iter 10/55 - loss 0.27368696 - samples/sec: 23.12 - lr: 0.006250
2021-05-17 09:02:53,874 epoch 27 - iter 15/55 - loss 0.50282374 - samples/sec: 19.57 - lr: 0.006250
2021-05-17 09:02:59,142 epoch 27 - iter 20/55 - loss 0.43580229 - samples/sec: 15.19 - lr: 0.006250
2021-05-17 09:03:04,044 epoch 27 - iter 25/55 - loss 0.40786001 - samples/sec: 16.32 - lr: 0.006250
2021-05-17 09:03:08,846 epoch 27 - iter 30/55 - loss 0.40300103 - samples/sec: 16.66 - lr: 0.006250
2021-05-17 09:03:14,058 epoch 27 - iter 35/55 - loss 0.38195988 - samples/sec: 15.35 - lr: 0.006250
2021-05-17 09:03:18,369 epoch 27 - iter 40/55 - loss 0.37262615 - samples/sec: 18.56 - lr: 0.006250
2021-05-17 09:03:23,155 epoch 27 - iter 45/55 - loss 0.35846935 - samples/sec: 16.72 - lr: 0.006250
2021-05-17 09:03:28,835 epoch 27 - iter 50/55 - loss 0.34415460 - samples/sec: 14.09 - lr: 0.006250
2021-05-17 09:03:32,538 epoch 27 - iter 55/55 - loss 0.37043346 - samples/sec: 21.61 - lr: 0.006250
2021-05-17 09:03:32,538 ----------------------------------------------------------------------------------------------------
2021-05-17 09:03:32,539 EPOCH 27 done: loss 0.3704 - lr 0.0062500
2021-05-17 09:03:37,239 DEV : loss 0.7816678881645203 - score 0.917
Epoch    27: reducing learning rate of group 0 to 3.1250e-03.
2021-05-17 09:03:37,276 BAD EPOCHS (no improvement): 4
2021-05-17 09:03:37,277 ----------------------------------------------------------------------------------------------------
2021-05-17 09:03:41,438 epoch 28 - iter 5/55 - loss 0.43678799 - samples/sec: 19.23 - lr: 0.003125
2021-05-17 09:03:46,382 epoch 28 - iter 10/55 - loss 0.35516882 - samples/sec: 16.19 - lr: 0.003125
2021-05-17 09:03:49,981 epoch 28 - iter 15/55 - loss 0.40836239 - samples/sec: 22.24 - lr: 0.003125
2021-05-17 09:03:53,909 epoch 28 - iter 20/55 - loss 0.34174788 - samples/sec: 20.39 - lr: 0.003125
2021-05-17 09:03:57,991 epoch 28 - iter 25/55 - loss 0.38664974 - samples/sec: 19.60 - lr: 0.003125
2021-05-17 09:04:02,709 epoch 28 - iter 30/55 - loss 0.38998843 - samples/sec: 16.96 - lr: 0.003125
2021-05-17 09:04:07,958 epoch 28 - iter 35/55 - loss 0.38319156 - samples/sec: 15.25 - lr: 0.003125
2021-05-17 09:04:12,204 epoch 28 - iter 40/55 - loss 0.38714058 - samples/sec: 18.84 - lr: 0.003125
2021-05-17 09:04:16,720 epoch 28 - iter 45/55 - loss 0.37524676 - samples/sec: 17.72 - lr: 0.003125
2021-05-17 09:04:20,867 epoch 28 - iter 50/55 - loss 0.37551487 - samples/sec: 19.30 - lr: 0.003125
2021-05-17 09:04:24,093 epoch 28 - iter 55/55 - loss 0.37387651 - samples/sec: 24.80 - lr: 0.003125
2021-05-17 09:04:24,093 ----------------------------------------------------------------------------------------------------
2021-05-17 09:04:24,094 EPOCH 28 done: loss 0.3739 - lr 0.0031250
2021-05-17 09:04:28,320 DEV : loss 0.7927533388137817 - score 0.9176
2021-05-17 09:04:28,357 BAD EPOCHS (no improvement): 1
2021-05-17 09:04:28,357 ----------------------------------------------------------------------------------------------------
2021-05-17 09:04:32,603 epoch 29 - iter 5/55 - loss 0.38658371 - samples/sec: 18.85 - lr: 0.003125
2021-05-17 09:04:36,807 epoch 29 - iter 10/55 - loss 0.40086138 - samples/sec: 19.04 - lr: 0.003125
2021-05-17 09:04:41,135 epoch 29 - iter 15/55 - loss 0.37696355 - samples/sec: 18.50 - lr: 0.003125
2021-05-17 09:04:45,020 epoch 29 - iter 20/55 - loss 0.37557533 - samples/sec: 20.60 - lr: 0.003125
2021-05-17 09:04:49,611 epoch 29 - iter 25/55 - loss 0.35512546 - samples/sec: 17.43 - lr: 0.003125
2021-05-17 09:04:53,808 epoch 29 - iter 30/55 - loss 0.35857003 - samples/sec: 19.06 - lr: 0.003125
2021-05-17 09:04:58,107 epoch 29 - iter 35/55 - loss 0.34110854 - samples/sec: 18.61 - lr: 0.003125
2021-05-17 09:05:02,463 epoch 29 - iter 40/55 - loss 0.34788749 - samples/sec: 18.37 - lr: 0.003125
2021-05-17 09:05:07,054 epoch 29 - iter 45/55 - loss 0.33962517 - samples/sec: 17.43 - lr: 0.003125
2021-05-17 09:05:11,802 epoch 29 - iter 50/55 - loss 0.32013073 - samples/sec: 16.85 - lr: 0.003125
2021-05-17 09:05:15,291 epoch 29 - iter 55/55 - loss 0.35197422 - samples/sec: 22.94 - lr: 0.003125
2021-05-17 09:05:15,292 ----------------------------------------------------------------------------------------------------
2021-05-17 09:05:15,292 EPOCH 29 done: loss 0.3520 - lr 0.0031250
2021-05-17 09:05:20,560 DEV : loss 0.7784459590911865 - score 0.9173
2021-05-17 09:05:20,640 BAD EPOCHS (no improvement): 2
2021-05-17 09:05:20,643 ----------------------------------------------------------------------------------------------------
2021-05-17 09:05:25,118 epoch 30 - iter 5/55 - loss 0.71846037 - samples/sec: 17.88 - lr: 0.003125
2021-05-17 09:05:30,269 epoch 30 - iter 10/55 - loss 0.50669663 - samples/sec: 15.53 - lr: 0.003125
2021-05-17 09:05:35,657 epoch 30 - iter 15/55 - loss 0.42647204 - samples/sec: 14.85 - lr: 0.003125
2021-05-17 09:05:39,518 epoch 30 - iter 20/55 - loss 0.38034666 - samples/sec: 20.73 - lr: 0.003125
2021-05-17 09:05:43,440 epoch 30 - iter 25/55 - loss 0.39941423 - samples/sec: 20.40 - lr: 0.003125
2021-05-17 09:05:47,146 epoch 30 - iter 30/55 - loss 0.38277473 - samples/sec: 21.59 - lr: 0.003125
2021-05-17 09:05:51,075 epoch 30 - iter 35/55 - loss 0.37910836 - samples/sec: 20.37 - lr: 0.003125
2021-05-17 09:05:55,764 epoch 30 - iter 40/55 - loss 0.37820523 - samples/sec: 17.08 - lr: 0.003125
2021-05-17 09:05:59,700 epoch 30 - iter 45/55 - loss 0.36798458 - samples/sec: 20.33 - lr: 0.003125
2021-05-17 09:06:04,038 epoch 30 - iter 50/55 - loss 0.35798653 - samples/sec: 18.47 - lr: 0.003125
2021-05-17 09:06:07,673 epoch 30 - iter 55/55 - loss 0.34163150 - samples/sec: 22.01 - lr: 0.003125
2021-05-17 09:06:07,674 ----------------------------------------------------------------------------------------------------
2021-05-17 09:06:07,674 EPOCH 30 done: loss 0.3416 - lr 0.0031250
2021-05-17 09:06:12,450 DEV : loss 0.7624803781509399 - score 0.9187
2021-05-17 09:06:12,487 BAD EPOCHS (no improvement): 3
2021-05-17 09:06:23,286 ----------------------------------------------------------------------------------------------------
2021-05-17 09:06:23,286 Testing using best model ...
2021-05-17 09:06:23,287 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/nld.rst.nldt/best-model.pt
2021-05-17 09:06:44,765 0.9514	0.9476	0.9495
2021-05-17 09:06:44,765 
Results:
- F1-score (micro) 0.9495
- F1-score (macro) 0.9495

By class:
SENT       tp: 235 - fp: 12 - fn: 13 - precision: 0.9514 - recall: 0.9476 - f1-score: 0.9495
2021-05-17 09:06:44,765 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn/
2021-05-17 09:06:44,798 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn
2021-05-17 09:06:44,798 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn/sent_train.txt
2021-05-17 09:06:44,800 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn/sent_dev.txt
2021-05-17 09:06:44,801 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn/sent_test.txt
Corpus: 2031 train + 282 dev + 164 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 09:07:02,557 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,563 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 09:07:02,563 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,563 Corpus: "Corpus: 2031 train + 282 dev + 164 test sentences"
2021-05-17 09:07:02,563 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,563 Parameters:
2021-05-17 09:07:02,563  - learning_rate: "0.1"
2021-05-17 09:07:02,563  - mini_batch_size: "16"
2021-05-17 09:07:02,563  - patience: "3"
2021-05-17 09:07:02,563  - anneal_factor: "0.5"
2021-05-17 09:07:02,564  - max_epochs: "30"
2021-05-17 09:07:02,564  - shuffle: "True"
2021-05-17 09:07:02,564  - train_with_dev: "False"
2021-05-17 09:07:02,564  - batch_growth_annealing: "False"
2021-05-17 09:07:02,564 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,564 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn"
2021-05-17 09:07:02,564 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,564 Device: cuda:0
2021-05-17 09:07:02,564 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:02,564 Embeddings storage mode: cpu
2021-05-17 09:07:02,567 ----------------------------------------------------------------------------------------------------
2021-05-17 09:07:32,171 epoch 1 - iter 12/127 - loss 9.08367620 - samples/sec: 6.49 - lr: 0.100000
2021-05-17 09:07:57,323 epoch 1 - iter 24/127 - loss 6.99360307 - samples/sec: 7.63 - lr: 0.100000
2021-05-17 09:08:23,928 epoch 1 - iter 36/127 - loss 5.98346097 - samples/sec: 7.22 - lr: 0.100000
2021-05-17 09:08:47,966 epoch 1 - iter 48/127 - loss 5.24713083 - samples/sec: 7.99 - lr: 0.100000
2021-05-17 09:09:10,854 epoch 1 - iter 60/127 - loss 4.73410389 - samples/sec: 8.39 - lr: 0.100000
2021-05-17 09:09:40,033 epoch 1 - iter 72/127 - loss 4.37535441 - samples/sec: 6.58 - lr: 0.100000
2021-05-17 09:10:03,981 epoch 1 - iter 84/127 - loss 3.98770060 - samples/sec: 8.02 - lr: 0.100000
2021-05-17 09:10:28,895 epoch 1 - iter 96/127 - loss 3.67884908 - samples/sec: 7.71 - lr: 0.100000
2021-05-17 09:10:50,867 epoch 1 - iter 108/127 - loss 3.40696373 - samples/sec: 8.74 - lr: 0.100000
2021-05-17 09:11:18,167 epoch 1 - iter 120/127 - loss 3.20123122 - samples/sec: 7.03 - lr: 0.100000
2021-05-17 09:11:34,600 ----------------------------------------------------------------------------------------------------
2021-05-17 09:11:34,601 EPOCH 1 done: loss 3.1017 - lr 0.1000000
2021-05-17 09:11:58,053 DEV : loss 0.8763049840927124 - score 0.7725
2021-05-17 09:11:58,107 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:12:08,656 ----------------------------------------------------------------------------------------------------
2021-05-17 09:12:18,647 epoch 2 - iter 12/127 - loss 1.00289329 - samples/sec: 19.22 - lr: 0.100000
2021-05-17 09:12:28,454 epoch 2 - iter 24/127 - loss 0.90534359 - samples/sec: 19.58 - lr: 0.100000
2021-05-17 09:12:38,323 epoch 2 - iter 36/127 - loss 0.83411920 - samples/sec: 19.46 - lr: 0.100000
2021-05-17 09:12:47,833 epoch 2 - iter 48/127 - loss 0.92682341 - samples/sec: 20.19 - lr: 0.100000
2021-05-17 09:12:57,792 epoch 2 - iter 60/127 - loss 0.95994828 - samples/sec: 19.28 - lr: 0.100000
2021-05-17 09:13:09,379 epoch 2 - iter 72/127 - loss 0.96878964 - samples/sec: 16.57 - lr: 0.100000
2021-05-17 09:13:20,986 epoch 2 - iter 84/127 - loss 0.95360032 - samples/sec: 16.55 - lr: 0.100000
2021-05-17 09:13:31,361 epoch 2 - iter 96/127 - loss 0.93256744 - samples/sec: 18.51 - lr: 0.100000
2021-05-17 09:13:41,554 epoch 2 - iter 108/127 - loss 0.91857611 - samples/sec: 18.84 - lr: 0.100000
2021-05-17 09:13:50,685 epoch 2 - iter 120/127 - loss 0.87291156 - samples/sec: 21.03 - lr: 0.100000
2021-05-17 09:13:57,477 ----------------------------------------------------------------------------------------------------
2021-05-17 09:13:57,478 EPOCH 2 done: loss 0.8591 - lr 0.1000000
2021-05-17 09:14:05,445 DEV : loss 0.47584861516952515 - score 0.9057
2021-05-17 09:14:05,499 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:14:15,957 ----------------------------------------------------------------------------------------------------
2021-05-17 09:14:25,767 epoch 3 - iter 12/127 - loss 0.72135363 - samples/sec: 19.57 - lr: 0.100000
2021-05-17 09:14:36,315 epoch 3 - iter 24/127 - loss 0.61461578 - samples/sec: 18.22 - lr: 0.100000
2021-05-17 09:14:45,869 epoch 3 - iter 36/127 - loss 0.67730285 - samples/sec: 20.10 - lr: 0.100000
2021-05-17 09:14:54,440 epoch 3 - iter 48/127 - loss 0.67623702 - samples/sec: 22.40 - lr: 0.100000
2021-05-17 09:15:05,411 epoch 3 - iter 60/127 - loss 0.66211454 - samples/sec: 17.50 - lr: 0.100000
2021-05-17 09:15:16,672 epoch 3 - iter 72/127 - loss 0.65593008 - samples/sec: 17.05 - lr: 0.100000
2021-05-17 09:15:27,870 epoch 3 - iter 84/127 - loss 0.68694405 - samples/sec: 17.15 - lr: 0.100000
2021-05-17 09:15:39,383 epoch 3 - iter 96/127 - loss 0.69582933 - samples/sec: 16.68 - lr: 0.100000
2021-05-17 09:15:49,392 epoch 3 - iter 108/127 - loss 0.70890079 - samples/sec: 19.19 - lr: 0.100000
2021-05-17 09:16:00,290 epoch 3 - iter 120/127 - loss 0.70290204 - samples/sec: 17.62 - lr: 0.100000
2021-05-17 09:16:05,625 ----------------------------------------------------------------------------------------------------
2021-05-17 09:16:05,625 EPOCH 3 done: loss 0.6860 - lr 0.1000000
2021-05-17 09:16:11,811 DEV : loss 0.6398735046386719 - score 0.8515
2021-05-17 09:16:11,866 BAD EPOCHS (no improvement): 1
2021-05-17 09:16:11,867 ----------------------------------------------------------------------------------------------------
2021-05-17 09:16:21,951 epoch 4 - iter 12/127 - loss 0.68499603 - samples/sec: 19.04 - lr: 0.100000
2021-05-17 09:16:30,334 epoch 4 - iter 24/127 - loss 0.65293238 - samples/sec: 22.91 - lr: 0.100000
2021-05-17 09:16:41,600 epoch 4 - iter 36/127 - loss 0.68837925 - samples/sec: 17.05 - lr: 0.100000
2021-05-17 09:16:51,823 epoch 4 - iter 48/127 - loss 0.66969413 - samples/sec: 18.79 - lr: 0.100000
2021-05-17 09:17:01,800 epoch 4 - iter 60/127 - loss 0.67437898 - samples/sec: 19.25 - lr: 0.100000
2021-05-17 09:17:12,852 epoch 4 - iter 72/127 - loss 0.64202136 - samples/sec: 17.37 - lr: 0.100000
2021-05-17 09:17:24,882 epoch 4 - iter 84/127 - loss 0.62298935 - samples/sec: 15.96 - lr: 0.100000
2021-05-17 09:17:37,049 epoch 4 - iter 96/127 - loss 0.61867895 - samples/sec: 15.78 - lr: 0.100000
2021-05-17 09:17:48,852 epoch 4 - iter 108/127 - loss 0.60116525 - samples/sec: 16.27 - lr: 0.100000
2021-05-17 09:17:59,816 epoch 4 - iter 120/127 - loss 0.60211095 - samples/sec: 17.51 - lr: 0.100000
2021-05-17 09:18:05,010 ----------------------------------------------------------------------------------------------------
2021-05-17 09:18:05,011 EPOCH 4 done: loss 0.5882 - lr 0.1000000
2021-05-17 09:18:11,574 DEV : loss 0.1701679229736328 - score 0.9803
2021-05-17 09:18:11,629 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:18:22,055 ----------------------------------------------------------------------------------------------------
2021-05-17 09:18:32,915 epoch 5 - iter 12/127 - loss 0.45304231 - samples/sec: 17.68 - lr: 0.100000
2021-05-17 09:18:43,789 epoch 5 - iter 24/127 - loss 0.39843337 - samples/sec: 17.68 - lr: 0.100000
2021-05-17 09:18:53,531 epoch 5 - iter 36/127 - loss 0.41867800 - samples/sec: 19.71 - lr: 0.100000
2021-05-17 09:19:04,507 epoch 5 - iter 48/127 - loss 0.39552965 - samples/sec: 17.49 - lr: 0.100000
2021-05-17 09:19:13,850 epoch 5 - iter 60/127 - loss 0.41079533 - samples/sec: 20.55 - lr: 0.100000
2021-05-17 09:19:24,420 epoch 5 - iter 72/127 - loss 0.41148306 - samples/sec: 18.17 - lr: 0.100000
2021-05-17 09:19:33,966 epoch 5 - iter 84/127 - loss 0.45599402 - samples/sec: 20.12 - lr: 0.100000
2021-05-17 09:19:42,373 epoch 5 - iter 96/127 - loss 0.46360912 - samples/sec: 22.84 - lr: 0.100000
2021-05-17 09:19:51,488 epoch 5 - iter 108/127 - loss 0.47733700 - samples/sec: 21.07 - lr: 0.100000
2021-05-17 09:20:00,271 epoch 5 - iter 120/127 - loss 0.48947440 - samples/sec: 21.86 - lr: 0.100000
2021-05-17 09:20:05,977 ----------------------------------------------------------------------------------------------------
2021-05-17 09:20:05,978 EPOCH 5 done: loss 0.4980 - lr 0.1000000
2021-05-17 09:20:12,455 DEV : loss 0.2999962866306305 - score 0.9478
2021-05-17 09:20:12,522 BAD EPOCHS (no improvement): 1
2021-05-17 09:20:12,523 ----------------------------------------------------------------------------------------------------
2021-05-17 09:20:23,249 epoch 6 - iter 12/127 - loss 0.42103203 - samples/sec: 17.90 - lr: 0.100000
2021-05-17 09:20:32,365 epoch 6 - iter 24/127 - loss 0.43444100 - samples/sec: 21.07 - lr: 0.100000
2021-05-17 09:20:41,764 epoch 6 - iter 36/127 - loss 0.55228127 - samples/sec: 20.43 - lr: 0.100000
2021-05-17 09:20:51,689 epoch 6 - iter 48/127 - loss 0.50643159 - samples/sec: 19.35 - lr: 0.100000
2021-05-17 09:21:01,373 epoch 6 - iter 60/127 - loss 0.47514368 - samples/sec: 19.83 - lr: 0.100000
2021-05-17 09:21:11,877 epoch 6 - iter 72/127 - loss 0.44968229 - samples/sec: 18.28 - lr: 0.100000
2021-05-17 09:21:22,977 epoch 6 - iter 84/127 - loss 0.44732454 - samples/sec: 17.30 - lr: 0.100000
2021-05-17 09:21:31,064 epoch 6 - iter 96/127 - loss 0.48151635 - samples/sec: 23.75 - lr: 0.100000
2021-05-17 09:21:41,263 epoch 6 - iter 108/127 - loss 0.48437278 - samples/sec: 18.83 - lr: 0.100000
2021-05-17 09:21:51,252 epoch 6 - iter 120/127 - loss 0.48999234 - samples/sec: 19.22 - lr: 0.100000
2021-05-17 09:21:56,916 ----------------------------------------------------------------------------------------------------
2021-05-17 09:21:56,916 EPOCH 6 done: loss 0.4978 - lr 0.1000000
2021-05-17 09:22:02,898 DEV : loss 0.2796253561973572 - score 0.9502
2021-05-17 09:22:02,979 BAD EPOCHS (no improvement): 2
2021-05-17 09:22:02,979 ----------------------------------------------------------------------------------------------------
2021-05-17 09:22:12,692 epoch 7 - iter 12/127 - loss 0.41309114 - samples/sec: 19.77 - lr: 0.100000
2021-05-17 09:22:22,297 epoch 7 - iter 24/127 - loss 0.41949930 - samples/sec: 19.99 - lr: 0.100000
2021-05-17 09:22:31,218 epoch 7 - iter 36/127 - loss 0.41286972 - samples/sec: 21.52 - lr: 0.100000
2021-05-17 09:22:40,558 epoch 7 - iter 48/127 - loss 0.43466753 - samples/sec: 20.56 - lr: 0.100000
2021-05-17 09:22:51,396 epoch 7 - iter 60/127 - loss 0.43476821 - samples/sec: 17.72 - lr: 0.100000
2021-05-17 09:23:01,329 epoch 7 - iter 72/127 - loss 0.42510318 - samples/sec: 19.33 - lr: 0.100000
2021-05-17 09:23:12,445 epoch 7 - iter 84/127 - loss 0.40788177 - samples/sec: 17.28 - lr: 0.100000
2021-05-17 09:23:23,053 epoch 7 - iter 96/127 - loss 0.41568861 - samples/sec: 18.10 - lr: 0.100000
2021-05-17 09:23:32,776 epoch 7 - iter 108/127 - loss 0.43106161 - samples/sec: 19.75 - lr: 0.100000
2021-05-17 09:23:42,312 epoch 7 - iter 120/127 - loss 0.42785364 - samples/sec: 20.14 - lr: 0.100000
2021-05-17 09:23:48,606 ----------------------------------------------------------------------------------------------------
2021-05-17 09:23:48,607 EPOCH 7 done: loss 0.4252 - lr 0.1000000
2021-05-17 09:23:54,280 DEV : loss 0.1532527208328247 - score 0.9825
2021-05-17 09:23:54,335 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:24:04,660 ----------------------------------------------------------------------------------------------------
2021-05-17 09:24:14,748 epoch 8 - iter 12/127 - loss 0.26387050 - samples/sec: 19.04 - lr: 0.100000
2021-05-17 09:24:24,949 epoch 8 - iter 24/127 - loss 0.32585988 - samples/sec: 18.83 - lr: 0.100000
2021-05-17 09:24:35,141 epoch 8 - iter 36/127 - loss 0.38984831 - samples/sec: 18.84 - lr: 0.100000
2021-05-17 09:24:44,090 epoch 8 - iter 48/127 - loss 0.40854191 - samples/sec: 21.46 - lr: 0.100000
2021-05-17 09:24:53,418 epoch 8 - iter 60/127 - loss 0.38218532 - samples/sec: 20.59 - lr: 0.100000
2021-05-17 09:25:04,263 epoch 8 - iter 72/127 - loss 0.38025013 - samples/sec: 17.71 - lr: 0.100000
2021-05-17 09:25:13,164 epoch 8 - iter 84/127 - loss 0.36502352 - samples/sec: 21.57 - lr: 0.100000
2021-05-17 09:25:23,431 epoch 8 - iter 96/127 - loss 0.37471812 - samples/sec: 18.70 - lr: 0.100000
2021-05-17 09:25:33,412 epoch 8 - iter 108/127 - loss 0.37183153 - samples/sec: 19.24 - lr: 0.100000
2021-05-17 09:25:44,142 epoch 8 - iter 120/127 - loss 0.36434867 - samples/sec: 17.89 - lr: 0.100000
2021-05-17 09:25:49,455 ----------------------------------------------------------------------------------------------------
2021-05-17 09:25:49,455 EPOCH 8 done: loss 0.3624 - lr 0.1000000
2021-05-17 09:25:54,231 DEV : loss 0.24867357313632965 - score 0.9713
2021-05-17 09:25:54,285 BAD EPOCHS (no improvement): 1
2021-05-17 09:25:54,285 ----------------------------------------------------------------------------------------------------
2021-05-17 09:26:03,456 epoch 9 - iter 12/127 - loss 0.32541691 - samples/sec: 20.94 - lr: 0.100000
2021-05-17 09:26:13,676 epoch 9 - iter 24/127 - loss 0.36742403 - samples/sec: 18.79 - lr: 0.100000
2021-05-17 09:26:22,767 epoch 9 - iter 36/127 - loss 0.36217540 - samples/sec: 21.12 - lr: 0.100000
2021-05-17 09:26:32,699 epoch 9 - iter 48/127 - loss 0.38291431 - samples/sec: 19.34 - lr: 0.100000
2021-05-17 09:26:42,488 epoch 9 - iter 60/127 - loss 0.36348003 - samples/sec: 19.62 - lr: 0.100000
2021-05-17 09:26:53,326 epoch 9 - iter 72/127 - loss 0.37016115 - samples/sec: 17.72 - lr: 0.100000
2021-05-17 09:27:03,062 epoch 9 - iter 84/127 - loss 0.38377362 - samples/sec: 19.73 - lr: 0.100000
2021-05-17 09:27:12,854 epoch 9 - iter 96/127 - loss 0.40509626 - samples/sec: 19.61 - lr: 0.100000
2021-05-17 09:27:21,562 epoch 9 - iter 108/127 - loss 0.41009762 - samples/sec: 22.05 - lr: 0.100000
2021-05-17 09:27:33,758 epoch 9 - iter 120/127 - loss 0.39848109 - samples/sec: 15.74 - lr: 0.100000
2021-05-17 09:27:39,212 ----------------------------------------------------------------------------------------------------
2021-05-17 09:27:39,213 EPOCH 9 done: loss 0.3897 - lr 0.1000000
2021-05-17 09:27:44,725 DEV : loss 0.143095463514328 - score 0.9825
2021-05-17 09:27:44,781 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:27:54,913 ----------------------------------------------------------------------------------------------------
2021-05-17 09:28:05,315 epoch 10 - iter 12/127 - loss 0.34806907 - samples/sec: 18.46 - lr: 0.100000
2021-05-17 09:28:15,458 epoch 10 - iter 24/127 - loss 0.30643499 - samples/sec: 18.93 - lr: 0.100000
2021-05-17 09:28:25,699 epoch 10 - iter 36/127 - loss 0.30655507 - samples/sec: 18.75 - lr: 0.100000
2021-05-17 09:28:34,290 epoch 10 - iter 48/127 - loss 0.32383658 - samples/sec: 22.35 - lr: 0.100000
2021-05-17 09:28:45,062 epoch 10 - iter 60/127 - loss 0.27410731 - samples/sec: 17.83 - lr: 0.100000
2021-05-17 09:28:57,164 epoch 10 - iter 72/127 - loss 0.29235854 - samples/sec: 15.87 - lr: 0.100000
2021-05-17 09:29:05,976 epoch 10 - iter 84/127 - loss 0.31908508 - samples/sec: 21.79 - lr: 0.100000
2021-05-17 09:29:16,005 epoch 10 - iter 96/127 - loss 0.33266254 - samples/sec: 19.15 - lr: 0.100000
2021-05-17 09:29:25,927 epoch 10 - iter 108/127 - loss 0.35970498 - samples/sec: 19.35 - lr: 0.100000
2021-05-17 09:29:35,281 epoch 10 - iter 120/127 - loss 0.36329706 - samples/sec: 20.53 - lr: 0.100000
2021-05-17 09:29:40,056 ----------------------------------------------------------------------------------------------------
2021-05-17 09:29:40,058 EPOCH 10 done: loss 0.3618 - lr 0.1000000
2021-05-17 09:29:44,691 DEV : loss 0.1371612548828125 - score 0.9803
2021-05-17 09:29:44,745 BAD EPOCHS (no improvement): 1
2021-05-17 09:29:44,745 ----------------------------------------------------------------------------------------------------
2021-05-17 09:29:55,127 epoch 11 - iter 12/127 - loss 0.35063042 - samples/sec: 18.50 - lr: 0.100000
2021-05-17 09:30:05,803 epoch 11 - iter 24/127 - loss 0.32209151 - samples/sec: 18.01 - lr: 0.100000
2021-05-17 09:30:15,102 epoch 11 - iter 36/127 - loss 0.34255299 - samples/sec: 20.65 - lr: 0.100000
2021-05-17 09:30:24,287 epoch 11 - iter 48/127 - loss 0.32431542 - samples/sec: 20.91 - lr: 0.100000
2021-05-17 09:30:34,241 epoch 11 - iter 60/127 - loss 0.33227767 - samples/sec: 19.29 - lr: 0.100000
2021-05-17 09:30:44,958 epoch 11 - iter 72/127 - loss 0.35088607 - samples/sec: 17.93 - lr: 0.100000
2021-05-17 09:30:54,753 epoch 11 - iter 84/127 - loss 0.36308011 - samples/sec: 19.61 - lr: 0.100000
2021-05-17 09:31:05,619 epoch 11 - iter 96/127 - loss 0.35894922 - samples/sec: 17.67 - lr: 0.100000
2021-05-17 09:31:16,470 epoch 11 - iter 108/127 - loss 0.35875994 - samples/sec: 17.70 - lr: 0.100000
2021-05-17 09:31:26,232 epoch 11 - iter 120/127 - loss 0.35542964 - samples/sec: 19.67 - lr: 0.100000
2021-05-17 09:31:31,790 ----------------------------------------------------------------------------------------------------
2021-05-17 09:31:31,791 EPOCH 11 done: loss 0.3546 - lr 0.1000000
2021-05-17 09:31:37,266 DEV : loss 0.15649059414863586 - score 0.9825
2021-05-17 09:31:37,322 BAD EPOCHS (no improvement): 2
2021-05-17 09:31:37,323 ----------------------------------------------------------------------------------------------------
2021-05-17 09:31:47,378 epoch 12 - iter 12/127 - loss 0.27390632 - samples/sec: 19.10 - lr: 0.100000
2021-05-17 09:31:57,948 epoch 12 - iter 24/127 - loss 0.24813892 - samples/sec: 18.17 - lr: 0.100000
2021-05-17 09:32:07,920 epoch 12 - iter 36/127 - loss 0.26749380 - samples/sec: 19.26 - lr: 0.100000
2021-05-17 09:32:16,084 epoch 12 - iter 48/127 - loss 0.29912151 - samples/sec: 23.52 - lr: 0.100000
2021-05-17 09:32:24,841 epoch 12 - iter 60/127 - loss 0.28785866 - samples/sec: 21.93 - lr: 0.100000
2021-05-17 09:32:35,985 epoch 12 - iter 72/127 - loss 0.28763476 - samples/sec: 17.23 - lr: 0.100000
2021-05-17 09:32:46,400 epoch 12 - iter 84/127 - loss 0.28154108 - samples/sec: 18.44 - lr: 0.100000
2021-05-17 09:32:55,510 epoch 12 - iter 96/127 - loss 0.27241887 - samples/sec: 21.08 - lr: 0.100000
2021-05-17 09:33:04,249 epoch 12 - iter 108/127 - loss 0.28476879 - samples/sec: 21.99 - lr: 0.100000
2021-05-17 09:33:14,374 epoch 12 - iter 120/127 - loss 0.29620496 - samples/sec: 18.97 - lr: 0.100000
2021-05-17 09:33:20,986 ----------------------------------------------------------------------------------------------------
2021-05-17 09:33:20,986 EPOCH 12 done: loss 0.2976 - lr 0.1000000
2021-05-17 09:33:26,227 DEV : loss 0.30049043893814087 - score 0.9526
2021-05-17 09:33:26,354 BAD EPOCHS (no improvement): 3
2021-05-17 09:33:26,355 ----------------------------------------------------------------------------------------------------
2021-05-17 09:33:36,186 epoch 13 - iter 12/127 - loss 0.36342371 - samples/sec: 19.53 - lr: 0.100000
2021-05-17 09:33:47,015 epoch 13 - iter 24/127 - loss 0.33870369 - samples/sec: 17.73 - lr: 0.100000
2021-05-17 09:33:57,473 epoch 13 - iter 36/127 - loss 0.30882699 - samples/sec: 18.36 - lr: 0.100000
2021-05-17 09:34:08,092 epoch 13 - iter 48/127 - loss 0.34424271 - samples/sec: 18.08 - lr: 0.100000
2021-05-17 09:34:18,603 epoch 13 - iter 60/127 - loss 0.35328313 - samples/sec: 18.27 - lr: 0.100000
2021-05-17 09:34:29,111 epoch 13 - iter 72/127 - loss 0.32909421 - samples/sec: 18.28 - lr: 0.100000
2021-05-17 09:34:40,594 epoch 13 - iter 84/127 - loss 0.32523351 - samples/sec: 16.73 - lr: 0.100000
2021-05-17 09:34:50,757 epoch 13 - iter 96/127 - loss 0.32468456 - samples/sec: 18.89 - lr: 0.100000
2021-05-17 09:35:00,878 epoch 13 - iter 108/127 - loss 0.31315325 - samples/sec: 18.97 - lr: 0.100000
2021-05-17 09:35:10,674 epoch 13 - iter 120/127 - loss 0.31550852 - samples/sec: 19.60 - lr: 0.100000
2021-05-17 09:35:15,651 ----------------------------------------------------------------------------------------------------
2021-05-17 09:35:15,651 EPOCH 13 done: loss 0.3166 - lr 0.1000000
2021-05-17 09:35:20,528 DEV : loss 0.21085414290428162 - score 0.9757
Epoch    13: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 09:35:20,582 BAD EPOCHS (no improvement): 4
2021-05-17 09:35:20,583 ----------------------------------------------------------------------------------------------------
2021-05-17 09:35:29,760 epoch 14 - iter 12/127 - loss 0.21167256 - samples/sec: 20.92 - lr: 0.050000
2021-05-17 09:35:39,643 epoch 14 - iter 24/127 - loss 0.34285167 - samples/sec: 19.43 - lr: 0.050000
2021-05-17 09:35:49,165 epoch 14 - iter 36/127 - loss 0.32133683 - samples/sec: 20.17 - lr: 0.050000
2021-05-17 09:35:58,241 epoch 14 - iter 48/127 - loss 0.32091022 - samples/sec: 21.16 - lr: 0.050000
2021-05-17 09:36:07,896 epoch 14 - iter 60/127 - loss 0.31730720 - samples/sec: 19.89 - lr: 0.050000
2021-05-17 09:36:18,261 epoch 14 - iter 72/127 - loss 0.30522581 - samples/sec: 18.53 - lr: 0.050000
2021-05-17 09:36:28,884 epoch 14 - iter 84/127 - loss 0.28125232 - samples/sec: 18.08 - lr: 0.050000
2021-05-17 09:36:38,363 epoch 14 - iter 96/127 - loss 0.26543572 - samples/sec: 20.26 - lr: 0.050000
2021-05-17 09:36:47,115 epoch 14 - iter 108/127 - loss 0.26651698 - samples/sec: 21.94 - lr: 0.050000
2021-05-17 09:36:56,854 epoch 14 - iter 120/127 - loss 0.26967407 - samples/sec: 19.72 - lr: 0.050000
2021-05-17 09:37:03,159 ----------------------------------------------------------------------------------------------------
2021-05-17 09:37:03,159 EPOCH 14 done: loss 0.2773 - lr 0.0500000
2021-05-17 09:37:09,210 DEV : loss 0.13725757598876953 - score 0.9804
2021-05-17 09:37:09,265 BAD EPOCHS (no improvement): 1
2021-05-17 09:37:09,265 ----------------------------------------------------------------------------------------------------
2021-05-17 09:37:17,886 epoch 15 - iter 12/127 - loss 0.12665862 - samples/sec: 22.27 - lr: 0.050000
2021-05-17 09:37:27,593 epoch 15 - iter 24/127 - loss 0.16833894 - samples/sec: 19.78 - lr: 0.050000
2021-05-17 09:37:38,377 epoch 15 - iter 36/127 - loss 0.27064017 - samples/sec: 17.81 - lr: 0.050000
2021-05-17 09:37:50,081 epoch 15 - iter 48/127 - loss 0.27120119 - samples/sec: 16.41 - lr: 0.050000
2021-05-17 09:37:59,754 epoch 15 - iter 60/127 - loss 0.26805320 - samples/sec: 19.85 - lr: 0.050000
2021-05-17 09:38:10,440 epoch 15 - iter 72/127 - loss 0.27081284 - samples/sec: 17.97 - lr: 0.050000
2021-05-17 09:38:21,768 epoch 15 - iter 84/127 - loss 0.26618523 - samples/sec: 16.95 - lr: 0.050000
2021-05-17 09:38:30,344 epoch 15 - iter 96/127 - loss 0.26227546 - samples/sec: 22.39 - lr: 0.050000
2021-05-17 09:38:38,528 epoch 15 - iter 108/127 - loss 0.25109894 - samples/sec: 23.47 - lr: 0.050000
2021-05-17 09:38:48,302 epoch 15 - iter 120/127 - loss 0.24686185 - samples/sec: 19.65 - lr: 0.050000
2021-05-17 09:38:54,782 ----------------------------------------------------------------------------------------------------
2021-05-17 09:38:54,782 EPOCH 15 done: loss 0.2512 - lr 0.0500000
2021-05-17 09:39:01,119 DEV : loss 0.16203902661800385 - score 0.9804
2021-05-17 09:39:01,173 BAD EPOCHS (no improvement): 2
2021-05-17 09:39:01,174 ----------------------------------------------------------------------------------------------------
2021-05-17 09:39:10,548 epoch 16 - iter 12/127 - loss 0.25176326 - samples/sec: 20.48 - lr: 0.050000
2021-05-17 09:39:19,542 epoch 16 - iter 24/127 - loss 0.20962229 - samples/sec: 21.35 - lr: 0.050000
2021-05-17 09:39:30,602 epoch 16 - iter 36/127 - loss 0.21417882 - samples/sec: 17.36 - lr: 0.050000
2021-05-17 09:39:41,511 epoch 16 - iter 48/127 - loss 0.18959752 - samples/sec: 17.60 - lr: 0.050000
2021-05-17 09:39:51,980 epoch 16 - iter 60/127 - loss 0.21242164 - samples/sec: 18.35 - lr: 0.050000
2021-05-17 09:40:02,606 epoch 16 - iter 72/127 - loss 0.21827928 - samples/sec: 18.07 - lr: 0.050000
2021-05-17 09:40:12,302 epoch 16 - iter 84/127 - loss 0.21085024 - samples/sec: 19.80 - lr: 0.050000
2021-05-17 09:40:22,475 epoch 16 - iter 96/127 - loss 0.21504866 - samples/sec: 18.88 - lr: 0.050000
2021-05-17 09:40:31,937 epoch 16 - iter 108/127 - loss 0.22878894 - samples/sec: 20.29 - lr: 0.050000
2021-05-17 09:40:41,830 epoch 16 - iter 120/127 - loss 0.22360437 - samples/sec: 19.41 - lr: 0.050000
2021-05-17 09:40:48,086 ----------------------------------------------------------------------------------------------------
2021-05-17 09:40:48,087 EPOCH 16 done: loss 0.2207 - lr 0.0500000
2021-05-17 09:40:54,021 DEV : loss 0.2414391040802002 - score 0.9643
2021-05-17 09:40:54,076 BAD EPOCHS (no improvement): 3
2021-05-17 09:40:54,077 ----------------------------------------------------------------------------------------------------
2021-05-17 09:41:03,897 epoch 17 - iter 12/127 - loss 0.25599962 - samples/sec: 19.55 - lr: 0.050000
2021-05-17 09:41:13,646 epoch 17 - iter 24/127 - loss 0.27651918 - samples/sec: 19.72 - lr: 0.050000
2021-05-17 09:41:23,141 epoch 17 - iter 36/127 - loss 0.29762433 - samples/sec: 20.22 - lr: 0.050000
2021-05-17 09:41:34,335 epoch 17 - iter 48/127 - loss 0.26591646 - samples/sec: 17.16 - lr: 0.050000
2021-05-17 09:41:43,445 epoch 17 - iter 60/127 - loss 0.26464931 - samples/sec: 21.08 - lr: 0.050000
2021-05-17 09:41:52,340 epoch 17 - iter 72/127 - loss 0.25408976 - samples/sec: 21.59 - lr: 0.050000
2021-05-17 09:42:02,056 epoch 17 - iter 84/127 - loss 0.23515017 - samples/sec: 19.76 - lr: 0.050000
2021-05-17 09:42:11,849 epoch 17 - iter 96/127 - loss 0.23884325 - samples/sec: 19.61 - lr: 0.050000
2021-05-17 09:42:22,242 epoch 17 - iter 108/127 - loss 0.24254465 - samples/sec: 18.48 - lr: 0.050000
2021-05-17 09:42:31,632 epoch 17 - iter 120/127 - loss 0.23375547 - samples/sec: 20.45 - lr: 0.050000
2021-05-17 09:42:38,183 ----------------------------------------------------------------------------------------------------
2021-05-17 09:42:38,184 EPOCH 17 done: loss 0.2354 - lr 0.0500000
2021-05-17 09:42:46,541 DEV : loss 0.15823031961917877 - score 0.9803
Epoch    17: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 09:42:46,633 BAD EPOCHS (no improvement): 4
2021-05-17 09:42:46,633 ----------------------------------------------------------------------------------------------------
2021-05-17 09:42:56,129 epoch 18 - iter 12/127 - loss 0.20377881 - samples/sec: 20.22 - lr: 0.025000
2021-05-17 09:43:06,740 epoch 18 - iter 24/127 - loss 0.19847757 - samples/sec: 18.10 - lr: 0.025000
2021-05-17 09:43:17,773 epoch 18 - iter 36/127 - loss 0.20881555 - samples/sec: 17.40 - lr: 0.025000
2021-05-17 09:43:28,340 epoch 18 - iter 48/127 - loss 0.22323166 - samples/sec: 18.17 - lr: 0.025000
2021-05-17 09:43:39,513 epoch 18 - iter 60/127 - loss 0.20504905 - samples/sec: 17.19 - lr: 0.025000
2021-05-17 09:43:49,816 epoch 18 - iter 72/127 - loss 0.19546258 - samples/sec: 18.64 - lr: 0.025000
2021-05-17 09:44:00,466 epoch 18 - iter 84/127 - loss 0.20224429 - samples/sec: 18.03 - lr: 0.025000
2021-05-17 09:44:11,006 epoch 18 - iter 96/127 - loss 0.20724126 - samples/sec: 18.22 - lr: 0.025000
2021-05-17 09:44:22,016 epoch 18 - iter 108/127 - loss 0.20808908 - samples/sec: 17.44 - lr: 0.025000
2021-05-17 09:44:32,540 epoch 18 - iter 120/127 - loss 0.20578470 - samples/sec: 18.25 - lr: 0.025000
2021-05-17 09:44:38,511 ----------------------------------------------------------------------------------------------------
2021-05-17 09:44:38,512 EPOCH 18 done: loss 0.2063 - lr 0.0250000
2021-05-17 09:44:44,453 DEV : loss 0.15229839086532593 - score 0.9825
2021-05-17 09:44:44,504 BAD EPOCHS (no improvement): 1
2021-05-17 09:44:44,505 ----------------------------------------------------------------------------------------------------
2021-05-17 09:44:55,524 epoch 19 - iter 12/127 - loss 0.15720920 - samples/sec: 17.43 - lr: 0.025000
2021-05-17 09:45:04,010 epoch 19 - iter 24/127 - loss 0.21438121 - samples/sec: 22.63 - lr: 0.025000
2021-05-17 09:45:14,711 epoch 19 - iter 36/127 - loss 0.19771496 - samples/sec: 17.94 - lr: 0.025000
2021-05-17 09:45:25,281 epoch 19 - iter 48/127 - loss 0.21015691 - samples/sec: 18.17 - lr: 0.025000
2021-05-17 09:45:36,027 epoch 19 - iter 60/127 - loss 0.21121527 - samples/sec: 17.87 - lr: 0.025000
2021-05-17 09:45:45,659 epoch 19 - iter 72/127 - loss 0.21793266 - samples/sec: 19.95 - lr: 0.025000
2021-05-17 09:45:55,624 epoch 19 - iter 84/127 - loss 0.20503822 - samples/sec: 19.28 - lr: 0.025000
2021-05-17 09:46:04,556 epoch 19 - iter 96/127 - loss 0.20305679 - samples/sec: 21.50 - lr: 0.025000
2021-05-17 09:46:15,817 epoch 19 - iter 108/127 - loss 0.20288543 - samples/sec: 17.05 - lr: 0.025000
2021-05-17 09:46:24,850 epoch 19 - iter 120/127 - loss 0.19666577 - samples/sec: 21.26 - lr: 0.025000
2021-05-17 09:46:30,855 ----------------------------------------------------------------------------------------------------
2021-05-17 09:46:30,856 EPOCH 19 done: loss 0.2038 - lr 0.0250000
2021-05-17 09:46:36,198 DEV : loss 0.1583760529756546 - score 0.9826
2021-05-17 09:46:36,286 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:46:46,583 ----------------------------------------------------------------------------------------------------
2021-05-17 09:46:56,950 epoch 20 - iter 12/127 - loss 0.20061247 - samples/sec: 18.52 - lr: 0.025000
2021-05-17 09:47:05,502 epoch 20 - iter 24/127 - loss 0.17136442 - samples/sec: 22.45 - lr: 0.025000
2021-05-17 09:47:15,849 epoch 20 - iter 36/127 - loss 0.20810330 - samples/sec: 18.56 - lr: 0.025000
2021-05-17 09:47:26,588 epoch 20 - iter 48/127 - loss 0.20131128 - samples/sec: 17.88 - lr: 0.025000
2021-05-17 09:47:36,203 epoch 20 - iter 60/127 - loss 0.17779746 - samples/sec: 19.97 - lr: 0.025000
2021-05-17 09:47:46,062 epoch 20 - iter 72/127 - loss 0.17276640 - samples/sec: 19.49 - lr: 0.025000
2021-05-17 09:47:55,942 epoch 20 - iter 84/127 - loss 0.20326959 - samples/sec: 19.44 - lr: 0.025000
2021-05-17 09:48:05,758 epoch 20 - iter 96/127 - loss 0.19344003 - samples/sec: 19.56 - lr: 0.025000
2021-05-17 09:48:15,877 epoch 20 - iter 108/127 - loss 0.19397425 - samples/sec: 18.98 - lr: 0.025000
2021-05-17 09:48:25,306 epoch 20 - iter 120/127 - loss 0.19034901 - samples/sec: 20.38 - lr: 0.025000
2021-05-17 09:48:31,539 ----------------------------------------------------------------------------------------------------
2021-05-17 09:48:31,540 EPOCH 20 done: loss 0.1885 - lr 0.0250000
2021-05-17 09:48:38,111 DEV : loss 0.15077559649944305 - score 0.9847
2021-05-17 09:48:38,178 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 09:48:48,579 ----------------------------------------------------------------------------------------------------
2021-05-17 09:48:57,816 epoch 21 - iter 12/127 - loss 0.24122807 - samples/sec: 20.79 - lr: 0.025000
2021-05-17 09:49:07,162 epoch 21 - iter 24/127 - loss 0.19852990 - samples/sec: 20.54 - lr: 0.025000
2021-05-17 09:49:15,335 epoch 21 - iter 36/127 - loss 0.20680312 - samples/sec: 23.50 - lr: 0.025000
2021-05-17 09:49:25,276 epoch 21 - iter 48/127 - loss 0.18127796 - samples/sec: 19.32 - lr: 0.025000
2021-05-17 09:49:34,139 epoch 21 - iter 60/127 - loss 0.17281206 - samples/sec: 21.67 - lr: 0.025000
2021-05-17 09:49:43,796 epoch 21 - iter 72/127 - loss 0.16995022 - samples/sec: 19.89 - lr: 0.025000
2021-05-17 09:49:52,465 epoch 21 - iter 84/127 - loss 0.16978286 - samples/sec: 22.15 - lr: 0.025000
2021-05-17 09:50:01,268 epoch 21 - iter 96/127 - loss 0.17824606 - samples/sec: 21.82 - lr: 0.025000
2021-05-17 09:50:11,663 epoch 21 - iter 108/127 - loss 0.20456788 - samples/sec: 18.47 - lr: 0.025000
2021-05-17 09:50:19,715 epoch 21 - iter 120/127 - loss 0.21903908 - samples/sec: 23.85 - lr: 0.025000
2021-05-17 09:50:24,814 ----------------------------------------------------------------------------------------------------
2021-05-17 09:50:24,820 EPOCH 21 done: loss 0.2176 - lr 0.0250000
2021-05-17 09:50:31,323 DEV : loss 0.16262060403823853 - score 0.9781
2021-05-17 09:50:31,379 BAD EPOCHS (no improvement): 1
2021-05-17 09:50:31,379 ----------------------------------------------------------------------------------------------------
2021-05-17 09:50:41,759 epoch 22 - iter 12/127 - loss 0.26234907 - samples/sec: 18.50 - lr: 0.025000
2021-05-17 09:50:51,356 epoch 22 - iter 24/127 - loss 0.18057548 - samples/sec: 20.01 - lr: 0.025000
2021-05-17 09:51:00,141 epoch 22 - iter 36/127 - loss 0.23038493 - samples/sec: 21.86 - lr: 0.025000
2021-05-17 09:51:09,552 epoch 22 - iter 48/127 - loss 0.21812755 - samples/sec: 20.41 - lr: 0.025000
2021-05-17 09:51:18,649 epoch 22 - iter 60/127 - loss 0.19681499 - samples/sec: 21.11 - lr: 0.025000
2021-05-17 09:51:27,996 epoch 22 - iter 72/127 - loss 0.18326736 - samples/sec: 20.54 - lr: 0.025000
2021-05-17 09:51:36,905 epoch 22 - iter 84/127 - loss 0.17464475 - samples/sec: 21.55 - lr: 0.025000
2021-05-17 09:51:46,028 epoch 22 - iter 96/127 - loss 0.17926230 - samples/sec: 21.05 - lr: 0.025000
2021-05-17 09:51:56,156 epoch 22 - iter 108/127 - loss 0.17784899 - samples/sec: 18.96 - lr: 0.025000
2021-05-17 09:52:05,323 epoch 22 - iter 120/127 - loss 0.18743375 - samples/sec: 20.95 - lr: 0.025000
2021-05-17 09:52:11,606 ----------------------------------------------------------------------------------------------------
2021-05-17 09:52:11,607 EPOCH 22 done: loss 0.1905 - lr 0.0250000
2021-05-17 09:52:18,027 DEV : loss 0.15197916328907013 - score 0.9826
2021-05-17 09:52:18,162 BAD EPOCHS (no improvement): 2
2021-05-17 09:52:18,171 ----------------------------------------------------------------------------------------------------
2021-05-17 09:52:28,440 epoch 23 - iter 12/127 - loss 0.13826485 - samples/sec: 18.70 - lr: 0.025000
2021-05-17 09:52:37,378 epoch 23 - iter 24/127 - loss 0.15306491 - samples/sec: 21.48 - lr: 0.025000
2021-05-17 09:52:47,318 epoch 23 - iter 36/127 - loss 0.19833205 - samples/sec: 19.32 - lr: 0.025000
2021-05-17 09:52:57,905 epoch 23 - iter 48/127 - loss 0.19835014 - samples/sec: 18.14 - lr: 0.025000
2021-05-17 09:53:07,438 epoch 23 - iter 60/127 - loss 0.19232884 - samples/sec: 20.14 - lr: 0.025000
2021-05-17 09:53:16,573 epoch 23 - iter 72/127 - loss 0.20590510 - samples/sec: 21.02 - lr: 0.025000
2021-05-17 09:53:26,038 epoch 23 - iter 84/127 - loss 0.19229363 - samples/sec: 20.29 - lr: 0.025000
2021-05-17 09:53:34,850 epoch 23 - iter 96/127 - loss 0.20224121 - samples/sec: 21.79 - lr: 0.025000
2021-05-17 09:53:43,890 epoch 23 - iter 108/127 - loss 0.19985488 - samples/sec: 21.24 - lr: 0.025000
2021-05-17 09:53:52,983 epoch 23 - iter 120/127 - loss 0.19351932 - samples/sec: 21.12 - lr: 0.025000
2021-05-17 09:53:59,084 ----------------------------------------------------------------------------------------------------
2021-05-17 09:53:59,084 EPOCH 23 done: loss 0.1939 - lr 0.0250000
2021-05-17 09:54:04,851 DEV : loss 0.15770091116428375 - score 0.9847
2021-05-17 09:54:04,906 BAD EPOCHS (no improvement): 3
2021-05-17 09:54:04,907 ----------------------------------------------------------------------------------------------------
2021-05-17 09:54:14,061 epoch 24 - iter 12/127 - loss 0.17794008 - samples/sec: 20.98 - lr: 0.025000
2021-05-17 09:54:22,900 epoch 24 - iter 24/127 - loss 0.16819612 - samples/sec: 21.72 - lr: 0.025000
2021-05-17 09:54:32,641 epoch 24 - iter 36/127 - loss 0.17132179 - samples/sec: 19.71 - lr: 0.025000
2021-05-17 09:54:43,573 epoch 24 - iter 48/127 - loss 0.18630427 - samples/sec: 17.57 - lr: 0.025000
2021-05-17 09:54:53,908 epoch 24 - iter 60/127 - loss 0.18550277 - samples/sec: 18.58 - lr: 0.025000
2021-05-17 09:55:03,372 epoch 24 - iter 72/127 - loss 0.19656442 - samples/sec: 20.30 - lr: 0.025000
2021-05-17 09:55:13,448 epoch 24 - iter 84/127 - loss 0.20561965 - samples/sec: 19.06 - lr: 0.025000
2021-05-17 09:55:23,023 epoch 24 - iter 96/127 - loss 0.19267219 - samples/sec: 20.05 - lr: 0.025000
2021-05-17 09:55:33,264 epoch 24 - iter 108/127 - loss 0.19833581 - samples/sec: 18.75 - lr: 0.025000
2021-05-17 09:55:42,234 epoch 24 - iter 120/127 - loss 0.19651864 - samples/sec: 21.41 - lr: 0.025000
2021-05-17 09:55:47,649 ----------------------------------------------------------------------------------------------------
2021-05-17 09:55:47,650 EPOCH 24 done: loss 0.1980 - lr 0.0250000
2021-05-17 09:55:52,460 DEV : loss 0.16153305768966675 - score 0.9847
Epoch    24: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 09:55:52,515 BAD EPOCHS (no improvement): 4
2021-05-17 09:55:52,516 ----------------------------------------------------------------------------------------------------
2021-05-17 09:56:01,454 epoch 25 - iter 12/127 - loss 0.18190993 - samples/sec: 21.48 - lr: 0.012500
2021-05-17 09:56:10,516 epoch 25 - iter 24/127 - loss 0.15888024 - samples/sec: 21.19 - lr: 0.012500
2021-05-17 09:56:19,318 epoch 25 - iter 36/127 - loss 0.19582846 - samples/sec: 21.82 - lr: 0.012500
2021-05-17 09:56:29,514 epoch 25 - iter 48/127 - loss 0.18521955 - samples/sec: 18.83 - lr: 0.012500
2021-05-17 09:56:37,804 epoch 25 - iter 60/127 - loss 0.17602512 - samples/sec: 23.16 - lr: 0.012500
2021-05-17 09:56:46,457 epoch 25 - iter 72/127 - loss 0.16728829 - samples/sec: 22.20 - lr: 0.012500
2021-05-17 09:56:55,838 epoch 25 - iter 84/127 - loss 0.15814749 - samples/sec: 20.47 - lr: 0.012500
2021-05-17 09:57:04,886 epoch 25 - iter 96/127 - loss 0.16536468 - samples/sec: 21.22 - lr: 0.012500
2021-05-17 09:57:16,782 epoch 25 - iter 108/127 - loss 0.17227541 - samples/sec: 16.14 - lr: 0.012500
2021-05-17 09:57:29,871 epoch 25 - iter 120/127 - loss 0.17057901 - samples/sec: 14.67 - lr: 0.012500
2021-05-17 09:57:35,797 ----------------------------------------------------------------------------------------------------
2021-05-17 09:57:35,798 EPOCH 25 done: loss 0.1712 - lr 0.0125000
2021-05-17 09:57:41,015 DEV : loss 0.16554737091064453 - score 0.9825
2021-05-17 09:57:41,069 BAD EPOCHS (no improvement): 1
2021-05-17 09:57:41,070 ----------------------------------------------------------------------------------------------------
2021-05-17 09:57:51,263 epoch 26 - iter 12/127 - loss 0.06644962 - samples/sec: 18.84 - lr: 0.012500
2021-05-17 09:58:01,683 epoch 26 - iter 24/127 - loss 0.09860970 - samples/sec: 18.43 - lr: 0.012500
2021-05-17 09:58:10,634 epoch 26 - iter 36/127 - loss 0.11192235 - samples/sec: 21.45 - lr: 0.012500
2021-05-17 09:58:20,137 epoch 26 - iter 48/127 - loss 0.18032826 - samples/sec: 20.21 - lr: 0.012500
2021-05-17 09:58:30,489 epoch 26 - iter 60/127 - loss 0.18616610 - samples/sec: 18.55 - lr: 0.012500
2021-05-17 09:58:41,583 epoch 26 - iter 72/127 - loss 0.16942472 - samples/sec: 17.31 - lr: 0.012500
2021-05-17 09:58:51,315 epoch 26 - iter 84/127 - loss 0.17048847 - samples/sec: 19.73 - lr: 0.012500
2021-05-17 09:59:00,940 epoch 26 - iter 96/127 - loss 0.16600958 - samples/sec: 19.95 - lr: 0.012500
2021-05-17 09:59:11,034 epoch 26 - iter 108/127 - loss 0.16969224 - samples/sec: 19.02 - lr: 0.012500
2021-05-17 09:59:19,812 epoch 26 - iter 120/127 - loss 0.17905704 - samples/sec: 21.88 - lr: 0.012500
2021-05-17 09:59:24,848 ----------------------------------------------------------------------------------------------------
2021-05-17 09:59:24,849 EPOCH 26 done: loss 0.1785 - lr 0.0125000
2021-05-17 09:59:30,352 DEV : loss 0.16491253674030304 - score 0.9847
2021-05-17 09:59:30,457 BAD EPOCHS (no improvement): 2
2021-05-17 09:59:30,458 ----------------------------------------------------------------------------------------------------
2021-05-17 09:59:40,829 epoch 27 - iter 12/127 - loss 0.15473238 - samples/sec: 18.52 - lr: 0.012500
2021-05-17 09:59:50,085 epoch 27 - iter 24/127 - loss 0.16359655 - samples/sec: 20.75 - lr: 0.012500
2021-05-17 10:00:00,379 epoch 27 - iter 36/127 - loss 0.14876848 - samples/sec: 18.65 - lr: 0.012500
2021-05-17 10:00:08,909 epoch 27 - iter 48/127 - loss 0.14948339 - samples/sec: 22.51 - lr: 0.012500
2021-05-17 10:00:18,893 epoch 27 - iter 60/127 - loss 0.15432956 - samples/sec: 19.23 - lr: 0.012500
2021-05-17 10:00:28,854 epoch 27 - iter 72/127 - loss 0.14670905 - samples/sec: 19.28 - lr: 0.012500
2021-05-17 10:00:37,615 epoch 27 - iter 84/127 - loss 0.14518033 - samples/sec: 21.92 - lr: 0.012500
2021-05-17 10:00:45,929 epoch 27 - iter 96/127 - loss 0.16071319 - samples/sec: 23.10 - lr: 0.012500
2021-05-17 10:00:55,767 epoch 27 - iter 108/127 - loss 0.15711454 - samples/sec: 19.52 - lr: 0.012500
2021-05-17 10:01:05,475 epoch 27 - iter 120/127 - loss 0.16129734 - samples/sec: 19.78 - lr: 0.012500
2021-05-17 10:01:10,438 ----------------------------------------------------------------------------------------------------
2021-05-17 10:01:10,439 EPOCH 27 done: loss 0.1642 - lr 0.0125000
2021-05-17 10:01:15,736 DEV : loss 0.15991643071174622 - score 0.9825
2021-05-17 10:01:15,791 BAD EPOCHS (no improvement): 3
2021-05-17 10:01:15,792 ----------------------------------------------------------------------------------------------------
2021-05-17 10:01:24,477 epoch 28 - iter 12/127 - loss 0.20956249 - samples/sec: 22.11 - lr: 0.012500
2021-05-17 10:01:34,160 epoch 28 - iter 24/127 - loss 0.20184488 - samples/sec: 19.83 - lr: 0.012500
2021-05-17 10:01:44,088 epoch 28 - iter 36/127 - loss 0.18428422 - samples/sec: 19.34 - lr: 0.012500
2021-05-17 10:01:54,794 epoch 28 - iter 48/127 - loss 0.18085956 - samples/sec: 17.94 - lr: 0.012500
2021-05-17 10:02:04,327 epoch 28 - iter 60/127 - loss 0.16571351 - samples/sec: 20.14 - lr: 0.012500
2021-05-17 10:02:15,085 epoch 28 - iter 72/127 - loss 0.16439932 - samples/sec: 17.85 - lr: 0.012500
2021-05-17 10:02:24,911 epoch 28 - iter 84/127 - loss 0.17295809 - samples/sec: 19.54 - lr: 0.012500
2021-05-17 10:02:34,197 epoch 28 - iter 96/127 - loss 0.17023454 - samples/sec: 20.68 - lr: 0.012500
2021-05-17 10:02:43,284 epoch 28 - iter 108/127 - loss 0.16366055 - samples/sec: 21.13 - lr: 0.012500
2021-05-17 10:02:52,820 epoch 28 - iter 120/127 - loss 0.15629136 - samples/sec: 20.14 - lr: 0.012500
2021-05-17 10:02:57,542 ----------------------------------------------------------------------------------------------------
2021-05-17 10:02:57,543 EPOCH 28 done: loss 0.1608 - lr 0.0125000
2021-05-17 10:03:03,459 DEV : loss 0.16782982647418976 - score 0.9825
Epoch    28: reducing learning rate of group 0 to 6.2500e-03.
2021-05-17 10:03:03,515 BAD EPOCHS (no improvement): 4
2021-05-17 10:03:03,515 ----------------------------------------------------------------------------------------------------
2021-05-17 10:03:13,247 epoch 29 - iter 12/127 - loss 0.11833695 - samples/sec: 19.73 - lr: 0.006250
2021-05-17 10:03:23,446 epoch 29 - iter 24/127 - loss 0.20471629 - samples/sec: 18.83 - lr: 0.006250
2021-05-17 10:03:33,714 epoch 29 - iter 36/127 - loss 0.17768220 - samples/sec: 18.70 - lr: 0.006250
2021-05-17 10:03:43,454 epoch 29 - iter 48/127 - loss 0.17471458 - samples/sec: 19.72 - lr: 0.006250
2021-05-17 10:03:52,471 epoch 29 - iter 60/127 - loss 0.17837887 - samples/sec: 21.30 - lr: 0.006250
2021-05-17 10:04:02,186 epoch 29 - iter 72/127 - loss 0.16922236 - samples/sec: 19.77 - lr: 0.006250
2021-05-17 10:04:11,430 epoch 29 - iter 84/127 - loss 0.16754228 - samples/sec: 20.77 - lr: 0.006250
2021-05-17 10:04:19,931 epoch 29 - iter 96/127 - loss 0.16901916 - samples/sec: 22.59 - lr: 0.006250
2021-05-17 10:04:29,690 epoch 29 - iter 108/127 - loss 0.16548101 - samples/sec: 19.68 - lr: 0.006250
2021-05-17 10:04:38,495 epoch 29 - iter 120/127 - loss 0.16731410 - samples/sec: 21.81 - lr: 0.006250
2021-05-17 10:04:43,196 ----------------------------------------------------------------------------------------------------
2021-05-17 10:04:43,197 EPOCH 29 done: loss 0.1698 - lr 0.0062500
2021-05-17 10:04:48,446 DEV : loss 0.1577041894197464 - score 0.9847
2021-05-17 10:04:48,502 BAD EPOCHS (no improvement): 1
2021-05-17 10:04:48,502 ----------------------------------------------------------------------------------------------------
2021-05-17 10:04:57,601 epoch 30 - iter 12/127 - loss 0.15233199 - samples/sec: 21.11 - lr: 0.006250
2021-05-17 10:05:06,909 epoch 30 - iter 24/127 - loss 0.14878786 - samples/sec: 20.63 - lr: 0.006250
2021-05-17 10:05:16,179 epoch 30 - iter 36/127 - loss 0.18646091 - samples/sec: 20.72 - lr: 0.006250
2021-05-17 10:05:25,166 epoch 30 - iter 48/127 - loss 0.15549252 - samples/sec: 21.37 - lr: 0.006250
2021-05-17 10:05:34,410 epoch 30 - iter 60/127 - loss 0.16533364 - samples/sec: 20.77 - lr: 0.006250
2021-05-17 10:05:43,744 epoch 30 - iter 72/127 - loss 0.16441045 - samples/sec: 20.57 - lr: 0.006250
2021-05-17 10:05:54,890 epoch 30 - iter 84/127 - loss 0.16015292 - samples/sec: 17.24 - lr: 0.006250
2021-05-17 10:06:06,748 epoch 30 - iter 96/127 - loss 0.16840659 - samples/sec: 16.19 - lr: 0.006250
2021-05-17 10:06:17,411 epoch 30 - iter 108/127 - loss 0.15909504 - samples/sec: 18.01 - lr: 0.006250
2021-05-17 10:06:27,575 epoch 30 - iter 120/127 - loss 0.15273210 - samples/sec: 18.89 - lr: 0.006250
2021-05-17 10:06:33,759 ----------------------------------------------------------------------------------------------------
2021-05-17 10:06:33,760 EPOCH 30 done: loss 0.1542 - lr 0.0062500
2021-05-17 10:06:40,447 DEV : loss 0.15650638937950134 - score 0.9825
2021-05-17 10:06:40,502 BAD EPOCHS (no improvement): 2
2021-05-17 10:06:51,076 ----------------------------------------------------------------------------------------------------
2021-05-17 10:06:51,077 Testing using best model ...
2021-05-17 10:06:51,077 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/por.rst.cstn/best-model.pt
2021-05-17 10:07:14,365 0.9840	1.0000	0.9919
2021-05-17 10:07:14,365 
Results:
- F1-score (micro) 0.9919
- F1-score (macro) 0.9919

By class:
SENT       tp: 123 - fp: 2 - fn: 0 - precision: 0.9840 - recall: 1.0000 - f1-score: 0.9919
2021-05-17 10:07:14,365 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb/
2021-05-17 10:07:14,406 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb
2021-05-17 10:07:14,406 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb/sent_train.txt
2021-05-17 10:07:14,409 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb/sent_dev.txt
2021-05-17 10:07:14,411 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb/sent_test.txt
Corpus: 18701 train + 2362 dev + 2245 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 10:07:46,778 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,783 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 10:07:46,784 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,784 Corpus: "Corpus: 18701 train + 2362 dev + 2245 test sentences"
2021-05-17 10:07:46,784 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,784 Parameters:
2021-05-17 10:07:46,784  - learning_rate: "0.1"
2021-05-17 10:07:46,784  - mini_batch_size: "16"
2021-05-17 10:07:46,784  - patience: "3"
2021-05-17 10:07:46,784  - anneal_factor: "0.5"
2021-05-17 10:07:46,784  - max_epochs: "30"
2021-05-17 10:07:46,784  - shuffle: "True"
2021-05-17 10:07:46,784  - train_with_dev: "False"
2021-05-17 10:07:46,784  - batch_growth_annealing: "False"
2021-05-17 10:07:46,785 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,785 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb"
2021-05-17 10:07:46,785 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,785 Device: cuda:0
2021-05-17 10:07:46,785 ----------------------------------------------------------------------------------------------------
2021-05-17 10:07:46,785 Embeddings storage mode: cpu
2021-05-17 10:07:46,788 ----------------------------------------------------------------------------------------------------
2021-05-17 10:11:52,136 epoch 1 - iter 116/1169 - loss 7.11639903 - samples/sec: 7.57 - lr: 0.100000
2021-05-17 10:15:55,196 epoch 1 - iter 232/1169 - loss 6.44579918 - samples/sec: 7.64 - lr: 0.100000
2021-05-17 10:19:59,073 epoch 1 - iter 348/1169 - loss 6.17032740 - samples/sec: 7.61 - lr: 0.100000
2021-05-17 10:23:53,024 epoch 1 - iter 464/1169 - loss 6.01592389 - samples/sec: 7.93 - lr: 0.100000
2021-05-17 10:28:10,204 epoch 1 - iter 580/1169 - loss 5.89103171 - samples/sec: 7.22 - lr: 0.100000
2021-05-17 10:32:22,989 epoch 1 - iter 696/1169 - loss 5.79957639 - samples/sec: 7.34 - lr: 0.100000
2021-05-17 10:36:25,803 epoch 1 - iter 812/1169 - loss 5.72532846 - samples/sec: 7.64 - lr: 0.100000
2021-05-17 10:40:10,356 epoch 1 - iter 928/1169 - loss 5.66158382 - samples/sec: 8.27 - lr: 0.100000
2021-05-17 10:43:58,598 epoch 1 - iter 1044/1169 - loss 5.60368006 - samples/sec: 8.13 - lr: 0.100000
2021-05-17 10:47:49,581 epoch 1 - iter 1160/1169 - loss 5.55987125 - samples/sec: 8.04 - lr: 0.100000
2021-05-17 10:48:07,188 ----------------------------------------------------------------------------------------------------
2021-05-17 10:48:07,189 EPOCH 1 done: loss 5.5632 - lr 0.1000000
2021-05-17 10:51:12,253 DEV : loss 5.303060531616211 - score 0.0
2021-05-17 10:51:12,803 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 10:51:23,054 ----------------------------------------------------------------------------------------------------
2021-05-17 10:52:58,548 epoch 2 - iter 116/1169 - loss 5.31170668 - samples/sec: 19.44 - lr: 0.100000
2021-05-17 10:54:37,013 epoch 2 - iter 232/1169 - loss 5.35124527 - samples/sec: 18.85 - lr: 0.100000
2021-05-17 10:56:13,566 epoch 2 - iter 348/1169 - loss 5.40247353 - samples/sec: 19.22 - lr: 0.100000
2021-05-17 10:57:48,082 epoch 2 - iter 464/1169 - loss 5.35200409 - samples/sec: 19.64 - lr: 0.100000
2021-05-17 10:59:19,537 epoch 2 - iter 580/1169 - loss 5.34886521 - samples/sec: 20.30 - lr: 0.100000
2021-05-17 11:01:02,784 epoch 2 - iter 696/1169 - loss 5.35779044 - samples/sec: 17.98 - lr: 0.100000
2021-05-17 11:02:35,188 epoch 2 - iter 812/1169 - loss 5.36810112 - samples/sec: 20.09 - lr: 0.100000
2021-05-17 11:04:10,256 epoch 2 - iter 928/1169 - loss 5.37304598 - samples/sec: 19.52 - lr: 0.100000
2021-05-17 11:05:43,047 epoch 2 - iter 1044/1169 - loss 5.37951118 - samples/sec: 20.00 - lr: 0.100000
2021-05-17 11:07:21,786 epoch 2 - iter 1160/1169 - loss 5.36875406 - samples/sec: 18.80 - lr: 0.100000
2021-05-17 11:07:28,920 ----------------------------------------------------------------------------------------------------
2021-05-17 11:07:28,920 EPOCH 2 done: loss 5.3651 - lr 0.1000000
2021-05-17 11:08:24,417 DEV : loss 5.095772743225098 - score 0.0
2021-05-17 11:08:24,881 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 11:08:35,053 ----------------------------------------------------------------------------------------------------
2021-05-17 11:10:03,377 epoch 3 - iter 116/1169 - loss 5.14900829 - samples/sec: 21.02 - lr: 0.100000
2021-05-17 11:11:46,668 epoch 3 - iter 232/1169 - loss 5.18298850 - samples/sec: 17.97 - lr: 0.100000
2021-05-17 11:13:25,381 epoch 3 - iter 348/1169 - loss 5.16566226 - samples/sec: 18.80 - lr: 0.100000
2021-05-17 11:15:03,571 epoch 3 - iter 464/1169 - loss 5.19043823 - samples/sec: 18.90 - lr: 0.100000
2021-05-17 11:16:45,018 epoch 3 - iter 580/1169 - loss 5.21552331 - samples/sec: 18.30 - lr: 0.100000
2021-05-17 11:18:25,547 epoch 3 - iter 696/1169 - loss 5.23473832 - samples/sec: 18.46 - lr: 0.100000
2021-05-17 11:20:08,628 epoch 3 - iter 812/1169 - loss 5.26520766 - samples/sec: 18.01 - lr: 0.100000
2021-05-17 11:21:42,360 epoch 3 - iter 928/1169 - loss 5.23889914 - samples/sec: 19.80 - lr: 0.100000
2021-05-17 11:23:26,214 epoch 3 - iter 1044/1169 - loss 5.24460474 - samples/sec: 17.87 - lr: 0.100000
2021-05-17 11:25:11,668 epoch 3 - iter 1160/1169 - loss 5.26878237 - samples/sec: 17.60 - lr: 0.100000
2021-05-17 11:25:18,668 ----------------------------------------------------------------------------------------------------
2021-05-17 11:25:18,669 EPOCH 3 done: loss 5.2719 - lr 0.1000000
2021-05-17 11:26:05,707 DEV : loss 5.521855354309082 - score 0.0
2021-05-17 11:26:06,275 BAD EPOCHS (no improvement): 1
2021-05-17 11:26:06,276 ----------------------------------------------------------------------------------------------------
2021-05-17 11:27:42,836 epoch 4 - iter 116/1169 - loss 5.50682417 - samples/sec: 19.22 - lr: 0.100000
2021-05-17 11:29:18,941 epoch 4 - iter 232/1169 - loss 5.46764066 - samples/sec: 19.31 - lr: 0.100000
2021-05-17 11:30:52,246 epoch 4 - iter 348/1169 - loss 5.42430940 - samples/sec: 19.89 - lr: 0.100000
2021-05-17 11:32:27,262 epoch 4 - iter 464/1169 - loss 5.42694109 - samples/sec: 19.53 - lr: 0.100000
2021-05-17 11:34:02,652 epoch 4 - iter 580/1169 - loss 5.43599254 - samples/sec: 19.46 - lr: 0.100000
2021-05-17 11:35:35,476 epoch 4 - iter 696/1169 - loss 5.41742551 - samples/sec: 20.00 - lr: 0.100000
2021-05-17 11:37:12,433 epoch 4 - iter 812/1169 - loss 5.40639158 - samples/sec: 19.14 - lr: 0.100000
2021-05-17 11:38:46,613 epoch 4 - iter 928/1169 - loss 5.37737861 - samples/sec: 19.71 - lr: 0.100000
2021-05-17 11:40:21,128 epoch 4 - iter 1044/1169 - loss 5.34497429 - samples/sec: 19.64 - lr: 0.100000
2021-05-17 11:41:57,480 epoch 4 - iter 1160/1169 - loss 5.31790239 - samples/sec: 19.26 - lr: 0.100000
2021-05-17 11:42:04,274 ----------------------------------------------------------------------------------------------------
2021-05-17 11:42:04,275 EPOCH 4 done: loss 5.3170 - lr 0.1000000
2021-05-17 11:42:51,048 DEV : loss 5.019209384918213 - score 0.0
2021-05-17 11:42:51,513 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 11:43:01,541 ----------------------------------------------------------------------------------------------------
2021-05-17 11:44:49,451 epoch 5 - iter 116/1169 - loss 5.41011554 - samples/sec: 17.20 - lr: 0.100000
2021-05-17 11:46:33,024 epoch 5 - iter 232/1169 - loss 5.26279371 - samples/sec: 17.92 - lr: 0.100000
2021-05-17 11:48:14,977 epoch 5 - iter 348/1169 - loss 5.25288147 - samples/sec: 18.21 - lr: 0.100000
2021-05-17 11:50:00,937 epoch 5 - iter 464/1169 - loss 5.22646462 - samples/sec: 17.52 - lr: 0.100000
2021-05-17 11:51:52,244 epoch 5 - iter 580/1169 - loss 5.23947881 - samples/sec: 16.68 - lr: 0.100000
2021-05-17 11:53:31,864 epoch 5 - iter 696/1169 - loss 5.25452327 - samples/sec: 18.63 - lr: 0.100000
2021-05-17 11:55:19,878 epoch 5 - iter 812/1169 - loss 5.26477746 - samples/sec: 17.18 - lr: 0.100000
2021-05-17 11:57:22,055 epoch 5 - iter 928/1169 - loss 5.25286523 - samples/sec: 15.19 - lr: 0.100000
2021-05-17 11:59:20,185 epoch 5 - iter 1044/1169 - loss 5.24749100 - samples/sec: 15.71 - lr: 0.100000
2021-05-17 12:01:17,864 epoch 5 - iter 1160/1169 - loss 5.23831349 - samples/sec: 15.77 - lr: 0.100000
2021-05-17 12:01:27,888 ----------------------------------------------------------------------------------------------------
2021-05-17 12:01:27,888 EPOCH 5 done: loss 5.2462 - lr 0.1000000
2021-05-17 12:02:35,673 DEV : loss 5.468130111694336 - score 0.0
2021-05-17 12:02:36,128 BAD EPOCHS (no improvement): 1
2021-05-17 12:02:36,128 ----------------------------------------------------------------------------------------------------
2021-05-17 12:04:39,258 epoch 6 - iter 116/1169 - loss 5.19605492 - samples/sec: 15.08 - lr: 0.100000
2021-05-17 12:06:39,562 epoch 6 - iter 232/1169 - loss 5.21382150 - samples/sec: 15.43 - lr: 0.100000
2021-05-17 12:08:36,611 epoch 6 - iter 348/1169 - loss 5.22825786 - samples/sec: 15.86 - lr: 0.100000
2021-05-17 12:10:33,466 epoch 6 - iter 464/1169 - loss 5.30038811 - samples/sec: 15.88 - lr: 0.100000
2021-05-17 12:12:24,696 epoch 6 - iter 580/1169 - loss 5.34908416 - samples/sec: 16.69 - lr: 0.100000
2021-05-17 12:14:22,194 epoch 6 - iter 696/1169 - loss 5.31395405 - samples/sec: 15.80 - lr: 0.100000
2021-05-17 12:16:19,861 epoch 6 - iter 812/1169 - loss 5.28077566 - samples/sec: 15.77 - lr: 0.100000
2021-05-17 12:18:19,961 epoch 6 - iter 928/1169 - loss 5.26544745 - samples/sec: 15.45 - lr: 0.100000
2021-05-17 12:20:15,002 epoch 6 - iter 1044/1169 - loss 5.25367640 - samples/sec: 16.13 - lr: 0.100000
2021-05-17 12:21:59,872 epoch 6 - iter 1160/1169 - loss 5.22705651 - samples/sec: 17.70 - lr: 0.100000
2021-05-17 12:22:06,900 ----------------------------------------------------------------------------------------------------
2021-05-17 12:22:06,900 EPOCH 6 done: loss 5.2271 - lr 0.1000000
2021-05-17 12:22:55,851 DEV : loss 4.936727046966553 - score 0.0228
2021-05-17 12:22:56,314 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 12:23:06,688 ----------------------------------------------------------------------------------------------------
2021-05-17 12:24:37,776 epoch 7 - iter 116/1169 - loss 5.07408702 - samples/sec: 20.38 - lr: 0.100000
2021-05-17 12:26:13,296 epoch 7 - iter 232/1169 - loss 5.12777530 - samples/sec: 19.43 - lr: 0.100000
2021-05-17 12:27:57,840 epoch 7 - iter 348/1169 - loss 5.10037038 - samples/sec: 17.76 - lr: 0.100000
2021-05-17 12:29:37,691 epoch 7 - iter 464/1169 - loss 5.06731918 - samples/sec: 18.59 - lr: 0.100000
2021-05-17 12:31:15,688 epoch 7 - iter 580/1169 - loss 5.05966485 - samples/sec: 18.94 - lr: 0.100000
2021-05-17 12:32:50,361 epoch 7 - iter 696/1169 - loss 5.05562645 - samples/sec: 19.61 - lr: 0.100000
2021-05-17 12:34:30,816 epoch 7 - iter 812/1169 - loss 5.06148997 - samples/sec: 18.48 - lr: 0.100000
2021-05-17 12:36:07,941 epoch 7 - iter 928/1169 - loss 5.03785015 - samples/sec: 19.11 - lr: 0.100000
2021-05-17 12:37:44,237 epoch 7 - iter 1044/1169 - loss 5.04011400 - samples/sec: 19.28 - lr: 0.100000
2021-05-17 12:39:18,354 epoch 7 - iter 1160/1169 - loss 5.03561555 - samples/sec: 19.72 - lr: 0.100000
2021-05-17 12:39:25,148 ----------------------------------------------------------------------------------------------------
2021-05-17 12:39:25,148 EPOCH 7 done: loss 5.0375 - lr 0.1000000
2021-05-17 12:40:20,923 DEV : loss 4.863562107086182 - score 0.0131
2021-05-17 12:40:21,389 BAD EPOCHS (no improvement): 1
2021-05-17 12:40:21,390 ----------------------------------------------------------------------------------------------------
2021-05-17 12:42:00,113 epoch 8 - iter 116/1169 - loss 4.96098098 - samples/sec: 18.80 - lr: 0.100000
2021-05-17 12:43:37,372 epoch 8 - iter 232/1169 - loss 5.08197150 - samples/sec: 19.09 - lr: 0.100000
2021-05-17 12:45:18,553 epoch 8 - iter 348/1169 - loss 5.05417497 - samples/sec: 18.35 - lr: 0.100000
2021-05-17 12:46:57,974 epoch 8 - iter 464/1169 - loss 4.99956434 - samples/sec: 18.67 - lr: 0.100000
2021-05-17 12:48:37,891 epoch 8 - iter 580/1169 - loss 5.01595940 - samples/sec: 18.58 - lr: 0.100000
2021-05-17 12:50:17,039 epoch 8 - iter 696/1169 - loss 5.02380650 - samples/sec: 18.72 - lr: 0.100000
2021-05-17 12:51:55,962 epoch 8 - iter 812/1169 - loss 5.02620350 - samples/sec: 18.76 - lr: 0.100000
2021-05-17 12:53:29,775 epoch 8 - iter 928/1169 - loss 5.05337244 - samples/sec: 19.79 - lr: 0.100000
2021-05-17 12:55:06,332 epoch 8 - iter 1044/1169 - loss 5.05403143 - samples/sec: 19.23 - lr: 0.100000
2021-05-17 12:56:43,973 epoch 8 - iter 1160/1169 - loss 5.06057607 - samples/sec: 19.01 - lr: 0.100000
2021-05-17 12:56:51,348 ----------------------------------------------------------------------------------------------------
2021-05-17 12:56:51,348 EPOCH 8 done: loss 5.0636 - lr 0.1000000
2021-05-17 12:57:37,272 DEV : loss 5.702185153961182 - score 0.0056
2021-05-17 12:57:37,730 BAD EPOCHS (no improvement): 2
2021-05-17 12:57:37,731 ----------------------------------------------------------------------------------------------------
2021-05-17 12:59:12,284 epoch 9 - iter 116/1169 - loss 5.11617132 - samples/sec: 19.63 - lr: 0.100000
2021-05-17 13:00:46,242 epoch 9 - iter 232/1169 - loss 5.10815221 - samples/sec: 19.76 - lr: 0.100000
2021-05-17 13:02:22,634 epoch 9 - iter 348/1169 - loss 5.17296754 - samples/sec: 19.26 - lr: 0.100000
2021-05-17 13:03:59,788 epoch 9 - iter 464/1169 - loss 5.18076593 - samples/sec: 19.11 - lr: 0.100000
2021-05-17 13:05:36,555 epoch 9 - iter 580/1169 - loss 5.18992541 - samples/sec: 19.18 - lr: 0.100000
2021-05-17 13:07:16,562 epoch 9 - iter 696/1169 - loss 5.18256338 - samples/sec: 18.56 - lr: 0.100000
2021-05-17 13:08:52,324 epoch 9 - iter 812/1169 - loss 5.15933272 - samples/sec: 19.38 - lr: 0.100000
2021-05-17 13:10:39,077 epoch 9 - iter 928/1169 - loss 5.14986908 - samples/sec: 17.39 - lr: 0.100000
2021-05-17 13:12:17,582 epoch 9 - iter 1044/1169 - loss 5.12941881 - samples/sec: 18.84 - lr: 0.100000
2021-05-17 13:14:01,958 epoch 9 - iter 1160/1169 - loss 5.09574919 - samples/sec: 17.78 - lr: 0.100000
2021-05-17 13:14:08,515 ----------------------------------------------------------------------------------------------------
2021-05-17 13:14:08,515 EPOCH 9 done: loss 5.0947 - lr 0.1000000
2021-05-17 13:15:04,541 DEV : loss 4.910151481628418 - score 0.0299
2021-05-17 13:15:05,104 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 13:15:15,068 ----------------------------------------------------------------------------------------------------
2021-05-17 13:16:53,397 epoch 10 - iter 116/1169 - loss 5.01199200 - samples/sec: 18.88 - lr: 0.100000
2021-05-17 13:18:27,928 epoch 10 - iter 232/1169 - loss 4.98652200 - samples/sec: 19.64 - lr: 0.100000
2021-05-17 13:20:04,261 epoch 10 - iter 348/1169 - loss 5.00369882 - samples/sec: 19.27 - lr: 0.100000
2021-05-17 13:21:45,906 epoch 10 - iter 464/1169 - loss 5.01339029 - samples/sec: 18.26 - lr: 0.100000
2021-05-17 13:22:40,745 epoch 10 - iter 580/1169 - loss 4.98793468 - samples/sec: 33.85 - lr: 0.100000
2021-05-17 13:23:15,702 epoch 10 - iter 696/1169 - loss 4.98022801 - samples/sec: 53.10 - lr: 0.100000
2021-05-17 13:23:50,801 epoch 10 - iter 812/1169 - loss 4.99745757 - samples/sec: 52.89 - lr: 0.100000
2021-05-17 13:24:25,685 epoch 10 - iter 928/1169 - loss 5.00043247 - samples/sec: 53.21 - lr: 0.100000
2021-05-17 13:25:00,718 epoch 10 - iter 1044/1169 - loss 4.97831183 - samples/sec: 52.99 - lr: 0.100000
2021-05-17 13:25:35,402 epoch 10 - iter 1160/1169 - loss 4.98971799 - samples/sec: 53.52 - lr: 0.100000
2021-05-17 13:25:38,072 ----------------------------------------------------------------------------------------------------
2021-05-17 13:25:38,072 EPOCH 10 done: loss 4.9942 - lr 0.1000000
2021-05-17 13:25:53,003 DEV : loss 4.834497928619385 - score 0.0289
2021-05-17 13:25:53,250 BAD EPOCHS (no improvement): 1
2021-05-17 13:25:53,250 ----------------------------------------------------------------------------------------------------
2021-05-17 13:26:27,811 epoch 11 - iter 116/1169 - loss 5.01391982 - samples/sec: 53.71 - lr: 0.100000
2021-05-17 13:27:02,217 epoch 11 - iter 232/1169 - loss 5.03364870 - samples/sec: 53.95 - lr: 0.100000
2021-05-17 13:27:38,000 epoch 11 - iter 348/1169 - loss 5.03031621 - samples/sec: 51.87 - lr: 0.100000
2021-05-17 13:28:13,745 epoch 11 - iter 464/1169 - loss 5.02925608 - samples/sec: 51.93 - lr: 0.100000
2021-05-17 13:29:00,577 epoch 11 - iter 580/1169 - loss 5.01913581 - samples/sec: 39.66 - lr: 0.100000
2021-05-17 13:31:39,979 epoch 11 - iter 696/1169 - loss 5.03302482 - samples/sec: 11.78 - lr: 0.100000
2021-05-17 13:33:02,979 epoch 11 - iter 812/1169 - loss 5.03636207 - samples/sec: 23.19 - lr: 0.100000
2021-05-17 13:34:10,452 epoch 11 - iter 928/1169 - loss 5.02983909 - samples/sec: 28.22 - lr: 0.100000
2021-05-17 13:35:20,796 epoch 11 - iter 1044/1169 - loss 5.03280728 - samples/sec: 26.85 - lr: 0.100000
2021-05-17 13:36:31,889 epoch 11 - iter 1160/1169 - loss 5.05351409 - samples/sec: 27.63 - lr: 0.100000
2021-05-17 13:36:37,042 ----------------------------------------------------------------------------------------------------
2021-05-17 13:36:37,043 EPOCH 11 done: loss 5.0535 - lr 0.1000000
2021-05-17 13:53:41,737 DEV : loss 5.035190582275391 - score 0.0143
2021-05-17 13:53:42,266 BAD EPOCHS (no improvement): 2
2021-05-17 13:53:42,296 ----------------------------------------------------------------------------------------------------
2021-05-17 13:56:06,201 epoch 12 - iter 116/1169 - loss 5.09345088 - samples/sec: 12.90 - lr: 0.100000
2021-05-17 13:58:05,234 epoch 12 - iter 232/1169 - loss 5.18336700 - samples/sec: 15.59 - lr: 0.100000
2021-05-17 13:59:53,452 epoch 12 - iter 348/1169 - loss 5.16468291 - samples/sec: 17.15 - lr: 0.100000
2021-05-17 14:01:39,964 epoch 12 - iter 464/1169 - loss 5.19479075 - samples/sec: 17.43 - lr: 0.100000
2021-05-17 14:03:40,149 epoch 12 - iter 580/1169 - loss 5.14626818 - samples/sec: 15.44 - lr: 0.100000
2021-05-17 14:05:33,330 epoch 12 - iter 696/1169 - loss 5.13643476 - samples/sec: 16.40 - lr: 0.100000
2021-05-17 14:07:15,375 epoch 12 - iter 812/1169 - loss 5.12803217 - samples/sec: 18.19 - lr: 0.100000
2021-05-17 14:09:00,559 epoch 12 - iter 928/1169 - loss 5.13042525 - samples/sec: 17.65 - lr: 0.100000
2021-05-17 14:10:48,676 epoch 12 - iter 1044/1169 - loss 5.13194618 - samples/sec: 17.17 - lr: 0.100000
2021-05-17 14:12:39,495 epoch 12 - iter 1160/1169 - loss 5.12826359 - samples/sec: 16.75 - lr: 0.100000
2021-05-17 14:12:47,921 ----------------------------------------------------------------------------------------------------
2021-05-17 14:12:47,921 EPOCH 12 done: loss 5.1262 - lr 0.1000000
2021-05-17 14:13:46,007 DEV : loss 4.857517719268799 - score 0.0299
2021-05-17 14:13:46,485 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 14:13:57,828 ----------------------------------------------------------------------------------------------------
2021-05-17 14:15:40,342 epoch 13 - iter 116/1169 - loss 5.06020297 - samples/sec: 18.11 - lr: 0.100000
2021-05-17 14:17:23,005 epoch 13 - iter 232/1169 - loss 5.04115385 - samples/sec: 18.08 - lr: 0.100000
2021-05-17 14:19:10,366 epoch 13 - iter 348/1169 - loss 5.01877030 - samples/sec: 17.29 - lr: 0.100000
2021-05-17 14:20:52,746 epoch 13 - iter 464/1169 - loss 5.02274979 - samples/sec: 18.13 - lr: 0.100000
2021-05-17 14:22:35,662 epoch 13 - iter 580/1169 - loss 5.03041677 - samples/sec: 18.04 - lr: 0.100000
2021-05-17 14:24:09,968 epoch 13 - iter 696/1169 - loss 5.00844420 - samples/sec: 19.68 - lr: 0.100000
2021-05-17 14:25:48,849 epoch 13 - iter 812/1169 - loss 5.02897740 - samples/sec: 18.77 - lr: 0.100000
2021-05-17 14:27:20,773 epoch 13 - iter 928/1169 - loss 5.03527217 - samples/sec: 20.19 - lr: 0.100000
2021-05-17 14:28:59,039 epoch 13 - iter 1044/1169 - loss 5.02202450 - samples/sec: 18.89 - lr: 0.100000
2021-05-17 14:30:35,924 epoch 13 - iter 1160/1169 - loss 5.01969851 - samples/sec: 19.16 - lr: 0.100000
2021-05-17 14:30:43,278 ----------------------------------------------------------------------------------------------------
2021-05-17 14:30:43,280 EPOCH 13 done: loss 5.0191 - lr 0.1000000
2021-05-17 14:31:30,936 DEV : loss 4.915874004364014 - score 0.0259
2021-05-17 14:31:31,558 BAD EPOCHS (no improvement): 1
2021-05-17 14:31:31,558 ----------------------------------------------------------------------------------------------------
2021-05-17 14:33:05,569 epoch 14 - iter 116/1169 - loss 5.13035313 - samples/sec: 19.74 - lr: 0.100000
2021-05-17 14:34:38,115 epoch 14 - iter 232/1169 - loss 5.07263798 - samples/sec: 20.06 - lr: 0.100000
2021-05-17 14:36:11,994 epoch 14 - iter 348/1169 - loss 5.03175412 - samples/sec: 19.77 - lr: 0.100000
2021-05-17 14:37:48,728 epoch 14 - iter 464/1169 - loss 5.03418995 - samples/sec: 19.19 - lr: 0.100000
2021-05-17 14:39:43,091 epoch 14 - iter 580/1169 - loss 5.04766065 - samples/sec: 16.23 - lr: 0.100000
2021-05-17 14:41:34,354 epoch 14 - iter 696/1169 - loss 5.04385762 - samples/sec: 16.68 - lr: 0.100000
2021-05-17 14:43:32,169 epoch 14 - iter 812/1169 - loss 5.06255981 - samples/sec: 15.76 - lr: 0.100000
2021-05-17 14:45:28,862 epoch 14 - iter 928/1169 - loss 5.05949259 - samples/sec: 15.91 - lr: 0.100000
2021-05-17 14:47:23,900 epoch 14 - iter 1044/1169 - loss 5.06898811 - samples/sec: 16.14 - lr: 0.100000
2021-05-17 14:49:12,523 epoch 14 - iter 1160/1169 - loss 5.05818969 - samples/sec: 17.09 - lr: 0.100000
2021-05-17 14:49:18,560 ----------------------------------------------------------------------------------------------------
2021-05-17 14:49:18,560 EPOCH 14 done: loss 5.0581 - lr 0.1000000
2021-05-17 14:50:04,490 DEV : loss 5.052065372467041 - score 0.0272
2021-05-17 14:50:04,948 BAD EPOCHS (no improvement): 2
2021-05-17 14:50:04,948 ----------------------------------------------------------------------------------------------------
2021-05-17 14:51:40,081 epoch 15 - iter 116/1169 - loss 4.96134791 - samples/sec: 19.51 - lr: 0.100000
2021-05-17 14:53:17,654 epoch 15 - iter 232/1169 - loss 4.98315493 - samples/sec: 19.02 - lr: 0.100000
2021-05-17 14:54:53,673 epoch 15 - iter 348/1169 - loss 4.96914598 - samples/sec: 19.33 - lr: 0.100000
2021-05-17 14:56:28,168 epoch 15 - iter 464/1169 - loss 4.96348649 - samples/sec: 19.64 - lr: 0.100000
2021-05-17 14:57:59,568 epoch 15 - iter 580/1169 - loss 4.97006129 - samples/sec: 20.31 - lr: 0.100000
2021-05-17 14:59:34,772 epoch 15 - iter 696/1169 - loss 4.97291022 - samples/sec: 19.50 - lr: 0.100000
2021-05-17 15:01:11,724 epoch 15 - iter 812/1169 - loss 5.00546895 - samples/sec: 19.14 - lr: 0.100000
2021-05-17 15:02:46,420 epoch 15 - iter 928/1169 - loss 5.01178440 - samples/sec: 19.60 - lr: 0.100000
2021-05-17 15:04:16,683 epoch 15 - iter 1044/1169 - loss 5.01749565 - samples/sec: 20.56 - lr: 0.100000
2021-05-17 15:05:53,150 epoch 15 - iter 1160/1169 - loss 5.01978484 - samples/sec: 19.24 - lr: 0.100000
2021-05-17 15:06:02,577 ----------------------------------------------------------------------------------------------------
2021-05-17 15:06:02,579 EPOCH 15 done: loss 5.0191 - lr 0.1000000
2021-05-17 15:06:54,385 DEV : loss 4.949116230010986 - score 0.0031
2021-05-17 15:06:54,855 BAD EPOCHS (no improvement): 3
2021-05-17 15:06:54,856 ----------------------------------------------------------------------------------------------------
2021-05-17 15:08:30,352 epoch 16 - iter 116/1169 - loss 5.15283848 - samples/sec: 19.44 - lr: 0.100000
2021-05-17 15:10:11,191 epoch 16 - iter 232/1169 - loss 5.12979801 - samples/sec: 18.41 - lr: 0.100000
2021-05-17 15:11:41,964 epoch 16 - iter 348/1169 - loss 5.11831413 - samples/sec: 20.45 - lr: 0.100000
2021-05-17 15:13:15,164 epoch 16 - iter 464/1169 - loss 5.09356293 - samples/sec: 19.92 - lr: 0.100000
2021-05-17 15:14:51,754 epoch 16 - iter 580/1169 - loss 5.06207144 - samples/sec: 19.22 - lr: 0.100000
2021-05-17 15:16:34,247 epoch 16 - iter 696/1169 - loss 5.03114428 - samples/sec: 18.11 - lr: 0.100000
2021-05-17 15:18:08,871 epoch 16 - iter 812/1169 - loss 5.01031953 - samples/sec: 19.62 - lr: 0.100000
2021-05-17 15:19:47,802 epoch 16 - iter 928/1169 - loss 5.00369535 - samples/sec: 18.76 - lr: 0.100000
2021-05-17 15:21:26,310 epoch 16 - iter 1044/1169 - loss 5.00659239 - samples/sec: 18.85 - lr: 0.100000
2021-05-17 15:23:07,843 epoch 16 - iter 1160/1169 - loss 5.01866297 - samples/sec: 18.28 - lr: 0.100000
2021-05-17 15:23:14,717 ----------------------------------------------------------------------------------------------------
2021-05-17 15:23:14,718 EPOCH 16 done: loss 5.0202 - lr 0.1000000
2021-05-17 15:24:06,715 DEV : loss 4.854010581970215 - score 0.0217
Epoch    16: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 15:24:07,174 BAD EPOCHS (no improvement): 4
2021-05-17 15:24:07,175 ----------------------------------------------------------------------------------------------------
2021-05-17 15:25:49,825 epoch 17 - iter 116/1169 - loss 4.92182314 - samples/sec: 18.08 - lr: 0.050000
2021-05-17 15:27:37,080 epoch 17 - iter 232/1169 - loss 4.85176289 - samples/sec: 17.31 - lr: 0.050000
2021-05-17 15:29:22,041 epoch 17 - iter 348/1169 - loss 4.85358139 - samples/sec: 17.69 - lr: 0.050000
2021-05-17 15:30:58,969 epoch 17 - iter 464/1169 - loss 4.84837337 - samples/sec: 19.15 - lr: 0.050000
2021-05-17 15:32:42,956 epoch 17 - iter 580/1169 - loss 4.85829455 - samples/sec: 17.85 - lr: 0.050000
2021-05-17 15:34:30,130 epoch 17 - iter 696/1169 - loss 4.86551908 - samples/sec: 17.32 - lr: 0.050000
2021-05-17 15:36:06,617 epoch 17 - iter 812/1169 - loss 4.87141913 - samples/sec: 19.24 - lr: 0.050000
2021-05-17 15:37:44,549 epoch 17 - iter 928/1169 - loss 4.86952587 - samples/sec: 18.95 - lr: 0.050000
2021-05-17 15:39:25,246 epoch 17 - iter 1044/1169 - loss 4.85919651 - samples/sec: 18.43 - lr: 0.050000
2021-05-17 15:41:10,065 epoch 17 - iter 1160/1169 - loss 4.85764764 - samples/sec: 17.71 - lr: 0.050000
2021-05-17 15:41:17,948 ----------------------------------------------------------------------------------------------------
2021-05-17 15:41:17,948 EPOCH 17 done: loss 4.8581 - lr 0.0500000
2021-05-17 15:42:08,294 DEV : loss 4.831878185272217 - score 0.0192
2021-05-17 15:42:08,876 BAD EPOCHS (no improvement): 1
2021-05-17 15:42:08,879 ----------------------------------------------------------------------------------------------------
2021-05-17 15:43:45,068 epoch 18 - iter 116/1169 - loss 4.87061576 - samples/sec: 19.30 - lr: 0.050000
2021-05-17 15:45:17,697 epoch 18 - iter 232/1169 - loss 4.84371418 - samples/sec: 20.04 - lr: 0.050000
2021-05-17 15:46:57,216 epoch 18 - iter 348/1169 - loss 4.82344801 - samples/sec: 18.66 - lr: 0.050000
2021-05-17 15:48:32,190 epoch 18 - iter 464/1169 - loss 4.84567098 - samples/sec: 19.54 - lr: 0.050000
2021-05-17 15:50:05,430 epoch 18 - iter 580/1169 - loss 4.84341068 - samples/sec: 19.91 - lr: 0.050000
2021-05-17 15:51:42,891 epoch 18 - iter 696/1169 - loss 4.84873667 - samples/sec: 19.04 - lr: 0.050000
2021-05-17 15:53:16,135 epoch 18 - iter 812/1169 - loss 4.85636872 - samples/sec: 19.91 - lr: 0.050000
2021-05-17 15:54:54,234 epoch 18 - iter 928/1169 - loss 4.86389685 - samples/sec: 18.92 - lr: 0.050000
2021-05-17 15:56:31,288 epoch 18 - iter 1044/1169 - loss 4.86705435 - samples/sec: 19.12 - lr: 0.050000
2021-05-17 15:58:12,885 epoch 18 - iter 1160/1169 - loss 4.86470165 - samples/sec: 18.27 - lr: 0.050000
2021-05-17 15:58:20,769 ----------------------------------------------------------------------------------------------------
2021-05-17 15:58:20,769 EPOCH 18 done: loss 4.8643 - lr 0.0500000
2021-05-17 15:59:10,708 DEV : loss 4.8266448974609375 - score 0.0311
2021-05-17 15:59:11,274 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 15:59:22,612 ----------------------------------------------------------------------------------------------------
2021-05-17 16:00:57,505 epoch 19 - iter 116/1169 - loss 4.84383796 - samples/sec: 19.56 - lr: 0.050000
2021-05-17 16:02:36,417 epoch 19 - iter 232/1169 - loss 4.80174044 - samples/sec: 18.77 - lr: 0.050000
2021-05-17 16:04:12,549 epoch 19 - iter 348/1169 - loss 4.82496655 - samples/sec: 19.31 - lr: 0.050000
2021-05-17 16:05:57,453 epoch 19 - iter 464/1169 - loss 4.82667332 - samples/sec: 17.70 - lr: 0.050000
2021-05-17 16:07:43,279 epoch 19 - iter 580/1169 - loss 4.86866099 - samples/sec: 17.54 - lr: 0.050000
2021-05-17 16:09:18,827 epoch 19 - iter 696/1169 - loss 4.86481713 - samples/sec: 19.43 - lr: 0.050000
2021-05-17 16:10:56,806 epoch 19 - iter 812/1169 - loss 4.85944542 - samples/sec: 18.94 - lr: 0.050000
2021-05-17 16:12:37,350 epoch 19 - iter 928/1169 - loss 4.85697333 - samples/sec: 18.46 - lr: 0.050000
2021-05-17 16:14:16,966 epoch 19 - iter 1044/1169 - loss 4.86122308 - samples/sec: 18.63 - lr: 0.050000
2021-05-17 16:16:02,966 epoch 19 - iter 1160/1169 - loss 4.86789862 - samples/sec: 17.51 - lr: 0.050000
2021-05-17 16:16:10,943 ----------------------------------------------------------------------------------------------------
2021-05-17 16:16:10,944 EPOCH 19 done: loss 4.8676 - lr 0.0500000
2021-05-17 16:17:00,340 DEV : loss 4.928374767303467 - score 0.0271
2021-05-17 16:17:00,797 BAD EPOCHS (no improvement): 1
2021-05-17 16:17:00,798 ----------------------------------------------------------------------------------------------------
2021-05-17 16:18:32,440 epoch 20 - iter 116/1169 - loss 4.87094154 - samples/sec: 20.25 - lr: 0.050000
2021-05-17 16:20:11,792 epoch 20 - iter 232/1169 - loss 4.86005050 - samples/sec: 18.68 - lr: 0.050000
2021-05-17 16:21:56,025 epoch 20 - iter 348/1169 - loss 4.85238995 - samples/sec: 17.81 - lr: 0.050000
2021-05-17 16:23:36,351 epoch 20 - iter 464/1169 - loss 4.84457873 - samples/sec: 18.50 - lr: 0.050000
2021-05-17 16:25:32,282 epoch 20 - iter 580/1169 - loss 4.83375181 - samples/sec: 16.01 - lr: 0.050000
2021-05-17 16:27:13,948 epoch 20 - iter 696/1169 - loss 4.83546960 - samples/sec: 18.26 - lr: 0.050000
2021-05-17 16:28:54,321 epoch 20 - iter 812/1169 - loss 4.84230048 - samples/sec: 18.49 - lr: 0.050000
2021-05-17 16:30:33,965 epoch 20 - iter 928/1169 - loss 4.84565377 - samples/sec: 18.63 - lr: 0.050000
2021-05-17 16:32:15,278 epoch 20 - iter 1044/1169 - loss 4.84769757 - samples/sec: 18.32 - lr: 0.050000
2021-05-17 16:33:54,735 epoch 20 - iter 1160/1169 - loss 4.85144473 - samples/sec: 18.66 - lr: 0.050000
2021-05-17 16:34:01,177 ----------------------------------------------------------------------------------------------------
2021-05-17 16:34:01,178 EPOCH 20 done: loss 4.8516 - lr 0.0500000
2021-05-17 16:34:49,687 DEV : loss 4.871040344238281 - score 0.0241
2021-05-17 16:34:50,272 BAD EPOCHS (no improvement): 2
2021-05-17 16:34:50,272 ----------------------------------------------------------------------------------------------------
2021-05-17 16:36:28,824 epoch 21 - iter 116/1169 - loss 4.82991308 - samples/sec: 18.83 - lr: 0.050000
2021-05-17 16:38:02,936 epoch 21 - iter 232/1169 - loss 4.82167389 - samples/sec: 19.72 - lr: 0.050000
2021-05-17 16:39:36,158 epoch 21 - iter 348/1169 - loss 4.83860075 - samples/sec: 19.91 - lr: 0.050000
2021-05-17 16:41:08,910 epoch 21 - iter 464/1169 - loss 4.84883025 - samples/sec: 20.01 - lr: 0.050000
2021-05-17 16:42:42,402 epoch 21 - iter 580/1169 - loss 4.84460139 - samples/sec: 19.85 - lr: 0.050000
2021-05-17 16:44:15,823 epoch 21 - iter 696/1169 - loss 4.85571633 - samples/sec: 19.87 - lr: 0.050000
2021-05-17 16:45:53,064 epoch 21 - iter 812/1169 - loss 4.84756757 - samples/sec: 19.09 - lr: 0.050000
2021-05-17 16:47:24,943 epoch 21 - iter 928/1169 - loss 4.85596199 - samples/sec: 20.20 - lr: 0.050000
2021-05-17 16:49:00,794 epoch 21 - iter 1044/1169 - loss 4.86528719 - samples/sec: 19.36 - lr: 0.050000
2021-05-17 16:50:34,056 epoch 21 - iter 1160/1169 - loss 4.85689681 - samples/sec: 19.90 - lr: 0.050000
2021-05-17 16:50:41,891 ----------------------------------------------------------------------------------------------------
2021-05-17 16:50:41,892 EPOCH 21 done: loss 4.8554 - lr 0.0500000
2021-05-17 16:51:29,334 DEV : loss 4.892035961151123 - score 0.0174
2021-05-17 16:51:29,800 BAD EPOCHS (no improvement): 3
2021-05-17 16:51:29,801 ----------------------------------------------------------------------------------------------------
2021-05-17 16:53:07,388 epoch 22 - iter 116/1169 - loss 4.84387778 - samples/sec: 19.02 - lr: 0.050000
2021-05-17 16:54:39,821 epoch 22 - iter 232/1169 - loss 4.86678874 - samples/sec: 20.08 - lr: 0.050000
2021-05-17 16:56:11,767 epoch 22 - iter 348/1169 - loss 4.86815787 - samples/sec: 20.19 - lr: 0.050000
2021-05-17 16:57:43,097 epoch 22 - iter 464/1169 - loss 4.84844270 - samples/sec: 20.32 - lr: 0.050000
2021-05-17 16:59:20,094 epoch 22 - iter 580/1169 - loss 4.84699978 - samples/sec: 19.14 - lr: 0.050000
2021-05-17 17:00:51,471 epoch 22 - iter 696/1169 - loss 4.84722303 - samples/sec: 20.31 - lr: 0.050000
2021-05-17 17:02:34,373 epoch 22 - iter 812/1169 - loss 4.85518426 - samples/sec: 18.04 - lr: 0.050000
2021-05-17 17:04:14,161 epoch 22 - iter 928/1169 - loss 4.85294781 - samples/sec: 18.60 - lr: 0.050000
2021-05-17 17:05:56,947 epoch 22 - iter 1044/1169 - loss 4.86589508 - samples/sec: 18.06 - lr: 0.050000
2021-05-17 17:07:40,927 epoch 22 - iter 1160/1169 - loss 4.86070466 - samples/sec: 17.85 - lr: 0.050000
2021-05-17 17:07:47,401 ----------------------------------------------------------------------------------------------------
2021-05-17 17:07:47,401 EPOCH 22 done: loss 4.8598 - lr 0.0500000
2021-05-17 17:08:37,716 DEV : loss 4.808829307556152 - score 0.0306
Epoch    22: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 17:08:38,178 BAD EPOCHS (no improvement): 4
2021-05-17 17:08:38,178 ----------------------------------------------------------------------------------------------------
2021-05-17 17:10:13,443 epoch 23 - iter 116/1169 - loss 4.80111567 - samples/sec: 19.48 - lr: 0.025000
2021-05-17 17:11:52,728 epoch 23 - iter 232/1169 - loss 4.78073121 - samples/sec: 18.70 - lr: 0.025000
2021-05-17 17:13:29,301 epoch 23 - iter 348/1169 - loss 4.80205015 - samples/sec: 19.22 - lr: 0.025000
2021-05-17 17:15:04,219 epoch 23 - iter 464/1169 - loss 4.81027814 - samples/sec: 19.56 - lr: 0.025000
2021-05-17 17:16:37,505 epoch 23 - iter 580/1169 - loss 4.80058280 - samples/sec: 19.90 - lr: 0.025000
2021-05-17 17:18:09,827 epoch 23 - iter 696/1169 - loss 4.79421589 - samples/sec: 20.10 - lr: 0.025000
2021-05-17 17:19:36,748 epoch 23 - iter 812/1169 - loss 4.79568585 - samples/sec: 21.35 - lr: 0.025000
2021-05-17 17:21:14,773 epoch 23 - iter 928/1169 - loss 4.80125249 - samples/sec: 18.94 - lr: 0.025000
2021-05-17 17:22:55,469 epoch 23 - iter 1044/1169 - loss 4.80174656 - samples/sec: 18.43 - lr: 0.025000
2021-05-17 17:24:30,028 epoch 23 - iter 1160/1169 - loss 4.80760041 - samples/sec: 19.63 - lr: 0.025000
2021-05-17 17:24:38,433 ----------------------------------------------------------------------------------------------------
2021-05-17 17:24:38,434 EPOCH 23 done: loss 4.8066 - lr 0.0250000
2021-05-17 17:25:27,816 DEV : loss 4.803623676300049 - score 0.03
2021-05-17 17:25:28,279 BAD EPOCHS (no improvement): 1
2021-05-17 17:25:28,279 ----------------------------------------------------------------------------------------------------
2021-05-17 17:27:06,616 epoch 24 - iter 116/1169 - loss 4.73993711 - samples/sec: 18.88 - lr: 0.025000
2021-05-17 17:28:40,473 epoch 24 - iter 232/1169 - loss 4.75648724 - samples/sec: 19.78 - lr: 0.025000
2021-05-17 17:30:26,734 epoch 24 - iter 348/1169 - loss 4.74835521 - samples/sec: 17.47 - lr: 0.025000
2021-05-17 17:32:06,808 epoch 24 - iter 464/1169 - loss 4.76436122 - samples/sec: 18.55 - lr: 0.025000
2021-05-17 17:33:42,585 epoch 24 - iter 580/1169 - loss 4.79175934 - samples/sec: 19.38 - lr: 0.025000
2021-05-17 17:35:17,281 epoch 24 - iter 696/1169 - loss 4.79133369 - samples/sec: 19.60 - lr: 0.025000
2021-05-17 17:36:51,890 epoch 24 - iter 812/1169 - loss 4.78496761 - samples/sec: 19.62 - lr: 0.025000
2021-05-17 17:38:31,184 epoch 24 - iter 928/1169 - loss 4.78892234 - samples/sec: 18.69 - lr: 0.025000
2021-05-17 17:40:07,973 epoch 24 - iter 1044/1169 - loss 4.78845946 - samples/sec: 19.18 - lr: 0.025000
2021-05-17 17:41:41,102 epoch 24 - iter 1160/1169 - loss 4.78991286 - samples/sec: 19.93 - lr: 0.025000
2021-05-17 17:41:48,901 ----------------------------------------------------------------------------------------------------
2021-05-17 17:41:48,901 EPOCH 24 done: loss 4.7907 - lr 0.0250000
2021-05-17 17:42:43,183 DEV : loss 4.786691188812256 - score 0.0289
2021-05-17 17:42:43,738 BAD EPOCHS (no improvement): 2
2021-05-17 17:42:43,738 ----------------------------------------------------------------------------------------------------
2021-05-17 17:44:18,629 epoch 25 - iter 116/1169 - loss 4.74599056 - samples/sec: 19.56 - lr: 0.025000
2021-05-17 17:46:02,282 epoch 25 - iter 232/1169 - loss 4.80048611 - samples/sec: 17.91 - lr: 0.025000
2021-05-17 17:47:50,258 epoch 25 - iter 348/1169 - loss 4.79732611 - samples/sec: 17.19 - lr: 0.025000
2021-05-17 17:49:29,991 epoch 25 - iter 464/1169 - loss 4.76465593 - samples/sec: 18.61 - lr: 0.025000
2021-05-17 17:51:09,181 epoch 25 - iter 580/1169 - loss 4.75040534 - samples/sec: 18.71 - lr: 0.025000
2021-05-17 17:52:49,491 epoch 25 - iter 696/1169 - loss 4.75657811 - samples/sec: 18.50 - lr: 0.025000
2021-05-17 17:54:36,061 epoch 25 - iter 812/1169 - loss 4.75372417 - samples/sec: 17.42 - lr: 0.025000
2021-05-17 17:56:17,297 epoch 25 - iter 928/1169 - loss 4.76445095 - samples/sec: 18.33 - lr: 0.025000
2021-05-17 17:57:57,620 epoch 25 - iter 1044/1169 - loss 4.77346119 - samples/sec: 18.50 - lr: 0.025000
2021-05-17 17:59:28,774 epoch 25 - iter 1160/1169 - loss 4.77553671 - samples/sec: 20.36 - lr: 0.025000
2021-05-17 17:59:36,129 ----------------------------------------------------------------------------------------------------
2021-05-17 17:59:36,129 EPOCH 25 done: loss 4.7774 - lr 0.0250000
2021-05-17 18:00:26,868 DEV : loss 4.791931629180908 - score 0.03
2021-05-17 18:00:27,327 BAD EPOCHS (no improvement): 3
2021-05-17 18:00:27,328 ----------------------------------------------------------------------------------------------------
2021-05-17 18:01:58,084 epoch 26 - iter 116/1169 - loss 4.76971693 - samples/sec: 20.45 - lr: 0.025000
2021-05-17 18:03:32,598 epoch 26 - iter 232/1169 - loss 4.73388763 - samples/sec: 19.64 - lr: 0.025000
2021-05-17 18:05:06,196 epoch 26 - iter 348/1169 - loss 4.75802804 - samples/sec: 19.83 - lr: 0.025000
2021-05-17 18:06:43,548 epoch 26 - iter 464/1169 - loss 4.75209648 - samples/sec: 19.07 - lr: 0.025000
2021-05-17 18:08:19,757 epoch 26 - iter 580/1169 - loss 4.78086557 - samples/sec: 19.29 - lr: 0.025000
2021-05-17 18:09:53,684 epoch 26 - iter 696/1169 - loss 4.76659478 - samples/sec: 19.76 - lr: 0.025000
2021-05-17 18:11:26,656 epoch 26 - iter 812/1169 - loss 4.77610627 - samples/sec: 19.96 - lr: 0.025000
2021-05-17 18:13:02,446 epoch 26 - iter 928/1169 - loss 4.77925512 - samples/sec: 19.38 - lr: 0.025000
2021-05-17 18:14:37,304 epoch 26 - iter 1044/1169 - loss 4.78197956 - samples/sec: 19.57 - lr: 0.025000
2021-05-17 18:16:09,608 epoch 26 - iter 1160/1169 - loss 4.77681808 - samples/sec: 20.11 - lr: 0.025000
2021-05-17 18:16:16,485 ----------------------------------------------------------------------------------------------------
2021-05-17 18:16:16,486 EPOCH 26 done: loss 4.7775 - lr 0.0250000
2021-05-17 18:17:07,151 DEV : loss 4.754419326782227 - score 0.0302
Epoch    26: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 18:17:07,609 BAD EPOCHS (no improvement): 4
2021-05-17 18:17:07,609 ----------------------------------------------------------------------------------------------------
2021-05-17 18:18:40,414 epoch 27 - iter 116/1169 - loss 4.78932392 - samples/sec: 20.00 - lr: 0.012500
2021-05-17 18:20:15,552 epoch 27 - iter 232/1169 - loss 4.76615264 - samples/sec: 19.51 - lr: 0.012500
2021-05-17 18:21:57,608 epoch 27 - iter 348/1169 - loss 4.77982554 - samples/sec: 18.19 - lr: 0.012500
2021-05-17 18:23:36,510 epoch 27 - iter 464/1169 - loss 4.76982974 - samples/sec: 18.77 - lr: 0.012500
2021-05-17 18:25:11,282 epoch 27 - iter 580/1169 - loss 4.76811139 - samples/sec: 19.59 - lr: 0.012500
2021-05-17 18:26:44,007 epoch 27 - iter 696/1169 - loss 4.75593913 - samples/sec: 20.02 - lr: 0.012500
2021-05-17 18:28:22,843 epoch 27 - iter 812/1169 - loss 4.75302461 - samples/sec: 18.78 - lr: 0.012500
2021-05-17 18:29:54,059 epoch 27 - iter 928/1169 - loss 4.74543631 - samples/sec: 20.35 - lr: 0.012500
2021-05-17 18:31:25,349 epoch 27 - iter 1044/1169 - loss 4.74917467 - samples/sec: 20.33 - lr: 0.012500
2021-05-17 18:32:59,150 epoch 27 - iter 1160/1169 - loss 4.73899842 - samples/sec: 19.79 - lr: 0.012500
2021-05-17 18:33:07,473 ----------------------------------------------------------------------------------------------------
2021-05-17 18:33:07,473 EPOCH 27 done: loss 4.7391 - lr 0.0125000
2021-05-17 18:33:53,842 DEV : loss 4.727595329284668 - score 0.0415
2021-05-17 18:33:54,298 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 18:34:05,435 ----------------------------------------------------------------------------------------------------
2021-05-17 18:35:40,498 epoch 28 - iter 116/1169 - loss 4.72895678 - samples/sec: 19.53 - lr: 0.012500
2021-05-17 18:37:12,770 epoch 28 - iter 232/1169 - loss 4.74283785 - samples/sec: 20.12 - lr: 0.012500
2021-05-17 18:38:48,102 epoch 28 - iter 348/1169 - loss 4.72142826 - samples/sec: 19.47 - lr: 0.012500
2021-05-17 18:40:21,868 epoch 28 - iter 464/1169 - loss 4.73691635 - samples/sec: 19.80 - lr: 0.012500
2021-05-17 18:41:59,975 epoch 28 - iter 580/1169 - loss 4.72267007 - samples/sec: 18.92 - lr: 0.012500
2021-05-17 18:43:33,720 epoch 28 - iter 696/1169 - loss 4.71274526 - samples/sec: 19.80 - lr: 0.012500
2021-05-17 18:45:09,227 epoch 28 - iter 812/1169 - loss 4.71326832 - samples/sec: 19.43 - lr: 0.012500
2021-05-17 18:46:50,686 epoch 28 - iter 928/1169 - loss 4.73032751 - samples/sec: 18.29 - lr: 0.012500
2021-05-17 18:48:28,493 epoch 28 - iter 1044/1169 - loss 4.72599965 - samples/sec: 18.98 - lr: 0.012500
2021-05-17 18:50:05,041 epoch 28 - iter 1160/1169 - loss 4.72591115 - samples/sec: 19.23 - lr: 0.012500
2021-05-17 18:50:12,281 ----------------------------------------------------------------------------------------------------
2021-05-17 18:50:12,281 EPOCH 28 done: loss 4.7245 - lr 0.0125000
2021-05-17 18:51:00,316 DEV : loss 4.726954460144043 - score 0.035
2021-05-17 18:51:00,780 BAD EPOCHS (no improvement): 1
2021-05-17 18:51:00,781 ----------------------------------------------------------------------------------------------------
2021-05-17 18:52:33,338 epoch 29 - iter 116/1169 - loss 4.64765854 - samples/sec: 20.05 - lr: 0.012500
2021-05-17 18:54:09,287 epoch 29 - iter 232/1169 - loss 4.67147181 - samples/sec: 19.35 - lr: 0.012500
2021-05-17 18:55:53,034 epoch 29 - iter 348/1169 - loss 4.67141571 - samples/sec: 17.89 - lr: 0.012500
2021-05-17 18:57:33,838 epoch 29 - iter 464/1169 - loss 4.69755081 - samples/sec: 18.41 - lr: 0.012500
2021-05-17 18:59:10,473 epoch 29 - iter 580/1169 - loss 4.71709311 - samples/sec: 19.21 - lr: 0.012500
2021-05-17 19:00:57,619 epoch 29 - iter 696/1169 - loss 4.71614336 - samples/sec: 17.32 - lr: 0.012500
2021-05-17 19:02:43,680 epoch 29 - iter 812/1169 - loss 4.73013869 - samples/sec: 17.50 - lr: 0.012500
2021-05-17 19:04:17,547 epoch 29 - iter 928/1169 - loss 4.74260718 - samples/sec: 19.77 - lr: 0.012500
2021-05-17 19:06:01,115 epoch 29 - iter 1044/1169 - loss 4.74120490 - samples/sec: 17.92 - lr: 0.012500
2021-05-17 19:07:42,944 epoch 29 - iter 1160/1169 - loss 4.72575689 - samples/sec: 18.23 - lr: 0.012500
2021-05-17 19:07:51,735 ----------------------------------------------------------------------------------------------------
2021-05-17 19:07:51,735 EPOCH 29 done: loss 4.7234 - lr 0.0125000
2021-05-17 19:08:40,804 DEV : loss 4.73424768447876 - score 0.0296
2021-05-17 19:08:41,266 BAD EPOCHS (no improvement): 2
2021-05-17 19:08:41,267 ----------------------------------------------------------------------------------------------------
2021-05-17 19:10:18,823 epoch 30 - iter 116/1169 - loss 4.70025968 - samples/sec: 19.03 - lr: 0.012500
2021-05-17 19:11:58,625 epoch 30 - iter 232/1169 - loss 4.69380911 - samples/sec: 18.60 - lr: 0.012500
2021-05-17 19:13:35,674 epoch 30 - iter 348/1169 - loss 4.71262674 - samples/sec: 19.13 - lr: 0.012500
2021-05-17 19:15:08,262 epoch 30 - iter 464/1169 - loss 4.73365001 - samples/sec: 20.05 - lr: 0.012500
2021-05-17 19:16:53,524 epoch 30 - iter 580/1169 - loss 4.72405298 - samples/sec: 17.63 - lr: 0.012500
2021-05-17 19:18:37,910 epoch 30 - iter 696/1169 - loss 4.72533496 - samples/sec: 17.78 - lr: 0.012500
2021-05-17 19:20:24,678 epoch 30 - iter 812/1169 - loss 4.72382837 - samples/sec: 17.38 - lr: 0.012500
2021-05-17 19:21:57,210 epoch 30 - iter 928/1169 - loss 4.72082504 - samples/sec: 20.06 - lr: 0.012500
2021-05-17 19:23:34,015 epoch 30 - iter 1044/1169 - loss 4.71875978 - samples/sec: 19.18 - lr: 0.012500
2021-05-17 19:25:15,013 epoch 30 - iter 1160/1169 - loss 4.72029738 - samples/sec: 18.38 - lr: 0.012500
2021-05-17 19:25:22,525 ----------------------------------------------------------------------------------------------------
2021-05-17 19:25:22,529 EPOCH 30 done: loss 4.7199 - lr 0.0125000
2021-05-17 19:26:11,873 DEV : loss 4.72442626953125 - score 0.0595
2021-05-17 19:26:12,341 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:26:35,459 ----------------------------------------------------------------------------------------------------
2021-05-17 19:26:35,460 Testing using best model ...
2021-05-17 19:26:35,551 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/tur.pdtb.tdb/best-model.pt
2021-05-17 19:29:49,195 0.6937	0.0378	0.0717
2021-05-17 19:29:49,196 
Results:
- F1-score (micro) 0.0717
- F1-score (macro) 0.0717

By class:
SENT       tp: 111 - fp: 49 - fn: 2826 - precision: 0.6937 - recall: 0.0378 - f1-score: 0.0717
2021-05-17 19:29:49,196 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac/
2021-05-17 19:29:49,267 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac
2021-05-17 19:29:49,268 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac/sent_train.txt
2021-05-17 19:29:49,271 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac/sent_dev.txt
2021-05-17 19:29:49,274 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac/sent_test.txt
Corpus: 1751 train + 228 dev + 315 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 19:30:11,911 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,924 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 19:30:11,924 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,924 Corpus: "Corpus: 1751 train + 228 dev + 315 test sentences"
2021-05-17 19:30:11,924 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,924 Parameters:
2021-05-17 19:30:11,924  - learning_rate: "0.1"
2021-05-17 19:30:11,924  - mini_batch_size: "16"
2021-05-17 19:30:11,924  - patience: "3"
2021-05-17 19:30:11,924  - anneal_factor: "0.5"
2021-05-17 19:30:11,924  - max_epochs: "30"
2021-05-17 19:30:11,925  - shuffle: "True"
2021-05-17 19:30:11,925  - train_with_dev: "False"
2021-05-17 19:30:11,925  - batch_growth_annealing: "False"
2021-05-17 19:30:11,925 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,925 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac"
2021-05-17 19:30:11,925 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,925 Device: cuda:0
2021-05-17 19:30:11,925 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:11,925 Embeddings storage mode: cpu
2021-05-17 19:30:11,928 ----------------------------------------------------------------------------------------------------
2021-05-17 19:30:36,887 epoch 1 - iter 11/110 - loss 16.40659913 - samples/sec: 7.05 - lr: 0.100000
2021-05-17 19:30:58,606 epoch 1 - iter 22/110 - loss 15.00980360 - samples/sec: 8.10 - lr: 0.100000
2021-05-17 19:31:18,311 epoch 1 - iter 33/110 - loss 14.09878774 - samples/sec: 8.93 - lr: 0.100000
2021-05-17 19:31:40,996 epoch 1 - iter 44/110 - loss 13.59858400 - samples/sec: 7.76 - lr: 0.100000
2021-05-17 19:32:03,138 epoch 1 - iter 55/110 - loss 13.17094850 - samples/sec: 7.95 - lr: 0.100000
2021-05-17 19:32:25,626 epoch 1 - iter 66/110 - loss 12.74926980 - samples/sec: 7.83 - lr: 0.100000
2021-05-17 19:32:49,791 epoch 1 - iter 77/110 - loss 12.33418201 - samples/sec: 7.28 - lr: 0.100000
2021-05-17 19:33:11,474 epoch 1 - iter 88/110 - loss 11.96370301 - samples/sec: 8.12 - lr: 0.100000
2021-05-17 19:33:31,842 epoch 1 - iter 99/110 - loss 11.62234822 - samples/sec: 8.64 - lr: 0.100000
2021-05-17 19:33:51,651 epoch 1 - iter 110/110 - loss 11.28993928 - samples/sec: 8.89 - lr: 0.100000
2021-05-17 19:33:51,652 ----------------------------------------------------------------------------------------------------
2021-05-17 19:33:51,652 EPOCH 1 done: loss 11.2899 - lr 0.1000000
2021-05-17 19:34:09,296 DEV : loss 6.29367208480835 - score 0.7299
2021-05-17 19:34:09,341 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:34:20,095 ----------------------------------------------------------------------------------------------------
2021-05-17 19:34:30,184 epoch 2 - iter 11/110 - loss 7.60891481 - samples/sec: 17.45 - lr: 0.100000
2021-05-17 19:34:40,061 epoch 2 - iter 22/110 - loss 7.87716348 - samples/sec: 17.82 - lr: 0.100000
2021-05-17 19:34:49,126 epoch 2 - iter 33/110 - loss 7.97067384 - samples/sec: 19.42 - lr: 0.100000
2021-05-17 19:34:57,255 epoch 2 - iter 44/110 - loss 7.86426869 - samples/sec: 21.65 - lr: 0.100000
2021-05-17 19:35:05,477 epoch 2 - iter 55/110 - loss 7.83889033 - samples/sec: 21.41 - lr: 0.100000
2021-05-17 19:35:14,773 epoch 2 - iter 66/110 - loss 7.81848209 - samples/sec: 18.93 - lr: 0.100000
2021-05-17 19:35:24,638 epoch 2 - iter 77/110 - loss 7.76859859 - samples/sec: 17.85 - lr: 0.100000
2021-05-17 19:35:33,347 epoch 2 - iter 88/110 - loss 7.73918636 - samples/sec: 20.21 - lr: 0.100000
2021-05-17 19:35:42,094 epoch 2 - iter 99/110 - loss 7.65062954 - samples/sec: 20.12 - lr: 0.100000
2021-05-17 19:35:50,528 epoch 2 - iter 110/110 - loss 7.53029289 - samples/sec: 20.87 - lr: 0.100000
2021-05-17 19:35:50,528 ----------------------------------------------------------------------------------------------------
2021-05-17 19:35:50,528 EPOCH 2 done: loss 7.5303 - lr 0.1000000
2021-05-17 19:35:55,651 DEV : loss 4.823390007019043 - score 0.7347
2021-05-17 19:35:55,695 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:36:06,480 ----------------------------------------------------------------------------------------------------
2021-05-17 19:36:15,615 epoch 3 - iter 11/110 - loss 6.49432451 - samples/sec: 19.27 - lr: 0.100000
2021-05-17 19:36:24,493 epoch 3 - iter 22/110 - loss 6.72032014 - samples/sec: 19.83 - lr: 0.100000
2021-05-17 19:36:32,811 epoch 3 - iter 33/110 - loss 6.60425286 - samples/sec: 21.16 - lr: 0.100000
2021-05-17 19:36:42,631 epoch 3 - iter 44/110 - loss 6.67733189 - samples/sec: 17.93 - lr: 0.100000
2021-05-17 19:36:51,540 epoch 3 - iter 55/110 - loss 6.69757703 - samples/sec: 19.77 - lr: 0.100000
2021-05-17 19:37:00,853 epoch 3 - iter 66/110 - loss 6.68150454 - samples/sec: 18.90 - lr: 0.100000
2021-05-17 19:37:09,005 epoch 3 - iter 77/110 - loss 6.57774326 - samples/sec: 21.59 - lr: 0.100000
2021-05-17 19:37:18,051 epoch 3 - iter 88/110 - loss 6.55178952 - samples/sec: 19.46 - lr: 0.100000
2021-05-17 19:37:26,167 epoch 3 - iter 99/110 - loss 6.52565774 - samples/sec: 21.69 - lr: 0.100000
2021-05-17 19:37:33,604 epoch 3 - iter 110/110 - loss 6.46861340 - samples/sec: 23.68 - lr: 0.100000
2021-05-17 19:37:33,605 ----------------------------------------------------------------------------------------------------
2021-05-17 19:37:33,605 EPOCH 3 done: loss 6.4686 - lr 0.1000000
2021-05-17 19:37:50,663 DEV : loss 5.487751007080078 - score 0.7611
2021-05-17 19:37:50,708 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:38:01,926 ----------------------------------------------------------------------------------------------------
2021-05-17 19:38:09,898 epoch 4 - iter 11/110 - loss 6.26913153 - samples/sec: 22.08 - lr: 0.100000
2021-05-17 19:38:19,036 epoch 4 - iter 22/110 - loss 5.85377184 - samples/sec: 19.26 - lr: 0.100000
2021-05-17 19:38:26,693 epoch 4 - iter 33/110 - loss 6.16587676 - samples/sec: 22.99 - lr: 0.100000
2021-05-17 19:38:33,894 epoch 4 - iter 44/110 - loss 6.01119708 - samples/sec: 24.45 - lr: 0.100000
2021-05-17 19:38:42,315 epoch 4 - iter 55/110 - loss 6.02343718 - samples/sec: 20.90 - lr: 0.100000
2021-05-17 19:38:50,872 epoch 4 - iter 66/110 - loss 6.05178785 - samples/sec: 20.57 - lr: 0.100000
2021-05-17 19:38:59,087 epoch 4 - iter 77/110 - loss 6.01959765 - samples/sec: 21.43 - lr: 0.100000
2021-05-17 19:39:07,201 epoch 4 - iter 88/110 - loss 6.03295955 - samples/sec: 21.69 - lr: 0.100000
2021-05-17 19:39:17,113 epoch 4 - iter 99/110 - loss 6.01170397 - samples/sec: 17.76 - lr: 0.100000
2021-05-17 19:39:25,591 epoch 4 - iter 110/110 - loss 5.97818795 - samples/sec: 20.76 - lr: 0.100000
2021-05-17 19:39:25,591 ----------------------------------------------------------------------------------------------------
2021-05-17 19:39:25,592 EPOCH 4 done: loss 5.9782 - lr 0.1000000
2021-05-17 19:39:29,725 DEV : loss 5.127121448516846 - score 0.7766
2021-05-17 19:39:29,770 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:39:40,729 ----------------------------------------------------------------------------------------------------
2021-05-17 19:39:49,502 epoch 5 - iter 11/110 - loss 6.52456531 - samples/sec: 20.07 - lr: 0.100000
2021-05-17 19:39:59,782 epoch 5 - iter 22/110 - loss 6.44894598 - samples/sec: 17.12 - lr: 0.100000
2021-05-17 19:40:08,188 epoch 5 - iter 33/110 - loss 6.35129319 - samples/sec: 20.94 - lr: 0.100000
2021-05-17 19:40:15,630 epoch 5 - iter 44/110 - loss 6.11363363 - samples/sec: 23.67 - lr: 0.100000
2021-05-17 19:40:23,417 epoch 5 - iter 55/110 - loss 6.08674815 - samples/sec: 22.60 - lr: 0.100000
2021-05-17 19:40:33,106 epoch 5 - iter 66/110 - loss 5.96245173 - samples/sec: 18.17 - lr: 0.100000
2021-05-17 19:40:43,112 epoch 5 - iter 77/110 - loss 5.95074482 - samples/sec: 17.60 - lr: 0.100000
2021-05-17 19:40:52,137 epoch 5 - iter 88/110 - loss 5.98683947 - samples/sec: 19.50 - lr: 0.100000
2021-05-17 19:41:00,594 epoch 5 - iter 99/110 - loss 5.93153728 - samples/sec: 20.84 - lr: 0.100000
2021-05-17 19:41:09,149 epoch 5 - iter 110/110 - loss 5.88251043 - samples/sec: 20.57 - lr: 0.100000
2021-05-17 19:41:09,150 ----------------------------------------------------------------------------------------------------
2021-05-17 19:41:09,150 EPOCH 5 done: loss 5.8825 - lr 0.1000000
2021-05-17 19:41:14,396 DEV : loss 3.900454521179199 - score 0.8284
2021-05-17 19:41:14,440 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:41:25,634 ----------------------------------------------------------------------------------------------------
2021-05-17 19:41:35,081 epoch 6 - iter 11/110 - loss 5.67746592 - samples/sec: 18.63 - lr: 0.100000
2021-05-17 19:41:45,471 epoch 6 - iter 22/110 - loss 5.72205257 - samples/sec: 16.94 - lr: 0.100000
2021-05-17 19:41:54,206 epoch 6 - iter 33/110 - loss 5.61564781 - samples/sec: 20.15 - lr: 0.100000
2021-05-17 19:42:03,416 epoch 6 - iter 44/110 - loss 5.58797657 - samples/sec: 19.13 - lr: 0.100000
2021-05-17 19:42:12,317 epoch 6 - iter 55/110 - loss 5.50871915 - samples/sec: 19.78 - lr: 0.100000
2021-05-17 19:42:21,157 epoch 6 - iter 66/110 - loss 5.49565052 - samples/sec: 19.91 - lr: 0.100000
2021-05-17 19:42:29,966 epoch 6 - iter 77/110 - loss 5.60149621 - samples/sec: 19.98 - lr: 0.100000
2021-05-17 19:42:39,881 epoch 6 - iter 88/110 - loss 5.52612539 - samples/sec: 17.76 - lr: 0.100000
2021-05-17 19:42:49,046 epoch 6 - iter 99/110 - loss 5.53698086 - samples/sec: 19.21 - lr: 0.100000
2021-05-17 19:42:58,623 epoch 6 - iter 110/110 - loss 5.49124125 - samples/sec: 18.38 - lr: 0.100000
2021-05-17 19:42:58,623 ----------------------------------------------------------------------------------------------------
2021-05-17 19:42:58,624 EPOCH 6 done: loss 5.4912 - lr 0.1000000
2021-05-17 19:43:03,211 DEV : loss 3.897594928741455 - score 0.8136
2021-05-17 19:43:03,255 BAD EPOCHS (no improvement): 1
2021-05-17 19:43:03,256 ----------------------------------------------------------------------------------------------------
2021-05-17 19:43:11,392 epoch 7 - iter 11/110 - loss 4.94245425 - samples/sec: 21.64 - lr: 0.100000
2021-05-17 19:43:19,742 epoch 7 - iter 22/110 - loss 5.06786849 - samples/sec: 21.08 - lr: 0.100000
2021-05-17 19:43:28,600 epoch 7 - iter 33/110 - loss 5.13885323 - samples/sec: 19.87 - lr: 0.100000
2021-05-17 19:43:38,327 epoch 7 - iter 44/110 - loss 5.25714848 - samples/sec: 18.10 - lr: 0.100000
2021-05-17 19:43:46,470 epoch 7 - iter 55/110 - loss 5.33482675 - samples/sec: 21.62 - lr: 0.100000
2021-05-17 19:43:55,747 epoch 7 - iter 66/110 - loss 5.23605711 - samples/sec: 18.97 - lr: 0.100000
2021-05-17 19:44:05,450 epoch 7 - iter 77/110 - loss 5.23135565 - samples/sec: 18.14 - lr: 0.100000
2021-05-17 19:44:13,579 epoch 7 - iter 88/110 - loss 5.33082544 - samples/sec: 21.67 - lr: 0.100000
2021-05-17 19:44:22,495 epoch 7 - iter 99/110 - loss 5.39170399 - samples/sec: 19.74 - lr: 0.100000
2021-05-17 19:44:30,910 epoch 7 - iter 110/110 - loss 5.37513780 - samples/sec: 20.92 - lr: 0.100000
2021-05-17 19:44:30,911 ----------------------------------------------------------------------------------------------------
2021-05-17 19:44:30,911 EPOCH 7 done: loss 5.3751 - lr 0.1000000
2021-05-17 19:44:35,209 DEV : loss 3.82231068611145 - score 0.8318
2021-05-17 19:44:35,254 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:44:45,964 ----------------------------------------------------------------------------------------------------
2021-05-17 19:44:55,869 epoch 8 - iter 11/110 - loss 5.23483580 - samples/sec: 17.77 - lr: 0.100000
2021-05-17 19:45:03,885 epoch 8 - iter 22/110 - loss 5.29372397 - samples/sec: 21.96 - lr: 0.100000
2021-05-17 19:45:13,795 epoch 8 - iter 33/110 - loss 5.26550323 - samples/sec: 17.76 - lr: 0.100000
2021-05-17 19:45:21,439 epoch 8 - iter 44/110 - loss 5.15148968 - samples/sec: 23.03 - lr: 0.100000
2021-05-17 19:45:30,787 epoch 8 - iter 55/110 - loss 5.26865133 - samples/sec: 18.83 - lr: 0.100000
2021-05-17 19:45:37,965 epoch 8 - iter 66/110 - loss 5.32370508 - samples/sec: 24.52 - lr: 0.100000
2021-05-17 19:45:46,542 epoch 8 - iter 77/110 - loss 5.27594458 - samples/sec: 20.52 - lr: 0.100000
2021-05-17 19:45:56,053 epoch 8 - iter 88/110 - loss 5.28180998 - samples/sec: 18.51 - lr: 0.100000
2021-05-17 19:46:05,174 epoch 8 - iter 99/110 - loss 5.28446621 - samples/sec: 19.30 - lr: 0.100000
2021-05-17 19:46:13,077 epoch 8 - iter 110/110 - loss 5.30051196 - samples/sec: 22.27 - lr: 0.100000
2021-05-17 19:46:13,078 ----------------------------------------------------------------------------------------------------
2021-05-17 19:46:13,078 EPOCH 8 done: loss 5.3005 - lr 0.1000000
2021-05-17 19:46:17,392 DEV : loss 4.090497016906738 - score 0.7895
2021-05-17 19:46:17,436 BAD EPOCHS (no improvement): 1
2021-05-17 19:46:17,437 ----------------------------------------------------------------------------------------------------
2021-05-17 19:46:24,935 epoch 9 - iter 11/110 - loss 5.06315390 - samples/sec: 23.48 - lr: 0.100000
2021-05-17 19:46:34,536 epoch 9 - iter 22/110 - loss 5.43526328 - samples/sec: 18.33 - lr: 0.100000
2021-05-17 19:46:43,735 epoch 9 - iter 33/110 - loss 5.46325978 - samples/sec: 19.13 - lr: 0.100000
2021-05-17 19:46:52,516 epoch 9 - iter 44/110 - loss 5.36952856 - samples/sec: 20.05 - lr: 0.100000
2021-05-17 19:47:01,462 epoch 9 - iter 55/110 - loss 5.34917452 - samples/sec: 19.68 - lr: 0.100000
2021-05-17 19:47:10,665 epoch 9 - iter 66/110 - loss 5.30841791 - samples/sec: 19.13 - lr: 0.100000
2021-05-17 19:47:21,111 epoch 9 - iter 77/110 - loss 5.30888285 - samples/sec: 16.85 - lr: 0.100000
2021-05-17 19:47:31,548 epoch 9 - iter 88/110 - loss 5.29766580 - samples/sec: 16.88 - lr: 0.100000
2021-05-17 19:47:41,323 epoch 9 - iter 99/110 - loss 5.25363984 - samples/sec: 18.01 - lr: 0.100000
2021-05-17 19:47:50,955 epoch 9 - iter 110/110 - loss 5.25423463 - samples/sec: 18.28 - lr: 0.100000
2021-05-17 19:47:50,956 ----------------------------------------------------------------------------------------------------
2021-05-17 19:47:50,956 EPOCH 9 done: loss 5.2542 - lr 0.1000000
2021-05-17 19:47:56,572 DEV : loss 3.932546854019165 - score 0.821
2021-05-17 19:47:56,617 BAD EPOCHS (no improvement): 2
2021-05-17 19:47:56,618 ----------------------------------------------------------------------------------------------------
2021-05-17 19:48:06,497 epoch 10 - iter 11/110 - loss 5.16351552 - samples/sec: 17.82 - lr: 0.100000
2021-05-17 19:48:17,057 epoch 10 - iter 22/110 - loss 5.11412859 - samples/sec: 16.68 - lr: 0.100000
2021-05-17 19:48:26,306 epoch 10 - iter 33/110 - loss 5.23370603 - samples/sec: 19.04 - lr: 0.100000
2021-05-17 19:48:35,019 epoch 10 - iter 44/110 - loss 5.24554269 - samples/sec: 20.20 - lr: 0.100000
2021-05-17 19:48:44,528 epoch 10 - iter 55/110 - loss 5.18278314 - samples/sec: 18.51 - lr: 0.100000
2021-05-17 19:48:54,917 epoch 10 - iter 66/110 - loss 5.22962731 - samples/sec: 16.95 - lr: 0.100000
2021-05-17 19:49:04,249 epoch 10 - iter 77/110 - loss 5.24305033 - samples/sec: 18.86 - lr: 0.100000
2021-05-17 19:49:13,578 epoch 10 - iter 88/110 - loss 5.23057562 - samples/sec: 18.87 - lr: 0.100000
2021-05-17 19:49:24,069 epoch 10 - iter 99/110 - loss 5.21605779 - samples/sec: 16.78 - lr: 0.100000
2021-05-17 19:49:33,694 epoch 10 - iter 110/110 - loss 5.22669325 - samples/sec: 18.29 - lr: 0.100000
2021-05-17 19:49:33,694 ----------------------------------------------------------------------------------------------------
2021-05-17 19:49:33,694 EPOCH 10 done: loss 5.2267 - lr 0.1000000
2021-05-17 19:49:39,576 DEV : loss 4.429699420928955 - score 0.8025
2021-05-17 19:49:39,620 BAD EPOCHS (no improvement): 3
2021-05-17 19:49:39,621 ----------------------------------------------------------------------------------------------------
2021-05-17 19:49:49,223 epoch 11 - iter 11/110 - loss 4.82244121 - samples/sec: 18.33 - lr: 0.100000
2021-05-17 19:49:59,368 epoch 11 - iter 22/110 - loss 5.19745550 - samples/sec: 17.35 - lr: 0.100000
2021-05-17 19:50:09,238 epoch 11 - iter 33/110 - loss 5.17967525 - samples/sec: 17.83 - lr: 0.100000
2021-05-17 19:50:19,745 epoch 11 - iter 44/110 - loss 5.09322055 - samples/sec: 16.75 - lr: 0.100000
2021-05-17 19:50:30,425 epoch 11 - iter 55/110 - loss 5.03446801 - samples/sec: 16.48 - lr: 0.100000
2021-05-17 19:50:40,796 epoch 11 - iter 66/110 - loss 5.05686738 - samples/sec: 16.98 - lr: 0.100000
2021-05-17 19:50:49,307 epoch 11 - iter 77/110 - loss 5.14256850 - samples/sec: 20.68 - lr: 0.100000
2021-05-17 19:50:57,223 epoch 11 - iter 88/110 - loss 5.09236774 - samples/sec: 22.24 - lr: 0.100000
2021-05-17 19:51:05,504 epoch 11 - iter 99/110 - loss 5.10078632 - samples/sec: 21.26 - lr: 0.100000
2021-05-17 19:51:14,552 epoch 11 - iter 110/110 - loss 5.11224217 - samples/sec: 19.45 - lr: 0.100000
2021-05-17 19:51:14,553 ----------------------------------------------------------------------------------------------------
2021-05-17 19:51:14,553 EPOCH 11 done: loss 5.1122 - lr 0.1000000
2021-05-17 19:51:20,559 DEV : loss 4.086042881011963 - score 0.8206
Epoch    11: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 19:51:20,606 BAD EPOCHS (no improvement): 4
2021-05-17 19:51:20,606 ----------------------------------------------------------------------------------------------------
2021-05-17 19:51:29,603 epoch 12 - iter 11/110 - loss 4.40972094 - samples/sec: 19.57 - lr: 0.050000
2021-05-17 19:51:38,839 epoch 12 - iter 22/110 - loss 4.41811842 - samples/sec: 19.06 - lr: 0.050000
2021-05-17 19:51:47,152 epoch 12 - iter 33/110 - loss 4.44681522 - samples/sec: 21.18 - lr: 0.050000
2021-05-17 19:51:57,135 epoch 12 - iter 44/110 - loss 4.55599518 - samples/sec: 17.63 - lr: 0.050000
2021-05-17 19:52:07,170 epoch 12 - iter 55/110 - loss 4.60007076 - samples/sec: 17.54 - lr: 0.050000
2021-05-17 19:52:17,732 epoch 12 - iter 66/110 - loss 4.60035961 - samples/sec: 16.67 - lr: 0.050000
2021-05-17 19:52:28,605 epoch 12 - iter 77/110 - loss 4.57999662 - samples/sec: 16.19 - lr: 0.050000
2021-05-17 19:52:38,952 epoch 12 - iter 88/110 - loss 4.61680334 - samples/sec: 17.01 - lr: 0.050000
2021-05-17 19:52:48,671 epoch 12 - iter 99/110 - loss 4.65534430 - samples/sec: 18.11 - lr: 0.050000
2021-05-17 19:52:58,127 epoch 12 - iter 110/110 - loss 4.63876551 - samples/sec: 18.62 - lr: 0.050000
2021-05-17 19:52:58,128 ----------------------------------------------------------------------------------------------------
2021-05-17 19:52:58,128 EPOCH 12 done: loss 4.6388 - lr 0.0500000
2021-05-17 19:53:03,736 DEV : loss 3.5714995861053467 - score 0.8393
2021-05-17 19:53:03,782 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:53:14,942 ----------------------------------------------------------------------------------------------------
2021-05-17 19:53:25,226 epoch 13 - iter 11/110 - loss 4.54173227 - samples/sec: 17.12 - lr: 0.050000
2021-05-17 19:53:34,235 epoch 13 - iter 22/110 - loss 4.55677176 - samples/sec: 19.54 - lr: 0.050000
2021-05-17 19:53:42,820 epoch 13 - iter 33/110 - loss 4.51943726 - samples/sec: 20.50 - lr: 0.050000
2021-05-17 19:53:51,834 epoch 13 - iter 44/110 - loss 4.46959690 - samples/sec: 19.53 - lr: 0.050000
2021-05-17 19:54:00,277 epoch 13 - iter 55/110 - loss 4.52355657 - samples/sec: 20.85 - lr: 0.050000
2021-05-17 19:54:09,740 epoch 13 - iter 66/110 - loss 4.51654222 - samples/sec: 18.62 - lr: 0.050000
2021-05-17 19:54:18,367 epoch 13 - iter 77/110 - loss 4.53149250 - samples/sec: 20.40 - lr: 0.050000
2021-05-17 19:54:27,145 epoch 13 - iter 88/110 - loss 4.54575733 - samples/sec: 20.05 - lr: 0.050000
2021-05-17 19:54:35,764 epoch 13 - iter 99/110 - loss 4.50416177 - samples/sec: 20.42 - lr: 0.050000
2021-05-17 19:54:46,047 epoch 13 - iter 110/110 - loss 4.51007723 - samples/sec: 17.12 - lr: 0.050000
2021-05-17 19:54:46,048 ----------------------------------------------------------------------------------------------------
2021-05-17 19:54:46,048 EPOCH 13 done: loss 4.5101 - lr 0.0500000
2021-05-17 19:54:51,075 DEV : loss 3.532637119293213 - score 0.8324
2021-05-17 19:54:51,121 BAD EPOCHS (no improvement): 1
2021-05-17 19:54:51,122 ----------------------------------------------------------------------------------------------------
2021-05-17 19:55:00,844 epoch 14 - iter 11/110 - loss 4.90676102 - samples/sec: 18.11 - lr: 0.050000
2021-05-17 19:55:10,738 epoch 14 - iter 22/110 - loss 4.69035646 - samples/sec: 17.79 - lr: 0.050000
2021-05-17 19:55:20,513 epoch 14 - iter 33/110 - loss 4.68165229 - samples/sec: 18.01 - lr: 0.050000
2021-05-17 19:55:30,133 epoch 14 - iter 44/110 - loss 4.60506860 - samples/sec: 18.30 - lr: 0.050000
2021-05-17 19:55:38,788 epoch 14 - iter 55/110 - loss 4.53140697 - samples/sec: 20.34 - lr: 0.050000
2021-05-17 19:55:48,440 epoch 14 - iter 66/110 - loss 4.52718592 - samples/sec: 18.24 - lr: 0.050000
2021-05-17 19:55:55,996 epoch 14 - iter 77/110 - loss 4.50977995 - samples/sec: 23.30 - lr: 0.050000
2021-05-17 19:56:05,419 epoch 14 - iter 88/110 - loss 4.47525450 - samples/sec: 18.68 - lr: 0.050000
2021-05-17 19:56:14,050 epoch 14 - iter 99/110 - loss 4.48032272 - samples/sec: 20.40 - lr: 0.050000
2021-05-17 19:56:21,777 epoch 14 - iter 110/110 - loss 4.50034828 - samples/sec: 22.78 - lr: 0.050000
2021-05-17 19:56:21,778 ----------------------------------------------------------------------------------------------------
2021-05-17 19:56:21,778 EPOCH 14 done: loss 4.5003 - lr 0.0500000
2021-05-17 19:56:26,606 DEV : loss 4.218347072601318 - score 0.8061
2021-05-17 19:56:26,651 BAD EPOCHS (no improvement): 2
2021-05-17 19:56:26,651 ----------------------------------------------------------------------------------------------------
2021-05-17 19:56:35,878 epoch 15 - iter 11/110 - loss 4.19252272 - samples/sec: 19.08 - lr: 0.050000
2021-05-17 19:56:46,857 epoch 15 - iter 22/110 - loss 4.42031818 - samples/sec: 16.03 - lr: 0.050000
2021-05-17 19:56:55,360 epoch 15 - iter 33/110 - loss 4.44641855 - samples/sec: 20.70 - lr: 0.050000
2021-05-17 19:57:03,569 epoch 15 - iter 44/110 - loss 4.45523329 - samples/sec: 21.44 - lr: 0.050000
2021-05-17 19:57:13,088 epoch 15 - iter 55/110 - loss 4.47173194 - samples/sec: 18.49 - lr: 0.050000
2021-05-17 19:57:21,350 epoch 15 - iter 66/110 - loss 4.45607551 - samples/sec: 21.31 - lr: 0.050000
2021-05-17 19:57:28,944 epoch 15 - iter 77/110 - loss 4.49945992 - samples/sec: 23.18 - lr: 0.050000
2021-05-17 19:57:36,253 epoch 15 - iter 88/110 - loss 4.50112797 - samples/sec: 24.08 - lr: 0.050000
2021-05-17 19:57:46,168 epoch 15 - iter 99/110 - loss 4.51183589 - samples/sec: 17.75 - lr: 0.050000
2021-05-17 19:57:53,453 epoch 15 - iter 110/110 - loss 4.50869670 - samples/sec: 24.16 - lr: 0.050000
2021-05-17 19:57:53,454 ----------------------------------------------------------------------------------------------------
2021-05-17 19:57:53,454 EPOCH 15 done: loss 4.5087 - lr 0.0500000
2021-05-17 19:57:57,585 DEV : loss 3.6075568199157715 - score 0.8318
2021-05-17 19:57:57,629 BAD EPOCHS (no improvement): 3
2021-05-17 19:57:57,629 ----------------------------------------------------------------------------------------------------
2021-05-17 19:58:05,650 epoch 16 - iter 11/110 - loss 4.46440571 - samples/sec: 21.95 - lr: 0.050000
2021-05-17 19:58:13,678 epoch 16 - iter 22/110 - loss 4.41022195 - samples/sec: 21.93 - lr: 0.050000
2021-05-17 19:58:22,365 epoch 16 - iter 33/110 - loss 4.38662627 - samples/sec: 20.26 - lr: 0.050000
2021-05-17 19:58:31,657 epoch 16 - iter 44/110 - loss 4.34261627 - samples/sec: 18.95 - lr: 0.050000
2021-05-17 19:58:39,408 epoch 16 - iter 55/110 - loss 4.38507962 - samples/sec: 22.71 - lr: 0.050000
2021-05-17 19:58:48,468 epoch 16 - iter 66/110 - loss 4.46891900 - samples/sec: 19.43 - lr: 0.050000
2021-05-17 19:58:58,094 epoch 16 - iter 77/110 - loss 4.47974255 - samples/sec: 18.30 - lr: 0.050000
2021-05-17 19:59:06,740 epoch 16 - iter 88/110 - loss 4.47126061 - samples/sec: 20.37 - lr: 0.050000
2021-05-17 19:59:16,765 epoch 16 - iter 99/110 - loss 4.43741515 - samples/sec: 17.56 - lr: 0.050000
2021-05-17 19:59:24,352 epoch 16 - iter 110/110 - loss 4.42934007 - samples/sec: 23.20 - lr: 0.050000
2021-05-17 19:59:24,353 ----------------------------------------------------------------------------------------------------
2021-05-17 19:59:24,353 EPOCH 16 done: loss 4.4293 - lr 0.0500000
2021-05-17 19:59:29,007 DEV : loss 3.500920295715332 - score 0.8423
2021-05-17 19:59:29,136 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 19:59:40,564 ----------------------------------------------------------------------------------------------------
2021-05-17 19:59:49,213 epoch 17 - iter 11/110 - loss 4.78948888 - samples/sec: 20.36 - lr: 0.050000
2021-05-17 19:59:57,694 epoch 17 - iter 22/110 - loss 4.55454162 - samples/sec: 20.76 - lr: 0.050000
2021-05-17 20:00:08,448 epoch 17 - iter 33/110 - loss 4.48120994 - samples/sec: 16.37 - lr: 0.050000
2021-05-17 20:00:20,041 epoch 17 - iter 44/110 - loss 4.45345529 - samples/sec: 15.18 - lr: 0.050000
2021-05-17 20:00:29,193 epoch 17 - iter 55/110 - loss 4.39520211 - samples/sec: 19.23 - lr: 0.050000
2021-05-17 20:00:38,943 epoch 17 - iter 66/110 - loss 4.37451629 - samples/sec: 18.05 - lr: 0.050000
2021-05-17 20:00:48,380 epoch 17 - iter 77/110 - loss 4.38405478 - samples/sec: 18.65 - lr: 0.050000
2021-05-17 20:00:57,296 epoch 17 - iter 88/110 - loss 4.36324025 - samples/sec: 19.74 - lr: 0.050000
2021-05-17 20:01:06,478 epoch 17 - iter 99/110 - loss 4.38273266 - samples/sec: 19.17 - lr: 0.050000
2021-05-17 20:01:15,838 epoch 17 - iter 110/110 - loss 4.38914707 - samples/sec: 18.81 - lr: 0.050000
2021-05-17 20:01:15,839 ----------------------------------------------------------------------------------------------------
2021-05-17 20:01:15,839 EPOCH 17 done: loss 4.3891 - lr 0.0500000
2021-05-17 20:01:21,402 DEV : loss 3.7558937072753906 - score 0.8282
2021-05-17 20:01:21,457 BAD EPOCHS (no improvement): 1
2021-05-17 20:01:21,457 ----------------------------------------------------------------------------------------------------
2021-05-17 20:01:31,027 epoch 18 - iter 11/110 - loss 4.73945215 - samples/sec: 18.39 - lr: 0.050000
2021-05-17 20:01:39,653 epoch 18 - iter 22/110 - loss 4.50301896 - samples/sec: 20.41 - lr: 0.050000
2021-05-17 20:01:48,968 epoch 18 - iter 33/110 - loss 4.45513676 - samples/sec: 18.90 - lr: 0.050000
2021-05-17 20:01:59,168 epoch 18 - iter 44/110 - loss 4.40752749 - samples/sec: 17.26 - lr: 0.050000
2021-05-17 20:02:06,963 epoch 18 - iter 55/110 - loss 4.34932505 - samples/sec: 22.58 - lr: 0.050000
2021-05-17 20:02:15,557 epoch 18 - iter 66/110 - loss 4.36970695 - samples/sec: 20.48 - lr: 0.050000
2021-05-17 20:02:24,305 epoch 18 - iter 77/110 - loss 4.38815255 - samples/sec: 20.12 - lr: 0.050000
2021-05-17 20:02:32,538 epoch 18 - iter 88/110 - loss 4.36936224 - samples/sec: 21.38 - lr: 0.050000
2021-05-17 20:02:41,293 epoch 18 - iter 99/110 - loss 4.35943255 - samples/sec: 20.11 - lr: 0.050000
2021-05-17 20:02:49,552 epoch 18 - iter 110/110 - loss 4.37263694 - samples/sec: 21.33 - lr: 0.050000
2021-05-17 20:02:49,553 ----------------------------------------------------------------------------------------------------
2021-05-17 20:02:49,553 EPOCH 18 done: loss 4.3726 - lr 0.0500000
2021-05-17 20:02:54,702 DEV : loss 3.466289758682251 - score 0.8438
2021-05-17 20:02:54,746 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:03:05,766 ----------------------------------------------------------------------------------------------------
2021-05-17 20:03:14,007 epoch 19 - iter 11/110 - loss 4.24615429 - samples/sec: 21.36 - lr: 0.050000
2021-05-17 20:03:22,514 epoch 19 - iter 22/110 - loss 4.26377558 - samples/sec: 20.69 - lr: 0.050000
2021-05-17 20:03:30,699 epoch 19 - iter 33/110 - loss 4.21565380 - samples/sec: 21.51 - lr: 0.050000
2021-05-17 20:03:38,118 epoch 19 - iter 44/110 - loss 4.23542667 - samples/sec: 23.73 - lr: 0.050000
2021-05-17 20:03:46,823 epoch 19 - iter 55/110 - loss 4.21035398 - samples/sec: 20.22 - lr: 0.050000
2021-05-17 20:03:55,545 epoch 19 - iter 66/110 - loss 4.21675656 - samples/sec: 20.18 - lr: 0.050000
2021-05-17 20:04:03,448 epoch 19 - iter 77/110 - loss 4.26158218 - samples/sec: 22.27 - lr: 0.050000
2021-05-17 20:04:11,735 epoch 19 - iter 88/110 - loss 4.29316818 - samples/sec: 21.24 - lr: 0.050000
2021-05-17 20:04:19,559 epoch 19 - iter 99/110 - loss 4.29236735 - samples/sec: 22.50 - lr: 0.050000
2021-05-17 20:04:28,099 epoch 19 - iter 110/110 - loss 4.32334340 - samples/sec: 20.61 - lr: 0.050000
2021-05-17 20:04:28,100 ----------------------------------------------------------------------------------------------------
2021-05-17 20:04:28,101 EPOCH 19 done: loss 4.3233 - lr 0.0500000
2021-05-17 20:04:33,449 DEV : loss 3.5290019512176514 - score 0.8362
2021-05-17 20:04:33,533 BAD EPOCHS (no improvement): 1
2021-05-17 20:04:33,536 ----------------------------------------------------------------------------------------------------
2021-05-17 20:04:41,490 epoch 20 - iter 11/110 - loss 4.31404096 - samples/sec: 22.13 - lr: 0.050000
2021-05-17 20:04:49,409 epoch 20 - iter 22/110 - loss 4.28470814 - samples/sec: 22.23 - lr: 0.050000
2021-05-17 20:04:58,326 epoch 20 - iter 33/110 - loss 4.33278882 - samples/sec: 19.74 - lr: 0.050000
2021-05-17 20:05:07,015 epoch 20 - iter 44/110 - loss 4.35769832 - samples/sec: 20.26 - lr: 0.050000
2021-05-17 20:05:16,090 epoch 20 - iter 55/110 - loss 4.42869445 - samples/sec: 19.40 - lr: 0.050000
2021-05-17 20:05:23,487 epoch 20 - iter 66/110 - loss 4.36513877 - samples/sec: 23.81 - lr: 0.050000
2021-05-17 20:05:31,023 epoch 20 - iter 77/110 - loss 4.34537645 - samples/sec: 23.36 - lr: 0.050000
2021-05-17 20:05:39,620 epoch 20 - iter 88/110 - loss 4.34568178 - samples/sec: 20.48 - lr: 0.050000
2021-05-17 20:05:49,571 epoch 20 - iter 99/110 - loss 4.32569669 - samples/sec: 17.70 - lr: 0.050000
2021-05-17 20:05:57,318 epoch 20 - iter 110/110 - loss 4.31499468 - samples/sec: 22.72 - lr: 0.050000
2021-05-17 20:05:57,318 ----------------------------------------------------------------------------------------------------
2021-05-17 20:05:57,318 EPOCH 20 done: loss 4.3150 - lr 0.0500000
2021-05-17 20:06:02,690 DEV : loss 3.539405584335327 - score 0.8322
2021-05-17 20:06:02,737 BAD EPOCHS (no improvement): 2
2021-05-17 20:06:02,737 ----------------------------------------------------------------------------------------------------
2021-05-17 20:06:11,680 epoch 21 - iter 11/110 - loss 4.17676028 - samples/sec: 19.68 - lr: 0.050000
2021-05-17 20:06:21,130 epoch 21 - iter 22/110 - loss 4.27091032 - samples/sec: 18.63 - lr: 0.050000
2021-05-17 20:06:29,279 epoch 21 - iter 33/110 - loss 4.30611552 - samples/sec: 21.60 - lr: 0.050000
2021-05-17 20:06:37,293 epoch 21 - iter 44/110 - loss 4.31184676 - samples/sec: 21.96 - lr: 0.050000
2021-05-17 20:06:46,321 epoch 21 - iter 55/110 - loss 4.30570073 - samples/sec: 19.50 - lr: 0.050000
2021-05-17 20:06:55,045 epoch 21 - iter 66/110 - loss 4.29417170 - samples/sec: 20.18 - lr: 0.050000
2021-05-17 20:07:03,174 epoch 21 - iter 77/110 - loss 4.27589579 - samples/sec: 21.66 - lr: 0.050000
2021-05-17 20:07:12,373 epoch 21 - iter 88/110 - loss 4.27468021 - samples/sec: 19.13 - lr: 0.050000
2021-05-17 20:07:20,440 epoch 21 - iter 99/110 - loss 4.28582189 - samples/sec: 21.82 - lr: 0.050000
2021-05-17 20:07:28,350 epoch 21 - iter 110/110 - loss 4.30688997 - samples/sec: 22.25 - lr: 0.050000
2021-05-17 20:07:28,351 ----------------------------------------------------------------------------------------------------
2021-05-17 20:07:28,352 EPOCH 21 done: loss 4.3069 - lr 0.0500000
2021-05-17 20:07:33,029 DEV : loss 3.6754448413848877 - score 0.8322
2021-05-17 20:07:33,074 BAD EPOCHS (no improvement): 3
2021-05-17 20:07:33,074 ----------------------------------------------------------------------------------------------------
2021-05-17 20:07:41,220 epoch 22 - iter 11/110 - loss 4.48869449 - samples/sec: 21.61 - lr: 0.050000
2021-05-17 20:07:49,615 epoch 22 - iter 22/110 - loss 4.43799051 - samples/sec: 20.97 - lr: 0.050000
2021-05-17 20:07:58,222 epoch 22 - iter 33/110 - loss 4.34096904 - samples/sec: 20.45 - lr: 0.050000
2021-05-17 20:08:07,155 epoch 22 - iter 44/110 - loss 4.46816491 - samples/sec: 19.71 - lr: 0.050000
2021-05-17 20:08:15,473 epoch 22 - iter 55/110 - loss 4.45681738 - samples/sec: 21.16 - lr: 0.050000
2021-05-17 20:08:25,519 epoch 22 - iter 66/110 - loss 4.43062722 - samples/sec: 17.52 - lr: 0.050000
2021-05-17 20:08:34,798 epoch 22 - iter 77/110 - loss 4.32996615 - samples/sec: 18.97 - lr: 0.050000
2021-05-17 20:08:42,429 epoch 22 - iter 88/110 - loss 4.30581980 - samples/sec: 23.07 - lr: 0.050000
2021-05-17 20:08:52,666 epoch 22 - iter 99/110 - loss 4.29181948 - samples/sec: 17.20 - lr: 0.050000
2021-05-17 20:09:00,516 epoch 22 - iter 110/110 - loss 4.29896040 - samples/sec: 22.42 - lr: 0.050000
2021-05-17 20:09:00,517 ----------------------------------------------------------------------------------------------------
2021-05-17 20:09:00,517 EPOCH 22 done: loss 4.2990 - lr 0.0500000
2021-05-17 20:09:05,076 DEV : loss 3.603147029876709 - score 0.8319
Epoch    22: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 20:09:05,131 BAD EPOCHS (no improvement): 4
2021-05-17 20:09:05,131 ----------------------------------------------------------------------------------------------------
2021-05-17 20:09:14,679 epoch 23 - iter 11/110 - loss 4.01809177 - samples/sec: 18.46 - lr: 0.025000
2021-05-17 20:09:23,807 epoch 23 - iter 22/110 - loss 3.91145407 - samples/sec: 19.29 - lr: 0.025000
2021-05-17 20:09:34,074 epoch 23 - iter 33/110 - loss 4.05939114 - samples/sec: 17.14 - lr: 0.025000
2021-05-17 20:09:42,794 epoch 23 - iter 44/110 - loss 4.13378803 - samples/sec: 20.19 - lr: 0.025000
2021-05-17 20:09:51,905 epoch 23 - iter 55/110 - loss 4.13158935 - samples/sec: 19.32 - lr: 0.025000
2021-05-17 20:10:00,294 epoch 23 - iter 66/110 - loss 4.13521298 - samples/sec: 21.00 - lr: 0.025000
2021-05-17 20:10:08,687 epoch 23 - iter 77/110 - loss 4.14608609 - samples/sec: 20.98 - lr: 0.025000
2021-05-17 20:10:18,653 epoch 23 - iter 88/110 - loss 4.16058319 - samples/sec: 17.66 - lr: 0.025000
2021-05-17 20:10:27,002 epoch 23 - iter 99/110 - loss 4.16263724 - samples/sec: 21.08 - lr: 0.025000
2021-05-17 20:10:36,874 epoch 23 - iter 110/110 - loss 4.15129394 - samples/sec: 17.83 - lr: 0.025000
2021-05-17 20:10:36,875 ----------------------------------------------------------------------------------------------------
2021-05-17 20:10:36,876 EPOCH 23 done: loss 4.1513 - lr 0.0250000
2021-05-17 20:10:40,913 DEV : loss 3.5215299129486084 - score 0.8373
2021-05-17 20:10:40,957 BAD EPOCHS (no improvement): 1
2021-05-17 20:10:40,957 ----------------------------------------------------------------------------------------------------
2021-05-17 20:10:48,879 epoch 24 - iter 11/110 - loss 4.26632131 - samples/sec: 22.22 - lr: 0.025000
2021-05-17 20:10:58,561 epoch 24 - iter 22/110 - loss 4.20073041 - samples/sec: 18.19 - lr: 0.025000
2021-05-17 20:11:06,552 epoch 24 - iter 33/110 - loss 4.09448453 - samples/sec: 22.03 - lr: 0.025000
2021-05-17 20:11:15,290 epoch 24 - iter 44/110 - loss 4.09575267 - samples/sec: 20.15 - lr: 0.025000
2021-05-17 20:11:24,082 epoch 24 - iter 55/110 - loss 4.11165228 - samples/sec: 20.03 - lr: 0.025000
2021-05-17 20:11:32,451 epoch 24 - iter 66/110 - loss 4.11686482 - samples/sec: 21.03 - lr: 0.025000
2021-05-17 20:11:40,375 epoch 24 - iter 77/110 - loss 4.12922391 - samples/sec: 22.21 - lr: 0.025000
2021-05-17 20:11:48,528 epoch 24 - iter 88/110 - loss 4.14234072 - samples/sec: 21.59 - lr: 0.025000
2021-05-17 20:11:57,239 epoch 24 - iter 99/110 - loss 4.14749741 - samples/sec: 20.21 - lr: 0.025000
2021-05-17 20:12:04,859 epoch 24 - iter 110/110 - loss 4.13159861 - samples/sec: 23.11 - lr: 0.025000
2021-05-17 20:12:04,860 ----------------------------------------------------------------------------------------------------
2021-05-17 20:12:04,860 EPOCH 24 done: loss 4.1316 - lr 0.0250000
2021-05-17 20:12:10,155 DEV : loss 3.393495798110962 - score 0.8424
2021-05-17 20:12:10,201 BAD EPOCHS (no improvement): 2
2021-05-17 20:12:10,201 ----------------------------------------------------------------------------------------------------
2021-05-17 20:12:18,769 epoch 25 - iter 11/110 - loss 4.02533913 - samples/sec: 20.54 - lr: 0.025000
2021-05-17 20:12:28,352 epoch 25 - iter 22/110 - loss 3.99464978 - samples/sec: 18.37 - lr: 0.025000
2021-05-17 20:12:37,220 epoch 25 - iter 33/110 - loss 4.02458374 - samples/sec: 19.85 - lr: 0.025000
2021-05-17 20:12:46,369 epoch 25 - iter 44/110 - loss 4.04150809 - samples/sec: 19.24 - lr: 0.025000
2021-05-17 20:12:55,591 epoch 25 - iter 55/110 - loss 4.08296080 - samples/sec: 19.09 - lr: 0.025000
2021-05-17 20:13:04,302 epoch 25 - iter 66/110 - loss 4.09399979 - samples/sec: 20.21 - lr: 0.025000
2021-05-17 20:13:12,686 epoch 25 - iter 77/110 - loss 4.11571194 - samples/sec: 20.99 - lr: 0.025000
2021-05-17 20:13:20,290 epoch 25 - iter 88/110 - loss 4.10738381 - samples/sec: 23.15 - lr: 0.025000
2021-05-17 20:13:29,861 epoch 25 - iter 99/110 - loss 4.08954202 - samples/sec: 18.39 - lr: 0.025000
2021-05-17 20:13:37,040 epoch 25 - iter 110/110 - loss 4.12351193 - samples/sec: 24.52 - lr: 0.025000
2021-05-17 20:13:37,040 ----------------------------------------------------------------------------------------------------
2021-05-17 20:13:37,040 EPOCH 25 done: loss 4.1235 - lr 0.0250000
2021-05-17 20:13:41,310 DEV : loss 3.686960220336914 - score 0.8346
2021-05-17 20:13:41,354 BAD EPOCHS (no improvement): 3
2021-05-17 20:13:41,354 ----------------------------------------------------------------------------------------------------
2021-05-17 20:13:48,888 epoch 26 - iter 11/110 - loss 4.41430057 - samples/sec: 23.37 - lr: 0.025000
2021-05-17 20:13:56,357 epoch 26 - iter 22/110 - loss 4.22104077 - samples/sec: 23.57 - lr: 0.025000
2021-05-17 20:14:04,635 epoch 26 - iter 33/110 - loss 4.10231324 - samples/sec: 21.26 - lr: 0.025000
2021-05-17 20:14:14,648 epoch 26 - iter 44/110 - loss 4.08301457 - samples/sec: 17.58 - lr: 0.025000
2021-05-17 20:14:23,507 epoch 26 - iter 55/110 - loss 4.02672299 - samples/sec: 19.87 - lr: 0.025000
2021-05-17 20:14:32,955 epoch 26 - iter 66/110 - loss 4.04893849 - samples/sec: 18.63 - lr: 0.025000
2021-05-17 20:14:42,101 epoch 26 - iter 77/110 - loss 4.09496660 - samples/sec: 19.25 - lr: 0.025000
2021-05-17 20:14:50,943 epoch 26 - iter 88/110 - loss 4.08506770 - samples/sec: 19.92 - lr: 0.025000
2021-05-17 20:14:59,440 epoch 26 - iter 99/110 - loss 4.07214385 - samples/sec: 20.73 - lr: 0.025000
2021-05-17 20:15:07,963 epoch 26 - iter 110/110 - loss 4.09180567 - samples/sec: 20.65 - lr: 0.025000
2021-05-17 20:15:07,964 ----------------------------------------------------------------------------------------------------
2021-05-17 20:15:07,964 EPOCH 26 done: loss 4.0918 - lr 0.0250000
2021-05-17 20:15:12,324 DEV : loss 3.4876136779785156 - score 0.8367
Epoch    26: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 20:15:12,370 BAD EPOCHS (no improvement): 4
2021-05-17 20:15:12,371 ----------------------------------------------------------------------------------------------------
2021-05-17 20:15:22,135 epoch 27 - iter 11/110 - loss 4.27370327 - samples/sec: 18.03 - lr: 0.012500
2021-05-17 20:15:30,751 epoch 27 - iter 22/110 - loss 4.13820256 - samples/sec: 20.43 - lr: 0.012500
2021-05-17 20:15:38,894 epoch 27 - iter 33/110 - loss 4.11647044 - samples/sec: 21.62 - lr: 0.012500
2021-05-17 20:15:47,661 epoch 27 - iter 44/110 - loss 4.11538024 - samples/sec: 20.08 - lr: 0.012500
2021-05-17 20:15:58,030 epoch 27 - iter 55/110 - loss 4.07999170 - samples/sec: 16.98 - lr: 0.012500
2021-05-17 20:16:08,907 epoch 27 - iter 66/110 - loss 4.08502261 - samples/sec: 16.19 - lr: 0.012500
2021-05-17 20:16:17,771 epoch 27 - iter 77/110 - loss 4.02031244 - samples/sec: 19.86 - lr: 0.012500
2021-05-17 20:16:26,854 epoch 27 - iter 88/110 - loss 4.03397618 - samples/sec: 19.38 - lr: 0.012500
2021-05-17 20:16:36,204 epoch 27 - iter 99/110 - loss 4.03895124 - samples/sec: 18.83 - lr: 0.012500
2021-05-17 20:16:45,257 epoch 27 - iter 110/110 - loss 4.04630112 - samples/sec: 19.44 - lr: 0.012500
2021-05-17 20:16:45,257 ----------------------------------------------------------------------------------------------------
2021-05-17 20:16:45,258 EPOCH 27 done: loss 4.0463 - lr 0.0125000
2021-05-17 20:16:50,123 DEV : loss 3.44303035736084 - score 0.8414
2021-05-17 20:16:50,168 BAD EPOCHS (no improvement): 1
2021-05-17 20:16:50,168 ----------------------------------------------------------------------------------------------------
2021-05-17 20:16:59,002 epoch 28 - iter 11/110 - loss 4.04018675 - samples/sec: 19.93 - lr: 0.012500
2021-05-17 20:17:07,026 epoch 28 - iter 22/110 - loss 4.17921621 - samples/sec: 21.94 - lr: 0.012500
2021-05-17 20:17:15,444 epoch 28 - iter 33/110 - loss 4.09481141 - samples/sec: 20.91 - lr: 0.012500
2021-05-17 20:17:25,099 epoch 28 - iter 44/110 - loss 4.19309832 - samples/sec: 18.23 - lr: 0.012500
2021-05-17 20:17:34,774 epoch 28 - iter 55/110 - loss 4.14040483 - samples/sec: 18.19 - lr: 0.012500
2021-05-17 20:17:42,215 epoch 28 - iter 66/110 - loss 4.13624605 - samples/sec: 23.66 - lr: 0.012500
2021-05-17 20:17:51,724 epoch 28 - iter 77/110 - loss 4.10836619 - samples/sec: 18.51 - lr: 0.012500
2021-05-17 20:17:59,061 epoch 28 - iter 88/110 - loss 4.05499669 - samples/sec: 23.99 - lr: 0.012500
2021-05-17 20:18:06,513 epoch 28 - iter 99/110 - loss 4.03985944 - samples/sec: 23.62 - lr: 0.012500
2021-05-17 20:18:16,326 epoch 28 - iter 110/110 - loss 3.99847491 - samples/sec: 17.94 - lr: 0.012500
2021-05-17 20:18:16,327 ----------------------------------------------------------------------------------------------------
2021-05-17 20:18:16,327 EPOCH 28 done: loss 3.9985 - lr 0.0125000
2021-05-17 20:18:22,154 DEV : loss 3.346100091934204 - score 0.8425
2021-05-17 20:18:22,199 BAD EPOCHS (no improvement): 2
2021-05-17 20:18:22,200 ----------------------------------------------------------------------------------------------------
2021-05-17 20:18:31,690 epoch 29 - iter 11/110 - loss 3.96878615 - samples/sec: 18.55 - lr: 0.012500
2021-05-17 20:18:40,923 epoch 29 - iter 22/110 - loss 4.10795250 - samples/sec: 19.06 - lr: 0.012500
2021-05-17 20:18:49,791 epoch 29 - iter 33/110 - loss 3.97491860 - samples/sec: 19.85 - lr: 0.012500
2021-05-17 20:18:57,508 epoch 29 - iter 44/110 - loss 3.96344393 - samples/sec: 22.81 - lr: 0.012500
2021-05-17 20:19:04,906 epoch 29 - iter 55/110 - loss 3.95897697 - samples/sec: 23.79 - lr: 0.012500
2021-05-17 20:19:12,873 epoch 29 - iter 66/110 - loss 3.98645815 - samples/sec: 22.10 - lr: 0.012500
2021-05-17 20:19:21,991 epoch 29 - iter 77/110 - loss 3.94261115 - samples/sec: 19.31 - lr: 0.012500
2021-05-17 20:19:31,052 epoch 29 - iter 88/110 - loss 3.93316282 - samples/sec: 19.43 - lr: 0.012500
2021-05-17 20:19:39,112 epoch 29 - iter 99/110 - loss 3.94771096 - samples/sec: 21.84 - lr: 0.012500
2021-05-17 20:19:47,660 epoch 29 - iter 110/110 - loss 3.91674373 - samples/sec: 20.59 - lr: 0.012500
2021-05-17 20:19:47,661 ----------------------------------------------------------------------------------------------------
2021-05-17 20:19:47,661 EPOCH 29 done: loss 3.9167 - lr 0.0125000
2021-05-17 20:19:52,185 DEV : loss 3.39617919921875 - score 0.8385
2021-05-17 20:19:52,230 BAD EPOCHS (no improvement): 3
2021-05-17 20:19:52,231 ----------------------------------------------------------------------------------------------------
2021-05-17 20:20:01,328 epoch 30 - iter 11/110 - loss 4.02702581 - samples/sec: 19.35 - lr: 0.012500
2021-05-17 20:20:09,534 epoch 30 - iter 22/110 - loss 4.17522700 - samples/sec: 21.45 - lr: 0.012500
2021-05-17 20:20:18,669 epoch 30 - iter 33/110 - loss 4.16299695 - samples/sec: 19.27 - lr: 0.012500
2021-05-17 20:20:27,203 epoch 30 - iter 44/110 - loss 4.12425046 - samples/sec: 20.63 - lr: 0.012500
2021-05-17 20:20:35,069 epoch 30 - iter 55/110 - loss 4.12440627 - samples/sec: 22.38 - lr: 0.012500
2021-05-17 20:20:43,838 epoch 30 - iter 66/110 - loss 4.06422240 - samples/sec: 20.07 - lr: 0.012500
2021-05-17 20:20:53,352 epoch 30 - iter 77/110 - loss 4.02652436 - samples/sec: 18.50 - lr: 0.012500
2021-05-17 20:21:02,973 epoch 30 - iter 88/110 - loss 4.00169692 - samples/sec: 18.31 - lr: 0.012500
2021-05-17 20:21:12,312 epoch 30 - iter 99/110 - loss 4.01783741 - samples/sec: 18.85 - lr: 0.012500
2021-05-17 20:21:21,590 epoch 30 - iter 110/110 - loss 4.01731455 - samples/sec: 18.97 - lr: 0.012500
2021-05-17 20:21:21,590 ----------------------------------------------------------------------------------------------------
2021-05-17 20:21:21,590 EPOCH 30 done: loss 4.0173 - lr 0.0125000
2021-05-17 20:21:26,171 DEV : loss 3.4351212978363037 - score 0.8416
Epoch    30: reducing learning rate of group 0 to 6.2500e-03.
2021-05-17 20:21:26,215 BAD EPOCHS (no improvement): 4
2021-05-17 20:21:37,436 ----------------------------------------------------------------------------------------------------
2021-05-17 20:21:37,437 Testing using best model ...
2021-05-17 20:21:37,437 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.sdrt.stac/best-model.pt
2021-05-17 20:22:01,652 0.8410	0.7956	0.8177
2021-05-17 20:22:01,652 
Results:
- F1-score (micro) 0.8177
- F1-score (macro) 0.8177

By class:
SENT       tp: 1074 - fp: 203 - fn: 276 - precision: 0.8410 - recall: 0.7956 - f1-score: 0.8177
2021-05-17 20:22:01,653 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb/
2021-05-17 20:22:01,691 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb
2021-05-17 20:22:01,692 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb/sent_train.txt
2021-05-17 20:22:01,695 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb/sent_dev.txt
2021-05-17 20:22:01,695 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb/sent_test.txt
Corpus: 501 train + 119 dev + 186 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 20:22:17,028 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,036 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 20:22:17,037 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,037 Corpus: "Corpus: 501 train + 119 dev + 186 test sentences"
2021-05-17 20:22:17,037 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,037 Parameters:
2021-05-17 20:22:17,037  - learning_rate: "0.1"
2021-05-17 20:22:17,037  - mini_batch_size: "16"
2021-05-17 20:22:17,037  - patience: "3"
2021-05-17 20:22:17,037  - anneal_factor: "0.5"
2021-05-17 20:22:17,037  - max_epochs: "30"
2021-05-17 20:22:17,037  - shuffle: "True"
2021-05-17 20:22:17,037  - train_with_dev: "False"
2021-05-17 20:22:17,037  - batch_growth_annealing: "False"
2021-05-17 20:22:17,038 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,038 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb"
2021-05-17 20:22:17,038 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,038 Device: cuda:0
2021-05-17 20:22:17,038 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:17,038 Embeddings storage mode: cpu
2021-05-17 20:22:17,041 ----------------------------------------------------------------------------------------------------
2021-05-17 20:22:22,026 epoch 1 - iter 3/32 - loss 5.08072678 - samples/sec: 9.63 - lr: 0.100000
2021-05-17 20:22:27,737 epoch 1 - iter 6/32 - loss 4.38630418 - samples/sec: 8.41 - lr: 0.100000
2021-05-17 20:22:32,815 epoch 1 - iter 9/32 - loss 4.02870459 - samples/sec: 9.45 - lr: 0.100000
2021-05-17 20:22:39,167 epoch 1 - iter 12/32 - loss 3.99877761 - samples/sec: 7.56 - lr: 0.100000
2021-05-17 20:22:45,187 epoch 1 - iter 15/32 - loss 3.83036551 - samples/sec: 7.97 - lr: 0.100000
2021-05-17 20:22:51,071 epoch 1 - iter 18/32 - loss 3.84035790 - samples/sec: 8.16 - lr: 0.100000
2021-05-17 20:22:57,704 epoch 1 - iter 21/32 - loss 3.81090652 - samples/sec: 7.24 - lr: 0.100000
2021-05-17 20:23:02,529 epoch 1 - iter 24/32 - loss 3.65601968 - samples/sec: 9.95 - lr: 0.100000
2021-05-17 20:23:08,417 epoch 1 - iter 27/32 - loss 3.51554329 - samples/sec: 8.15 - lr: 0.100000
2021-05-17 20:23:13,853 epoch 1 - iter 30/32 - loss 3.37832136 - samples/sec: 8.86 - lr: 0.100000
2021-05-17 20:23:16,549 ----------------------------------------------------------------------------------------------------
2021-05-17 20:23:16,549 EPOCH 1 done: loss 3.2990 - lr 0.1000000
2021-05-17 20:23:25,080 DEV : loss 1.7288858890533447 - score 0.0
2021-05-17 20:23:25,103 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:23:36,164 ----------------------------------------------------------------------------------------------------
2021-05-17 20:23:38,661 epoch 2 - iter 3/32 - loss 1.87298886 - samples/sec: 19.24 - lr: 0.100000
2021-05-17 20:23:41,462 epoch 2 - iter 6/32 - loss 2.68536294 - samples/sec: 17.14 - lr: 0.100000
2021-05-17 20:23:43,804 epoch 2 - iter 9/32 - loss 2.63677035 - samples/sec: 20.51 - lr: 0.100000
2021-05-17 20:23:46,415 epoch 2 - iter 12/32 - loss 2.55234478 - samples/sec: 18.39 - lr: 0.100000
2021-05-17 20:23:49,058 epoch 2 - iter 15/32 - loss 2.41549797 - samples/sec: 18.17 - lr: 0.100000
2021-05-17 20:23:51,061 epoch 2 - iter 18/32 - loss 2.50643174 - samples/sec: 23.97 - lr: 0.100000
2021-05-17 20:23:53,262 epoch 2 - iter 21/32 - loss 2.52585485 - samples/sec: 21.82 - lr: 0.100000
2021-05-17 20:23:55,782 epoch 2 - iter 24/32 - loss 2.39040583 - samples/sec: 19.05 - lr: 0.100000
2021-05-17 20:23:58,402 epoch 2 - iter 27/32 - loss 2.32446202 - samples/sec: 18.33 - lr: 0.100000
2021-05-17 20:24:00,703 epoch 2 - iter 30/32 - loss 2.22603636 - samples/sec: 20.87 - lr: 0.100000
2021-05-17 20:24:01,886 ----------------------------------------------------------------------------------------------------
2021-05-17 20:24:01,886 EPOCH 2 done: loss 2.2009 - lr 0.1000000
2021-05-17 20:24:04,518 DEV : loss 2.160759210586548 - score 0.0
2021-05-17 20:24:04,552 BAD EPOCHS (no improvement): 1
2021-05-17 20:24:04,552 ----------------------------------------------------------------------------------------------------
2021-05-17 20:24:06,774 epoch 3 - iter 3/32 - loss 2.21065505 - samples/sec: 21.62 - lr: 0.100000
2021-05-17 20:24:09,239 epoch 3 - iter 6/32 - loss 1.94410928 - samples/sec: 19.48 - lr: 0.100000
2021-05-17 20:24:11,559 epoch 3 - iter 9/32 - loss 1.82083959 - samples/sec: 20.70 - lr: 0.100000
2021-05-17 20:24:13,526 epoch 3 - iter 12/32 - loss 1.71783508 - samples/sec: 24.41 - lr: 0.100000
2021-05-17 20:24:15,823 epoch 3 - iter 15/32 - loss 1.68075429 - samples/sec: 20.91 - lr: 0.100000
2021-05-17 20:24:17,795 epoch 3 - iter 18/32 - loss 1.60697816 - samples/sec: 24.35 - lr: 0.100000
2021-05-17 20:24:19,776 epoch 3 - iter 21/32 - loss 1.52149502 - samples/sec: 24.23 - lr: 0.100000
2021-05-17 20:24:22,499 epoch 3 - iter 24/32 - loss 1.53579673 - samples/sec: 17.63 - lr: 0.100000
2021-05-17 20:24:24,746 epoch 3 - iter 27/32 - loss 1.52270460 - samples/sec: 21.37 - lr: 0.100000
2021-05-17 20:24:27,219 epoch 3 - iter 30/32 - loss 1.54094628 - samples/sec: 19.42 - lr: 0.100000
2021-05-17 20:24:28,347 ----------------------------------------------------------------------------------------------------
2021-05-17 20:24:28,348 EPOCH 3 done: loss 1.4826 - lr 0.1000000
2021-05-17 20:24:30,562 DEV : loss 0.8238828182220459 - score 0.6364
2021-05-17 20:24:30,585 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:24:41,362 ----------------------------------------------------------------------------------------------------
2021-05-17 20:24:43,789 epoch 4 - iter 3/32 - loss 1.25491683 - samples/sec: 19.79 - lr: 0.100000
2021-05-17 20:24:46,495 epoch 4 - iter 6/32 - loss 1.03303162 - samples/sec: 17.74 - lr: 0.100000
2021-05-17 20:24:48,833 epoch 4 - iter 9/32 - loss 1.09880768 - samples/sec: 20.56 - lr: 0.100000
2021-05-17 20:24:51,636 epoch 4 - iter 12/32 - loss 1.12069714 - samples/sec: 17.13 - lr: 0.100000
2021-05-17 20:24:53,997 epoch 4 - iter 15/32 - loss 1.10178676 - samples/sec: 20.34 - lr: 0.100000
2021-05-17 20:24:56,049 epoch 4 - iter 18/32 - loss 1.05801262 - samples/sec: 23.40 - lr: 0.100000
2021-05-17 20:24:58,270 epoch 4 - iter 21/32 - loss 1.01612989 - samples/sec: 21.61 - lr: 0.100000
2021-05-17 20:25:00,267 epoch 4 - iter 24/32 - loss 0.99732129 - samples/sec: 24.04 - lr: 0.100000
2021-05-17 20:25:02,758 epoch 4 - iter 27/32 - loss 1.04202705 - samples/sec: 19.29 - lr: 0.100000
2021-05-17 20:25:05,482 epoch 4 - iter 30/32 - loss 1.09519414 - samples/sec: 17.63 - lr: 0.100000
2021-05-17 20:25:06,741 ----------------------------------------------------------------------------------------------------
2021-05-17 20:25:06,742 EPOCH 4 done: loss 1.0832 - lr 0.1000000
2021-05-17 20:25:09,882 DEV : loss 0.4585699737071991 - score 0.8467
2021-05-17 20:25:09,906 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:25:20,941 ----------------------------------------------------------------------------------------------------
2021-05-17 20:25:23,226 epoch 5 - iter 3/32 - loss 0.87092036 - samples/sec: 21.02 - lr: 0.100000
2021-05-17 20:25:26,073 epoch 5 - iter 6/32 - loss 1.12469336 - samples/sec: 16.87 - lr: 0.100000
2021-05-17 20:25:28,964 epoch 5 - iter 9/32 - loss 1.02518821 - samples/sec: 16.61 - lr: 0.100000
2021-05-17 20:25:31,416 epoch 5 - iter 12/32 - loss 0.92602091 - samples/sec: 19.59 - lr: 0.100000
2021-05-17 20:25:33,860 epoch 5 - iter 15/32 - loss 0.95263589 - samples/sec: 19.64 - lr: 0.100000
2021-05-17 20:25:36,902 epoch 5 - iter 18/32 - loss 0.97932747 - samples/sec: 15.84 - lr: 0.100000
2021-05-17 20:25:39,166 epoch 5 - iter 21/32 - loss 0.96609832 - samples/sec: 21.21 - lr: 0.100000
2021-05-17 20:25:41,490 epoch 5 - iter 24/32 - loss 0.93799460 - samples/sec: 20.65 - lr: 0.100000
2021-05-17 20:25:44,123 epoch 5 - iter 27/32 - loss 0.91823899 - samples/sec: 18.26 - lr: 0.100000
2021-05-17 20:25:46,658 epoch 5 - iter 30/32 - loss 0.91417836 - samples/sec: 18.97 - lr: 0.100000
2021-05-17 20:25:47,638 ----------------------------------------------------------------------------------------------------
2021-05-17 20:25:47,638 EPOCH 5 done: loss 0.9078 - lr 0.1000000
2021-05-17 20:25:50,048 DEV : loss 0.9029998183250427 - score 0.8156
2021-05-17 20:25:50,071 BAD EPOCHS (no improvement): 1
2021-05-17 20:25:50,071 ----------------------------------------------------------------------------------------------------
2021-05-17 20:25:51,956 epoch 6 - iter 3/32 - loss 1.28118106 - samples/sec: 25.47 - lr: 0.100000
2021-05-17 20:25:53,977 epoch 6 - iter 6/32 - loss 1.14660014 - samples/sec: 23.77 - lr: 0.100000
2021-05-17 20:25:55,912 epoch 6 - iter 9/32 - loss 1.16998138 - samples/sec: 24.83 - lr: 0.100000
2021-05-17 20:25:57,997 epoch 6 - iter 12/32 - loss 1.08917590 - samples/sec: 23.14 - lr: 0.100000
2021-05-17 20:26:00,005 epoch 6 - iter 15/32 - loss 1.11717801 - samples/sec: 23.92 - lr: 0.100000
2021-05-17 20:26:02,649 epoch 6 - iter 18/32 - loss 1.09526032 - samples/sec: 18.16 - lr: 0.100000
2021-05-17 20:26:05,322 epoch 6 - iter 21/32 - loss 1.06686253 - samples/sec: 17.96 - lr: 0.100000
2021-05-17 20:26:08,435 epoch 6 - iter 24/32 - loss 1.00306486 - samples/sec: 15.43 - lr: 0.100000
2021-05-17 20:26:10,604 epoch 6 - iter 27/32 - loss 1.03255368 - samples/sec: 22.15 - lr: 0.100000
2021-05-17 20:26:12,956 epoch 6 - iter 30/32 - loss 0.97929339 - samples/sec: 20.42 - lr: 0.100000
2021-05-17 20:26:14,026 ----------------------------------------------------------------------------------------------------
2021-05-17 20:26:14,026 EPOCH 6 done: loss 0.9458 - lr 0.1000000
2021-05-17 20:26:16,294 DEV : loss 0.4898853003978729 - score 0.8333
2021-05-17 20:26:16,317 BAD EPOCHS (no improvement): 2
2021-05-17 20:26:16,318 ----------------------------------------------------------------------------------------------------
2021-05-17 20:26:19,155 epoch 7 - iter 3/32 - loss 1.33960533 - samples/sec: 16.92 - lr: 0.100000
2021-05-17 20:26:22,068 epoch 7 - iter 6/32 - loss 0.95663357 - samples/sec: 16.48 - lr: 0.100000
2021-05-17 20:26:24,806 epoch 7 - iter 9/32 - loss 1.13263920 - samples/sec: 17.54 - lr: 0.100000
2021-05-17 20:26:27,420 epoch 7 - iter 12/32 - loss 1.13090454 - samples/sec: 18.45 - lr: 0.100000
2021-05-17 20:26:29,686 epoch 7 - iter 15/32 - loss 1.06589141 - samples/sec: 21.19 - lr: 0.100000
2021-05-17 20:26:32,333 epoch 7 - iter 18/32 - loss 1.02144892 - samples/sec: 18.14 - lr: 0.100000
2021-05-17 20:26:35,198 epoch 7 - iter 21/32 - loss 1.00844719 - samples/sec: 16.76 - lr: 0.100000
2021-05-17 20:26:37,791 epoch 7 - iter 24/32 - loss 1.03763188 - samples/sec: 18.52 - lr: 0.100000
2021-05-17 20:26:40,542 epoch 7 - iter 27/32 - loss 0.99993673 - samples/sec: 17.46 - lr: 0.100000
2021-05-17 20:26:42,592 epoch 7 - iter 30/32 - loss 0.97179502 - samples/sec: 23.42 - lr: 0.100000
2021-05-17 20:26:43,510 ----------------------------------------------------------------------------------------------------
2021-05-17 20:26:43,511 EPOCH 7 done: loss 0.9411 - lr 0.1000000
2021-05-17 20:26:45,778 DEV : loss 0.43905797600746155 - score 0.8613
2021-05-17 20:26:45,801 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:26:56,637 ----------------------------------------------------------------------------------------------------
2021-05-17 20:26:58,903 epoch 8 - iter 3/32 - loss 1.34192785 - samples/sec: 21.20 - lr: 0.100000
2021-05-17 20:27:01,035 epoch 8 - iter 6/32 - loss 1.02827533 - samples/sec: 22.53 - lr: 0.100000
2021-05-17 20:27:03,555 epoch 8 - iter 9/32 - loss 1.13600747 - samples/sec: 19.05 - lr: 0.100000
2021-05-17 20:27:05,870 epoch 8 - iter 12/32 - loss 1.10540420 - samples/sec: 20.76 - lr: 0.100000
2021-05-17 20:27:08,182 epoch 8 - iter 15/32 - loss 1.02747706 - samples/sec: 20.77 - lr: 0.100000
2021-05-17 20:27:10,412 epoch 8 - iter 18/32 - loss 1.00268182 - samples/sec: 21.53 - lr: 0.100000
2021-05-17 20:27:12,743 epoch 8 - iter 21/32 - loss 0.94509319 - samples/sec: 20.61 - lr: 0.100000
2021-05-17 20:27:14,631 epoch 8 - iter 24/32 - loss 0.96148173 - samples/sec: 25.44 - lr: 0.100000
2021-05-17 20:27:17,323 epoch 8 - iter 27/32 - loss 0.99439827 - samples/sec: 17.83 - lr: 0.100000
2021-05-17 20:27:19,971 epoch 8 - iter 30/32 - loss 0.95290537 - samples/sec: 18.13 - lr: 0.100000
2021-05-17 20:27:21,307 ----------------------------------------------------------------------------------------------------
2021-05-17 20:27:21,308 EPOCH 8 done: loss 0.9631 - lr 0.1000000
2021-05-17 20:27:24,030 DEV : loss 0.3687981367111206 - score 0.8889
2021-05-17 20:27:24,054 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:27:34,869 ----------------------------------------------------------------------------------------------------
2021-05-17 20:27:37,273 epoch 9 - iter 3/32 - loss 0.66598829 - samples/sec: 19.98 - lr: 0.100000
2021-05-17 20:27:39,765 epoch 9 - iter 6/32 - loss 0.59555729 - samples/sec: 19.27 - lr: 0.100000
2021-05-17 20:27:41,956 epoch 9 - iter 9/32 - loss 0.62395059 - samples/sec: 21.91 - lr: 0.100000
2021-05-17 20:27:44,297 epoch 9 - iter 12/32 - loss 0.71674128 - samples/sec: 20.52 - lr: 0.100000
2021-05-17 20:27:46,741 epoch 9 - iter 15/32 - loss 0.69194329 - samples/sec: 19.65 - lr: 0.100000
2021-05-17 20:27:48,840 epoch 9 - iter 18/32 - loss 0.76274851 - samples/sec: 22.88 - lr: 0.100000
2021-05-17 20:27:50,947 epoch 9 - iter 21/32 - loss 0.75692969 - samples/sec: 22.79 - lr: 0.100000
2021-05-17 20:27:53,171 epoch 9 - iter 24/32 - loss 0.72759974 - samples/sec: 21.59 - lr: 0.100000
2021-05-17 20:27:55,223 epoch 9 - iter 27/32 - loss 0.75761025 - samples/sec: 23.40 - lr: 0.100000
2021-05-17 20:27:57,362 epoch 9 - iter 30/32 - loss 0.75623137 - samples/sec: 22.46 - lr: 0.100000
2021-05-17 20:27:58,557 ----------------------------------------------------------------------------------------------------
2021-05-17 20:27:58,557 EPOCH 9 done: loss 0.7521 - lr 0.1000000
2021-05-17 20:28:00,781 DEV : loss 0.33155956864356995 - score 0.8889
2021-05-17 20:28:00,805 BAD EPOCHS (no improvement): 1
2021-05-17 20:28:00,805 ----------------------------------------------------------------------------------------------------
2021-05-17 20:28:03,364 epoch 10 - iter 3/32 - loss 0.65753063 - samples/sec: 18.77 - lr: 0.100000
2021-05-17 20:28:05,386 epoch 10 - iter 6/32 - loss 0.75288820 - samples/sec: 23.74 - lr: 0.100000
2021-05-17 20:28:07,165 epoch 10 - iter 9/32 - loss 0.73499126 - samples/sec: 26.98 - lr: 0.100000
2021-05-17 20:28:10,063 epoch 10 - iter 12/32 - loss 0.80234255 - samples/sec: 16.57 - lr: 0.100000
2021-05-17 20:28:12,931 epoch 10 - iter 15/32 - loss 0.85088913 - samples/sec: 16.74 - lr: 0.100000
2021-05-17 20:28:15,746 epoch 10 - iter 18/32 - loss 0.85359880 - samples/sec: 17.05 - lr: 0.100000
2021-05-17 20:28:18,465 epoch 10 - iter 21/32 - loss 0.82155975 - samples/sec: 17.67 - lr: 0.100000
2021-05-17 20:28:21,295 epoch 10 - iter 24/32 - loss 0.78373386 - samples/sec: 16.97 - lr: 0.100000
2021-05-17 20:28:23,895 epoch 10 - iter 27/32 - loss 0.72790587 - samples/sec: 18.52 - lr: 0.100000
2021-05-17 20:28:26,073 epoch 10 - iter 30/32 - loss 0.69952259 - samples/sec: 22.04 - lr: 0.100000
2021-05-17 20:28:27,410 ----------------------------------------------------------------------------------------------------
2021-05-17 20:28:27,410 EPOCH 10 done: loss 0.6717 - lr 0.1000000
2021-05-17 20:28:29,473 DEV : loss 0.29158225655555725 - score 0.9128
2021-05-17 20:28:29,496 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:28:40,379 ----------------------------------------------------------------------------------------------------
2021-05-17 20:28:42,771 epoch 11 - iter 3/32 - loss 0.67358442 - samples/sec: 20.07 - lr: 0.100000
2021-05-17 20:28:44,970 epoch 11 - iter 6/32 - loss 0.67489304 - samples/sec: 21.84 - lr: 0.100000
2021-05-17 20:28:47,531 epoch 11 - iter 9/32 - loss 0.65983191 - samples/sec: 18.77 - lr: 0.100000
2021-05-17 20:28:50,237 epoch 11 - iter 12/32 - loss 0.66341564 - samples/sec: 17.75 - lr: 0.100000
2021-05-17 20:28:52,748 epoch 11 - iter 15/32 - loss 0.66628260 - samples/sec: 19.16 - lr: 0.100000
2021-05-17 20:28:55,713 epoch 11 - iter 18/32 - loss 0.67528876 - samples/sec: 16.22 - lr: 0.100000
2021-05-17 20:28:58,200 epoch 11 - iter 21/32 - loss 0.65935476 - samples/sec: 19.30 - lr: 0.100000
2021-05-17 20:29:00,794 epoch 11 - iter 24/32 - loss 0.68936980 - samples/sec: 18.51 - lr: 0.100000
2021-05-17 20:29:02,929 epoch 11 - iter 27/32 - loss 0.66932620 - samples/sec: 22.49 - lr: 0.100000
2021-05-17 20:29:04,993 epoch 11 - iter 30/32 - loss 0.63612755 - samples/sec: 23.27 - lr: 0.100000
2021-05-17 20:29:05,931 ----------------------------------------------------------------------------------------------------
2021-05-17 20:29:05,932 EPOCH 11 done: loss 0.6536 - lr 0.1000000
2021-05-17 20:29:08,221 DEV : loss 1.5140012502670288 - score 0.7019
2021-05-17 20:29:08,244 BAD EPOCHS (no improvement): 1
2021-05-17 20:29:08,245 ----------------------------------------------------------------------------------------------------
2021-05-17 20:29:10,596 epoch 12 - iter 3/32 - loss 0.74824206 - samples/sec: 20.42 - lr: 0.100000
2021-05-17 20:29:12,808 epoch 12 - iter 6/32 - loss 0.74969610 - samples/sec: 21.74 - lr: 0.100000
2021-05-17 20:29:15,083 epoch 12 - iter 9/32 - loss 0.70668414 - samples/sec: 21.11 - lr: 0.100000
2021-05-17 20:29:17,296 epoch 12 - iter 12/32 - loss 0.65530092 - samples/sec: 21.70 - lr: 0.100000
2021-05-17 20:29:19,668 epoch 12 - iter 15/32 - loss 0.63865949 - samples/sec: 20.25 - lr: 0.100000
2021-05-17 20:29:21,891 epoch 12 - iter 18/32 - loss 0.60162583 - samples/sec: 21.60 - lr: 0.100000
2021-05-17 20:29:24,330 epoch 12 - iter 21/32 - loss 0.59100098 - samples/sec: 19.69 - lr: 0.100000
2021-05-17 20:29:27,048 epoch 12 - iter 24/32 - loss 0.65895774 - samples/sec: 17.74 - lr: 0.100000
2021-05-17 20:29:29,505 epoch 12 - iter 27/32 - loss 0.65577137 - samples/sec: 19.54 - lr: 0.100000
2021-05-17 20:29:31,788 epoch 12 - iter 30/32 - loss 0.63661401 - samples/sec: 21.03 - lr: 0.100000
2021-05-17 20:29:32,928 ----------------------------------------------------------------------------------------------------
2021-05-17 20:29:32,928 EPOCH 12 done: loss 0.6194 - lr 0.1000000
2021-05-17 20:29:35,179 DEV : loss 0.4220840632915497 - score 0.8834
2021-05-17 20:29:35,203 BAD EPOCHS (no improvement): 2
2021-05-17 20:29:35,203 ----------------------------------------------------------------------------------------------------
2021-05-17 20:29:38,149 epoch 13 - iter 3/32 - loss 0.70066770 - samples/sec: 16.30 - lr: 0.100000
2021-05-17 20:29:40,473 epoch 13 - iter 6/32 - loss 0.57334810 - samples/sec: 20.70 - lr: 0.100000
2021-05-17 20:29:42,815 epoch 13 - iter 9/32 - loss 0.65744083 - samples/sec: 20.50 - lr: 0.100000
2021-05-17 20:29:45,457 epoch 13 - iter 12/32 - loss 0.58151393 - samples/sec: 18.17 - lr: 0.100000
2021-05-17 20:29:48,189 epoch 13 - iter 15/32 - loss 0.59121860 - samples/sec: 17.57 - lr: 0.100000
2021-05-17 20:29:51,033 epoch 13 - iter 18/32 - loss 0.57821876 - samples/sec: 16.88 - lr: 0.100000
2021-05-17 20:29:53,669 epoch 13 - iter 21/32 - loss 0.57598987 - samples/sec: 18.26 - lr: 0.100000
2021-05-17 20:29:55,863 epoch 13 - iter 24/32 - loss 0.57736243 - samples/sec: 21.89 - lr: 0.100000
2021-05-17 20:29:59,066 epoch 13 - iter 27/32 - loss 0.57352868 - samples/sec: 14.99 - lr: 0.100000
2021-05-17 20:30:01,187 epoch 13 - iter 30/32 - loss 0.55157735 - samples/sec: 22.65 - lr: 0.100000
2021-05-17 20:30:02,151 ----------------------------------------------------------------------------------------------------
2021-05-17 20:30:02,152 EPOCH 13 done: loss 0.5275 - lr 0.1000000
2021-05-17 20:30:05,473 DEV : loss 0.3179597556591034 - score 0.8966
2021-05-17 20:30:05,496 BAD EPOCHS (no improvement): 3
2021-05-17 20:30:05,496 ----------------------------------------------------------------------------------------------------
2021-05-17 20:30:07,911 epoch 14 - iter 3/32 - loss 0.39451981 - samples/sec: 19.88 - lr: 0.100000
2021-05-17 20:30:10,655 epoch 14 - iter 6/32 - loss 0.59337221 - samples/sec: 17.50 - lr: 0.100000
2021-05-17 20:30:13,235 epoch 14 - iter 9/32 - loss 0.63328082 - samples/sec: 18.61 - lr: 0.100000
2021-05-17 20:30:15,846 epoch 14 - iter 12/32 - loss 0.64951675 - samples/sec: 18.40 - lr: 0.100000
2021-05-17 20:30:18,612 epoch 14 - iter 15/32 - loss 0.62750938 - samples/sec: 17.38 - lr: 0.100000
2021-05-17 20:30:21,171 epoch 14 - iter 18/32 - loss 0.63040855 - samples/sec: 18.77 - lr: 0.100000
2021-05-17 20:30:23,796 epoch 14 - iter 21/32 - loss 0.60933455 - samples/sec: 18.29 - lr: 0.100000
2021-05-17 20:30:26,211 epoch 14 - iter 24/32 - loss 0.63326849 - samples/sec: 19.88 - lr: 0.100000
2021-05-17 20:30:28,618 epoch 14 - iter 27/32 - loss 0.65578893 - samples/sec: 19.95 - lr: 0.100000
2021-05-17 20:30:30,636 epoch 14 - iter 30/32 - loss 0.62794276 - samples/sec: 23.80 - lr: 0.100000
2021-05-17 20:30:31,577 ----------------------------------------------------------------------------------------------------
2021-05-17 20:30:31,578 EPOCH 14 done: loss 0.6022 - lr 0.1000000
2021-05-17 20:30:34,214 DEV : loss 0.3417746424674988 - score 0.8944
Epoch    14: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 20:30:34,242 BAD EPOCHS (no improvement): 4
2021-05-17 20:30:34,243 ----------------------------------------------------------------------------------------------------
2021-05-17 20:30:36,770 epoch 15 - iter 3/32 - loss 0.81819987 - samples/sec: 19.00 - lr: 0.050000
2021-05-17 20:30:39,206 epoch 15 - iter 6/32 - loss 0.67312229 - samples/sec: 19.71 - lr: 0.050000
2021-05-17 20:30:41,770 epoch 15 - iter 9/32 - loss 0.62368628 - samples/sec: 18.73 - lr: 0.050000
2021-05-17 20:30:44,361 epoch 15 - iter 12/32 - loss 0.57367436 - samples/sec: 18.53 - lr: 0.050000
2021-05-17 20:30:46,494 epoch 15 - iter 15/32 - loss 0.55419559 - samples/sec: 22.52 - lr: 0.050000
2021-05-17 20:30:48,645 epoch 15 - iter 18/32 - loss 0.47851103 - samples/sec: 22.32 - lr: 0.050000
2021-05-17 20:30:51,080 epoch 15 - iter 21/32 - loss 0.43964588 - samples/sec: 19.72 - lr: 0.050000
2021-05-17 20:30:53,524 epoch 15 - iter 24/32 - loss 0.41646370 - samples/sec: 19.65 - lr: 0.050000
2021-05-17 20:30:56,289 epoch 15 - iter 27/32 - loss 0.40047460 - samples/sec: 17.40 - lr: 0.050000
2021-05-17 20:30:58,972 epoch 15 - iter 30/32 - loss 0.38591884 - samples/sec: 17.92 - lr: 0.050000
2021-05-17 20:31:00,403 ----------------------------------------------------------------------------------------------------
2021-05-17 20:31:00,404 EPOCH 15 done: loss 0.3812 - lr 0.0500000
2021-05-17 20:31:04,321 DEV : loss 0.32541319727897644 - score 0.8811
2021-05-17 20:31:04,350 BAD EPOCHS (no improvement): 1
2021-05-17 20:31:04,351 ----------------------------------------------------------------------------------------------------
2021-05-17 20:31:07,472 epoch 16 - iter 3/32 - loss 0.33707444 - samples/sec: 15.38 - lr: 0.050000
2021-05-17 20:31:10,283 epoch 16 - iter 6/32 - loss 0.32560500 - samples/sec: 17.08 - lr: 0.050000
2021-05-17 20:31:12,967 epoch 16 - iter 9/32 - loss 0.28328144 - samples/sec: 17.89 - lr: 0.050000
2021-05-17 20:31:15,484 epoch 16 - iter 12/32 - loss 0.30377960 - samples/sec: 19.08 - lr: 0.050000
2021-05-17 20:31:18,023 epoch 16 - iter 15/32 - loss 0.29325917 - samples/sec: 18.92 - lr: 0.050000
2021-05-17 20:31:20,414 epoch 16 - iter 18/32 - loss 0.31115605 - samples/sec: 20.08 - lr: 0.050000
2021-05-17 20:31:22,388 epoch 16 - iter 21/32 - loss 0.33521628 - samples/sec: 24.33 - lr: 0.050000
2021-05-17 20:31:24,653 epoch 16 - iter 24/32 - loss 0.32714579 - samples/sec: 21.19 - lr: 0.050000
2021-05-17 20:31:26,731 epoch 16 - iter 27/32 - loss 0.35843765 - samples/sec: 23.12 - lr: 0.050000
2021-05-17 20:31:28,780 epoch 16 - iter 30/32 - loss 0.36235574 - samples/sec: 23.43 - lr: 0.050000
2021-05-17 20:31:29,802 ----------------------------------------------------------------------------------------------------
2021-05-17 20:31:29,803 EPOCH 16 done: loss 0.4205 - lr 0.0500000
2021-05-17 20:31:33,166 DEV : loss 0.3006204664707184 - score 0.9211
2021-05-17 20:31:33,190 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:31:44,018 ----------------------------------------------------------------------------------------------------
2021-05-17 20:31:46,095 epoch 17 - iter 3/32 - loss 0.30804777 - samples/sec: 23.13 - lr: 0.050000
2021-05-17 20:31:48,146 epoch 17 - iter 6/32 - loss 0.26120543 - samples/sec: 23.41 - lr: 0.050000
2021-05-17 20:31:50,539 epoch 17 - iter 9/32 - loss 0.31611147 - samples/sec: 20.07 - lr: 0.050000
2021-05-17 20:31:53,488 epoch 17 - iter 12/32 - loss 0.36362041 - samples/sec: 16.29 - lr: 0.050000
2021-05-17 20:31:56,099 epoch 17 - iter 15/32 - loss 0.34338555 - samples/sec: 18.43 - lr: 0.050000
2021-05-17 20:31:58,345 epoch 17 - iter 18/32 - loss 0.38442156 - samples/sec: 21.39 - lr: 0.050000
2021-05-17 20:32:00,477 epoch 17 - iter 21/32 - loss 0.42860110 - samples/sec: 22.52 - lr: 0.050000
2021-05-17 20:32:02,649 epoch 17 - iter 24/32 - loss 0.42670995 - samples/sec: 22.12 - lr: 0.050000
2021-05-17 20:32:05,467 epoch 17 - iter 27/32 - loss 0.43026444 - samples/sec: 17.04 - lr: 0.050000
2021-05-17 20:32:08,305 epoch 17 - iter 30/32 - loss 0.41717208 - samples/sec: 16.92 - lr: 0.050000
2021-05-17 20:32:09,761 ----------------------------------------------------------------------------------------------------
2021-05-17 20:32:09,762 EPOCH 17 done: loss 0.4027 - lr 0.0500000
2021-05-17 20:32:12,934 DEV : loss 0.3003029227256775 - score 0.9116
2021-05-17 20:32:12,964 BAD EPOCHS (no improvement): 1
2021-05-17 20:32:12,964 ----------------------------------------------------------------------------------------------------
2021-05-17 20:32:15,250 epoch 18 - iter 3/32 - loss 0.24099640 - samples/sec: 21.01 - lr: 0.050000
2021-05-17 20:32:17,777 epoch 18 - iter 6/32 - loss 0.27611523 - samples/sec: 19.00 - lr: 0.050000
2021-05-17 20:32:20,644 epoch 18 - iter 9/32 - loss 0.33923242 - samples/sec: 16.75 - lr: 0.050000
2021-05-17 20:32:22,976 epoch 18 - iter 12/32 - loss 0.35259142 - samples/sec: 20.60 - lr: 0.050000
2021-05-17 20:32:25,211 epoch 18 - iter 15/32 - loss 0.36036357 - samples/sec: 21.49 - lr: 0.050000
2021-05-17 20:32:27,270 epoch 18 - iter 18/32 - loss 0.39716822 - samples/sec: 23.32 - lr: 0.050000
2021-05-17 20:32:29,501 epoch 18 - iter 21/32 - loss 0.40354816 - samples/sec: 21.52 - lr: 0.050000
2021-05-17 20:32:32,441 epoch 18 - iter 24/32 - loss 0.42560463 - samples/sec: 16.35 - lr: 0.050000
2021-05-17 20:32:34,743 epoch 18 - iter 27/32 - loss 0.41890245 - samples/sec: 20.86 - lr: 0.050000
2021-05-17 20:32:36,955 epoch 18 - iter 30/32 - loss 0.40015065 - samples/sec: 21.70 - lr: 0.050000
2021-05-17 20:32:37,874 ----------------------------------------------------------------------------------------------------
2021-05-17 20:32:37,875 EPOCH 18 done: loss 0.4276 - lr 0.0500000
2021-05-17 20:32:40,208 DEV : loss 0.28810977935791016 - score 0.915
2021-05-17 20:32:40,232 BAD EPOCHS (no improvement): 2
2021-05-17 20:32:40,232 ----------------------------------------------------------------------------------------------------
2021-05-17 20:32:42,299 epoch 19 - iter 3/32 - loss 0.31446558 - samples/sec: 23.23 - lr: 0.050000
2021-05-17 20:32:44,295 epoch 19 - iter 6/32 - loss 0.25398249 - samples/sec: 24.06 - lr: 0.050000
2021-05-17 20:32:46,188 epoch 19 - iter 9/32 - loss 0.32378681 - samples/sec: 25.37 - lr: 0.050000
2021-05-17 20:32:48,260 epoch 19 - iter 12/32 - loss 0.37784502 - samples/sec: 23.16 - lr: 0.050000
2021-05-17 20:32:50,267 epoch 19 - iter 15/32 - loss 0.38860882 - samples/sec: 23.93 - lr: 0.050000
2021-05-17 20:32:52,853 epoch 19 - iter 18/32 - loss 0.36801774 - samples/sec: 18.58 - lr: 0.050000
2021-05-17 20:32:55,301 epoch 19 - iter 21/32 - loss 0.35704623 - samples/sec: 19.63 - lr: 0.050000
2021-05-17 20:32:57,567 epoch 19 - iter 24/32 - loss 0.35853019 - samples/sec: 21.19 - lr: 0.050000
2021-05-17 20:33:00,075 epoch 19 - iter 27/32 - loss 0.35722771 - samples/sec: 19.15 - lr: 0.050000
2021-05-17 20:33:02,509 epoch 19 - iter 30/32 - loss 0.37232618 - samples/sec: 19.82 - lr: 0.050000
2021-05-17 20:33:03,720 ----------------------------------------------------------------------------------------------------
2021-05-17 20:33:03,721 EPOCH 19 done: loss 0.3651 - lr 0.0500000
2021-05-17 20:33:06,140 DEV : loss 0.28321996331214905 - score 0.9211
2021-05-17 20:33:06,169 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:33:17,453 ----------------------------------------------------------------------------------------------------
2021-05-17 20:33:20,405 epoch 20 - iter 3/32 - loss 0.55319897 - samples/sec: 16.27 - lr: 0.050000
2021-05-17 20:33:23,300 epoch 20 - iter 6/32 - loss 0.50660225 - samples/sec: 16.58 - lr: 0.050000
2021-05-17 20:33:26,085 epoch 20 - iter 9/32 - loss 0.47525209 - samples/sec: 17.24 - lr: 0.050000
2021-05-17 20:33:28,597 epoch 20 - iter 12/32 - loss 0.43855318 - samples/sec: 19.11 - lr: 0.050000
2021-05-17 20:33:31,549 epoch 20 - iter 15/32 - loss 0.42762684 - samples/sec: 16.28 - lr: 0.050000
2021-05-17 20:33:33,651 epoch 20 - iter 18/32 - loss 0.44620470 - samples/sec: 22.85 - lr: 0.050000
2021-05-17 20:33:36,200 epoch 20 - iter 21/32 - loss 0.45453677 - samples/sec: 18.86 - lr: 0.050000
2021-05-17 20:33:39,146 epoch 20 - iter 24/32 - loss 0.42394659 - samples/sec: 16.30 - lr: 0.050000
2021-05-17 20:33:41,580 epoch 20 - iter 27/32 - loss 0.41114773 - samples/sec: 19.73 - lr: 0.050000
2021-05-17 20:33:43,957 epoch 20 - iter 30/32 - loss 0.41622792 - samples/sec: 20.20 - lr: 0.050000
2021-05-17 20:33:44,970 ----------------------------------------------------------------------------------------------------
2021-05-17 20:33:44,971 EPOCH 20 done: loss 0.4264 - lr 0.0500000
2021-05-17 20:33:47,459 DEV : loss 0.29456064105033875 - score 0.9128
2021-05-17 20:33:47,482 BAD EPOCHS (no improvement): 1
2021-05-17 20:33:47,482 ----------------------------------------------------------------------------------------------------
2021-05-17 20:33:49,799 epoch 21 - iter 3/32 - loss 0.36945089 - samples/sec: 20.72 - lr: 0.050000
2021-05-17 20:33:52,004 epoch 21 - iter 6/32 - loss 0.26672240 - samples/sec: 21.78 - lr: 0.050000
2021-05-17 20:33:53,992 epoch 21 - iter 9/32 - loss 0.36518282 - samples/sec: 24.14 - lr: 0.050000
2021-05-17 20:33:56,398 epoch 21 - iter 12/32 - loss 0.33282284 - samples/sec: 19.96 - lr: 0.050000
2021-05-17 20:33:58,749 epoch 21 - iter 15/32 - loss 0.36232704 - samples/sec: 20.43 - lr: 0.050000
2021-05-17 20:34:00,778 epoch 21 - iter 18/32 - loss 0.35413570 - samples/sec: 23.66 - lr: 0.050000
2021-05-17 20:34:03,510 epoch 21 - iter 21/32 - loss 0.36069020 - samples/sec: 17.63 - lr: 0.050000
2021-05-17 20:34:06,268 epoch 21 - iter 24/32 - loss 0.35102915 - samples/sec: 17.44 - lr: 0.050000
2021-05-17 20:34:08,852 epoch 21 - iter 27/32 - loss 0.35068950 - samples/sec: 18.58 - lr: 0.050000
2021-05-17 20:34:11,403 epoch 21 - iter 30/32 - loss 0.33558038 - samples/sec: 18.83 - lr: 0.050000
2021-05-17 20:34:12,523 ----------------------------------------------------------------------------------------------------
2021-05-17 20:34:12,524 EPOCH 21 done: loss 0.3364 - lr 0.0500000
2021-05-17 20:34:15,520 DEV : loss 0.2987869083881378 - score 0.9324
2021-05-17 20:34:15,547 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:34:26,430 ----------------------------------------------------------------------------------------------------
2021-05-17 20:34:29,137 epoch 22 - iter 3/32 - loss 0.20347595 - samples/sec: 17.74 - lr: 0.050000
2021-05-17 20:34:31,931 epoch 22 - iter 6/32 - loss 0.30483554 - samples/sec: 17.19 - lr: 0.050000
2021-05-17 20:34:34,866 epoch 22 - iter 9/32 - loss 0.29506883 - samples/sec: 16.36 - lr: 0.050000
2021-05-17 20:34:37,393 epoch 22 - iter 12/32 - loss 0.34020371 - samples/sec: 19.00 - lr: 0.050000
2021-05-17 20:34:39,925 epoch 22 - iter 15/32 - loss 0.34393683 - samples/sec: 18.96 - lr: 0.050000
2021-05-17 20:34:42,058 epoch 22 - iter 18/32 - loss 0.34461210 - samples/sec: 22.52 - lr: 0.050000
2021-05-17 20:34:44,718 epoch 22 - iter 21/32 - loss 0.32296574 - samples/sec: 18.04 - lr: 0.050000
2021-05-17 20:34:47,190 epoch 22 - iter 24/32 - loss 0.34700264 - samples/sec: 19.42 - lr: 0.050000
2021-05-17 20:34:49,686 epoch 22 - iter 27/32 - loss 0.36450356 - samples/sec: 19.24 - lr: 0.050000
2021-05-17 20:34:51,900 epoch 22 - iter 30/32 - loss 0.36240273 - samples/sec: 21.68 - lr: 0.050000
2021-05-17 20:34:52,873 ----------------------------------------------------------------------------------------------------
2021-05-17 20:34:52,874 EPOCH 22 done: loss 0.3808 - lr 0.0500000
2021-05-17 20:34:55,345 DEV : loss 0.468802809715271 - score 0.878
2021-05-17 20:34:55,368 BAD EPOCHS (no improvement): 1
2021-05-17 20:34:55,369 ----------------------------------------------------------------------------------------------------
2021-05-17 20:34:58,102 epoch 23 - iter 3/32 - loss 0.50979288 - samples/sec: 17.57 - lr: 0.050000
2021-05-17 20:35:01,001 epoch 23 - iter 6/32 - loss 0.34637541 - samples/sec: 16.56 - lr: 0.050000
2021-05-17 20:35:03,484 epoch 23 - iter 9/32 - loss 0.31163424 - samples/sec: 19.34 - lr: 0.050000
2021-05-17 20:35:05,982 epoch 23 - iter 12/32 - loss 0.31994229 - samples/sec: 19.23 - lr: 0.050000
2021-05-17 20:35:08,030 epoch 23 - iter 15/32 - loss 0.30490659 - samples/sec: 23.44 - lr: 0.050000
2021-05-17 20:35:10,124 epoch 23 - iter 18/32 - loss 0.30849373 - samples/sec: 22.93 - lr: 0.050000
2021-05-17 20:35:12,033 epoch 23 - iter 21/32 - loss 0.32544019 - samples/sec: 25.15 - lr: 0.050000
2021-05-17 20:35:14,349 epoch 23 - iter 24/32 - loss 0.32268418 - samples/sec: 20.74 - lr: 0.050000
2021-05-17 20:35:17,258 epoch 23 - iter 27/32 - loss 0.31083539 - samples/sec: 16.51 - lr: 0.050000
2021-05-17 20:35:19,707 epoch 23 - iter 30/32 - loss 0.30760698 - samples/sec: 19.60 - lr: 0.050000
2021-05-17 20:35:20,824 ----------------------------------------------------------------------------------------------------
2021-05-17 20:35:20,825 EPOCH 23 done: loss 0.2992 - lr 0.0500000
2021-05-17 20:35:23,514 DEV : loss 0.348202645778656 - score 0.9057
2021-05-17 20:35:23,539 BAD EPOCHS (no improvement): 2
2021-05-17 20:35:23,540 ----------------------------------------------------------------------------------------------------
2021-05-17 20:35:25,598 epoch 24 - iter 3/32 - loss 0.33385245 - samples/sec: 23.33 - lr: 0.050000
2021-05-17 20:35:27,833 epoch 24 - iter 6/32 - loss 0.29869207 - samples/sec: 21.49 - lr: 0.050000
2021-05-17 20:35:30,162 epoch 24 - iter 9/32 - loss 0.31017503 - samples/sec: 20.62 - lr: 0.050000
2021-05-17 20:35:32,645 epoch 24 - iter 12/32 - loss 0.27860429 - samples/sec: 19.34 - lr: 0.050000
2021-05-17 20:35:34,810 epoch 24 - iter 15/32 - loss 0.24823632 - samples/sec: 22.18 - lr: 0.050000
2021-05-17 20:35:36,946 epoch 24 - iter 18/32 - loss 0.25464680 - samples/sec: 22.47 - lr: 0.050000
2021-05-17 20:35:39,040 epoch 24 - iter 21/32 - loss 0.27396985 - samples/sec: 22.93 - lr: 0.050000
2021-05-17 20:35:41,440 epoch 24 - iter 24/32 - loss 0.26592278 - samples/sec: 20.01 - lr: 0.050000
2021-05-17 20:35:43,484 epoch 24 - iter 27/32 - loss 0.27288816 - samples/sec: 23.50 - lr: 0.050000
2021-05-17 20:35:45,609 epoch 24 - iter 30/32 - loss 0.26692589 - samples/sec: 22.60 - lr: 0.050000
2021-05-17 20:35:47,003 ----------------------------------------------------------------------------------------------------
2021-05-17 20:35:47,004 EPOCH 24 done: loss 0.2845 - lr 0.0500000
2021-05-17 20:35:49,725 DEV : loss 0.2999819815158844 - score 0.92
2021-05-17 20:35:49,756 BAD EPOCHS (no improvement): 3
2021-05-17 20:35:49,757 ----------------------------------------------------------------------------------------------------
2021-05-17 20:35:51,928 epoch 25 - iter 3/32 - loss 0.18645652 - samples/sec: 22.11 - lr: 0.050000
2021-05-17 20:35:54,038 epoch 25 - iter 6/32 - loss 0.20368296 - samples/sec: 22.81 - lr: 0.050000
2021-05-17 20:35:56,421 epoch 25 - iter 9/32 - loss 0.17865312 - samples/sec: 20.15 - lr: 0.050000
2021-05-17 20:35:58,610 epoch 25 - iter 12/32 - loss 0.21844938 - samples/sec: 21.93 - lr: 0.050000
2021-05-17 20:36:00,600 epoch 25 - iter 15/32 - loss 0.23903395 - samples/sec: 24.13 - lr: 0.050000
2021-05-17 20:36:02,963 epoch 25 - iter 18/32 - loss 0.25043497 - samples/sec: 20.32 - lr: 0.050000
2021-05-17 20:36:05,082 epoch 25 - iter 21/32 - loss 0.25585000 - samples/sec: 22.67 - lr: 0.050000
2021-05-17 20:36:07,136 epoch 25 - iter 24/32 - loss 0.28465247 - samples/sec: 23.38 - lr: 0.050000
2021-05-17 20:36:09,588 epoch 25 - iter 27/32 - loss 0.28849002 - samples/sec: 19.59 - lr: 0.050000
2021-05-17 20:36:12,136 epoch 25 - iter 30/32 - loss 0.28809913 - samples/sec: 18.84 - lr: 0.050000
2021-05-17 20:36:13,150 ----------------------------------------------------------------------------------------------------
2021-05-17 20:36:13,151 EPOCH 25 done: loss 0.2751 - lr 0.0500000
2021-05-17 20:36:15,572 DEV : loss 0.3249751627445221 - score 0.9221
Epoch    25: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 20:36:15,596 BAD EPOCHS (no improvement): 4
2021-05-17 20:36:15,597 ----------------------------------------------------------------------------------------------------
2021-05-17 20:36:18,079 epoch 26 - iter 3/32 - loss 0.39241091 - samples/sec: 19.35 - lr: 0.025000
2021-05-17 20:36:20,839 epoch 26 - iter 6/32 - loss 0.32161289 - samples/sec: 17.40 - lr: 0.025000
2021-05-17 20:36:23,607 epoch 26 - iter 9/32 - loss 0.45443224 - samples/sec: 17.38 - lr: 0.025000
2021-05-17 20:36:26,704 epoch 26 - iter 12/32 - loss 0.46446014 - samples/sec: 15.52 - lr: 0.025000
2021-05-17 20:36:29,770 epoch 26 - iter 15/32 - loss 0.43121370 - samples/sec: 15.66 - lr: 0.025000
2021-05-17 20:36:32,304 epoch 26 - iter 18/32 - loss 0.37354692 - samples/sec: 18.95 - lr: 0.025000
2021-05-17 20:36:34,965 epoch 26 - iter 21/32 - loss 0.36164212 - samples/sec: 18.04 - lr: 0.025000
2021-05-17 20:36:37,273 epoch 26 - iter 24/32 - loss 0.33308646 - samples/sec: 20.84 - lr: 0.025000
2021-05-17 20:36:39,394 epoch 26 - iter 27/32 - loss 0.32644433 - samples/sec: 22.64 - lr: 0.025000
2021-05-17 20:36:41,670 epoch 26 - iter 30/32 - loss 0.32393936 - samples/sec: 21.10 - lr: 0.025000
2021-05-17 20:36:42,864 ----------------------------------------------------------------------------------------------------
2021-05-17 20:36:42,865 EPOCH 26 done: loss 0.3151 - lr 0.0250000
2021-05-17 20:36:45,794 DEV : loss 0.2911435663700104 - score 0.9342
2021-05-17 20:36:45,823 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:36:57,518 ----------------------------------------------------------------------------------------------------
2021-05-17 20:37:00,234 epoch 27 - iter 3/32 - loss 0.25711838 - samples/sec: 17.68 - lr: 0.025000
2021-05-17 20:37:02,700 epoch 27 - iter 6/32 - loss 0.23291746 - samples/sec: 19.47 - lr: 0.025000
2021-05-17 20:37:05,393 epoch 27 - iter 9/32 - loss 0.25041251 - samples/sec: 17.83 - lr: 0.025000
2021-05-17 20:37:07,426 epoch 27 - iter 12/32 - loss 0.24975347 - samples/sec: 23.63 - lr: 0.025000
2021-05-17 20:37:09,696 epoch 27 - iter 15/32 - loss 0.25699571 - samples/sec: 21.16 - lr: 0.025000
2021-05-17 20:37:12,066 epoch 27 - iter 18/32 - loss 0.25063817 - samples/sec: 20.26 - lr: 0.025000
2021-05-17 20:37:14,189 epoch 27 - iter 21/32 - loss 0.24436231 - samples/sec: 22.62 - lr: 0.025000
2021-05-17 20:37:16,250 epoch 27 - iter 24/32 - loss 0.26115431 - samples/sec: 23.30 - lr: 0.025000
2021-05-17 20:37:18,611 epoch 27 - iter 27/32 - loss 0.25693119 - samples/sec: 20.34 - lr: 0.025000
2021-05-17 20:37:21,493 epoch 27 - iter 30/32 - loss 0.27254143 - samples/sec: 16.67 - lr: 0.025000
2021-05-17 20:37:22,921 ----------------------------------------------------------------------------------------------------
2021-05-17 20:37:22,921 EPOCH 27 done: loss 0.2686 - lr 0.0250000
2021-05-17 20:37:25,278 DEV : loss 0.28075700998306274 - score 0.9281
2021-05-17 20:37:25,299 BAD EPOCHS (no improvement): 1
2021-05-17 20:37:25,299 ----------------------------------------------------------------------------------------------------
2021-05-17 20:37:27,695 epoch 28 - iter 3/32 - loss 0.21735128 - samples/sec: 20.04 - lr: 0.025000
2021-05-17 20:37:29,984 epoch 28 - iter 6/32 - loss 0.19805934 - samples/sec: 21.05 - lr: 0.025000
2021-05-17 20:37:32,431 epoch 28 - iter 9/32 - loss 0.19097700 - samples/sec: 19.62 - lr: 0.025000
2021-05-17 20:37:34,660 epoch 28 - iter 12/32 - loss 0.24488585 - samples/sec: 21.55 - lr: 0.025000
2021-05-17 20:37:37,004 epoch 28 - iter 15/32 - loss 0.24762663 - samples/sec: 20.49 - lr: 0.025000
2021-05-17 20:37:39,230 epoch 28 - iter 18/32 - loss 0.26264626 - samples/sec: 21.58 - lr: 0.025000
2021-05-17 20:37:41,458 epoch 28 - iter 21/32 - loss 0.26823342 - samples/sec: 21.56 - lr: 0.025000
2021-05-17 20:37:43,913 epoch 28 - iter 24/32 - loss 0.26976187 - samples/sec: 19.55 - lr: 0.025000
2021-05-17 20:37:46,776 epoch 28 - iter 27/32 - loss 0.27735398 - samples/sec: 16.77 - lr: 0.025000
2021-05-17 20:37:49,302 epoch 28 - iter 30/32 - loss 0.28062876 - samples/sec: 19.01 - lr: 0.025000
2021-05-17 20:37:50,465 ----------------------------------------------------------------------------------------------------
2021-05-17 20:37:50,466 EPOCH 28 done: loss 0.2776 - lr 0.0250000
2021-05-17 20:37:52,937 DEV : loss 0.3073178231716156 - score 0.9167
2021-05-17 20:37:52,960 BAD EPOCHS (no improvement): 2
2021-05-17 20:37:52,960 ----------------------------------------------------------------------------------------------------
2021-05-17 20:37:55,470 epoch 29 - iter 3/32 - loss 0.15582762 - samples/sec: 19.13 - lr: 0.025000
2021-05-17 20:37:58,027 epoch 29 - iter 6/32 - loss 0.26041638 - samples/sec: 18.78 - lr: 0.025000
2021-05-17 20:38:00,520 epoch 29 - iter 9/32 - loss 0.25714211 - samples/sec: 19.27 - lr: 0.025000
2021-05-17 20:38:03,397 epoch 29 - iter 12/32 - loss 0.24453898 - samples/sec: 16.71 - lr: 0.025000
2021-05-17 20:38:05,807 epoch 29 - iter 15/32 - loss 0.26619790 - samples/sec: 19.93 - lr: 0.025000
2021-05-17 20:38:08,162 epoch 29 - iter 18/32 - loss 0.27494026 - samples/sec: 20.40 - lr: 0.025000
2021-05-17 20:38:10,244 epoch 29 - iter 21/32 - loss 0.28117973 - samples/sec: 23.07 - lr: 0.025000
2021-05-17 20:38:12,278 epoch 29 - iter 24/32 - loss 0.28688630 - samples/sec: 23.60 - lr: 0.025000
2021-05-17 20:38:14,385 epoch 29 - iter 27/32 - loss 0.27975175 - samples/sec: 22.79 - lr: 0.025000
2021-05-17 20:38:17,224 epoch 29 - iter 30/32 - loss 0.26987329 - samples/sec: 16.98 - lr: 0.025000
2021-05-17 20:38:18,401 ----------------------------------------------------------------------------------------------------
2021-05-17 20:38:18,402 EPOCH 29 done: loss 0.2580 - lr 0.0250000
2021-05-17 20:38:21,395 DEV : loss 0.34096914529800415 - score 0.9057
2021-05-17 20:38:21,419 BAD EPOCHS (no improvement): 3
2021-05-17 20:38:21,420 ----------------------------------------------------------------------------------------------------
2021-05-17 20:38:23,891 epoch 30 - iter 3/32 - loss 0.11279472 - samples/sec: 19.42 - lr: 0.025000
2021-05-17 20:38:26,341 epoch 30 - iter 6/32 - loss 0.09155613 - samples/sec: 19.64 - lr: 0.025000
2021-05-17 20:38:28,805 epoch 30 - iter 9/32 - loss 0.14984013 - samples/sec: 19.49 - lr: 0.025000
2021-05-17 20:38:30,982 epoch 30 - iter 12/32 - loss 0.19838837 - samples/sec: 22.06 - lr: 0.025000
2021-05-17 20:38:33,038 epoch 30 - iter 15/32 - loss 0.21156573 - samples/sec: 23.34 - lr: 0.025000
2021-05-17 20:38:35,368 epoch 30 - iter 18/32 - loss 0.22164359 - samples/sec: 20.61 - lr: 0.025000
2021-05-17 20:38:37,679 epoch 30 - iter 21/32 - loss 0.22392103 - samples/sec: 20.79 - lr: 0.025000
2021-05-17 20:38:40,018 epoch 30 - iter 24/32 - loss 0.23069498 - samples/sec: 20.53 - lr: 0.025000
2021-05-17 20:38:42,059 epoch 30 - iter 27/32 - loss 0.24339843 - samples/sec: 23.52 - lr: 0.025000
2021-05-17 20:38:44,121 epoch 30 - iter 30/32 - loss 0.25344007 - samples/sec: 23.30 - lr: 0.025000
2021-05-17 20:38:45,173 ----------------------------------------------------------------------------------------------------
2021-05-17 20:38:45,174 EPOCH 30 done: loss 0.2473 - lr 0.0250000
2021-05-17 20:38:47,821 DEV : loss 0.2809917628765106 - score 0.9211
Epoch    30: reducing learning rate of group 0 to 1.2500e-02.
2021-05-17 20:38:47,844 BAD EPOCHS (no improvement): 4
2021-05-17 20:38:59,115 ----------------------------------------------------------------------------------------------------
2021-05-17 20:38:59,117 Testing using best model ...
2021-05-17 20:38:59,118 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/spa.rst.sctb/best-model.pt
2021-05-17 20:39:14,914 0.7444	0.9900	0.8498
2021-05-17 20:39:14,914 
Results:
- F1-score (micro) 0.8498
- F1-score (macro) 0.8498

By class:
SENT       tp: 99 - fp: 34 - fn: 1 - precision: 0.7444 - recall: 0.9900 - f1-score: 0.8498
2021-05-17 20:39:14,914 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt/
2021-05-17 20:39:14,951 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt
2021-05-17 20:39:14,954 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt/sent_train.txt
2021-05-17 20:39:14,955 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt/sent_dev.txt
2021-05-17 20:39:14,955 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt/sent_test.txt
Corpus: 8099 train + 840 dev + 1047 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-17 20:39:36,747 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,753 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-17 20:39:36,753 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,753 Corpus: "Corpus: 8099 train + 840 dev + 1047 test sentences"
2021-05-17 20:39:36,753 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,753 Parameters:
2021-05-17 20:39:36,753  - learning_rate: "0.1"
2021-05-17 20:39:36,753  - mini_batch_size: "16"
2021-05-17 20:39:36,753  - patience: "3"
2021-05-17 20:39:36,753  - anneal_factor: "0.5"
2021-05-17 20:39:36,753  - max_epochs: "30"
2021-05-17 20:39:36,753  - shuffle: "True"
2021-05-17 20:39:36,754  - train_with_dev: "False"
2021-05-17 20:39:36,754  - batch_growth_annealing: "False"
2021-05-17 20:39:36,754 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,754 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt"
2021-05-17 20:39:36,754 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,754 Device: cuda:0
2021-05-17 20:39:36,754 ----------------------------------------------------------------------------------------------------
2021-05-17 20:39:36,754 Embeddings storage mode: cpu
2021-05-17 20:39:36,758 ----------------------------------------------------------------------------------------------------
2021-05-17 20:41:18,720 epoch 1 - iter 50/507 - loss 4.41786274 - samples/sec: 7.85 - lr: 0.100000
2021-05-17 20:42:54,135 epoch 1 - iter 100/507 - loss 4.13152374 - samples/sec: 8.38 - lr: 0.100000
2021-05-17 20:44:39,825 epoch 1 - iter 150/507 - loss 3.97143035 - samples/sec: 7.57 - lr: 0.100000
2021-05-17 20:46:15,148 epoch 1 - iter 200/507 - loss 3.90884629 - samples/sec: 8.39 - lr: 0.100000
2021-05-17 20:47:50,187 epoch 1 - iter 250/507 - loss 3.84071752 - samples/sec: 8.42 - lr: 0.100000
2021-05-17 20:49:27,087 epoch 1 - iter 300/507 - loss 3.80803507 - samples/sec: 8.26 - lr: 0.100000
2021-05-17 20:51:00,903 epoch 1 - iter 350/507 - loss 3.77935762 - samples/sec: 8.53 - lr: 0.100000
2021-05-17 20:52:35,458 epoch 1 - iter 400/507 - loss 3.74680783 - samples/sec: 8.46 - lr: 0.100000
2021-05-17 20:54:12,288 epoch 1 - iter 450/507 - loss 3.71603314 - samples/sec: 8.26 - lr: 0.100000
2021-05-17 20:55:52,535 epoch 1 - iter 500/507 - loss 3.70932117 - samples/sec: 7.98 - lr: 0.100000
2021-05-17 20:56:08,314 ----------------------------------------------------------------------------------------------------
2021-05-17 20:56:08,314 EPOCH 1 done: loss 3.7141 - lr 0.1000000
2021-05-17 20:57:26,393 DEV : loss 3.5421719551086426 - score 0.0
2021-05-17 20:57:26,553 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 20:57:37,972 ----------------------------------------------------------------------------------------------------
2021-05-17 20:58:25,680 epoch 2 - iter 50/507 - loss 3.50461987 - samples/sec: 16.77 - lr: 0.100000
2021-05-17 20:59:09,989 epoch 2 - iter 100/507 - loss 3.56684094 - samples/sec: 18.06 - lr: 0.100000
2021-05-17 20:59:46,080 epoch 2 - iter 150/507 - loss 3.54926296 - samples/sec: 22.17 - lr: 0.100000
2021-05-17 21:00:29,260 epoch 2 - iter 200/507 - loss 3.53894655 - samples/sec: 18.53 - lr: 0.100000
2021-05-17 21:01:10,266 epoch 2 - iter 250/507 - loss 3.54343751 - samples/sec: 19.51 - lr: 0.100000
2021-05-17 21:01:47,690 epoch 2 - iter 300/507 - loss 3.52257412 - samples/sec: 21.38 - lr: 0.100000
2021-05-17 21:02:28,997 epoch 2 - iter 350/507 - loss 3.50678000 - samples/sec: 19.37 - lr: 0.100000
2021-05-17 21:03:07,872 epoch 2 - iter 400/507 - loss 3.50848666 - samples/sec: 20.58 - lr: 0.100000
2021-05-17 21:03:48,143 epoch 2 - iter 450/507 - loss 3.51247353 - samples/sec: 19.87 - lr: 0.100000
2021-05-17 21:04:30,084 epoch 2 - iter 500/507 - loss 3.50123344 - samples/sec: 19.08 - lr: 0.100000
2021-05-17 21:04:35,492 ----------------------------------------------------------------------------------------------------
2021-05-17 21:04:35,493 EPOCH 2 done: loss 3.5005 - lr 0.1000000
2021-05-17 21:04:55,507 DEV : loss 3.808274030685425 - score 0.0
2021-05-17 21:04:55,667 BAD EPOCHS (no improvement): 1
2021-05-17 21:04:55,667 ----------------------------------------------------------------------------------------------------
2021-05-17 21:05:38,005 epoch 3 - iter 50/507 - loss 3.62171867 - samples/sec: 18.90 - lr: 0.100000
2021-05-17 21:06:18,171 epoch 3 - iter 100/507 - loss 3.58175389 - samples/sec: 19.92 - lr: 0.100000
2021-05-17 21:07:00,642 epoch 3 - iter 150/507 - loss 3.54227257 - samples/sec: 18.84 - lr: 0.100000
2021-05-17 21:07:40,289 epoch 3 - iter 200/507 - loss 3.53291245 - samples/sec: 20.18 - lr: 0.100000
2021-05-17 21:08:21,880 epoch 3 - iter 250/507 - loss 3.53094312 - samples/sec: 19.24 - lr: 0.100000
2021-05-17 21:09:01,482 epoch 3 - iter 300/507 - loss 3.51509659 - samples/sec: 20.20 - lr: 0.100000
2021-05-17 21:09:38,692 epoch 3 - iter 350/507 - loss 3.51127611 - samples/sec: 21.50 - lr: 0.100000
2021-05-17 21:10:18,635 epoch 3 - iter 400/507 - loss 3.49198530 - samples/sec: 20.03 - lr: 0.100000
2021-05-17 21:10:58,814 epoch 3 - iter 450/507 - loss 3.47527111 - samples/sec: 19.91 - lr: 0.100000
2021-05-17 21:11:39,812 epoch 3 - iter 500/507 - loss 3.47580831 - samples/sec: 19.52 - lr: 0.100000
2021-05-17 21:11:44,388 ----------------------------------------------------------------------------------------------------
2021-05-17 21:11:44,388 EPOCH 3 done: loss 3.4758 - lr 0.1000000
2021-05-17 21:12:00,760 DEV : loss 3.509883403778076 - score 0.0
2021-05-17 21:12:00,924 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 21:12:12,285 ----------------------------------------------------------------------------------------------------
2021-05-17 21:12:50,114 epoch 4 - iter 50/507 - loss 3.57471687 - samples/sec: 21.15 - lr: 0.100000
2021-05-17 21:13:31,090 epoch 4 - iter 100/507 - loss 3.52214918 - samples/sec: 19.52 - lr: 0.100000
2021-05-17 21:14:12,769 epoch 4 - iter 150/507 - loss 3.52137159 - samples/sec: 19.20 - lr: 0.100000
2021-05-17 21:14:50,415 epoch 4 - iter 200/507 - loss 3.50715285 - samples/sec: 21.25 - lr: 0.100000
2021-05-17 21:15:30,535 epoch 4 - iter 250/507 - loss 3.50016919 - samples/sec: 19.94 - lr: 0.100000
2021-05-17 21:16:12,188 epoch 4 - iter 300/507 - loss 3.49555379 - samples/sec: 19.21 - lr: 0.100000
2021-05-17 21:16:51,056 epoch 4 - iter 350/507 - loss 3.48774909 - samples/sec: 20.58 - lr: 0.100000
2021-05-17 21:17:31,398 epoch 4 - iter 400/507 - loss 3.49823283 - samples/sec: 19.83 - lr: 0.100000
2021-05-17 21:18:11,699 epoch 4 - iter 450/507 - loss 3.49125609 - samples/sec: 19.85 - lr: 0.100000
2021-05-17 21:18:49,693 epoch 4 - iter 500/507 - loss 3.49334428 - samples/sec: 21.06 - lr: 0.100000
2021-05-17 21:18:54,805 ----------------------------------------------------------------------------------------------------
2021-05-17 21:18:54,805 EPOCH 4 done: loss 3.4881 - lr 0.1000000
2021-05-17 21:19:10,832 DEV : loss 4.246805191040039 - score 0.0
2021-05-17 21:19:10,993 BAD EPOCHS (no improvement): 1
2021-05-17 21:19:10,994 ----------------------------------------------------------------------------------------------------
2021-05-17 21:19:53,307 epoch 5 - iter 50/507 - loss 3.55589582 - samples/sec: 18.91 - lr: 0.100000
2021-05-17 21:20:31,550 epoch 5 - iter 100/507 - loss 3.46546983 - samples/sec: 20.92 - lr: 0.100000
2021-05-17 21:21:10,821 epoch 5 - iter 150/507 - loss 3.52573315 - samples/sec: 20.37 - lr: 0.100000
2021-05-17 21:21:49,596 epoch 5 - iter 200/507 - loss 3.51515192 - samples/sec: 20.63 - lr: 0.100000
2021-05-17 21:22:27,229 epoch 5 - iter 250/507 - loss 3.50977789 - samples/sec: 21.26 - lr: 0.100000
2021-05-17 21:23:02,701 epoch 5 - iter 300/507 - loss 3.49963160 - samples/sec: 22.56 - lr: 0.100000
2021-05-17 21:23:41,986 epoch 5 - iter 350/507 - loss 3.48856495 - samples/sec: 20.37 - lr: 0.100000
2021-05-17 21:24:20,653 epoch 5 - iter 400/507 - loss 3.48288760 - samples/sec: 20.69 - lr: 0.100000
2021-05-17 21:24:59,536 epoch 5 - iter 450/507 - loss 3.48042781 - samples/sec: 20.58 - lr: 0.100000
2021-05-17 21:25:36,240 epoch 5 - iter 500/507 - loss 3.47904868 - samples/sec: 21.80 - lr: 0.100000
2021-05-17 21:25:40,469 ----------------------------------------------------------------------------------------------------
2021-05-17 21:25:40,470 EPOCH 5 done: loss 3.4856 - lr 0.1000000
2021-05-17 21:25:57,632 DEV : loss 3.5928494930267334 - score 0.0
2021-05-17 21:25:57,829 BAD EPOCHS (no improvement): 2
2021-05-17 21:25:57,829 ----------------------------------------------------------------------------------------------------
2021-05-17 21:26:39,386 epoch 6 - iter 50/507 - loss 3.46751624 - samples/sec: 19.25 - lr: 0.100000
2021-05-17 21:27:19,187 epoch 6 - iter 100/507 - loss 3.49568579 - samples/sec: 20.10 - lr: 0.100000
2021-05-17 21:27:58,720 epoch 6 - iter 150/507 - loss 3.48018150 - samples/sec: 20.24 - lr: 0.100000
2021-05-17 21:28:39,129 epoch 6 - iter 200/507 - loss 3.48308177 - samples/sec: 19.80 - lr: 0.100000
2021-05-17 21:29:18,818 epoch 6 - iter 250/507 - loss 3.47034504 - samples/sec: 20.16 - lr: 0.100000
2021-05-17 21:30:00,086 epoch 6 - iter 300/507 - loss 3.47660111 - samples/sec: 19.39 - lr: 0.100000
2021-05-17 21:30:41,505 epoch 6 - iter 350/507 - loss 3.47867136 - samples/sec: 19.32 - lr: 0.100000
2021-05-17 21:31:19,342 epoch 6 - iter 400/507 - loss 3.47927529 - samples/sec: 21.14 - lr: 0.100000
2021-05-17 21:31:57,823 epoch 6 - iter 450/507 - loss 3.47114214 - samples/sec: 20.79 - lr: 0.100000
2021-05-17 21:32:39,080 epoch 6 - iter 500/507 - loss 3.47027605 - samples/sec: 19.39 - lr: 0.100000
2021-05-17 21:32:44,469 ----------------------------------------------------------------------------------------------------
2021-05-17 21:32:44,469 EPOCH 6 done: loss 3.4754 - lr 0.1000000
2021-05-17 21:33:02,519 DEV : loss 3.528590679168701 - score 0.0
2021-05-17 21:33:02,681 BAD EPOCHS (no improvement): 3
2021-05-17 21:33:02,682 ----------------------------------------------------------------------------------------------------
2021-05-17 21:33:42,630 epoch 7 - iter 50/507 - loss 3.56014782 - samples/sec: 20.03 - lr: 0.100000
2021-05-17 21:34:23,173 epoch 7 - iter 100/507 - loss 3.56205087 - samples/sec: 19.73 - lr: 0.100000
2021-05-17 21:35:04,218 epoch 7 - iter 150/507 - loss 3.54476617 - samples/sec: 19.49 - lr: 0.100000
2021-05-17 21:35:41,421 epoch 7 - iter 200/507 - loss 3.52341500 - samples/sec: 21.51 - lr: 0.100000
2021-05-17 21:36:21,936 epoch 7 - iter 250/507 - loss 3.49587793 - samples/sec: 19.75 - lr: 0.100000
2021-05-17 21:36:57,960 epoch 7 - iter 300/507 - loss 3.49035401 - samples/sec: 22.21 - lr: 0.100000
2021-05-17 21:37:36,769 epoch 7 - iter 350/507 - loss 3.49307385 - samples/sec: 20.62 - lr: 0.100000
2021-05-17 21:38:17,693 epoch 7 - iter 400/507 - loss 3.47471458 - samples/sec: 19.55 - lr: 0.100000
2021-05-17 21:38:55,988 epoch 7 - iter 450/507 - loss 3.47682897 - samples/sec: 20.89 - lr: 0.100000
2021-05-17 21:39:38,497 epoch 7 - iter 500/507 - loss 3.47314710 - samples/sec: 18.82 - lr: 0.100000
2021-05-17 21:39:43,479 ----------------------------------------------------------------------------------------------------
2021-05-17 21:39:43,479 EPOCH 7 done: loss 3.4710 - lr 0.1000000
2021-05-17 21:40:00,304 DEV : loss 3.595059394836426 - score 0.0
Epoch     7: reducing learning rate of group 0 to 5.0000e-02.
2021-05-17 21:40:00,466 BAD EPOCHS (no improvement): 4
2021-05-17 21:40:00,466 ----------------------------------------------------------------------------------------------------
2021-05-17 21:40:38,057 epoch 8 - iter 50/507 - loss 3.45200210 - samples/sec: 21.28 - lr: 0.050000
2021-05-17 21:41:14,601 epoch 8 - iter 100/507 - loss 3.40038068 - samples/sec: 21.89 - lr: 0.050000
2021-05-17 21:41:55,372 epoch 8 - iter 150/507 - loss 3.41030492 - samples/sec: 19.62 - lr: 0.050000
2021-05-17 21:42:37,604 epoch 8 - iter 200/507 - loss 3.40740182 - samples/sec: 18.94 - lr: 0.050000
2021-05-17 21:43:19,621 epoch 8 - iter 250/507 - loss 3.41319209 - samples/sec: 19.04 - lr: 0.050000
2021-05-17 21:44:01,272 epoch 8 - iter 300/507 - loss 3.41117879 - samples/sec: 19.21 - lr: 0.050000
2021-05-17 21:44:44,122 epoch 8 - iter 350/507 - loss 3.41244674 - samples/sec: 18.67 - lr: 0.050000
2021-05-17 21:45:24,462 epoch 8 - iter 400/507 - loss 3.41538082 - samples/sec: 19.83 - lr: 0.050000
2021-05-17 21:46:08,385 epoch 8 - iter 450/507 - loss 3.42395958 - samples/sec: 18.22 - lr: 0.050000
2021-05-17 21:46:49,456 epoch 8 - iter 500/507 - loss 3.43113240 - samples/sec: 19.48 - lr: 0.050000
2021-05-17 21:46:55,758 ----------------------------------------------------------------------------------------------------
2021-05-17 21:46:55,758 EPOCH 8 done: loss 3.4326 - lr 0.0500000
2021-05-17 21:47:12,313 DEV : loss 3.5259339809417725 - score 0.0
2021-05-17 21:47:12,475 BAD EPOCHS (no improvement): 1
2021-05-17 21:47:12,476 ----------------------------------------------------------------------------------------------------
2021-05-17 21:47:52,460 epoch 9 - iter 50/507 - loss 3.50378666 - samples/sec: 20.01 - lr: 0.050000
2021-05-17 21:48:34,804 epoch 9 - iter 100/507 - loss 3.46533435 - samples/sec: 18.89 - lr: 0.050000
2021-05-17 21:49:12,787 epoch 9 - iter 150/507 - loss 3.43861879 - samples/sec: 21.07 - lr: 0.050000
2021-05-17 21:49:54,035 epoch 9 - iter 200/507 - loss 3.43942533 - samples/sec: 19.40 - lr: 0.050000
2021-05-17 21:50:38,082 epoch 9 - iter 250/507 - loss 3.43604268 - samples/sec: 18.16 - lr: 0.050000
2021-05-17 21:51:19,517 epoch 9 - iter 300/507 - loss 3.44843937 - samples/sec: 19.31 - lr: 0.050000
2021-05-17 21:52:03,385 epoch 9 - iter 350/507 - loss 3.42587906 - samples/sec: 18.24 - lr: 0.050000
2021-05-17 21:52:48,598 epoch 9 - iter 400/507 - loss 3.43076741 - samples/sec: 17.70 - lr: 0.050000
2021-05-17 21:53:27,293 epoch 9 - iter 450/507 - loss 3.43543560 - samples/sec: 20.68 - lr: 0.050000
2021-05-17 21:54:08,738 epoch 9 - iter 500/507 - loss 3.43263280 - samples/sec: 19.30 - lr: 0.050000
2021-05-17 21:54:13,226 ----------------------------------------------------------------------------------------------------
2021-05-17 21:54:13,226 EPOCH 9 done: loss 3.4303 - lr 0.0500000
2021-05-17 21:54:28,295 DEV : loss 3.6023969650268555 - score 0.0
2021-05-17 21:54:28,464 BAD EPOCHS (no improvement): 2
2021-05-17 21:54:28,464 ----------------------------------------------------------------------------------------------------
2021-05-17 21:55:06,338 epoch 10 - iter 50/507 - loss 3.29315494 - samples/sec: 21.12 - lr: 0.050000
2021-05-17 21:55:45,718 epoch 10 - iter 100/507 - loss 3.39855018 - samples/sec: 20.32 - lr: 0.050000
2021-05-17 21:56:23,983 epoch 10 - iter 150/507 - loss 3.44220596 - samples/sec: 20.91 - lr: 0.050000
2021-05-17 21:57:13,433 epoch 10 - iter 200/507 - loss 3.39878441 - samples/sec: 16.18 - lr: 0.050000
2021-05-17 21:57:55,443 epoch 10 - iter 250/507 - loss 3.40574410 - samples/sec: 19.04 - lr: 0.050000
2021-05-17 21:58:34,095 epoch 10 - iter 300/507 - loss 3.40640132 - samples/sec: 20.70 - lr: 0.050000
2021-05-17 21:59:15,227 epoch 10 - iter 350/507 - loss 3.42596868 - samples/sec: 19.45 - lr: 0.050000
2021-05-17 21:59:59,948 epoch 10 - iter 400/507 - loss 3.42681958 - samples/sec: 17.89 - lr: 0.050000
2021-05-17 22:00:43,671 epoch 10 - iter 450/507 - loss 3.42553473 - samples/sec: 18.30 - lr: 0.050000
2021-05-17 22:01:27,219 epoch 10 - iter 500/507 - loss 3.42939383 - samples/sec: 18.37 - lr: 0.050000
2021-05-17 22:01:32,084 ----------------------------------------------------------------------------------------------------
2021-05-17 22:01:32,085 EPOCH 10 done: loss 3.4302 - lr 0.0500000
2021-05-17 22:01:49,048 DEV : loss 3.508666753768921 - score 0.0
2021-05-17 22:01:49,210 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 22:02:00,605 ----------------------------------------------------------------------------------------------------
2021-05-17 22:02:43,162 epoch 11 - iter 50/507 - loss 3.51631351 - samples/sec: 18.80 - lr: 0.050000
2021-05-17 22:03:23,317 epoch 11 - iter 100/507 - loss 3.41963025 - samples/sec: 19.92 - lr: 0.050000
2021-05-17 22:04:03,439 epoch 11 - iter 150/507 - loss 3.42800825 - samples/sec: 19.94 - lr: 0.050000
2021-05-17 22:04:45,303 epoch 11 - iter 200/507 - loss 3.46350782 - samples/sec: 19.11 - lr: 0.050000
2021-05-17 22:05:27,879 epoch 11 - iter 250/507 - loss 3.45654127 - samples/sec: 18.79 - lr: 0.050000
2021-05-17 22:06:05,478 epoch 11 - iter 300/507 - loss 3.44154129 - samples/sec: 21.28 - lr: 0.050000
2021-05-17 22:06:42,486 epoch 11 - iter 350/507 - loss 3.45070084 - samples/sec: 21.62 - lr: 0.050000
2021-05-17 22:07:23,304 epoch 11 - iter 400/507 - loss 3.42716983 - samples/sec: 19.60 - lr: 0.050000
2021-05-17 22:08:03,851 epoch 11 - iter 450/507 - loss 3.42784426 - samples/sec: 19.73 - lr: 0.050000
2021-05-17 22:08:44,194 epoch 11 - iter 500/507 - loss 3.42873808 - samples/sec: 19.83 - lr: 0.050000
2021-05-17 22:08:49,628 ----------------------------------------------------------------------------------------------------
2021-05-17 22:08:49,628 EPOCH 11 done: loss 3.4295 - lr 0.0500000
2021-05-17 22:09:06,045 DEV : loss 3.5047595500946045 - score 0.0
2021-05-17 22:09:06,207 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 22:09:17,099 ----------------------------------------------------------------------------------------------------
2021-05-17 22:10:01,805 epoch 12 - iter 50/507 - loss 3.45714056 - samples/sec: 17.90 - lr: 0.050000
2021-05-17 22:10:46,314 epoch 12 - iter 100/507 - loss 3.41279621 - samples/sec: 17.98 - lr: 0.050000
2021-05-17 22:11:26,285 epoch 12 - iter 150/507 - loss 3.43788843 - samples/sec: 20.02 - lr: 0.050000
2021-05-17 22:12:04,956 epoch 12 - iter 200/507 - loss 3.43169592 - samples/sec: 20.69 - lr: 0.050000
2021-05-17 22:12:51,758 epoch 12 - iter 250/507 - loss 3.43914957 - samples/sec: 17.09 - lr: 0.050000
2021-05-17 22:13:35,569 epoch 12 - iter 300/507 - loss 3.41504309 - samples/sec: 18.26 - lr: 0.050000
2021-05-17 22:14:15,460 epoch 12 - iter 350/507 - loss 3.42387899 - samples/sec: 20.06 - lr: 0.050000
2021-05-17 22:14:56,780 epoch 12 - iter 400/507 - loss 3.41984531 - samples/sec: 19.36 - lr: 0.050000
2021-05-17 22:15:44,105 epoch 12 - iter 450/507 - loss 3.42649529 - samples/sec: 16.91 - lr: 0.050000
2021-05-17 22:16:27,758 epoch 12 - iter 500/507 - loss 3.43344854 - samples/sec: 18.33 - lr: 0.050000
2021-05-17 22:16:32,381 ----------------------------------------------------------------------------------------------------
2021-05-17 22:16:32,391 EPOCH 12 done: loss 3.4313 - lr 0.0500000
2021-05-17 22:16:49,917 DEV : loss 3.5029678344726562 - score 0.0
2021-05-17 22:16:50,078 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 22:17:01,261 ----------------------------------------------------------------------------------------------------
2021-05-17 22:17:39,666 epoch 13 - iter 50/507 - loss 3.36373434 - samples/sec: 20.83 - lr: 0.050000
2021-05-17 22:18:18,006 epoch 13 - iter 100/507 - loss 3.43405476 - samples/sec: 20.87 - lr: 0.050000
2021-05-17 22:18:54,859 epoch 13 - iter 150/507 - loss 3.49474442 - samples/sec: 21.71 - lr: 0.050000
2021-05-17 22:19:34,218 epoch 13 - iter 200/507 - loss 3.46160812 - samples/sec: 20.33 - lr: 0.050000
2021-05-17 22:20:12,960 epoch 13 - iter 250/507 - loss 3.47233087 - samples/sec: 20.65 - lr: 0.050000
2021-05-17 22:20:54,360 epoch 13 - iter 300/507 - loss 3.44678826 - samples/sec: 19.33 - lr: 0.050000
2021-05-17 22:21:34,480 epoch 13 - iter 350/507 - loss 3.43656859 - samples/sec: 19.94 - lr: 0.050000
2021-05-17 22:22:19,465 epoch 13 - iter 400/507 - loss 3.43245499 - samples/sec: 17.78 - lr: 0.050000
2021-05-17 22:23:00,026 epoch 13 - iter 450/507 - loss 3.44163255 - samples/sec: 19.73 - lr: 0.050000
2021-05-17 22:23:38,568 epoch 13 - iter 500/507 - loss 3.43555464 - samples/sec: 20.76 - lr: 0.050000
2021-05-17 22:23:43,186 ----------------------------------------------------------------------------------------------------
2021-05-17 22:23:43,187 EPOCH 13 done: loss 3.4320 - lr 0.0500000
2021-05-17 22:23:59,540 DEV : loss 3.51118803024292 - score 0.0
2021-05-17 22:23:59,780 BAD EPOCHS (no improvement): 1
2021-05-17 22:23:59,781 ----------------------------------------------------------------------------------------------------
2021-05-17 22:24:35,867 epoch 14 - iter 50/507 - loss 3.52409796 - samples/sec: 22.17 - lr: 0.050000
2021-05-17 22:25:15,242 epoch 14 - iter 100/507 - loss 3.49984569 - samples/sec: 20.32 - lr: 0.050000
2021-05-17 22:25:53,141 epoch 14 - iter 150/507 - loss 3.46638762 - samples/sec: 21.11 - lr: 0.050000
2021-05-17 22:26:32,708 epoch 14 - iter 200/507 - loss 3.46117523 - samples/sec: 20.22 - lr: 0.050000
2021-05-17 22:27:13,077 epoch 14 - iter 250/507 - loss 3.45952998 - samples/sec: 19.82 - lr: 0.050000
2021-05-17 22:27:53,914 epoch 14 - iter 300/507 - loss 3.43615693 - samples/sec: 19.59 - lr: 0.050000
2021-05-17 22:28:34,249 epoch 14 - iter 350/507 - loss 3.43239760 - samples/sec: 19.84 - lr: 0.050000
2021-05-17 22:29:14,407 epoch 14 - iter 400/507 - loss 3.44380080 - samples/sec: 19.92 - lr: 0.050000
2021-05-17 22:29:51,190 epoch 14 - iter 450/507 - loss 3.43818871 - samples/sec: 21.75 - lr: 0.050000
2021-05-17 22:30:29,265 epoch 14 - iter 500/507 - loss 3.43238640 - samples/sec: 21.01 - lr: 0.050000
2021-05-17 22:30:33,920 ----------------------------------------------------------------------------------------------------
2021-05-17 22:30:33,921 EPOCH 14 done: loss 3.4307 - lr 0.0500000
2021-05-17 22:30:50,368 DEV : loss 3.5227508544921875 - score 0.0
2021-05-17 22:30:50,566 BAD EPOCHS (no improvement): 2
2021-05-17 22:30:50,567 ----------------------------------------------------------------------------------------------------
2021-05-17 22:31:29,588 epoch 15 - iter 50/507 - loss 3.41635297 - samples/sec: 20.50 - lr: 0.050000
2021-05-17 22:32:09,187 epoch 15 - iter 100/507 - loss 3.40225931 - samples/sec: 20.21 - lr: 0.050000
2021-05-17 22:32:48,642 epoch 15 - iter 150/507 - loss 3.39997420 - samples/sec: 20.28 - lr: 0.050000
2021-05-17 22:33:27,427 epoch 15 - iter 200/507 - loss 3.38952017 - samples/sec: 20.63 - lr: 0.050000
2021-05-17 22:34:05,931 epoch 15 - iter 250/507 - loss 3.38378234 - samples/sec: 20.78 - lr: 0.050000
2021-05-17 22:34:45,374 epoch 15 - iter 300/507 - loss 3.40790396 - samples/sec: 20.28 - lr: 0.050000
2021-05-17 22:35:27,377 epoch 15 - iter 350/507 - loss 3.41303596 - samples/sec: 19.05 - lr: 0.050000
2021-05-17 22:36:08,215 epoch 15 - iter 400/507 - loss 3.42293280 - samples/sec: 19.59 - lr: 0.050000
2021-05-17 22:36:45,639 epoch 15 - iter 450/507 - loss 3.43292900 - samples/sec: 21.38 - lr: 0.050000
2021-05-17 22:37:26,299 epoch 15 - iter 500/507 - loss 3.43365088 - samples/sec: 19.68 - lr: 0.050000
2021-05-17 22:37:30,928 ----------------------------------------------------------------------------------------------------
2021-05-17 22:37:30,928 EPOCH 15 done: loss 3.4299 - lr 0.0500000
2021-05-17 22:37:49,447 DEV : loss 3.543325901031494 - score 0.0
2021-05-17 22:37:49,614 BAD EPOCHS (no improvement): 3
2021-05-17 22:37:49,615 ----------------------------------------------------------------------------------------------------
2021-05-17 22:38:29,095 epoch 16 - iter 50/507 - loss 3.42154009 - samples/sec: 20.27 - lr: 0.050000
2021-05-17 22:39:08,932 epoch 16 - iter 100/507 - loss 3.36736894 - samples/sec: 20.08 - lr: 0.050000
2021-05-17 22:39:49,639 epoch 16 - iter 150/507 - loss 3.37571614 - samples/sec: 19.65 - lr: 0.050000
2021-05-17 22:40:32,416 epoch 16 - iter 200/507 - loss 3.38801561 - samples/sec: 18.70 - lr: 0.050000
2021-05-17 22:41:16,873 epoch 16 - iter 250/507 - loss 3.42741999 - samples/sec: 18.00 - lr: 0.050000
2021-05-17 22:42:01,351 epoch 16 - iter 300/507 - loss 3.43812695 - samples/sec: 17.99 - lr: 0.050000
2021-05-17 22:42:46,684 epoch 16 - iter 350/507 - loss 3.43054096 - samples/sec: 17.65 - lr: 0.050000
2021-05-17 22:43:35,153 epoch 16 - iter 400/507 - loss 3.43119217 - samples/sec: 16.51 - lr: 0.050000
2021-05-17 22:44:23,016 epoch 16 - iter 450/507 - loss 3.43656765 - samples/sec: 16.72 - lr: 0.050000
2021-05-17 22:45:06,534 epoch 16 - iter 500/507 - loss 3.43154056 - samples/sec: 18.39 - lr: 0.050000
2021-05-17 22:45:12,051 ----------------------------------------------------------------------------------------------------
2021-05-17 22:45:12,052 EPOCH 16 done: loss 3.4331 - lr 0.0500000
2021-05-17 22:45:29,380 DEV : loss 3.501904010772705 - score 0.0
2021-05-17 22:45:29,574 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 22:45:40,943 ----------------------------------------------------------------------------------------------------
2021-05-17 22:46:18,563 epoch 17 - iter 50/507 - loss 3.46128690 - samples/sec: 21.27 - lr: 0.050000
2021-05-17 22:46:57,071 epoch 17 - iter 100/507 - loss 3.49491340 - samples/sec: 20.78 - lr: 0.050000
2021-05-17 22:47:40,855 epoch 17 - iter 150/507 - loss 3.48293114 - samples/sec: 18.27 - lr: 0.050000
2021-05-17 22:48:17,328 epoch 17 - iter 200/507 - loss 3.50921682 - samples/sec: 21.94 - lr: 0.050000
2021-05-17 22:48:56,618 epoch 17 - iter 250/507 - loss 3.47208137 - samples/sec: 20.36 - lr: 0.050000
2021-05-17 22:49:39,390 epoch 17 - iter 300/507 - loss 3.47034631 - samples/sec: 18.71 - lr: 0.050000
2021-05-17 22:50:22,757 epoch 17 - iter 350/507 - loss 3.45649195 - samples/sec: 18.45 - lr: 0.050000
2021-05-17 22:51:08,058 epoch 17 - iter 400/507 - loss 3.43012374 - samples/sec: 17.66 - lr: 0.050000
2021-05-17 22:51:52,936 epoch 17 - iter 450/507 - loss 3.43758860 - samples/sec: 17.83 - lr: 0.050000
2021-05-17 22:52:37,941 epoch 17 - iter 500/507 - loss 3.43481557 - samples/sec: 17.78 - lr: 0.050000
2021-05-17 22:52:43,398 ----------------------------------------------------------------------------------------------------
2021-05-17 22:52:43,399 EPOCH 17 done: loss 3.4368 - lr 0.0500000
2021-05-17 22:53:03,524 DEV : loss 3.7618062496185303 - score 0.0
2021-05-17 22:53:03,687 BAD EPOCHS (no improvement): 1
2021-05-17 22:53:03,687 ----------------------------------------------------------------------------------------------------
2021-05-17 22:53:49,338 epoch 18 - iter 50/507 - loss 3.43297773 - samples/sec: 17.53 - lr: 0.050000
2021-05-17 22:54:34,840 epoch 18 - iter 100/507 - loss 3.38279338 - samples/sec: 17.58 - lr: 0.050000
2021-05-17 22:55:19,354 epoch 18 - iter 150/507 - loss 3.40013863 - samples/sec: 17.97 - lr: 0.050000
2021-05-17 22:56:05,277 epoch 18 - iter 200/507 - loss 3.41474265 - samples/sec: 17.42 - lr: 0.050000
2021-05-17 22:56:54,266 epoch 18 - iter 250/507 - loss 3.40237246 - samples/sec: 16.33 - lr: 0.050000
2021-05-17 22:57:45,467 epoch 18 - iter 300/507 - loss 3.41372016 - samples/sec: 15.63 - lr: 0.050000
2021-05-17 22:58:38,741 epoch 18 - iter 350/507 - loss 3.40951747 - samples/sec: 15.02 - lr: 0.050000
2021-05-17 22:59:29,626 epoch 18 - iter 400/507 - loss 3.41209952 - samples/sec: 15.72 - lr: 0.050000
2021-05-17 23:00:18,183 epoch 18 - iter 450/507 - loss 3.43343704 - samples/sec: 16.48 - lr: 0.050000
2021-05-17 23:01:02,473 epoch 18 - iter 500/507 - loss 3.42972707 - samples/sec: 18.07 - lr: 0.050000
2021-05-17 23:01:08,236 ----------------------------------------------------------------------------------------------------
2021-05-17 23:01:08,236 EPOCH 18 done: loss 3.4321 - lr 0.0500000
2021-05-17 23:01:30,980 DEV : loss 3.5252907276153564 - score 0.0
2021-05-17 23:01:31,145 BAD EPOCHS (no improvement): 2
2021-05-17 23:01:31,146 ----------------------------------------------------------------------------------------------------
2021-05-17 23:02:17,932 epoch 19 - iter 50/507 - loss 3.47629459 - samples/sec: 17.10 - lr: 0.050000
2021-05-17 23:03:09,309 epoch 19 - iter 100/507 - loss 3.41483169 - samples/sec: 15.57 - lr: 0.050000
2021-05-17 23:03:56,994 epoch 19 - iter 150/507 - loss 3.42374626 - samples/sec: 16.78 - lr: 0.050000
2021-05-17 23:04:44,715 epoch 19 - iter 200/507 - loss 3.44060524 - samples/sec: 16.77 - lr: 0.050000
2021-05-17 23:05:33,899 epoch 19 - iter 250/507 - loss 3.44365633 - samples/sec: 16.27 - lr: 0.050000
2021-05-17 23:06:23,048 epoch 19 - iter 300/507 - loss 3.44599332 - samples/sec: 16.28 - lr: 0.050000
2021-05-17 23:07:11,273 epoch 19 - iter 350/507 - loss 3.42889360 - samples/sec: 16.59 - lr: 0.050000
2021-05-17 23:08:00,399 epoch 19 - iter 400/507 - loss 3.43160421 - samples/sec: 16.29 - lr: 0.050000
2021-05-17 23:08:51,475 epoch 19 - iter 450/507 - loss 3.43802316 - samples/sec: 15.66 - lr: 0.050000
2021-05-17 23:09:35,918 epoch 19 - iter 500/507 - loss 3.43025048 - samples/sec: 18.00 - lr: 0.050000
2021-05-17 23:09:41,919 ----------------------------------------------------------------------------------------------------
2021-05-17 23:09:41,919 EPOCH 19 done: loss 3.4347 - lr 0.0500000
2021-05-17 23:10:04,243 DEV : loss 3.6832213401794434 - score 0.0
2021-05-17 23:10:04,496 BAD EPOCHS (no improvement): 3
2021-05-17 23:10:04,496 ----------------------------------------------------------------------------------------------------
2021-05-17 23:10:57,205 epoch 20 - iter 50/507 - loss 3.31900672 - samples/sec: 15.18 - lr: 0.050000
2021-05-17 23:11:48,491 epoch 20 - iter 100/507 - loss 3.37641326 - samples/sec: 15.60 - lr: 0.050000
2021-05-17 23:12:37,372 epoch 20 - iter 150/507 - loss 3.38219187 - samples/sec: 16.37 - lr: 0.050000
2021-05-17 23:13:26,708 epoch 20 - iter 200/507 - loss 3.43397134 - samples/sec: 16.22 - lr: 0.050000
2021-05-17 23:14:15,405 epoch 20 - iter 250/507 - loss 3.43154780 - samples/sec: 16.43 - lr: 0.050000
2021-05-17 23:15:04,139 epoch 20 - iter 300/507 - loss 3.43978260 - samples/sec: 16.42 - lr: 0.050000
2021-05-17 23:15:51,190 epoch 20 - iter 350/507 - loss 3.44354558 - samples/sec: 17.00 - lr: 0.050000
2021-05-17 23:16:40,581 epoch 20 - iter 400/507 - loss 3.43014830 - samples/sec: 16.20 - lr: 0.050000
2021-05-17 23:17:27,933 epoch 20 - iter 450/507 - loss 3.43569763 - samples/sec: 16.90 - lr: 0.050000
2021-05-17 23:18:14,479 epoch 20 - iter 500/507 - loss 3.43413442 - samples/sec: 17.19 - lr: 0.050000
2021-05-17 23:18:20,559 ----------------------------------------------------------------------------------------------------
2021-05-17 23:18:20,561 EPOCH 20 done: loss 3.4355 - lr 0.0500000
2021-05-17 23:18:44,869 DEV : loss 3.5078539848327637 - score 0.0
Epoch    20: reducing learning rate of group 0 to 2.5000e-02.
2021-05-17 23:18:45,033 BAD EPOCHS (no improvement): 4
2021-05-17 23:18:45,034 ----------------------------------------------------------------------------------------------------
2021-05-17 23:19:34,787 epoch 21 - iter 50/507 - loss 3.54090005 - samples/sec: 16.08 - lr: 0.025000
2021-05-17 23:20:19,272 epoch 21 - iter 100/507 - loss 3.45669009 - samples/sec: 17.98 - lr: 0.025000
2021-05-17 23:21:05,514 epoch 21 - iter 150/507 - loss 3.43960566 - samples/sec: 17.30 - lr: 0.025000
2021-05-17 23:21:50,549 epoch 21 - iter 200/507 - loss 3.47222587 - samples/sec: 17.77 - lr: 0.025000
2021-05-17 23:22:32,861 epoch 21 - iter 250/507 - loss 3.45263821 - samples/sec: 18.91 - lr: 0.025000
2021-05-17 23:23:13,564 epoch 21 - iter 300/507 - loss 3.45254893 - samples/sec: 19.66 - lr: 0.025000
2021-05-17 23:23:54,383 epoch 21 - iter 350/507 - loss 3.43584727 - samples/sec: 19.60 - lr: 0.025000
2021-05-17 23:24:35,295 epoch 21 - iter 400/507 - loss 3.43727120 - samples/sec: 19.56 - lr: 0.025000
2021-05-17 23:25:13,820 epoch 21 - iter 450/507 - loss 3.43249443 - samples/sec: 20.77 - lr: 0.025000
2021-05-17 23:25:54,256 epoch 21 - iter 500/507 - loss 3.42701919 - samples/sec: 19.79 - lr: 0.025000
2021-05-17 23:25:58,869 ----------------------------------------------------------------------------------------------------
2021-05-17 23:25:58,869 EPOCH 21 done: loss 3.4263 - lr 0.0250000
2021-05-17 23:26:16,186 DEV : loss 3.503167152404785 - score 0.0
2021-05-17 23:26:16,383 BAD EPOCHS (no improvement): 1
2021-05-17 23:26:16,384 ----------------------------------------------------------------------------------------------------
2021-05-17 23:26:59,375 epoch 22 - iter 50/507 - loss 3.37364385 - samples/sec: 18.61 - lr: 0.025000
2021-05-17 23:27:40,300 epoch 22 - iter 100/507 - loss 3.38963930 - samples/sec: 19.55 - lr: 0.025000
2021-05-17 23:28:20,793 epoch 22 - iter 150/507 - loss 3.38434655 - samples/sec: 19.76 - lr: 0.025000
2021-05-17 23:29:03,415 epoch 22 - iter 200/507 - loss 3.40613548 - samples/sec: 18.77 - lr: 0.025000
2021-05-17 23:29:43,120 epoch 22 - iter 250/507 - loss 3.40331955 - samples/sec: 20.15 - lr: 0.025000
2021-05-17 23:30:27,062 epoch 22 - iter 300/507 - loss 3.40379885 - samples/sec: 18.21 - lr: 0.025000
2021-05-17 23:31:07,950 epoch 22 - iter 350/507 - loss 3.41494117 - samples/sec: 19.57 - lr: 0.025000
2021-05-17 23:31:45,324 epoch 22 - iter 400/507 - loss 3.41165314 - samples/sec: 21.41 - lr: 0.025000
2021-05-17 23:32:26,870 epoch 22 - iter 450/507 - loss 3.42329570 - samples/sec: 19.26 - lr: 0.025000
2021-05-17 23:33:07,637 epoch 22 - iter 500/507 - loss 3.42144916 - samples/sec: 19.63 - lr: 0.025000
2021-05-17 23:33:12,466 ----------------------------------------------------------------------------------------------------
2021-05-17 23:33:12,467 EPOCH 22 done: loss 3.4258 - lr 0.0250000
2021-05-17 23:33:28,289 DEV : loss 3.5195999145507812 - score 0.0
2021-05-17 23:33:28,450 BAD EPOCHS (no improvement): 2
2021-05-17 23:33:28,451 ----------------------------------------------------------------------------------------------------
2021-05-17 23:34:12,077 epoch 23 - iter 50/507 - loss 3.43039482 - samples/sec: 18.34 - lr: 0.025000
2021-05-17 23:34:56,655 epoch 23 - iter 100/507 - loss 3.46000986 - samples/sec: 17.95 - lr: 0.025000
2021-05-17 23:35:35,334 epoch 23 - iter 150/507 - loss 3.47255558 - samples/sec: 20.69 - lr: 0.025000
2021-05-17 23:36:16,357 epoch 23 - iter 200/507 - loss 3.46241215 - samples/sec: 19.50 - lr: 0.025000
2021-05-17 23:37:00,160 epoch 23 - iter 250/507 - loss 3.43651962 - samples/sec: 18.26 - lr: 0.025000
2021-05-17 23:37:39,907 epoch 23 - iter 300/507 - loss 3.43618031 - samples/sec: 20.13 - lr: 0.025000
2021-05-17 23:38:21,180 epoch 23 - iter 350/507 - loss 3.42099742 - samples/sec: 19.38 - lr: 0.025000
2021-05-17 23:39:03,396 epoch 23 - iter 400/507 - loss 3.43172756 - samples/sec: 18.95 - lr: 0.025000
2021-05-17 23:39:49,254 epoch 23 - iter 450/507 - loss 3.43053584 - samples/sec: 17.45 - lr: 0.025000
2021-05-17 23:40:27,138 epoch 23 - iter 500/507 - loss 3.41866274 - samples/sec: 21.12 - lr: 0.025000
2021-05-17 23:40:31,901 ----------------------------------------------------------------------------------------------------
2021-05-17 23:40:31,901 EPOCH 23 done: loss 3.4243 - lr 0.0250000
2021-05-17 23:40:47,680 DEV : loss 3.5050792694091797 - score 0.0
2021-05-17 23:40:47,843 BAD EPOCHS (no improvement): 3
2021-05-17 23:40:47,843 ----------------------------------------------------------------------------------------------------
2021-05-17 23:41:28,059 epoch 24 - iter 50/507 - loss 3.45868023 - samples/sec: 19.89 - lr: 0.025000
2021-05-17 23:42:06,298 epoch 24 - iter 100/507 - loss 3.44677876 - samples/sec: 20.92 - lr: 0.025000
2021-05-17 23:42:45,354 epoch 24 - iter 150/507 - loss 3.44144510 - samples/sec: 20.49 - lr: 0.025000
2021-05-17 23:43:27,019 epoch 24 - iter 200/507 - loss 3.46213950 - samples/sec: 19.20 - lr: 0.025000
2021-05-17 23:44:08,262 epoch 24 - iter 250/507 - loss 3.45423728 - samples/sec: 19.40 - lr: 0.025000
2021-05-17 23:44:47,901 epoch 24 - iter 300/507 - loss 3.41912774 - samples/sec: 20.18 - lr: 0.025000
2021-05-17 23:45:26,942 epoch 24 - iter 350/507 - loss 3.41633718 - samples/sec: 20.49 - lr: 0.025000
2021-05-17 23:46:06,423 epoch 24 - iter 400/507 - loss 3.42851000 - samples/sec: 20.26 - lr: 0.025000
2021-05-17 23:46:48,376 epoch 24 - iter 450/507 - loss 3.42400810 - samples/sec: 19.08 - lr: 0.025000
2021-05-17 23:47:30,869 epoch 24 - iter 500/507 - loss 3.42209756 - samples/sec: 18.83 - lr: 0.025000
2021-05-17 23:47:35,488 ----------------------------------------------------------------------------------------------------
2021-05-17 23:47:35,488 EPOCH 24 done: loss 3.4235 - lr 0.0250000
2021-05-17 23:47:52,225 DEV : loss 3.50162935256958 - score 0.0
2021-05-17 23:47:52,438 BAD EPOCHS (no improvement): 0
saving best model
2021-05-17 23:48:03,332 ----------------------------------------------------------------------------------------------------
2021-05-17 23:48:42,844 epoch 25 - iter 50/507 - loss 3.40500177 - samples/sec: 20.25 - lr: 0.025000
2021-05-17 23:49:24,659 epoch 25 - iter 100/507 - loss 3.44275723 - samples/sec: 19.13 - lr: 0.025000
2021-05-17 23:50:06,117 epoch 25 - iter 150/507 - loss 3.41185311 - samples/sec: 19.30 - lr: 0.025000
2021-05-17 23:50:47,702 epoch 25 - iter 200/507 - loss 3.43633194 - samples/sec: 19.24 - lr: 0.025000
2021-05-17 23:51:26,878 epoch 25 - iter 250/507 - loss 3.42607859 - samples/sec: 20.42 - lr: 0.025000
2021-05-17 23:52:04,659 epoch 25 - iter 300/507 - loss 3.41689771 - samples/sec: 21.18 - lr: 0.025000
2021-05-17 23:52:45,143 epoch 25 - iter 350/507 - loss 3.42187360 - samples/sec: 19.76 - lr: 0.025000
2021-05-17 23:53:27,252 epoch 25 - iter 400/507 - loss 3.42776701 - samples/sec: 19.00 - lr: 0.025000
2021-05-17 23:54:05,782 epoch 25 - iter 450/507 - loss 3.42849370 - samples/sec: 20.76 - lr: 0.025000
2021-05-17 23:54:45,236 epoch 25 - iter 500/507 - loss 3.42208758 - samples/sec: 20.28 - lr: 0.025000
2021-05-17 23:54:50,007 ----------------------------------------------------------------------------------------------------
2021-05-17 23:54:50,007 EPOCH 25 done: loss 3.4255 - lr 0.0250000
2021-05-17 23:55:06,808 DEV : loss 3.506106376647949 - score 0.0
2021-05-17 23:55:06,971 BAD EPOCHS (no improvement): 1
2021-05-17 23:55:06,972 ----------------------------------------------------------------------------------------------------
2021-05-17 23:55:51,395 epoch 26 - iter 50/507 - loss 3.40174494 - samples/sec: 18.01 - lr: 0.025000
2021-05-17 23:56:32,148 epoch 26 - iter 100/507 - loss 3.44380440 - samples/sec: 19.63 - lr: 0.025000
2021-05-17 23:57:11,396 epoch 26 - iter 150/507 - loss 3.42477270 - samples/sec: 20.39 - lr: 0.025000
2021-05-17 23:57:51,171 epoch 26 - iter 200/507 - loss 3.41535527 - samples/sec: 20.12 - lr: 0.025000
2021-05-17 23:58:30,425 epoch 26 - iter 250/507 - loss 3.41572052 - samples/sec: 20.38 - lr: 0.025000
2021-05-17 23:59:11,444 epoch 26 - iter 300/507 - loss 3.41463568 - samples/sec: 19.50 - lr: 0.025000
2021-05-17 23:59:53,226 epoch 26 - iter 350/507 - loss 3.43157999 - samples/sec: 19.15 - lr: 0.025000
2021-05-18 00:00:33,631 epoch 26 - iter 400/507 - loss 3.42129743 - samples/sec: 19.80 - lr: 0.025000
2021-05-18 00:01:15,392 epoch 26 - iter 450/507 - loss 3.41952587 - samples/sec: 19.16 - lr: 0.025000
2021-05-18 00:01:53,574 epoch 26 - iter 500/507 - loss 3.42185386 - samples/sec: 20.95 - lr: 0.025000
2021-05-18 00:01:58,962 ----------------------------------------------------------------------------------------------------
2021-05-18 00:01:58,963 EPOCH 26 done: loss 3.4202 - lr 0.0250000
2021-05-18 00:02:16,456 DEV : loss 3.5560200214385986 - score 0.0
2021-05-18 00:02:16,620 BAD EPOCHS (no improvement): 2
2021-05-18 00:02:16,621 ----------------------------------------------------------------------------------------------------
2021-05-18 00:02:57,712 epoch 27 - iter 50/507 - loss 3.47805234 - samples/sec: 19.47 - lr: 0.025000
2021-05-18 00:03:35,740 epoch 27 - iter 100/507 - loss 3.48535350 - samples/sec: 21.04 - lr: 0.025000
2021-05-18 00:04:19,056 epoch 27 - iter 150/507 - loss 3.48321999 - samples/sec: 18.47 - lr: 0.025000
2021-05-18 00:05:01,964 epoch 27 - iter 200/507 - loss 3.47771772 - samples/sec: 18.65 - lr: 0.025000
2021-05-18 00:05:45,732 epoch 27 - iter 250/507 - loss 3.45152512 - samples/sec: 18.28 - lr: 0.025000
2021-05-18 00:06:25,363 epoch 27 - iter 300/507 - loss 3.43874877 - samples/sec: 20.19 - lr: 0.025000
2021-05-18 00:07:04,403 epoch 27 - iter 350/507 - loss 3.43070811 - samples/sec: 20.49 - lr: 0.025000
2021-05-18 00:07:44,645 epoch 27 - iter 400/507 - loss 3.42431692 - samples/sec: 19.88 - lr: 0.025000
2021-05-18 00:08:25,894 epoch 27 - iter 450/507 - loss 3.42228563 - samples/sec: 19.40 - lr: 0.025000
2021-05-18 00:09:06,159 epoch 27 - iter 500/507 - loss 3.42698391 - samples/sec: 19.87 - lr: 0.025000
2021-05-18 00:09:11,720 ----------------------------------------------------------------------------------------------------
2021-05-18 00:09:11,722 EPOCH 27 done: loss 3.4219 - lr 0.0250000
2021-05-18 00:09:29,997 DEV : loss 3.5051209926605225 - score 0.0
2021-05-18 00:09:30,171 BAD EPOCHS (no improvement): 3
2021-05-18 00:09:30,171 ----------------------------------------------------------------------------------------------------
2021-05-18 00:10:14,680 epoch 28 - iter 50/507 - loss 3.42052853 - samples/sec: 17.98 - lr: 0.025000
2021-05-18 00:10:54,804 epoch 28 - iter 100/507 - loss 3.45051638 - samples/sec: 19.94 - lr: 0.025000
2021-05-18 00:11:34,177 epoch 28 - iter 150/507 - loss 3.45989935 - samples/sec: 20.32 - lr: 0.025000
2021-05-18 00:12:13,987 epoch 28 - iter 200/507 - loss 3.45183851 - samples/sec: 20.10 - lr: 0.025000
2021-05-18 00:12:53,982 epoch 28 - iter 250/507 - loss 3.44284499 - samples/sec: 20.01 - lr: 0.025000
2021-05-18 00:13:35,004 epoch 28 - iter 300/507 - loss 3.44560222 - samples/sec: 19.50 - lr: 0.025000
2021-05-18 00:14:12,843 epoch 28 - iter 350/507 - loss 3.43448232 - samples/sec: 21.14 - lr: 0.025000
2021-05-18 00:14:53,612 epoch 28 - iter 400/507 - loss 3.43298261 - samples/sec: 19.62 - lr: 0.025000
2021-05-18 00:15:35,130 epoch 28 - iter 450/507 - loss 3.42582931 - samples/sec: 19.27 - lr: 0.025000
2021-05-18 00:16:15,882 epoch 28 - iter 500/507 - loss 3.42441284 - samples/sec: 19.63 - lr: 0.025000
2021-05-18 00:16:20,893 ----------------------------------------------------------------------------------------------------
2021-05-18 00:16:20,893 EPOCH 28 done: loss 3.4238 - lr 0.0250000
2021-05-18 00:16:39,494 DEV : loss 3.5147948265075684 - score 0.0
Epoch    28: reducing learning rate of group 0 to 1.2500e-02.
2021-05-18 00:16:39,672 BAD EPOCHS (no improvement): 4
2021-05-18 00:16:39,672 ----------------------------------------------------------------------------------------------------
2021-05-18 00:17:15,673 epoch 29 - iter 50/507 - loss 3.44143930 - samples/sec: 22.22 - lr: 0.012500
2021-05-18 00:17:57,821 epoch 29 - iter 100/507 - loss 3.43337102 - samples/sec: 18.98 - lr: 0.012500
2021-05-18 00:18:38,265 epoch 29 - iter 150/507 - loss 3.46426165 - samples/sec: 19.78 - lr: 0.012500
2021-05-18 00:19:20,358 epoch 29 - iter 200/507 - loss 3.43158583 - samples/sec: 19.01 - lr: 0.012500
2021-05-18 00:20:01,570 epoch 29 - iter 250/507 - loss 3.42797584 - samples/sec: 19.41 - lr: 0.012500
2021-05-18 00:20:42,512 epoch 29 - iter 300/507 - loss 3.41653942 - samples/sec: 19.54 - lr: 0.012500
2021-05-18 00:21:21,078 epoch 29 - iter 350/507 - loss 3.40738923 - samples/sec: 20.75 - lr: 0.012500
2021-05-18 00:22:02,221 epoch 29 - iter 400/507 - loss 3.41796291 - samples/sec: 19.45 - lr: 0.012500
2021-05-18 00:22:39,448 epoch 29 - iter 450/507 - loss 3.41234309 - samples/sec: 21.49 - lr: 0.012500
2021-05-18 00:23:19,086 epoch 29 - iter 500/507 - loss 3.41477194 - samples/sec: 20.18 - lr: 0.012500
2021-05-18 00:23:23,725 ----------------------------------------------------------------------------------------------------
2021-05-18 00:23:23,725 EPOCH 29 done: loss 3.4203 - lr 0.0125000
2021-05-18 00:23:40,653 DEV : loss 3.507077217102051 - score 0.0
2021-05-18 00:23:40,816 BAD EPOCHS (no improvement): 1
2021-05-18 00:23:40,817 ----------------------------------------------------------------------------------------------------
2021-05-18 00:24:21,724 epoch 30 - iter 50/507 - loss 3.44458429 - samples/sec: 19.56 - lr: 0.012500
2021-05-18 00:25:03,301 epoch 30 - iter 100/507 - loss 3.47890459 - samples/sec: 19.24 - lr: 0.012500
2021-05-18 00:25:43,193 epoch 30 - iter 150/507 - loss 3.44449983 - samples/sec: 20.06 - lr: 0.012500
2021-05-18 00:26:19,876 epoch 30 - iter 200/507 - loss 3.43129150 - samples/sec: 21.81 - lr: 0.012500
2021-05-18 00:27:00,372 epoch 30 - iter 250/507 - loss 3.42954649 - samples/sec: 19.76 - lr: 0.012500
2021-05-18 00:27:36,709 epoch 30 - iter 300/507 - loss 3.41738130 - samples/sec: 22.02 - lr: 0.012500
2021-05-18 00:28:15,886 epoch 30 - iter 350/507 - loss 3.42049696 - samples/sec: 20.42 - lr: 0.012500
2021-05-18 00:28:55,005 epoch 30 - iter 400/507 - loss 3.41983346 - samples/sec: 20.45 - lr: 0.012500
2021-05-18 00:29:33,622 epoch 30 - iter 450/507 - loss 3.41724855 - samples/sec: 20.72 - lr: 0.012500
2021-05-18 00:30:11,234 epoch 30 - iter 500/507 - loss 3.41266246 - samples/sec: 21.27 - lr: 0.012500
2021-05-18 00:30:16,763 ----------------------------------------------------------------------------------------------------
2021-05-18 00:30:16,763 EPOCH 30 done: loss 3.4172 - lr 0.0125000
2021-05-18 00:30:33,363 DEV : loss 3.502173900604248 - score 0.0
2021-05-18 00:30:33,525 BAD EPOCHS (no improvement): 2
2021-05-18 00:30:45,167 ----------------------------------------------------------------------------------------------------
2021-05-18 00:30:45,168 Testing using best model ...
2021-05-18 00:30:45,168 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/eng.rst.rstdt/best-model.pt
2021-05-18 00:32:02,731 0.0000	0.0000	0.0000
2021-05-18 00:32:02,731 
Results:
- F1-score (micro) 0.0000
- F1-score (macro) 0.0000

By class:
SENT       tp: 0 - fp: 0 - fn: 929 - precision: 0.0000 - recall: 0.0000 - f1-score: 0.0000
2021-05-18 00:32:02,731 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc/
2021-05-18 00:32:02,768 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc
2021-05-18 00:32:02,768 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc/sent_train.txt
2021-05-18 00:32:02,771 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc/sent_dev.txt
2021-05-18 00:32:02,772 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc/sent_test.txt
Corpus: 1343 train + 158 dev + 161 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-18 00:32:22,265 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,270 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-18 00:32:22,275 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,275 Corpus: "Corpus: 1343 train + 158 dev + 161 test sentences"
2021-05-18 00:32:22,276 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,276 Parameters:
2021-05-18 00:32:22,276  - learning_rate: "0.1"
2021-05-18 00:32:22,276  - mini_batch_size: "16"
2021-05-18 00:32:22,276  - patience: "3"
2021-05-18 00:32:22,276  - anneal_factor: "0.5"
2021-05-18 00:32:22,276  - max_epochs: "30"
2021-05-18 00:32:22,276  - shuffle: "True"
2021-05-18 00:32:22,276  - train_with_dev: "False"
2021-05-18 00:32:22,277  - batch_growth_annealing: "False"
2021-05-18 00:32:22,277 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,277 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc"
2021-05-18 00:32:22,277 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,277 Device: cuda:0
2021-05-18 00:32:22,277 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:22,277 Embeddings storage mode: cpu
2021-05-18 00:32:22,280 ----------------------------------------------------------------------------------------------------
2021-05-18 00:32:37,374 epoch 1 - iter 8/84 - loss 13.85782897 - samples/sec: 8.48 - lr: 0.100000
2021-05-18 00:32:51,445 epoch 1 - iter 16/84 - loss 10.63702855 - samples/sec: 9.10 - lr: 0.100000
2021-05-18 00:33:06,136 epoch 1 - iter 24/84 - loss 9.44750879 - samples/sec: 8.71 - lr: 0.100000
2021-05-18 00:33:19,036 epoch 1 - iter 32/84 - loss 8.59233999 - samples/sec: 9.93 - lr: 0.100000
2021-05-18 00:33:31,672 epoch 1 - iter 40/84 - loss 7.87548328 - samples/sec: 10.13 - lr: 0.100000
2021-05-18 00:33:45,965 epoch 1 - iter 48/84 - loss 7.25120436 - samples/sec: 8.96 - lr: 0.100000
2021-05-18 00:33:59,844 epoch 1 - iter 56/84 - loss 6.80050996 - samples/sec: 9.22 - lr: 0.100000
2021-05-18 00:34:14,296 epoch 1 - iter 64/84 - loss 6.40843888 - samples/sec: 8.86 - lr: 0.100000
2021-05-18 00:34:29,369 epoch 1 - iter 72/84 - loss 6.05921784 - samples/sec: 8.49 - lr: 0.100000
2021-05-18 00:34:43,813 epoch 1 - iter 80/84 - loss 5.74112796 - samples/sec: 8.86 - lr: 0.100000
2021-05-18 00:34:50,412 ----------------------------------------------------------------------------------------------------
2021-05-18 00:34:50,413 EPOCH 1 done: loss 5.5346 - lr 0.1000000
2021-05-18 00:35:03,623 DEV : loss 3.809072256088257 - score 0.6327
2021-05-18 00:35:03,653 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:35:14,613 ----------------------------------------------------------------------------------------------------
2021-05-18 00:35:20,557 epoch 2 - iter 8/84 - loss 2.85438980 - samples/sec: 21.54 - lr: 0.100000
2021-05-18 00:35:27,374 epoch 2 - iter 16/84 - loss 2.13924802 - samples/sec: 18.78 - lr: 0.100000
2021-05-18 00:35:33,916 epoch 2 - iter 24/84 - loss 1.91531908 - samples/sec: 19.57 - lr: 0.100000
2021-05-18 00:35:40,216 epoch 2 - iter 32/84 - loss 1.91617531 - samples/sec: 20.32 - lr: 0.100000
2021-05-18 00:35:46,676 epoch 2 - iter 40/84 - loss 1.92685897 - samples/sec: 19.82 - lr: 0.100000
2021-05-18 00:35:52,851 epoch 2 - iter 48/84 - loss 1.93948176 - samples/sec: 20.73 - lr: 0.100000
2021-05-18 00:35:59,444 epoch 2 - iter 56/84 - loss 1.88114074 - samples/sec: 19.42 - lr: 0.100000
2021-05-18 00:36:05,142 epoch 2 - iter 64/84 - loss 1.84698773 - samples/sec: 22.47 - lr: 0.100000
2021-05-18 00:36:11,430 epoch 2 - iter 72/84 - loss 1.79936361 - samples/sec: 20.36 - lr: 0.100000
2021-05-18 00:36:17,455 epoch 2 - iter 80/84 - loss 1.73460102 - samples/sec: 21.25 - lr: 0.100000
2021-05-18 00:36:20,312 ----------------------------------------------------------------------------------------------------
2021-05-18 00:36:20,312 EPOCH 2 done: loss 1.7090 - lr 0.1000000
2021-05-18 00:36:23,776 DEV : loss 0.49908390641212463 - score 0.927
2021-05-18 00:36:23,806 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:36:34,510 ----------------------------------------------------------------------------------------------------
2021-05-18 00:36:40,872 epoch 3 - iter 8/84 - loss 1.09842210 - samples/sec: 20.13 - lr: 0.100000
2021-05-18 00:36:46,379 epoch 3 - iter 16/84 - loss 1.26750976 - samples/sec: 23.25 - lr: 0.100000
2021-05-18 00:36:52,334 epoch 3 - iter 24/84 - loss 1.24747527 - samples/sec: 21.50 - lr: 0.100000
2021-05-18 00:36:58,596 epoch 3 - iter 32/84 - loss 1.31506669 - samples/sec: 20.44 - lr: 0.100000
2021-05-18 00:37:05,365 epoch 3 - iter 40/84 - loss 1.30665684 - samples/sec: 18.91 - lr: 0.100000
2021-05-18 00:37:11,596 epoch 3 - iter 48/84 - loss 1.23848615 - samples/sec: 20.54 - lr: 0.100000
2021-05-18 00:37:18,253 epoch 3 - iter 56/84 - loss 1.18267979 - samples/sec: 19.23 - lr: 0.100000
2021-05-18 00:37:26,234 epoch 3 - iter 64/84 - loss 1.19642724 - samples/sec: 16.04 - lr: 0.100000
2021-05-18 00:37:33,706 epoch 3 - iter 72/84 - loss 1.17145035 - samples/sec: 17.13 - lr: 0.100000
2021-05-18 00:37:41,019 epoch 3 - iter 80/84 - loss 1.14849607 - samples/sec: 17.50 - lr: 0.100000
2021-05-18 00:37:44,645 ----------------------------------------------------------------------------------------------------
2021-05-18 00:37:44,646 EPOCH 3 done: loss 1.1325 - lr 0.1000000
2021-05-18 00:37:47,675 DEV : loss 0.7546796202659607 - score 0.9067
2021-05-18 00:37:47,705 BAD EPOCHS (no improvement): 1
2021-05-18 00:37:47,705 ----------------------------------------------------------------------------------------------------
2021-05-18 00:37:53,753 epoch 4 - iter 8/84 - loss 0.81638166 - samples/sec: 21.17 - lr: 0.100000
2021-05-18 00:37:59,881 epoch 4 - iter 16/84 - loss 0.85553764 - samples/sec: 20.89 - lr: 0.100000
2021-05-18 00:38:06,238 epoch 4 - iter 24/84 - loss 0.97693978 - samples/sec: 20.14 - lr: 0.100000
2021-05-18 00:38:12,950 epoch 4 - iter 32/84 - loss 0.96024215 - samples/sec: 19.08 - lr: 0.100000
2021-05-18 00:38:18,826 epoch 4 - iter 40/84 - loss 1.02380170 - samples/sec: 21.79 - lr: 0.100000
2021-05-18 00:38:24,894 epoch 4 - iter 48/84 - loss 1.04717179 - samples/sec: 21.10 - lr: 0.100000
2021-05-18 00:38:32,048 epoch 4 - iter 56/84 - loss 1.04975301 - samples/sec: 17.93 - lr: 0.100000
2021-05-18 00:38:38,156 epoch 4 - iter 64/84 - loss 1.04935329 - samples/sec: 20.96 - lr: 0.100000
2021-05-18 00:38:45,000 epoch 4 - iter 72/84 - loss 1.03402220 - samples/sec: 18.70 - lr: 0.100000
2021-05-18 00:38:51,974 epoch 4 - iter 80/84 - loss 1.04674533 - samples/sec: 18.36 - lr: 0.100000
2021-05-18 00:38:55,559 ----------------------------------------------------------------------------------------------------
2021-05-18 00:38:55,562 EPOCH 4 done: loss 1.0471 - lr 0.1000000
2021-05-18 00:38:58,861 DEV : loss 0.9823707938194275 - score 0.8794
2021-05-18 00:38:58,892 BAD EPOCHS (no improvement): 2
2021-05-18 00:38:58,893 ----------------------------------------------------------------------------------------------------
2021-05-18 00:39:05,450 epoch 5 - iter 8/84 - loss 1.04096539 - samples/sec: 19.52 - lr: 0.100000
2021-05-18 00:39:12,893 epoch 5 - iter 16/84 - loss 1.19451494 - samples/sec: 17.20 - lr: 0.100000
2021-05-18 00:39:19,226 epoch 5 - iter 24/84 - loss 1.12769376 - samples/sec: 20.22 - lr: 0.100000
2021-05-18 00:39:26,599 epoch 5 - iter 32/84 - loss 1.00344399 - samples/sec: 17.37 - lr: 0.100000
2021-05-18 00:39:33,418 epoch 5 - iter 40/84 - loss 1.08472201 - samples/sec: 18.78 - lr: 0.100000
2021-05-18 00:39:39,941 epoch 5 - iter 48/84 - loss 1.05963202 - samples/sec: 19.63 - lr: 0.100000
2021-05-18 00:39:46,587 epoch 5 - iter 56/84 - loss 1.02528411 - samples/sec: 19.26 - lr: 0.100000
2021-05-18 00:39:52,888 epoch 5 - iter 64/84 - loss 1.03895903 - samples/sec: 20.32 - lr: 0.100000
2021-05-18 00:40:00,594 epoch 5 - iter 72/84 - loss 1.04169783 - samples/sec: 16.61 - lr: 0.100000
2021-05-18 00:40:07,459 epoch 5 - iter 80/84 - loss 1.01684698 - samples/sec: 18.65 - lr: 0.100000
2021-05-18 00:40:10,832 ----------------------------------------------------------------------------------------------------
2021-05-18 00:40:10,833 EPOCH 5 done: loss 1.0062 - lr 0.1000000
2021-05-18 00:40:14,656 DEV : loss 0.5554642081260681 - score 0.9294
2021-05-18 00:40:14,692 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:40:25,486 ----------------------------------------------------------------------------------------------------
2021-05-18 00:40:32,360 epoch 6 - iter 8/84 - loss 1.03187668 - samples/sec: 18.63 - lr: 0.100000
2021-05-18 00:40:39,344 epoch 6 - iter 16/84 - loss 0.97234894 - samples/sec: 18.33 - lr: 0.100000
2021-05-18 00:40:44,893 epoch 6 - iter 24/84 - loss 0.98065218 - samples/sec: 23.07 - lr: 0.100000
2021-05-18 00:40:50,961 epoch 6 - iter 32/84 - loss 0.92004773 - samples/sec: 21.10 - lr: 0.100000
2021-05-18 00:40:57,168 epoch 6 - iter 40/84 - loss 0.90832036 - samples/sec: 20.63 - lr: 0.100000
2021-05-18 00:41:03,368 epoch 6 - iter 48/84 - loss 0.89791419 - samples/sec: 20.65 - lr: 0.100000
2021-05-18 00:41:09,519 epoch 6 - iter 56/84 - loss 0.87381588 - samples/sec: 20.81 - lr: 0.100000
2021-05-18 00:41:16,317 epoch 6 - iter 64/84 - loss 0.85230576 - samples/sec: 18.83 - lr: 0.100000
2021-05-18 00:41:22,483 epoch 6 - iter 72/84 - loss 0.86270947 - samples/sec: 20.76 - lr: 0.100000
2021-05-18 00:41:29,081 epoch 6 - iter 80/84 - loss 0.90604266 - samples/sec: 19.40 - lr: 0.100000
2021-05-18 00:41:32,710 ----------------------------------------------------------------------------------------------------
2021-05-18 00:41:32,711 EPOCH 6 done: loss 0.8959 - lr 0.1000000
2021-05-18 00:41:35,840 DEV : loss 0.5062841773033142 - score 0.9175
2021-05-18 00:41:35,870 BAD EPOCHS (no improvement): 1
2021-05-18 00:41:35,870 ----------------------------------------------------------------------------------------------------
2021-05-18 00:41:42,578 epoch 7 - iter 8/84 - loss 0.69241077 - samples/sec: 19.09 - lr: 0.100000
2021-05-18 00:41:49,570 epoch 7 - iter 16/84 - loss 0.75107318 - samples/sec: 18.31 - lr: 0.100000
2021-05-18 00:41:56,360 epoch 7 - iter 24/84 - loss 0.74858514 - samples/sec: 18.85 - lr: 0.100000
2021-05-18 00:42:02,898 epoch 7 - iter 32/84 - loss 0.79009728 - samples/sec: 19.58 - lr: 0.100000
2021-05-18 00:42:09,229 epoch 7 - iter 40/84 - loss 0.82318643 - samples/sec: 20.22 - lr: 0.100000
2021-05-18 00:42:15,055 epoch 7 - iter 48/84 - loss 0.81761083 - samples/sec: 21.97 - lr: 0.100000
2021-05-18 00:42:22,209 epoch 7 - iter 56/84 - loss 0.82917573 - samples/sec: 17.90 - lr: 0.100000
2021-05-18 00:42:28,270 epoch 7 - iter 64/84 - loss 0.86696836 - samples/sec: 21.12 - lr: 0.100000
2021-05-18 00:42:34,757 epoch 7 - iter 72/84 - loss 0.87514691 - samples/sec: 19.74 - lr: 0.100000
2021-05-18 00:42:42,632 epoch 7 - iter 80/84 - loss 0.88755987 - samples/sec: 16.26 - lr: 0.100000
2021-05-18 00:42:46,156 ----------------------------------------------------------------------------------------------------
2021-05-18 00:42:46,157 EPOCH 7 done: loss 0.8940 - lr 0.1000000
2021-05-18 00:42:49,555 DEV : loss 0.4147736132144928 - score 0.9484
2021-05-18 00:42:49,585 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:43:00,097 ----------------------------------------------------------------------------------------------------
2021-05-18 00:43:07,398 epoch 8 - iter 8/84 - loss 0.88308235 - samples/sec: 17.54 - lr: 0.100000
2021-05-18 00:43:13,862 epoch 8 - iter 16/84 - loss 0.90193219 - samples/sec: 19.81 - lr: 0.100000
2021-05-18 00:43:19,396 epoch 8 - iter 24/84 - loss 0.85757646 - samples/sec: 23.13 - lr: 0.100000
2021-05-18 00:43:25,146 epoch 8 - iter 32/84 - loss 0.86542246 - samples/sec: 22.26 - lr: 0.100000
2021-05-18 00:43:31,469 epoch 8 - iter 40/84 - loss 0.79892826 - samples/sec: 20.26 - lr: 0.100000
2021-05-18 00:43:38,461 epoch 8 - iter 48/84 - loss 0.79051904 - samples/sec: 18.31 - lr: 0.100000
2021-05-18 00:43:45,744 epoch 8 - iter 56/84 - loss 0.78286266 - samples/sec: 17.58 - lr: 0.100000
2021-05-18 00:43:53,255 epoch 8 - iter 64/84 - loss 0.77542885 - samples/sec: 17.04 - lr: 0.100000
2021-05-18 00:44:00,458 epoch 8 - iter 72/84 - loss 0.78944143 - samples/sec: 17.78 - lr: 0.100000
2021-05-18 00:44:07,696 epoch 8 - iter 80/84 - loss 0.79920608 - samples/sec: 17.71 - lr: 0.100000
2021-05-18 00:44:10,730 ----------------------------------------------------------------------------------------------------
2021-05-18 00:44:10,733 EPOCH 8 done: loss 0.8045 - lr 0.1000000
2021-05-18 00:44:15,059 DEV : loss 0.3732416331768036 - score 0.9476
2021-05-18 00:44:15,090 BAD EPOCHS (no improvement): 1
2021-05-18 00:44:15,090 ----------------------------------------------------------------------------------------------------
2021-05-18 00:44:22,150 epoch 9 - iter 8/84 - loss 0.71999267 - samples/sec: 18.13 - lr: 0.100000
2021-05-18 00:44:29,354 epoch 9 - iter 16/84 - loss 0.66019144 - samples/sec: 17.80 - lr: 0.100000
2021-05-18 00:44:36,275 epoch 9 - iter 24/84 - loss 0.76384471 - samples/sec: 18.50 - lr: 0.100000
2021-05-18 00:44:41,842 epoch 9 - iter 32/84 - loss 0.79542074 - samples/sec: 23.00 - lr: 0.100000
2021-05-18 00:44:47,547 epoch 9 - iter 40/84 - loss 0.80432773 - samples/sec: 22.44 - lr: 0.100000
2021-05-18 00:44:54,018 epoch 9 - iter 48/84 - loss 0.79926884 - samples/sec: 19.79 - lr: 0.100000
2021-05-18 00:45:00,103 epoch 9 - iter 56/84 - loss 0.80686295 - samples/sec: 21.04 - lr: 0.100000
2021-05-18 00:45:06,734 epoch 9 - iter 64/84 - loss 0.77609456 - samples/sec: 19.31 - lr: 0.100000
2021-05-18 00:45:12,524 epoch 9 - iter 72/84 - loss 0.74746849 - samples/sec: 22.11 - lr: 0.100000
2021-05-18 00:45:18,571 epoch 9 - iter 80/84 - loss 0.74447274 - samples/sec: 21.18 - lr: 0.100000
2021-05-18 00:45:21,929 ----------------------------------------------------------------------------------------------------
2021-05-18 00:45:21,930 EPOCH 9 done: loss 0.7640 - lr 0.1000000
2021-05-18 00:45:25,051 DEV : loss 0.4105165898799896 - score 0.9412
2021-05-18 00:45:25,080 BAD EPOCHS (no improvement): 2
2021-05-18 00:45:25,081 ----------------------------------------------------------------------------------------------------
2021-05-18 00:45:31,373 epoch 10 - iter 8/84 - loss 0.55580964 - samples/sec: 20.35 - lr: 0.100000
2021-05-18 00:45:37,689 epoch 10 - iter 16/84 - loss 0.67937847 - samples/sec: 20.27 - lr: 0.100000
2021-05-18 00:45:44,602 epoch 10 - iter 24/84 - loss 0.65535600 - samples/sec: 18.52 - lr: 0.100000
2021-05-18 00:45:50,255 epoch 10 - iter 32/84 - loss 0.67066134 - samples/sec: 22.65 - lr: 0.100000
2021-05-18 00:45:56,140 epoch 10 - iter 40/84 - loss 0.64799154 - samples/sec: 21.75 - lr: 0.100000
2021-05-18 00:46:02,439 epoch 10 - iter 48/84 - loss 0.66818911 - samples/sec: 20.33 - lr: 0.100000
2021-05-18 00:46:10,035 epoch 10 - iter 56/84 - loss 0.66757063 - samples/sec: 16.85 - lr: 0.100000
2021-05-18 00:46:17,072 epoch 10 - iter 64/84 - loss 0.69659694 - samples/sec: 18.19 - lr: 0.100000
2021-05-18 00:46:23,913 epoch 10 - iter 72/84 - loss 0.69840815 - samples/sec: 18.72 - lr: 0.100000
2021-05-18 00:46:30,887 epoch 10 - iter 80/84 - loss 0.71496438 - samples/sec: 18.38 - lr: 0.100000
2021-05-18 00:46:33,769 ----------------------------------------------------------------------------------------------------
2021-05-18 00:46:33,779 EPOCH 10 done: loss 0.7141 - lr 0.1000000
2021-05-18 00:46:37,044 DEV : loss 0.4124203622341156 - score 0.935
2021-05-18 00:46:37,074 BAD EPOCHS (no improvement): 3
2021-05-18 00:46:37,074 ----------------------------------------------------------------------------------------------------
2021-05-18 00:46:42,999 epoch 11 - iter 8/84 - loss 0.55706010 - samples/sec: 21.61 - lr: 0.100000
2021-05-18 00:46:50,871 epoch 11 - iter 16/84 - loss 0.72419170 - samples/sec: 16.26 - lr: 0.100000
2021-05-18 00:46:58,341 epoch 11 - iter 24/84 - loss 0.77325119 - samples/sec: 17.14 - lr: 0.100000
2021-05-18 00:47:06,426 epoch 11 - iter 32/84 - loss 0.81327949 - samples/sec: 15.83 - lr: 0.100000
2021-05-18 00:47:14,091 epoch 11 - iter 40/84 - loss 0.77396539 - samples/sec: 16.70 - lr: 0.100000
2021-05-18 00:47:20,672 epoch 11 - iter 48/84 - loss 0.80115558 - samples/sec: 19.45 - lr: 0.100000
2021-05-18 00:47:27,025 epoch 11 - iter 56/84 - loss 0.76054778 - samples/sec: 20.15 - lr: 0.100000
2021-05-18 00:47:33,446 epoch 11 - iter 64/84 - loss 0.74834579 - samples/sec: 19.94 - lr: 0.100000
2021-05-18 00:47:40,936 epoch 11 - iter 72/84 - loss 0.72290754 - samples/sec: 17.09 - lr: 0.100000
2021-05-18 00:47:47,340 epoch 11 - iter 80/84 - loss 0.70551033 - samples/sec: 19.99 - lr: 0.100000
2021-05-18 00:47:51,268 ----------------------------------------------------------------------------------------------------
2021-05-18 00:47:51,269 EPOCH 11 done: loss 0.7207 - lr 0.1000000
2021-05-18 00:47:55,363 DEV : loss 0.4935677945613861 - score 0.9336
Epoch    11: reducing learning rate of group 0 to 5.0000e-02.
2021-05-18 00:47:55,393 BAD EPOCHS (no improvement): 4
2021-05-18 00:47:55,393 ----------------------------------------------------------------------------------------------------
2021-05-18 00:48:02,126 epoch 12 - iter 8/84 - loss 0.57124427 - samples/sec: 19.02 - lr: 0.050000
2021-05-18 00:48:08,713 epoch 12 - iter 16/84 - loss 0.54865417 - samples/sec: 19.44 - lr: 0.050000
2021-05-18 00:48:16,318 epoch 12 - iter 24/84 - loss 0.59457604 - samples/sec: 16.84 - lr: 0.050000
2021-05-18 00:48:22,830 epoch 12 - iter 32/84 - loss 0.57259511 - samples/sec: 19.66 - lr: 0.050000
2021-05-18 00:48:29,505 epoch 12 - iter 40/84 - loss 0.56344368 - samples/sec: 19.18 - lr: 0.050000
2021-05-18 00:48:35,703 epoch 12 - iter 48/84 - loss 0.55479410 - samples/sec: 20.67 - lr: 0.050000
2021-05-18 00:48:41,875 epoch 12 - iter 56/84 - loss 0.58192363 - samples/sec: 20.74 - lr: 0.050000
2021-05-18 00:48:48,587 epoch 12 - iter 64/84 - loss 0.58168245 - samples/sec: 19.07 - lr: 0.050000
2021-05-18 00:48:54,894 epoch 12 - iter 72/84 - loss 0.57094548 - samples/sec: 20.30 - lr: 0.050000
2021-05-18 00:49:01,569 epoch 12 - iter 80/84 - loss 0.57329626 - samples/sec: 19.18 - lr: 0.050000
2021-05-18 00:49:04,647 ----------------------------------------------------------------------------------------------------
2021-05-18 00:49:04,648 EPOCH 12 done: loss 0.5693 - lr 0.0500000
2021-05-18 00:49:08,009 DEV : loss 0.45975279808044434 - score 0.9466
2021-05-18 00:49:08,039 BAD EPOCHS (no improvement): 1
2021-05-18 00:49:08,039 ----------------------------------------------------------------------------------------------------
2021-05-18 00:49:14,819 epoch 13 - iter 8/84 - loss 0.84348132 - samples/sec: 18.88 - lr: 0.050000
2021-05-18 00:49:20,972 epoch 13 - iter 16/84 - loss 0.65278651 - samples/sec: 20.81 - lr: 0.050000
2021-05-18 00:49:27,190 epoch 13 - iter 24/84 - loss 0.58368092 - samples/sec: 20.59 - lr: 0.050000
2021-05-18 00:49:34,762 epoch 13 - iter 32/84 - loss 0.56997791 - samples/sec: 16.91 - lr: 0.050000
2021-05-18 00:49:41,504 epoch 13 - iter 40/84 - loss 0.55684071 - samples/sec: 18.99 - lr: 0.050000
2021-05-18 00:49:48,429 epoch 13 - iter 48/84 - loss 0.57914361 - samples/sec: 18.49 - lr: 0.050000
2021-05-18 00:49:54,533 epoch 13 - iter 56/84 - loss 0.57404618 - samples/sec: 20.97 - lr: 0.050000
2021-05-18 00:50:00,832 epoch 13 - iter 64/84 - loss 0.55089088 - samples/sec: 20.32 - lr: 0.050000
2021-05-18 00:50:07,740 epoch 13 - iter 72/84 - loss 0.53515450 - samples/sec: 18.53 - lr: 0.050000
2021-05-18 00:50:15,330 epoch 13 - iter 80/84 - loss 0.53272337 - samples/sec: 16.87 - lr: 0.050000
2021-05-18 00:50:18,080 ----------------------------------------------------------------------------------------------------
2021-05-18 00:50:18,081 EPOCH 13 done: loss 0.5386 - lr 0.0500000
2021-05-18 00:50:21,148 DEV : loss 0.60390704870224 - score 0.9339
2021-05-18 00:50:21,178 BAD EPOCHS (no improvement): 2
2021-05-18 00:50:21,179 ----------------------------------------------------------------------------------------------------
2021-05-18 00:50:27,704 epoch 14 - iter 8/84 - loss 0.56948262 - samples/sec: 19.62 - lr: 0.050000
2021-05-18 00:50:33,227 epoch 14 - iter 16/84 - loss 0.56235229 - samples/sec: 23.18 - lr: 0.050000
2021-05-18 00:50:40,347 epoch 14 - iter 24/84 - loss 0.53098439 - samples/sec: 17.98 - lr: 0.050000
2021-05-18 00:50:47,200 epoch 14 - iter 32/84 - loss 0.55445870 - samples/sec: 18.68 - lr: 0.050000
2021-05-18 00:50:53,392 epoch 14 - iter 40/84 - loss 0.51890497 - samples/sec: 20.68 - lr: 0.050000
2021-05-18 00:50:59,253 epoch 14 - iter 48/84 - loss 0.48653024 - samples/sec: 21.84 - lr: 0.050000
2021-05-18 00:51:05,273 epoch 14 - iter 56/84 - loss 0.48620283 - samples/sec: 21.26 - lr: 0.050000
2021-05-18 00:51:11,120 epoch 14 - iter 64/84 - loss 0.50116839 - samples/sec: 21.89 - lr: 0.050000
2021-05-18 00:51:18,309 epoch 14 - iter 72/84 - loss 0.52269308 - samples/sec: 17.81 - lr: 0.050000
2021-05-18 00:51:25,259 epoch 14 - iter 80/84 - loss 0.51230471 - samples/sec: 18.42 - lr: 0.050000
2021-05-18 00:51:28,283 ----------------------------------------------------------------------------------------------------
2021-05-18 00:51:28,283 EPOCH 14 done: loss 0.5119 - lr 0.0500000
2021-05-18 00:51:31,213 DEV : loss 0.46358194947242737 - score 0.9466
2021-05-18 00:51:31,243 BAD EPOCHS (no improvement): 3
2021-05-18 00:51:31,244 ----------------------------------------------------------------------------------------------------
2021-05-18 00:51:38,608 epoch 15 - iter 8/84 - loss 0.63439649 - samples/sec: 17.38 - lr: 0.050000
2021-05-18 00:51:45,599 epoch 15 - iter 16/84 - loss 0.56211155 - samples/sec: 18.31 - lr: 0.050000
2021-05-18 00:51:51,815 epoch 15 - iter 24/84 - loss 0.51916194 - samples/sec: 20.59 - lr: 0.050000
2021-05-18 00:51:58,680 epoch 15 - iter 32/84 - loss 0.50785198 - samples/sec: 18.65 - lr: 0.050000
2021-05-18 00:52:05,591 epoch 15 - iter 40/84 - loss 0.48220446 - samples/sec: 18.52 - lr: 0.050000
2021-05-18 00:52:11,430 epoch 15 - iter 48/84 - loss 0.46930142 - samples/sec: 21.93 - lr: 0.050000
2021-05-18 00:52:17,843 epoch 15 - iter 56/84 - loss 0.46298920 - samples/sec: 19.96 - lr: 0.050000
2021-05-18 00:52:24,356 epoch 15 - iter 64/84 - loss 0.46714477 - samples/sec: 19.65 - lr: 0.050000
2021-05-18 00:52:30,190 epoch 15 - iter 72/84 - loss 0.47334974 - samples/sec: 21.94 - lr: 0.050000
2021-05-18 00:52:37,371 epoch 15 - iter 80/84 - loss 0.47298831 - samples/sec: 17.84 - lr: 0.050000
2021-05-18 00:52:40,784 ----------------------------------------------------------------------------------------------------
2021-05-18 00:52:40,785 EPOCH 15 done: loss 0.4722 - lr 0.0500000
2021-05-18 00:52:44,290 DEV : loss 0.3668765425682068 - score 0.9531
2021-05-18 00:52:44,320 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:52:55,090 ----------------------------------------------------------------------------------------------------
2021-05-18 00:53:01,429 epoch 16 - iter 8/84 - loss 0.53005023 - samples/sec: 20.20 - lr: 0.050000
2021-05-18 00:53:08,222 epoch 16 - iter 16/84 - loss 0.49015910 - samples/sec: 18.85 - lr: 0.050000
2021-05-18 00:53:14,775 epoch 16 - iter 24/84 - loss 0.44750745 - samples/sec: 19.54 - lr: 0.050000
2021-05-18 00:53:22,096 epoch 16 - iter 32/84 - loss 0.47062739 - samples/sec: 17.49 - lr: 0.050000
2021-05-18 00:53:28,812 epoch 16 - iter 40/84 - loss 0.48447661 - samples/sec: 19.07 - lr: 0.050000
2021-05-18 00:53:35,761 epoch 16 - iter 48/84 - loss 0.49849522 - samples/sec: 18.42 - lr: 0.050000
2021-05-18 00:53:41,349 epoch 16 - iter 56/84 - loss 0.47363408 - samples/sec: 22.91 - lr: 0.050000
2021-05-18 00:53:47,941 epoch 16 - iter 64/84 - loss 0.48154527 - samples/sec: 19.42 - lr: 0.050000
2021-05-18 00:53:54,652 epoch 16 - iter 72/84 - loss 0.48604910 - samples/sec: 19.08 - lr: 0.050000
2021-05-18 00:54:01,919 epoch 16 - iter 80/84 - loss 0.47514908 - samples/sec: 17.62 - lr: 0.050000
2021-05-18 00:54:05,329 ----------------------------------------------------------------------------------------------------
2021-05-18 00:54:05,330 EPOCH 16 done: loss 0.4958 - lr 0.0500000
2021-05-18 00:54:09,184 DEV : loss 0.3815603256225586 - score 0.9492
2021-05-18 00:54:09,215 BAD EPOCHS (no improvement): 1
2021-05-18 00:54:09,216 ----------------------------------------------------------------------------------------------------
2021-05-18 00:54:15,865 epoch 17 - iter 8/84 - loss 0.42905608 - samples/sec: 19.25 - lr: 0.050000
2021-05-18 00:54:22,085 epoch 17 - iter 16/84 - loss 0.53848383 - samples/sec: 20.59 - lr: 0.050000
2021-05-18 00:54:28,348 epoch 17 - iter 24/84 - loss 0.53266579 - samples/sec: 20.44 - lr: 0.050000
2021-05-18 00:54:34,407 epoch 17 - iter 32/84 - loss 0.48404446 - samples/sec: 21.15 - lr: 0.050000
2021-05-18 00:54:41,798 epoch 17 - iter 40/84 - loss 0.47002073 - samples/sec: 17.32 - lr: 0.050000
2021-05-18 00:54:47,815 epoch 17 - iter 48/84 - loss 0.45337694 - samples/sec: 21.28 - lr: 0.050000
2021-05-18 00:54:53,537 epoch 17 - iter 56/84 - loss 0.47411790 - samples/sec: 22.39 - lr: 0.050000
2021-05-18 00:55:00,090 epoch 17 - iter 64/84 - loss 0.47540430 - samples/sec: 19.54 - lr: 0.050000
2021-05-18 00:55:06,255 epoch 17 - iter 72/84 - loss 0.49735511 - samples/sec: 20.77 - lr: 0.050000
2021-05-18 00:55:11,935 epoch 17 - iter 80/84 - loss 0.49394150 - samples/sec: 22.54 - lr: 0.050000
2021-05-18 00:55:15,339 ----------------------------------------------------------------------------------------------------
2021-05-18 00:55:15,340 EPOCH 17 done: loss 0.4944 - lr 0.0500000
2021-05-18 00:55:18,733 DEV : loss 0.5897009372711182 - score 0.9255
2021-05-18 00:55:18,767 BAD EPOCHS (no improvement): 2
2021-05-18 00:55:18,767 ----------------------------------------------------------------------------------------------------
2021-05-18 00:55:26,051 epoch 18 - iter 8/84 - loss 0.60772601 - samples/sec: 17.58 - lr: 0.050000
2021-05-18 00:55:33,249 epoch 18 - iter 16/84 - loss 0.55562063 - samples/sec: 17.79 - lr: 0.050000
2021-05-18 00:55:39,059 epoch 18 - iter 24/84 - loss 0.50838732 - samples/sec: 22.06 - lr: 0.050000
2021-05-18 00:55:45,458 epoch 18 - iter 32/84 - loss 0.52694776 - samples/sec: 20.01 - lr: 0.050000
2021-05-18 00:55:52,229 epoch 18 - iter 40/84 - loss 0.52944529 - samples/sec: 18.91 - lr: 0.050000
2021-05-18 00:55:58,242 epoch 18 - iter 48/84 - loss 0.48805865 - samples/sec: 21.29 - lr: 0.050000
2021-05-18 00:56:04,402 epoch 18 - iter 56/84 - loss 0.49546028 - samples/sec: 20.78 - lr: 0.050000
2021-05-18 00:56:10,280 epoch 18 - iter 64/84 - loss 0.51503974 - samples/sec: 21.78 - lr: 0.050000
2021-05-18 00:56:16,995 epoch 18 - iter 72/84 - loss 0.50769960 - samples/sec: 19.06 - lr: 0.050000
2021-05-18 00:56:23,409 epoch 18 - iter 80/84 - loss 0.50846397 - samples/sec: 19.96 - lr: 0.050000
2021-05-18 00:56:26,933 ----------------------------------------------------------------------------------------------------
2021-05-18 00:56:26,934 EPOCH 18 done: loss 0.5147 - lr 0.0500000
2021-05-18 00:56:30,140 DEV : loss 0.43387728929519653 - score 0.9533
2021-05-18 00:56:30,197 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:56:41,443 ----------------------------------------------------------------------------------------------------
2021-05-18 00:56:47,879 epoch 19 - iter 8/84 - loss 0.41556484 - samples/sec: 19.89 - lr: 0.050000
2021-05-18 00:56:54,430 epoch 19 - iter 16/84 - loss 0.52410074 - samples/sec: 19.54 - lr: 0.050000
2021-05-18 00:57:01,537 epoch 19 - iter 24/84 - loss 0.54606982 - samples/sec: 18.01 - lr: 0.050000
2021-05-18 00:57:08,921 epoch 19 - iter 32/84 - loss 0.53506246 - samples/sec: 17.37 - lr: 0.050000
2021-05-18 00:57:15,339 epoch 19 - iter 40/84 - loss 0.55374176 - samples/sec: 19.95 - lr: 0.050000
2021-05-18 00:57:21,213 epoch 19 - iter 48/84 - loss 0.50920705 - samples/sec: 21.80 - lr: 0.050000
2021-05-18 00:57:27,747 epoch 19 - iter 56/84 - loss 0.45801707 - samples/sec: 19.59 - lr: 0.050000
2021-05-18 00:57:34,236 epoch 19 - iter 64/84 - loss 0.46115650 - samples/sec: 19.73 - lr: 0.050000
2021-05-18 00:57:40,647 epoch 19 - iter 72/84 - loss 0.45899603 - samples/sec: 19.97 - lr: 0.050000
2021-05-18 00:57:46,816 epoch 19 - iter 80/84 - loss 0.45833483 - samples/sec: 20.75 - lr: 0.050000
2021-05-18 00:57:49,822 ----------------------------------------------------------------------------------------------------
2021-05-18 00:57:49,822 EPOCH 19 done: loss 0.4544 - lr 0.0500000
2021-05-18 00:57:53,197 DEV : loss 0.36888864636421204 - score 0.954
2021-05-18 00:57:53,227 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 00:58:04,080 ----------------------------------------------------------------------------------------------------
2021-05-18 00:58:10,429 epoch 20 - iter 8/84 - loss 0.42783719 - samples/sec: 20.17 - lr: 0.050000
2021-05-18 00:58:17,262 epoch 20 - iter 16/84 - loss 0.48079514 - samples/sec: 18.73 - lr: 0.050000
2021-05-18 00:58:23,131 epoch 20 - iter 24/84 - loss 0.47867333 - samples/sec: 21.81 - lr: 0.050000
2021-05-18 00:58:28,647 epoch 20 - iter 32/84 - loss 0.56522829 - samples/sec: 23.21 - lr: 0.050000
2021-05-18 00:58:34,090 epoch 20 - iter 40/84 - loss 0.54681011 - samples/sec: 23.52 - lr: 0.050000
2021-05-18 00:58:40,207 epoch 20 - iter 48/84 - loss 0.56074663 - samples/sec: 20.93 - lr: 0.050000
2021-05-18 00:58:46,051 epoch 20 - iter 56/84 - loss 0.54152341 - samples/sec: 21.91 - lr: 0.050000
2021-05-18 00:58:52,534 epoch 20 - iter 64/84 - loss 0.53734988 - samples/sec: 19.74 - lr: 0.050000
2021-05-18 00:58:57,955 epoch 20 - iter 72/84 - loss 0.50733120 - samples/sec: 23.62 - lr: 0.050000
2021-05-18 00:59:04,005 epoch 20 - iter 80/84 - loss 0.49564754 - samples/sec: 21.17 - lr: 0.050000
2021-05-18 00:59:06,871 ----------------------------------------------------------------------------------------------------
2021-05-18 00:59:06,872 EPOCH 20 done: loss 0.5009 - lr 0.0500000
2021-05-18 00:59:09,636 DEV : loss 0.3829657733440399 - score 0.9508
2021-05-18 00:59:09,666 BAD EPOCHS (no improvement): 1
2021-05-18 00:59:09,667 ----------------------------------------------------------------------------------------------------
2021-05-18 00:59:15,054 epoch 21 - iter 8/84 - loss 0.49540577 - samples/sec: 23.77 - lr: 0.050000
2021-05-18 00:59:20,813 epoch 21 - iter 16/84 - loss 0.53572224 - samples/sec: 22.23 - lr: 0.050000
2021-05-18 00:59:25,922 epoch 21 - iter 24/84 - loss 0.47366703 - samples/sec: 25.06 - lr: 0.050000
2021-05-18 00:59:31,965 epoch 21 - iter 32/84 - loss 0.48743635 - samples/sec: 21.19 - lr: 0.050000
2021-05-18 00:59:38,198 epoch 21 - iter 40/84 - loss 0.47729414 - samples/sec: 20.54 - lr: 0.050000
2021-05-18 00:59:43,970 epoch 21 - iter 48/84 - loss 0.46291382 - samples/sec: 22.18 - lr: 0.050000
2021-05-18 00:59:50,121 epoch 21 - iter 56/84 - loss 0.48173002 - samples/sec: 20.85 - lr: 0.050000
2021-05-18 00:59:56,479 epoch 21 - iter 64/84 - loss 0.50391413 - samples/sec: 20.14 - lr: 0.050000
2021-05-18 01:00:02,620 epoch 21 - iter 72/84 - loss 0.49651159 - samples/sec: 20.85 - lr: 0.050000
2021-05-18 01:00:08,807 epoch 21 - iter 80/84 - loss 0.48305920 - samples/sec: 20.69 - lr: 0.050000
2021-05-18 01:00:11,953 ----------------------------------------------------------------------------------------------------
2021-05-18 01:00:11,959 EPOCH 21 done: loss 0.4825 - lr 0.0500000
2021-05-18 01:00:14,974 DEV : loss 0.42400941252708435 - score 0.9356
2021-05-18 01:00:15,004 BAD EPOCHS (no improvement): 2
2021-05-18 01:00:15,004 ----------------------------------------------------------------------------------------------------
2021-05-18 01:00:21,403 epoch 22 - iter 8/84 - loss 0.66314417 - samples/sec: 20.01 - lr: 0.050000
2021-05-18 01:00:27,893 epoch 22 - iter 16/84 - loss 0.59782240 - samples/sec: 19.73 - lr: 0.050000
2021-05-18 01:00:34,333 epoch 22 - iter 24/84 - loss 0.55778628 - samples/sec: 19.88 - lr: 0.050000
2021-05-18 01:00:40,154 epoch 22 - iter 32/84 - loss 0.50741404 - samples/sec: 21.99 - lr: 0.050000
2021-05-18 01:00:46,626 epoch 22 - iter 40/84 - loss 0.49419850 - samples/sec: 19.78 - lr: 0.050000
2021-05-18 01:00:52,776 epoch 22 - iter 48/84 - loss 0.50897331 - samples/sec: 20.82 - lr: 0.050000
2021-05-18 01:00:59,035 epoch 22 - iter 56/84 - loss 0.51888452 - samples/sec: 20.46 - lr: 0.050000
2021-05-18 01:01:04,966 epoch 22 - iter 64/84 - loss 0.50110252 - samples/sec: 21.59 - lr: 0.050000
2021-05-18 01:01:11,894 epoch 22 - iter 72/84 - loss 0.47857302 - samples/sec: 18.48 - lr: 0.050000
2021-05-18 01:01:18,404 epoch 22 - iter 80/84 - loss 0.46950783 - samples/sec: 19.66 - lr: 0.050000
2021-05-18 01:01:21,795 ----------------------------------------------------------------------------------------------------
2021-05-18 01:01:21,796 EPOCH 22 done: loss 0.4683 - lr 0.0500000
2021-05-18 01:01:24,635 DEV : loss 0.3379322588443756 - score 0.959
2021-05-18 01:01:24,665 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:01:35,370 ----------------------------------------------------------------------------------------------------
2021-05-18 01:01:41,282 epoch 23 - iter 8/84 - loss 0.65049570 - samples/sec: 21.66 - lr: 0.050000
2021-05-18 01:01:47,934 epoch 23 - iter 16/84 - loss 0.67087180 - samples/sec: 19.24 - lr: 0.050000
2021-05-18 01:01:54,201 epoch 23 - iter 24/84 - loss 0.54523551 - samples/sec: 20.43 - lr: 0.050000
2021-05-18 01:01:59,940 epoch 23 - iter 32/84 - loss 0.48028261 - samples/sec: 22.31 - lr: 0.050000
2021-05-18 01:02:06,247 epoch 23 - iter 40/84 - loss 0.48604212 - samples/sec: 20.30 - lr: 0.050000
2021-05-18 01:02:13,954 epoch 23 - iter 48/84 - loss 0.47896788 - samples/sec: 16.61 - lr: 0.050000
2021-05-18 01:02:20,840 epoch 23 - iter 56/84 - loss 0.49770984 - samples/sec: 18.59 - lr: 0.050000
2021-05-18 01:02:28,414 epoch 23 - iter 64/84 - loss 0.48827519 - samples/sec: 16.90 - lr: 0.050000
2021-05-18 01:02:35,039 epoch 23 - iter 72/84 - loss 0.46166221 - samples/sec: 19.33 - lr: 0.050000
2021-05-18 01:02:42,073 epoch 23 - iter 80/84 - loss 0.46598433 - samples/sec: 18.20 - lr: 0.050000
2021-05-18 01:02:45,369 ----------------------------------------------------------------------------------------------------
2021-05-18 01:02:45,370 EPOCH 23 done: loss 0.4719 - lr 0.0500000
2021-05-18 01:02:48,476 DEV : loss 0.443378210067749 - score 0.9404
2021-05-18 01:02:48,506 BAD EPOCHS (no improvement): 1
2021-05-18 01:02:48,507 ----------------------------------------------------------------------------------------------------
2021-05-18 01:02:54,475 epoch 24 - iter 8/84 - loss 0.38102767 - samples/sec: 21.45 - lr: 0.050000
2021-05-18 01:03:00,792 epoch 24 - iter 16/84 - loss 0.47111398 - samples/sec: 20.27 - lr: 0.050000
2021-05-18 01:03:07,125 epoch 24 - iter 24/84 - loss 0.48569596 - samples/sec: 20.22 - lr: 0.050000
2021-05-18 01:03:13,792 epoch 24 - iter 32/84 - loss 0.40640871 - samples/sec: 19.20 - lr: 0.050000
2021-05-18 01:03:21,148 epoch 24 - iter 40/84 - loss 0.39074743 - samples/sec: 17.40 - lr: 0.050000
2021-05-18 01:03:27,080 epoch 24 - iter 48/84 - loss 0.39464801 - samples/sec: 21.58 - lr: 0.050000
2021-05-18 01:03:32,370 epoch 24 - iter 56/84 - loss 0.42601727 - samples/sec: 24.20 - lr: 0.050000
2021-05-18 01:03:38,621 epoch 24 - iter 64/84 - loss 0.44333389 - samples/sec: 20.48 - lr: 0.050000
2021-05-18 01:03:45,197 epoch 24 - iter 72/84 - loss 0.46182355 - samples/sec: 19.49 - lr: 0.050000
2021-05-18 01:03:51,170 epoch 24 - iter 80/84 - loss 0.47150851 - samples/sec: 21.44 - lr: 0.050000
2021-05-18 01:03:54,219 ----------------------------------------------------------------------------------------------------
2021-05-18 01:03:54,224 EPOCH 24 done: loss 0.4735 - lr 0.0500000
2021-05-18 01:03:57,469 DEV : loss 0.32572001218795776 - score 0.9531
2021-05-18 01:03:57,506 BAD EPOCHS (no improvement): 2
2021-05-18 01:03:57,507 ----------------------------------------------------------------------------------------------------
2021-05-18 01:04:04,126 epoch 25 - iter 8/84 - loss 0.50884141 - samples/sec: 19.34 - lr: 0.050000
2021-05-18 01:04:09,918 epoch 25 - iter 16/84 - loss 0.59073658 - samples/sec: 22.10 - lr: 0.050000
2021-05-18 01:04:15,184 epoch 25 - iter 24/84 - loss 0.54549219 - samples/sec: 24.31 - lr: 0.050000
2021-05-18 01:04:22,273 epoch 25 - iter 32/84 - loss 0.52205156 - samples/sec: 18.06 - lr: 0.050000
2021-05-18 01:04:28,957 epoch 25 - iter 40/84 - loss 0.49249189 - samples/sec: 19.15 - lr: 0.050000
2021-05-18 01:04:34,546 epoch 25 - iter 48/84 - loss 0.50024230 - samples/sec: 22.90 - lr: 0.050000
2021-05-18 01:04:40,862 epoch 25 - iter 56/84 - loss 0.46367211 - samples/sec: 20.27 - lr: 0.050000
2021-05-18 01:04:47,312 epoch 25 - iter 64/84 - loss 0.43590510 - samples/sec: 19.89 - lr: 0.050000
2021-05-18 01:04:54,176 epoch 25 - iter 72/84 - loss 0.42073155 - samples/sec: 18.65 - lr: 0.050000
2021-05-18 01:04:59,656 epoch 25 - iter 80/84 - loss 0.41417779 - samples/sec: 23.36 - lr: 0.050000
2021-05-18 01:05:02,224 ----------------------------------------------------------------------------------------------------
2021-05-18 01:05:02,225 EPOCH 25 done: loss 0.4354 - lr 0.0500000
2021-05-18 01:05:05,322 DEV : loss 0.4486263394355774 - score 0.9491
2021-05-18 01:05:05,352 BAD EPOCHS (no improvement): 3
2021-05-18 01:05:05,352 ----------------------------------------------------------------------------------------------------
2021-05-18 01:05:12,032 epoch 26 - iter 8/84 - loss 0.58774640 - samples/sec: 19.17 - lr: 0.050000
2021-05-18 01:05:17,928 epoch 26 - iter 16/84 - loss 0.49555351 - samples/sec: 21.71 - lr: 0.050000
2021-05-18 01:05:23,845 epoch 26 - iter 24/84 - loss 0.48380300 - samples/sec: 21.64 - lr: 0.050000
2021-05-18 01:05:30,667 epoch 26 - iter 32/84 - loss 0.47284488 - samples/sec: 18.77 - lr: 0.050000
2021-05-18 01:05:37,734 epoch 26 - iter 40/84 - loss 0.47563716 - samples/sec: 18.11 - lr: 0.050000
2021-05-18 01:05:44,125 epoch 26 - iter 48/84 - loss 0.44682147 - samples/sec: 20.03 - lr: 0.050000
2021-05-18 01:05:50,235 epoch 26 - iter 56/84 - loss 0.46019502 - samples/sec: 20.95 - lr: 0.050000
2021-05-18 01:05:55,915 epoch 26 - iter 64/84 - loss 0.44328191 - samples/sec: 22.54 - lr: 0.050000
2021-05-18 01:06:01,555 epoch 26 - iter 72/84 - loss 0.45043747 - samples/sec: 22.70 - lr: 0.050000
2021-05-18 01:06:07,842 epoch 26 - iter 80/84 - loss 0.45481965 - samples/sec: 20.37 - lr: 0.050000
2021-05-18 01:06:11,134 ----------------------------------------------------------------------------------------------------
2021-05-18 01:06:11,138 EPOCH 26 done: loss 0.4582 - lr 0.0500000
2021-05-18 01:06:14,418 DEV : loss 0.3839414119720459 - score 0.9531
Epoch    26: reducing learning rate of group 0 to 2.5000e-02.
2021-05-18 01:06:14,449 BAD EPOCHS (no improvement): 4
2021-05-18 01:06:14,449 ----------------------------------------------------------------------------------------------------
2021-05-18 01:06:20,421 epoch 27 - iter 8/84 - loss 0.42624040 - samples/sec: 21.44 - lr: 0.025000
2021-05-18 01:06:26,664 epoch 27 - iter 16/84 - loss 0.47837362 - samples/sec: 20.54 - lr: 0.025000
2021-05-18 01:06:33,232 epoch 27 - iter 24/84 - loss 0.46287845 - samples/sec: 19.49 - lr: 0.025000
2021-05-18 01:06:40,304 epoch 27 - iter 32/84 - loss 0.40545144 - samples/sec: 18.10 - lr: 0.025000
2021-05-18 01:06:46,372 epoch 27 - iter 40/84 - loss 0.36686405 - samples/sec: 21.10 - lr: 0.025000
2021-05-18 01:06:52,645 epoch 27 - iter 48/84 - loss 0.37691237 - samples/sec: 20.41 - lr: 0.025000
2021-05-18 01:06:59,834 epoch 27 - iter 56/84 - loss 0.38771376 - samples/sec: 17.81 - lr: 0.025000
2021-05-18 01:07:06,362 epoch 27 - iter 64/84 - loss 0.38242984 - samples/sec: 19.61 - lr: 0.025000
2021-05-18 01:07:13,597 epoch 27 - iter 72/84 - loss 0.38554821 - samples/sec: 17.70 - lr: 0.025000
2021-05-18 01:07:19,944 epoch 27 - iter 80/84 - loss 0.40088741 - samples/sec: 20.17 - lr: 0.025000
2021-05-18 01:07:23,167 ----------------------------------------------------------------------------------------------------
2021-05-18 01:07:23,168 EPOCH 27 done: loss 0.4110 - lr 0.0250000
2021-05-18 01:07:26,581 DEV : loss 0.3870689868927002 - score 0.9464
2021-05-18 01:07:26,611 BAD EPOCHS (no improvement): 1
2021-05-18 01:07:26,611 ----------------------------------------------------------------------------------------------------
2021-05-18 01:07:34,175 epoch 28 - iter 8/84 - loss 0.34514107 - samples/sec: 16.92 - lr: 0.025000
2021-05-18 01:07:41,120 epoch 28 - iter 16/84 - loss 0.42186827 - samples/sec: 18.47 - lr: 0.025000
2021-05-18 01:07:47,602 epoch 28 - iter 24/84 - loss 0.39479560 - samples/sec: 19.75 - lr: 0.025000
2021-05-18 01:07:53,566 epoch 28 - iter 32/84 - loss 0.40037522 - samples/sec: 21.48 - lr: 0.025000
2021-05-18 01:08:00,570 epoch 28 - iter 40/84 - loss 0.43176681 - samples/sec: 18.29 - lr: 0.025000
2021-05-18 01:08:06,943 epoch 28 - iter 48/84 - loss 0.43316157 - samples/sec: 20.09 - lr: 0.025000
2021-05-18 01:08:13,488 epoch 28 - iter 56/84 - loss 0.42650205 - samples/sec: 19.56 - lr: 0.025000
2021-05-18 01:08:20,370 epoch 28 - iter 64/84 - loss 0.41857772 - samples/sec: 18.60 - lr: 0.025000
2021-05-18 01:08:26,015 epoch 28 - iter 72/84 - loss 0.41233375 - samples/sec: 22.68 - lr: 0.025000
2021-05-18 01:08:31,545 epoch 28 - iter 80/84 - loss 0.41021357 - samples/sec: 23.15 - lr: 0.025000
2021-05-18 01:08:34,200 ----------------------------------------------------------------------------------------------------
2021-05-18 01:08:34,201 EPOCH 28 done: loss 0.3996 - lr 0.0250000
2021-05-18 01:08:37,085 DEV : loss 0.4223548471927643 - score 0.9533
2021-05-18 01:08:37,115 BAD EPOCHS (no improvement): 2
2021-05-18 01:08:37,115 ----------------------------------------------------------------------------------------------------
2021-05-18 01:08:42,407 epoch 29 - iter 8/84 - loss 0.35309601 - samples/sec: 24.19 - lr: 0.025000
2021-05-18 01:08:48,063 epoch 29 - iter 16/84 - loss 0.44106610 - samples/sec: 22.63 - lr: 0.025000
2021-05-18 01:08:54,261 epoch 29 - iter 24/84 - loss 0.40991032 - samples/sec: 20.69 - lr: 0.025000
2021-05-18 01:09:02,257 epoch 29 - iter 32/84 - loss 0.42166993 - samples/sec: 16.01 - lr: 0.025000
2021-05-18 01:09:08,645 epoch 29 - iter 40/84 - loss 0.41590392 - samples/sec: 20.04 - lr: 0.025000
2021-05-18 01:09:14,735 epoch 29 - iter 48/84 - loss 0.38306389 - samples/sec: 21.02 - lr: 0.025000
2021-05-18 01:09:21,522 epoch 29 - iter 56/84 - loss 0.37871651 - samples/sec: 18.87 - lr: 0.025000
2021-05-18 01:09:27,776 epoch 29 - iter 64/84 - loss 0.36973044 - samples/sec: 20.49 - lr: 0.025000
2021-05-18 01:09:33,603 epoch 29 - iter 72/84 - loss 0.38316549 - samples/sec: 21.97 - lr: 0.025000
2021-05-18 01:09:39,154 epoch 29 - iter 80/84 - loss 0.37284981 - samples/sec: 23.06 - lr: 0.025000
2021-05-18 01:09:42,443 ----------------------------------------------------------------------------------------------------
2021-05-18 01:09:42,444 EPOCH 29 done: loss 0.3685 - lr 0.0250000
2021-05-18 01:09:46,242 DEV : loss 0.35915544629096985 - score 0.9531
2021-05-18 01:09:46,273 BAD EPOCHS (no improvement): 3
2021-05-18 01:09:46,274 ----------------------------------------------------------------------------------------------------
2021-05-18 01:09:51,966 epoch 30 - iter 8/84 - loss 0.45247187 - samples/sec: 22.49 - lr: 0.025000
2021-05-18 01:09:58,257 epoch 30 - iter 16/84 - loss 0.45403788 - samples/sec: 20.35 - lr: 0.025000
2021-05-18 01:10:05,548 epoch 30 - iter 24/84 - loss 0.42835639 - samples/sec: 17.56 - lr: 0.025000
2021-05-18 01:10:11,345 epoch 30 - iter 32/84 - loss 0.42658318 - samples/sec: 22.08 - lr: 0.025000
2021-05-18 01:10:16,878 epoch 30 - iter 40/84 - loss 0.40467010 - samples/sec: 23.14 - lr: 0.025000
2021-05-18 01:10:23,598 epoch 30 - iter 48/84 - loss 0.44963341 - samples/sec: 19.05 - lr: 0.025000
2021-05-18 01:10:30,849 epoch 30 - iter 56/84 - loss 0.45183692 - samples/sec: 17.66 - lr: 0.025000
2021-05-18 01:10:39,104 epoch 30 - iter 64/84 - loss 0.45871868 - samples/sec: 15.51 - lr: 0.025000
2021-05-18 01:10:46,038 epoch 30 - iter 72/84 - loss 0.43508592 - samples/sec: 18.46 - lr: 0.025000
2021-05-18 01:10:52,821 epoch 30 - iter 80/84 - loss 0.43039889 - samples/sec: 18.92 - lr: 0.025000
2021-05-18 01:10:55,781 ----------------------------------------------------------------------------------------------------
2021-05-18 01:10:55,783 EPOCH 30 done: loss 0.4185 - lr 0.0250000
2021-05-18 01:10:58,667 DEV : loss 0.5284842848777771 - score 0.9406
Epoch    30: reducing learning rate of group 0 to 1.2500e-02.
2021-05-18 01:10:58,711 BAD EPOCHS (no improvement): 4
2021-05-18 01:11:09,617 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:09,617 Testing using best model ...
2021-05-18 01:11:09,618 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/deu.rst.pcc/best-model.pt
2021-05-18 01:11:25,185 0.9802	0.9296	0.9542
2021-05-18 01:11:25,186 
Results:
- F1-score (micro) 0.9542
- F1-score (macro) 0.9542

By class:
SENT       tp: 198 - fp: 4 - fn: 15 - precision: 0.9802 - recall: 0.9296 - f1-score: 0.9542
2021-05-18 01:11:25,186 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt/
2021-05-18 01:11:25,227 Reading data from /home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt
2021-05-18 01:11:25,227 Train: /home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt/sent_train.txt
2021-05-18 01:11:25,230 Dev: /home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt/sent_dev.txt
2021-05-18 01:11:25,233 Test: /home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt/sent_test.txt
Corpus: 10286 train + 1410 dev + 1379 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-18 01:11:50,991 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:50,996 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-18 01:11:51,000 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:51,000 Corpus: "Corpus: 10286 train + 1410 dev + 1379 test sentences"
2021-05-18 01:11:51,000 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:51,000 Parameters:
2021-05-18 01:11:51,001  - learning_rate: "0.1"
2021-05-18 01:11:51,001  - mini_batch_size: "16"
2021-05-18 01:11:51,001  - patience: "3"
2021-05-18 01:11:51,001  - anneal_factor: "0.5"
2021-05-18 01:11:51,001  - max_epochs: "30"
2021-05-18 01:11:51,001  - shuffle: "True"
2021-05-18 01:11:51,001  - train_with_dev: "False"
2021-05-18 01:11:51,001  - batch_growth_annealing: "False"
2021-05-18 01:11:51,001 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:51,002 Model training base path: "/home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt"
2021-05-18 01:11:51,002 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:51,002 Device: cuda:0
2021-05-18 01:11:51,002 ----------------------------------------------------------------------------------------------------
2021-05-18 01:11:51,002 Embeddings storage mode: cpu
2021-05-18 01:11:51,005 ----------------------------------------------------------------------------------------------------
2021-05-18 01:13:38,490 epoch 1 - iter 64/643 - loss 4.64768062 - samples/sec: 9.53 - lr: 0.100000
2021-05-18 01:15:11,489 epoch 1 - iter 128/643 - loss 3.33911781 - samples/sec: 11.01 - lr: 0.100000
2021-05-18 01:16:45,381 epoch 1 - iter 192/643 - loss 2.82634022 - samples/sec: 10.91 - lr: 0.100000
2021-05-18 01:18:18,264 epoch 1 - iter 256/643 - loss 2.44609600 - samples/sec: 11.03 - lr: 0.100000
2021-05-18 01:19:53,188 epoch 1 - iter 320/643 - loss 2.23942039 - samples/sec: 10.79 - lr: 0.100000
2021-05-18 01:21:28,230 epoch 1 - iter 384/643 - loss 2.09743879 - samples/sec: 10.77 - lr: 0.100000
2021-05-18 01:23:01,205 epoch 1 - iter 448/643 - loss 1.95568411 - samples/sec: 11.01 - lr: 0.100000
2021-05-18 01:24:35,863 epoch 1 - iter 512/643 - loss 1.85540780 - samples/sec: 10.82 - lr: 0.100000
2021-05-18 01:26:06,664 epoch 1 - iter 576/643 - loss 1.75870644 - samples/sec: 11.28 - lr: 0.100000
2021-05-18 01:27:39,451 epoch 1 - iter 640/643 - loss 1.67596769 - samples/sec: 11.04 - lr: 0.100000
2021-05-18 01:27:43,490 ----------------------------------------------------------------------------------------------------
2021-05-18 01:27:43,490 EPOCH 1 done: loss 1.6709 - lr 0.1000000
2021-05-18 01:29:04,987 DEV : loss 0.6353048086166382 - score 0.8738
2021-05-18 01:29:05,218 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:29:16,364 ----------------------------------------------------------------------------------------------------
2021-05-18 01:29:48,367 epoch 2 - iter 64/643 - loss 1.01201980 - samples/sec: 32.00 - lr: 0.100000
2021-05-18 01:30:19,731 epoch 2 - iter 128/643 - loss 0.95096705 - samples/sec: 32.65 - lr: 0.100000
2021-05-18 01:30:53,365 epoch 2 - iter 192/643 - loss 0.91276550 - samples/sec: 30.45 - lr: 0.100000
2021-05-18 01:31:27,092 epoch 2 - iter 256/643 - loss 0.95230270 - samples/sec: 30.37 - lr: 0.100000
2021-05-18 01:32:00,809 epoch 2 - iter 320/643 - loss 0.90533415 - samples/sec: 30.37 - lr: 0.100000
2021-05-18 01:32:33,556 epoch 2 - iter 384/643 - loss 0.91295597 - samples/sec: 31.27 - lr: 0.100000
2021-05-18 01:33:05,878 epoch 2 - iter 448/643 - loss 0.92450470 - samples/sec: 31.69 - lr: 0.100000
2021-05-18 01:33:36,864 epoch 2 - iter 512/643 - loss 0.90268085 - samples/sec: 33.05 - lr: 0.100000
2021-05-18 01:34:07,897 epoch 2 - iter 576/643 - loss 0.89252633 - samples/sec: 33.00 - lr: 0.100000
2021-05-18 01:34:41,438 epoch 2 - iter 640/643 - loss 0.87250385 - samples/sec: 30.53 - lr: 0.100000
2021-05-18 01:34:42,925 ----------------------------------------------------------------------------------------------------
2021-05-18 01:34:42,925 EPOCH 2 done: loss 0.8723 - lr 0.1000000
2021-05-18 01:34:58,797 DEV : loss 0.5807299613952637 - score 0.8871
2021-05-18 01:34:59,027 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:35:10,373 ----------------------------------------------------------------------------------------------------
2021-05-18 01:35:42,290 epoch 3 - iter 64/643 - loss 0.71436103 - samples/sec: 32.09 - lr: 0.100000
2021-05-18 01:36:14,146 epoch 3 - iter 128/643 - loss 0.71975565 - samples/sec: 32.15 - lr: 0.100000
2021-05-18 01:36:43,259 epoch 3 - iter 192/643 - loss 0.70522371 - samples/sec: 35.18 - lr: 0.100000
2021-05-18 01:37:12,428 epoch 3 - iter 256/643 - loss 0.69846277 - samples/sec: 35.11 - lr: 0.100000
2021-05-18 01:37:43,397 epoch 3 - iter 320/643 - loss 0.71056378 - samples/sec: 33.07 - lr: 0.100000
2021-05-18 01:38:13,860 epoch 3 - iter 384/643 - loss 0.70288847 - samples/sec: 33.62 - lr: 0.100000
2021-05-18 01:38:46,971 epoch 3 - iter 448/643 - loss 0.71535717 - samples/sec: 30.93 - lr: 0.100000
2021-05-18 01:39:18,289 epoch 3 - iter 512/643 - loss 0.71514532 - samples/sec: 32.70 - lr: 0.100000
2021-05-18 01:39:50,978 epoch 3 - iter 576/643 - loss 0.71473229 - samples/sec: 31.33 - lr: 0.100000
2021-05-18 01:40:24,897 epoch 3 - iter 640/643 - loss 0.72814195 - samples/sec: 30.19 - lr: 0.100000
2021-05-18 01:40:26,693 ----------------------------------------------------------------------------------------------------
2021-05-18 01:40:26,693 EPOCH 3 done: loss 0.7272 - lr 0.1000000
2021-05-18 01:40:41,935 DEV : loss 0.6266545057296753 - score 0.8895
2021-05-18 01:40:42,177 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:40:53,354 ----------------------------------------------------------------------------------------------------
2021-05-18 01:41:26,545 epoch 4 - iter 64/643 - loss 0.83102907 - samples/sec: 30.86 - lr: 0.100000
2021-05-18 01:41:54,545 epoch 4 - iter 128/643 - loss 0.76244564 - samples/sec: 36.58 - lr: 0.100000
2021-05-18 01:42:27,200 epoch 4 - iter 192/643 - loss 0.75241318 - samples/sec: 31.36 - lr: 0.100000
2021-05-18 01:42:59,723 epoch 4 - iter 256/643 - loss 0.72915511 - samples/sec: 31.49 - lr: 0.100000
2021-05-18 01:43:28,860 epoch 4 - iter 320/643 - loss 0.70943882 - samples/sec: 35.15 - lr: 0.100000
2021-05-18 01:43:57,589 epoch 4 - iter 384/643 - loss 0.69796571 - samples/sec: 35.65 - lr: 0.100000
2021-05-18 01:44:32,588 epoch 4 - iter 448/643 - loss 0.68525037 - samples/sec: 29.26 - lr: 0.100000
2021-05-18 01:45:03,215 epoch 4 - iter 512/643 - loss 0.68495469 - samples/sec: 33.44 - lr: 0.100000
2021-05-18 01:45:35,376 epoch 4 - iter 576/643 - loss 0.68346571 - samples/sec: 31.84 - lr: 0.100000
2021-05-18 01:46:06,926 epoch 4 - iter 640/643 - loss 0.67068379 - samples/sec: 32.46 - lr: 0.100000
2021-05-18 01:46:08,104 ----------------------------------------------------------------------------------------------------
2021-05-18 01:46:08,104 EPOCH 4 done: loss 0.6687 - lr 0.1000000
2021-05-18 01:46:22,878 DEV : loss 0.395761638879776 - score 0.9277
2021-05-18 01:46:23,107 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:46:34,361 ----------------------------------------------------------------------------------------------------
2021-05-18 01:47:05,317 epoch 5 - iter 64/643 - loss 0.67204686 - samples/sec: 33.08 - lr: 0.100000
2021-05-18 01:47:38,075 epoch 5 - iter 128/643 - loss 0.60182567 - samples/sec: 31.26 - lr: 0.100000
2021-05-18 01:48:11,158 epoch 5 - iter 192/643 - loss 0.60370568 - samples/sec: 30.96 - lr: 0.100000
2021-05-18 01:48:42,195 epoch 5 - iter 256/643 - loss 0.61478157 - samples/sec: 33.00 - lr: 0.100000
2021-05-18 01:49:15,090 epoch 5 - iter 320/643 - loss 0.61602499 - samples/sec: 31.13 - lr: 0.100000
2021-05-18 01:49:44,975 epoch 5 - iter 384/643 - loss 0.61774082 - samples/sec: 34.27 - lr: 0.100000
2021-05-18 01:50:13,381 epoch 5 - iter 448/643 - loss 0.60898051 - samples/sec: 36.05 - lr: 0.100000
2021-05-18 01:50:43,591 epoch 5 - iter 512/643 - loss 0.61132409 - samples/sec: 33.90 - lr: 0.100000
2021-05-18 01:51:13,386 epoch 5 - iter 576/643 - loss 0.59791552 - samples/sec: 34.37 - lr: 0.100000
2021-05-18 01:51:44,105 epoch 5 - iter 640/643 - loss 0.59117111 - samples/sec: 33.34 - lr: 0.100000
2021-05-18 01:51:45,673 ----------------------------------------------------------------------------------------------------
2021-05-18 01:51:45,674 EPOCH 5 done: loss 0.5919 - lr 0.1000000
2021-05-18 01:52:01,172 DEV : loss 0.3527889847755432 - score 0.931
2021-05-18 01:52:01,410 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 01:52:12,648 ----------------------------------------------------------------------------------------------------
2021-05-18 01:52:42,797 epoch 6 - iter 64/643 - loss 0.53598802 - samples/sec: 33.97 - lr: 0.100000
2021-05-18 01:53:11,223 epoch 6 - iter 128/643 - loss 0.55981845 - samples/sec: 36.03 - lr: 0.100000
2021-05-18 01:53:43,527 epoch 6 - iter 192/643 - loss 0.55494921 - samples/sec: 31.70 - lr: 0.100000
2021-05-18 01:54:14,390 epoch 6 - iter 256/643 - loss 0.55834586 - samples/sec: 33.18 - lr: 0.100000
2021-05-18 01:54:45,569 epoch 6 - iter 320/643 - loss 0.56225953 - samples/sec: 32.85 - lr: 0.100000
2021-05-18 01:55:17,689 epoch 6 - iter 384/643 - loss 0.56886614 - samples/sec: 31.88 - lr: 0.100000
2021-05-18 01:55:47,477 epoch 6 - iter 448/643 - loss 0.56854058 - samples/sec: 34.38 - lr: 0.100000
2021-05-18 01:56:19,239 epoch 6 - iter 512/643 - loss 0.56486672 - samples/sec: 32.24 - lr: 0.100000
2021-05-18 01:56:49,030 epoch 6 - iter 576/643 - loss 0.57244620 - samples/sec: 34.38 - lr: 0.100000
2021-05-18 01:57:21,732 epoch 6 - iter 640/643 - loss 0.55667802 - samples/sec: 31.32 - lr: 0.100000
2021-05-18 01:57:23,011 ----------------------------------------------------------------------------------------------------
2021-05-18 01:57:23,011 EPOCH 6 done: loss 0.5571 - lr 0.1000000
2021-05-18 01:57:37,064 DEV : loss 0.46899381279945374 - score 0.9059
2021-05-18 01:57:37,292 BAD EPOCHS (no improvement): 1
2021-05-18 01:57:37,292 ----------------------------------------------------------------------------------------------------
2021-05-18 01:58:06,226 epoch 7 - iter 64/643 - loss 0.51614246 - samples/sec: 35.40 - lr: 0.100000
2021-05-18 01:58:37,236 epoch 7 - iter 128/643 - loss 0.52732058 - samples/sec: 33.03 - lr: 0.100000
2021-05-18 01:59:08,865 epoch 7 - iter 192/643 - loss 0.50874359 - samples/sec: 32.38 - lr: 0.100000
2021-05-18 01:59:40,869 epoch 7 - iter 256/643 - loss 0.51404946 - samples/sec: 32.00 - lr: 0.100000
2021-05-18 02:00:10,801 epoch 7 - iter 320/643 - loss 0.52229008 - samples/sec: 34.21 - lr: 0.100000
2021-05-18 02:00:40,166 epoch 7 - iter 384/643 - loss 0.50918847 - samples/sec: 34.88 - lr: 0.100000
2021-05-18 02:01:10,834 epoch 7 - iter 448/643 - loss 0.50003163 - samples/sec: 33.39 - lr: 0.100000
2021-05-18 02:01:42,903 epoch 7 - iter 512/643 - loss 0.48948314 - samples/sec: 31.94 - lr: 0.100000
2021-05-18 02:02:14,854 epoch 7 - iter 576/643 - loss 0.49632642 - samples/sec: 32.05 - lr: 0.100000
2021-05-18 02:02:45,936 epoch 7 - iter 640/643 - loss 0.49796672 - samples/sec: 32.95 - lr: 0.100000
2021-05-18 02:02:47,234 ----------------------------------------------------------------------------------------------------
2021-05-18 02:02:47,235 EPOCH 7 done: loss 0.4982 - lr 0.1000000
2021-05-18 02:03:01,872 DEV : loss 0.32211795449256897 - score 0.9496
2021-05-18 02:03:02,126 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 02:03:13,294 ----------------------------------------------------------------------------------------------------
2021-05-18 02:03:42,907 epoch 8 - iter 64/643 - loss 0.48814525 - samples/sec: 34.59 - lr: 0.100000
2021-05-18 02:04:13,299 epoch 8 - iter 128/643 - loss 0.49910227 - samples/sec: 33.70 - lr: 0.100000
2021-05-18 02:04:43,445 epoch 8 - iter 192/643 - loss 0.50575387 - samples/sec: 33.97 - lr: 0.100000
2021-05-18 02:05:15,847 epoch 8 - iter 256/643 - loss 0.50182269 - samples/sec: 31.61 - lr: 0.100000
2021-05-18 02:05:46,689 epoch 8 - iter 320/643 - loss 0.49566709 - samples/sec: 33.21 - lr: 0.100000
2021-05-18 02:06:16,145 epoch 8 - iter 384/643 - loss 0.49684728 - samples/sec: 34.77 - lr: 0.100000
2021-05-18 02:06:48,308 epoch 8 - iter 448/643 - loss 0.50059323 - samples/sec: 31.84 - lr: 0.100000
2021-05-18 02:07:17,560 epoch 8 - iter 512/643 - loss 0.48880259 - samples/sec: 35.01 - lr: 0.100000
2021-05-18 02:07:50,819 epoch 8 - iter 576/643 - loss 0.49014904 - samples/sec: 30.79 - lr: 0.100000
2021-05-18 02:08:19,604 epoch 8 - iter 640/643 - loss 0.48765622 - samples/sec: 35.58 - lr: 0.100000
2021-05-18 02:08:20,993 ----------------------------------------------------------------------------------------------------
2021-05-18 02:08:20,993 EPOCH 8 done: loss 0.4870 - lr 0.1000000
2021-05-18 02:08:35,560 DEV : loss 0.4600290060043335 - score 0.9272
2021-05-18 02:08:35,790 BAD EPOCHS (no improvement): 1
2021-05-18 02:08:35,790 ----------------------------------------------------------------------------------------------------
2021-05-18 02:09:05,840 epoch 9 - iter 64/643 - loss 0.45261470 - samples/sec: 34.08 - lr: 0.100000
2021-05-18 02:09:36,354 epoch 9 - iter 128/643 - loss 0.45956952 - samples/sec: 33.56 - lr: 0.100000
2021-05-18 02:10:09,384 epoch 9 - iter 192/643 - loss 0.46090193 - samples/sec: 31.01 - lr: 0.100000
2021-05-18 02:10:40,839 epoch 9 - iter 256/643 - loss 0.46162774 - samples/sec: 32.56 - lr: 0.100000
2021-05-18 02:11:13,377 epoch 9 - iter 320/643 - loss 0.45192026 - samples/sec: 31.47 - lr: 0.100000
2021-05-18 02:11:45,071 epoch 9 - iter 384/643 - loss 0.44502005 - samples/sec: 32.31 - lr: 0.100000
2021-05-18 02:12:15,710 epoch 9 - iter 448/643 - loss 0.44650645 - samples/sec: 33.43 - lr: 0.100000
2021-05-18 02:12:47,252 epoch 9 - iter 512/643 - loss 0.44522833 - samples/sec: 32.47 - lr: 0.100000
2021-05-18 02:13:17,605 epoch 9 - iter 576/643 - loss 0.44136884 - samples/sec: 33.74 - lr: 0.100000
2021-05-18 02:13:47,091 epoch 9 - iter 640/643 - loss 0.43442153 - samples/sec: 34.73 - lr: 0.100000
2021-05-18 02:13:48,230 ----------------------------------------------------------------------------------------------------
2021-05-18 02:13:48,230 EPOCH 9 done: loss 0.4339 - lr 0.1000000
2021-05-18 02:14:02,263 DEV : loss 0.26245808601379395 - score 0.952
2021-05-18 02:14:02,538 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 02:14:13,834 ----------------------------------------------------------------------------------------------------
2021-05-18 02:14:44,819 epoch 10 - iter 64/643 - loss 0.42904068 - samples/sec: 33.05 - lr: 0.100000
2021-05-18 02:15:18,190 epoch 10 - iter 128/643 - loss 0.46466526 - samples/sec: 30.69 - lr: 0.100000
2021-05-18 02:15:49,140 epoch 10 - iter 192/643 - loss 0.45826343 - samples/sec: 33.09 - lr: 0.100000
2021-05-18 02:16:19,876 epoch 10 - iter 256/643 - loss 0.44415932 - samples/sec: 33.32 - lr: 0.100000
2021-05-18 02:16:50,428 epoch 10 - iter 320/643 - loss 0.43515178 - samples/sec: 33.52 - lr: 0.100000
2021-05-18 02:17:22,936 epoch 10 - iter 384/643 - loss 0.42302073 - samples/sec: 31.50 - lr: 0.100000
2021-05-18 02:17:56,109 epoch 10 - iter 448/643 - loss 0.41553746 - samples/sec: 30.87 - lr: 0.100000
2021-05-18 02:18:27,633 epoch 10 - iter 512/643 - loss 0.41538403 - samples/sec: 32.49 - lr: 0.100000
2021-05-18 02:18:59,839 epoch 10 - iter 576/643 - loss 0.41054173 - samples/sec: 31.80 - lr: 0.100000
2021-05-18 02:19:31,597 epoch 10 - iter 640/643 - loss 0.41186302 - samples/sec: 32.25 - lr: 0.100000
2021-05-18 02:19:32,971 ----------------------------------------------------------------------------------------------------
2021-05-18 02:19:32,972 EPOCH 10 done: loss 0.4120 - lr 0.1000000
2021-05-18 02:19:47,999 DEV : loss 0.24387668073177338 - score 0.962
2021-05-18 02:19:48,231 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 02:19:59,501 ----------------------------------------------------------------------------------------------------
2021-05-18 02:20:30,780 epoch 11 - iter 64/643 - loss 0.39495780 - samples/sec: 32.74 - lr: 0.100000
2021-05-18 02:21:00,111 epoch 11 - iter 128/643 - loss 0.40001676 - samples/sec: 34.92 - lr: 0.100000
2021-05-18 02:21:31,725 epoch 11 - iter 192/643 - loss 0.39234736 - samples/sec: 32.39 - lr: 0.100000
2021-05-18 02:22:06,208 epoch 11 - iter 256/643 - loss 0.39037918 - samples/sec: 29.70 - lr: 0.100000
2021-05-18 02:22:36,824 epoch 11 - iter 320/643 - loss 0.39239528 - samples/sec: 33.45 - lr: 0.100000
2021-05-18 02:23:07,303 epoch 11 - iter 384/643 - loss 0.39920814 - samples/sec: 33.60 - lr: 0.100000
2021-05-18 02:23:40,616 epoch 11 - iter 448/643 - loss 0.40160001 - samples/sec: 30.74 - lr: 0.100000
2021-05-18 02:24:12,529 epoch 11 - iter 512/643 - loss 0.40036257 - samples/sec: 32.09 - lr: 0.100000
2021-05-18 02:24:43,112 epoch 11 - iter 576/643 - loss 0.40054480 - samples/sec: 33.49 - lr: 0.100000
2021-05-18 02:25:14,348 epoch 11 - iter 640/643 - loss 0.39930107 - samples/sec: 32.79 - lr: 0.100000
2021-05-18 02:25:15,457 ----------------------------------------------------------------------------------------------------
2021-05-18 02:25:15,457 EPOCH 11 done: loss 0.3999 - lr 0.1000000
2021-05-18 02:25:30,920 DEV : loss 0.23764027655124664 - score 0.948
2021-05-18 02:25:31,153 BAD EPOCHS (no improvement): 1
2021-05-18 02:25:31,154 ----------------------------------------------------------------------------------------------------
2021-05-18 02:26:03,624 epoch 12 - iter 64/643 - loss 0.33267616 - samples/sec: 31.54 - lr: 0.100000
2021-05-18 02:26:38,686 epoch 12 - iter 128/643 - loss 0.35761712 - samples/sec: 29.21 - lr: 0.100000
2021-05-18 02:27:08,292 epoch 12 - iter 192/643 - loss 0.38928773 - samples/sec: 34.59 - lr: 0.100000
2021-05-18 02:27:39,155 epoch 12 - iter 256/643 - loss 0.38059464 - samples/sec: 33.18 - lr: 0.100000
2021-05-18 02:28:09,796 epoch 12 - iter 320/643 - loss 0.38463448 - samples/sec: 33.42 - lr: 0.100000
2021-05-18 02:28:39,493 epoch 12 - iter 384/643 - loss 0.39072380 - samples/sec: 34.49 - lr: 0.100000
2021-05-18 02:29:11,760 epoch 12 - iter 448/643 - loss 0.38574804 - samples/sec: 31.74 - lr: 0.100000
2021-05-18 02:29:44,160 epoch 12 - iter 512/643 - loss 0.38554856 - samples/sec: 31.61 - lr: 0.100000
2021-05-18 02:30:16,077 epoch 12 - iter 576/643 - loss 0.38827982 - samples/sec: 32.09 - lr: 0.100000
2021-05-18 02:30:46,888 epoch 12 - iter 640/643 - loss 0.39249958 - samples/sec: 33.24 - lr: 0.100000
2021-05-18 02:30:48,267 ----------------------------------------------------------------------------------------------------
2021-05-18 02:30:48,268 EPOCH 12 done: loss 0.3950 - lr 0.1000000
2021-05-18 02:31:02,393 DEV : loss 0.2834044396877289 - score 0.957
2021-05-18 02:31:02,624 BAD EPOCHS (no improvement): 2
2021-05-18 02:31:02,624 ----------------------------------------------------------------------------------------------------
2021-05-18 02:31:33,581 epoch 13 - iter 64/643 - loss 0.42450152 - samples/sec: 33.08 - lr: 0.100000
2021-05-18 02:32:05,949 epoch 13 - iter 128/643 - loss 0.38979269 - samples/sec: 31.64 - lr: 0.100000
2021-05-18 02:32:37,558 epoch 13 - iter 192/643 - loss 0.39258889 - samples/sec: 32.40 - lr: 0.100000
2021-05-18 02:33:09,108 epoch 13 - iter 256/643 - loss 0.38116681 - samples/sec: 32.46 - lr: 0.100000
2021-05-18 02:33:39,550 epoch 13 - iter 320/643 - loss 0.37233666 - samples/sec: 33.64 - lr: 0.100000
2021-05-18 02:34:10,194 epoch 13 - iter 384/643 - loss 0.38532157 - samples/sec: 33.42 - lr: 0.100000
2021-05-18 02:34:40,480 epoch 13 - iter 448/643 - loss 0.38136196 - samples/sec: 33.82 - lr: 0.100000
2021-05-18 02:35:10,206 epoch 13 - iter 512/643 - loss 0.37885916 - samples/sec: 34.45 - lr: 0.100000
2021-05-18 02:35:41,562 epoch 13 - iter 576/643 - loss 0.37780681 - samples/sec: 32.66 - lr: 0.100000
2021-05-18 02:36:13,448 epoch 13 - iter 640/643 - loss 0.38248821 - samples/sec: 32.12 - lr: 0.100000
2021-05-18 02:36:14,829 ----------------------------------------------------------------------------------------------------
2021-05-18 02:36:14,829 EPOCH 13 done: loss 0.3821 - lr 0.1000000
2021-05-18 02:36:28,704 DEV : loss 0.2731015980243683 - score 0.9395
2021-05-18 02:36:28,977 BAD EPOCHS (no improvement): 3
2021-05-18 02:36:28,977 ----------------------------------------------------------------------------------------------------
2021-05-18 02:37:02,076 epoch 14 - iter 64/643 - loss 0.36065765 - samples/sec: 30.94 - lr: 0.100000
2021-05-18 02:37:33,289 epoch 14 - iter 128/643 - loss 0.36877139 - samples/sec: 32.81 - lr: 0.100000
2021-05-18 02:38:05,306 epoch 14 - iter 192/643 - loss 0.36057814 - samples/sec: 31.99 - lr: 0.100000
2021-05-18 02:38:38,171 epoch 14 - iter 256/643 - loss 0.36100667 - samples/sec: 31.16 - lr: 0.100000
2021-05-18 02:39:11,566 epoch 14 - iter 320/643 - loss 0.35585940 - samples/sec: 30.67 - lr: 0.100000
2021-05-18 02:39:42,639 epoch 14 - iter 384/643 - loss 0.35886947 - samples/sec: 32.96 - lr: 0.100000
2021-05-18 02:40:13,066 epoch 14 - iter 448/643 - loss 0.35949622 - samples/sec: 33.66 - lr: 0.100000
2021-05-18 02:40:44,976 epoch 14 - iter 512/643 - loss 0.35952491 - samples/sec: 32.09 - lr: 0.100000
2021-05-18 02:41:16,201 epoch 14 - iter 576/643 - loss 0.35514179 - samples/sec: 32.80 - lr: 0.100000
2021-05-18 02:41:47,637 epoch 14 - iter 640/643 - loss 0.35757937 - samples/sec: 32.58 - lr: 0.100000
2021-05-18 02:41:49,097 ----------------------------------------------------------------------------------------------------
2021-05-18 02:41:49,098 EPOCH 14 done: loss 0.3570 - lr 0.1000000
2021-05-18 02:42:04,476 DEV : loss 0.2586304843425751 - score 0.9465
Epoch    14: reducing learning rate of group 0 to 5.0000e-02.
2021-05-18 02:42:04,719 BAD EPOCHS (no improvement): 4
2021-05-18 02:42:04,719 ----------------------------------------------------------------------------------------------------
2021-05-18 02:42:34,837 epoch 15 - iter 64/643 - loss 0.29831761 - samples/sec: 34.00 - lr: 0.050000
2021-05-18 02:43:05,270 epoch 15 - iter 128/643 - loss 0.29541467 - samples/sec: 33.65 - lr: 0.050000
2021-05-18 02:43:35,361 epoch 15 - iter 192/643 - loss 0.30381154 - samples/sec: 34.03 - lr: 0.050000
2021-05-18 02:44:07,413 epoch 15 - iter 256/643 - loss 0.31684987 - samples/sec: 31.95 - lr: 0.050000
2021-05-18 02:44:35,811 epoch 15 - iter 320/643 - loss 0.31322847 - samples/sec: 36.06 - lr: 0.050000
2021-05-18 02:45:06,979 epoch 15 - iter 384/643 - loss 0.31315301 - samples/sec: 32.86 - lr: 0.050000
2021-05-18 02:45:36,956 epoch 15 - iter 448/643 - loss 0.32151929 - samples/sec: 34.16 - lr: 0.050000
2021-05-18 02:46:09,045 epoch 15 - iter 512/643 - loss 0.31783527 - samples/sec: 31.91 - lr: 0.050000
2021-05-18 02:46:40,059 epoch 15 - iter 576/643 - loss 0.32388651 - samples/sec: 33.02 - lr: 0.050000
2021-05-18 02:47:09,034 epoch 15 - iter 640/643 - loss 0.32232986 - samples/sec: 35.35 - lr: 0.050000
2021-05-18 02:47:10,471 ----------------------------------------------------------------------------------------------------
2021-05-18 02:47:10,471 EPOCH 15 done: loss 0.3217 - lr 0.0500000
2021-05-18 02:47:28,110 DEV : loss 0.24616730213165283 - score 0.9682
2021-05-18 02:47:28,346 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 02:47:39,663 ----------------------------------------------------------------------------------------------------
2021-05-18 02:48:09,378 epoch 16 - iter 64/643 - loss 0.25420893 - samples/sec: 34.47 - lr: 0.050000
2021-05-18 02:48:42,863 epoch 16 - iter 128/643 - loss 0.27916322 - samples/sec: 30.58 - lr: 0.050000
2021-05-18 02:49:14,476 epoch 16 - iter 192/643 - loss 0.28864278 - samples/sec: 32.40 - lr: 0.050000
2021-05-18 02:49:46,038 epoch 16 - iter 256/643 - loss 0.28369081 - samples/sec: 32.45 - lr: 0.050000
2021-05-18 02:50:15,128 epoch 16 - iter 320/643 - loss 0.28766948 - samples/sec: 35.21 - lr: 0.050000
2021-05-18 02:50:46,883 epoch 16 - iter 384/643 - loss 0.28406366 - samples/sec: 32.25 - lr: 0.050000
2021-05-18 02:51:17,329 epoch 16 - iter 448/643 - loss 0.28611848 - samples/sec: 33.64 - lr: 0.050000
2021-05-18 02:51:47,597 epoch 16 - iter 512/643 - loss 0.28974473 - samples/sec: 33.84 - lr: 0.050000
2021-05-18 02:52:18,635 epoch 16 - iter 576/643 - loss 0.29203110 - samples/sec: 32.99 - lr: 0.050000
2021-05-18 02:52:48,569 epoch 16 - iter 640/643 - loss 0.29499946 - samples/sec: 34.21 - lr: 0.050000
2021-05-18 02:52:50,026 ----------------------------------------------------------------------------------------------------
2021-05-18 02:52:50,027 EPOCH 16 done: loss 0.2943 - lr 0.0500000
2021-05-18 02:53:03,993 DEV : loss 0.23551563918590546 - score 0.9617
2021-05-18 02:53:04,245 BAD EPOCHS (no improvement): 1
2021-05-18 02:53:04,245 ----------------------------------------------------------------------------------------------------
2021-05-18 02:53:37,973 epoch 17 - iter 64/643 - loss 0.31155245 - samples/sec: 30.36 - lr: 0.050000
2021-05-18 02:54:09,060 epoch 17 - iter 128/643 - loss 0.32685545 - samples/sec: 32.94 - lr: 0.050000
2021-05-18 02:54:41,702 epoch 17 - iter 192/643 - loss 0.30483521 - samples/sec: 31.37 - lr: 0.050000
2021-05-18 02:55:12,058 epoch 17 - iter 256/643 - loss 0.30569646 - samples/sec: 33.74 - lr: 0.050000
2021-05-18 02:55:41,902 epoch 17 - iter 320/643 - loss 0.29142448 - samples/sec: 34.32 - lr: 0.050000
2021-05-18 02:56:14,079 epoch 17 - iter 384/643 - loss 0.28343527 - samples/sec: 31.83 - lr: 0.050000
2021-05-18 02:56:44,960 epoch 17 - iter 448/643 - loss 0.28516765 - samples/sec: 33.16 - lr: 0.050000
2021-05-18 02:57:14,441 epoch 17 - iter 512/643 - loss 0.28084332 - samples/sec: 34.74 - lr: 0.050000
2021-05-18 02:57:44,536 epoch 17 - iter 576/643 - loss 0.27963779 - samples/sec: 34.03 - lr: 0.050000
2021-05-18 02:58:14,765 epoch 17 - iter 640/643 - loss 0.28348033 - samples/sec: 33.88 - lr: 0.050000
2021-05-18 02:58:16,245 ----------------------------------------------------------------------------------------------------
2021-05-18 02:58:16,245 EPOCH 17 done: loss 0.2833 - lr 0.0500000
2021-05-18 02:58:30,488 DEV : loss 0.23083682358264923 - score 0.9656
2021-05-18 02:58:30,722 BAD EPOCHS (no improvement): 2
2021-05-18 02:58:30,723 ----------------------------------------------------------------------------------------------------
2021-05-18 02:59:01,172 epoch 18 - iter 64/643 - loss 0.30220403 - samples/sec: 33.63 - lr: 0.050000
2021-05-18 02:59:33,730 epoch 18 - iter 128/643 - loss 0.28966930 - samples/sec: 31.46 - lr: 0.050000
2021-05-18 03:00:03,613 epoch 18 - iter 192/643 - loss 0.29571408 - samples/sec: 34.27 - lr: 0.050000
2021-05-18 03:00:33,686 epoch 18 - iter 256/643 - loss 0.29147151 - samples/sec: 34.05 - lr: 0.050000
2021-05-18 03:00:57,889 epoch 18 - iter 320/643 - loss 0.29435431 - samples/sec: 42.31 - lr: 0.050000
2021-05-18 03:01:16,947 epoch 18 - iter 384/643 - loss 0.29448659 - samples/sec: 53.74 - lr: 0.050000
2021-05-18 03:01:35,792 epoch 18 - iter 448/643 - loss 0.28528306 - samples/sec: 54.35 - lr: 0.050000
2021-05-18 03:01:54,654 epoch 18 - iter 512/643 - loss 0.28363308 - samples/sec: 54.30 - lr: 0.050000
2021-05-18 03:02:13,794 epoch 18 - iter 576/643 - loss 0.28482517 - samples/sec: 53.51 - lr: 0.050000
2021-05-18 03:02:32,555 epoch 18 - iter 640/643 - loss 0.28297249 - samples/sec: 54.59 - lr: 0.050000
2021-05-18 03:02:33,417 ----------------------------------------------------------------------------------------------------
2021-05-18 03:02:33,417 EPOCH 18 done: loss 0.2822 - lr 0.0500000
2021-05-18 03:02:42,379 DEV : loss 0.21183955669403076 - score 0.9619
2021-05-18 03:02:42,527 BAD EPOCHS (no improvement): 3
2021-05-18 03:02:42,527 ----------------------------------------------------------------------------------------------------
2021-05-18 03:03:01,447 epoch 19 - iter 64/643 - loss 0.24594682 - samples/sec: 54.13 - lr: 0.050000
2021-05-18 03:03:20,455 epoch 19 - iter 128/643 - loss 0.25644614 - samples/sec: 53.88 - lr: 0.050000
2021-05-18 03:03:39,435 epoch 19 - iter 192/643 - loss 0.26020970 - samples/sec: 53.96 - lr: 0.050000
2021-05-18 03:03:58,372 epoch 19 - iter 256/643 - loss 0.26389301 - samples/sec: 54.08 - lr: 0.050000
2021-05-18 03:04:17,451 epoch 19 - iter 320/643 - loss 0.27107599 - samples/sec: 53.68 - lr: 0.050000
2021-05-18 03:04:36,504 epoch 19 - iter 384/643 - loss 0.27132031 - samples/sec: 53.75 - lr: 0.050000
2021-05-18 03:04:55,299 epoch 19 - iter 448/643 - loss 0.27087249 - samples/sec: 54.49 - lr: 0.050000
2021-05-18 03:05:14,229 epoch 19 - iter 512/643 - loss 0.27335845 - samples/sec: 54.10 - lr: 0.050000
2021-05-18 03:05:33,173 epoch 19 - iter 576/643 - loss 0.27478163 - samples/sec: 54.06 - lr: 0.050000
2021-05-18 03:05:52,108 epoch 19 - iter 640/643 - loss 0.27578680 - samples/sec: 54.09 - lr: 0.050000
2021-05-18 03:05:52,965 ----------------------------------------------------------------------------------------------------
2021-05-18 03:05:52,965 EPOCH 19 done: loss 0.2757 - lr 0.0500000
2021-05-18 03:06:01,933 DEV : loss 0.20000265538692474 - score 0.9627
Epoch    19: reducing learning rate of group 0 to 2.5000e-02.
2021-05-18 03:06:02,080 BAD EPOCHS (no improvement): 4
2021-05-18 03:06:02,080 ----------------------------------------------------------------------------------------------------
2021-05-18 03:06:21,097 epoch 20 - iter 64/643 - loss 0.27290406 - samples/sec: 53.86 - lr: 0.025000
2021-05-18 03:06:40,187 epoch 20 - iter 128/643 - loss 0.23848271 - samples/sec: 53.65 - lr: 0.025000
2021-05-18 03:06:59,118 epoch 20 - iter 192/643 - loss 0.24255062 - samples/sec: 54.10 - lr: 0.025000
2021-05-18 03:07:17,924 epoch 20 - iter 256/643 - loss 0.24997159 - samples/sec: 54.46 - lr: 0.025000
2021-05-18 03:07:36,858 epoch 20 - iter 320/643 - loss 0.25302394 - samples/sec: 54.09 - lr: 0.025000
2021-05-18 03:07:55,596 epoch 20 - iter 384/643 - loss 0.26102438 - samples/sec: 54.65 - lr: 0.025000
2021-05-18 03:08:14,483 epoch 20 - iter 448/643 - loss 0.25650213 - samples/sec: 54.22 - lr: 0.025000
2021-05-18 03:08:33,453 epoch 20 - iter 512/643 - loss 0.25524591 - samples/sec: 53.99 - lr: 0.025000
2021-05-18 03:08:52,410 epoch 20 - iter 576/643 - loss 0.26023605 - samples/sec: 54.02 - lr: 0.025000
2021-05-18 03:09:12,301 epoch 20 - iter 640/643 - loss 0.26080484 - samples/sec: 51.49 - lr: 0.025000
2021-05-18 03:09:13,181 ----------------------------------------------------------------------------------------------------
2021-05-18 03:09:13,181 EPOCH 20 done: loss 0.2606 - lr 0.0250000
2021-05-18 03:09:22,187 DEV : loss 0.2054022252559662 - score 0.9645
2021-05-18 03:09:22,333 BAD EPOCHS (no improvement): 1
2021-05-18 03:09:22,333 ----------------------------------------------------------------------------------------------------
2021-05-18 03:09:41,443 epoch 21 - iter 64/643 - loss 0.26019177 - samples/sec: 53.59 - lr: 0.025000
2021-05-18 03:10:00,487 epoch 21 - iter 128/643 - loss 0.25454101 - samples/sec: 53.78 - lr: 0.025000
2021-05-18 03:10:19,404 epoch 21 - iter 192/643 - loss 0.25652005 - samples/sec: 54.14 - lr: 0.025000
2021-05-18 03:10:38,350 epoch 21 - iter 256/643 - loss 0.26341200 - samples/sec: 54.06 - lr: 0.025000
2021-05-18 03:10:57,314 epoch 21 - iter 320/643 - loss 0.25560586 - samples/sec: 54.00 - lr: 0.025000
2021-05-18 03:11:16,171 epoch 21 - iter 384/643 - loss 0.24800270 - samples/sec: 54.31 - lr: 0.025000
2021-05-18 03:11:35,234 epoch 21 - iter 448/643 - loss 0.24955916 - samples/sec: 53.72 - lr: 0.025000
2021-05-18 03:11:54,162 epoch 21 - iter 512/643 - loss 0.24890251 - samples/sec: 54.11 - lr: 0.025000
2021-05-18 03:12:13,005 epoch 21 - iter 576/643 - loss 0.25035553 - samples/sec: 54.35 - lr: 0.025000
2021-05-18 03:12:32,075 epoch 21 - iter 640/643 - loss 0.24702166 - samples/sec: 53.71 - lr: 0.025000
2021-05-18 03:12:32,920 ----------------------------------------------------------------------------------------------------
2021-05-18 03:12:32,921 EPOCH 21 done: loss 0.2472 - lr 0.0250000
2021-05-18 03:12:41,881 DEV : loss 0.20734718441963196 - score 0.9687
2021-05-18 03:12:42,028 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 03:12:52,599 ----------------------------------------------------------------------------------------------------
2021-05-18 03:13:11,673 epoch 22 - iter 64/643 - loss 0.25275532 - samples/sec: 53.70 - lr: 0.025000
2021-05-18 03:13:30,684 epoch 22 - iter 128/643 - loss 0.23419350 - samples/sec: 53.87 - lr: 0.025000
2021-05-18 03:13:49,474 epoch 22 - iter 192/643 - loss 0.24400471 - samples/sec: 54.51 - lr: 0.025000
2021-05-18 03:14:08,515 epoch 22 - iter 256/643 - loss 0.24134264 - samples/sec: 53.78 - lr: 0.025000
2021-05-18 03:14:27,447 epoch 22 - iter 320/643 - loss 0.24454350 - samples/sec: 54.09 - lr: 0.025000
2021-05-18 03:14:46,526 epoch 22 - iter 384/643 - loss 0.24445182 - samples/sec: 53.68 - lr: 0.025000
2021-05-18 03:15:05,459 epoch 22 - iter 448/643 - loss 0.24629614 - samples/sec: 54.09 - lr: 0.025000
2021-05-18 03:15:24,430 epoch 22 - iter 512/643 - loss 0.24489824 - samples/sec: 53.98 - lr: 0.025000
2021-05-18 03:15:43,281 epoch 22 - iter 576/643 - loss 0.24580979 - samples/sec: 54.33 - lr: 0.025000
2021-05-18 03:16:02,279 epoch 22 - iter 640/643 - loss 0.24308964 - samples/sec: 53.91 - lr: 0.025000
2021-05-18 03:16:03,145 ----------------------------------------------------------------------------------------------------
2021-05-18 03:16:03,145 EPOCH 22 done: loss 0.2442 - lr 0.0250000
2021-05-18 03:16:12,076 DEV : loss 0.2133687287569046 - score 0.9637
2021-05-18 03:16:12,223 BAD EPOCHS (no improvement): 1
2021-05-18 03:16:12,223 ----------------------------------------------------------------------------------------------------
2021-05-18 03:16:31,230 epoch 23 - iter 64/643 - loss 0.20491455 - samples/sec: 53.88 - lr: 0.025000
2021-05-18 03:16:50,198 epoch 23 - iter 128/643 - loss 0.22380318 - samples/sec: 53.99 - lr: 0.025000
2021-05-18 03:17:09,043 epoch 23 - iter 192/643 - loss 0.22572961 - samples/sec: 54.34 - lr: 0.025000
2021-05-18 03:17:27,961 epoch 23 - iter 256/643 - loss 0.22706628 - samples/sec: 54.14 - lr: 0.025000
2021-05-18 03:17:46,973 epoch 23 - iter 320/643 - loss 0.22577254 - samples/sec: 53.87 - lr: 0.025000
2021-05-18 03:18:05,990 epoch 23 - iter 384/643 - loss 0.23126617 - samples/sec: 53.85 - lr: 0.025000
2021-05-18 03:18:24,941 epoch 23 - iter 448/643 - loss 0.24211103 - samples/sec: 54.04 - lr: 0.025000
2021-05-18 03:18:44,050 epoch 23 - iter 512/643 - loss 0.23726997 - samples/sec: 53.60 - lr: 0.025000
2021-05-18 03:19:03,066 epoch 23 - iter 576/643 - loss 0.23516599 - samples/sec: 53.86 - lr: 0.025000
2021-05-18 03:19:22,132 epoch 23 - iter 640/643 - loss 0.23899232 - samples/sec: 53.72 - lr: 0.025000
2021-05-18 03:19:22,978 ----------------------------------------------------------------------------------------------------
2021-05-18 03:19:22,978 EPOCH 23 done: loss 0.2389 - lr 0.0250000
2021-05-18 03:19:31,947 DEV : loss 0.2006489634513855 - score 0.9682
2021-05-18 03:19:32,095 BAD EPOCHS (no improvement): 2
2021-05-18 03:19:32,095 ----------------------------------------------------------------------------------------------------
2021-05-18 03:19:51,060 epoch 24 - iter 64/643 - loss 0.26384567 - samples/sec: 54.00 - lr: 0.025000
2021-05-18 03:20:09,977 epoch 24 - iter 128/643 - loss 0.28435817 - samples/sec: 54.14 - lr: 0.025000
2021-05-18 03:20:28,948 epoch 24 - iter 192/643 - loss 0.25972292 - samples/sec: 53.98 - lr: 0.025000
2021-05-18 03:20:47,983 epoch 24 - iter 256/643 - loss 0.25268170 - samples/sec: 53.80 - lr: 0.025000
2021-05-18 03:21:06,970 epoch 24 - iter 320/643 - loss 0.25517803 - samples/sec: 53.94 - lr: 0.025000
2021-05-18 03:21:25,981 epoch 24 - iter 384/643 - loss 0.24713272 - samples/sec: 53.87 - lr: 0.025000
2021-05-18 03:21:45,083 epoch 24 - iter 448/643 - loss 0.24352187 - samples/sec: 53.62 - lr: 0.025000
2021-05-18 03:22:04,080 epoch 24 - iter 512/643 - loss 0.24575685 - samples/sec: 53.91 - lr: 0.025000
2021-05-18 03:22:23,147 epoch 24 - iter 576/643 - loss 0.24942684 - samples/sec: 53.71 - lr: 0.025000
2021-05-18 03:22:41,987 epoch 24 - iter 640/643 - loss 0.24984183 - samples/sec: 54.36 - lr: 0.025000
2021-05-18 03:22:42,844 ----------------------------------------------------------------------------------------------------
2021-05-18 03:22:42,844 EPOCH 24 done: loss 0.2491 - lr 0.0250000
2021-05-18 03:22:52,755 DEV : loss 0.210347980260849 - score 0.9691
2021-05-18 03:22:52,903 BAD EPOCHS (no improvement): 0
saving best model
2021-05-18 03:23:03,602 ----------------------------------------------------------------------------------------------------
2021-05-18 03:23:22,575 epoch 25 - iter 64/643 - loss 0.24681182 - samples/sec: 53.98 - lr: 0.025000
2021-05-18 03:23:41,354 epoch 25 - iter 128/643 - loss 0.24139381 - samples/sec: 54.54 - lr: 0.025000
2021-05-18 03:24:00,198 epoch 25 - iter 192/643 - loss 0.24290992 - samples/sec: 54.35 - lr: 0.025000
2021-05-18 03:24:19,145 epoch 25 - iter 256/643 - loss 0.24798421 - samples/sec: 54.05 - lr: 0.025000
2021-05-18 03:24:38,229 epoch 25 - iter 320/643 - loss 0.23500897 - samples/sec: 53.66 - lr: 0.025000
2021-05-18 03:24:57,075 epoch 25 - iter 384/643 - loss 0.23461550 - samples/sec: 54.34 - lr: 0.025000
2021-05-18 03:25:15,882 epoch 25 - iter 448/643 - loss 0.23347867 - samples/sec: 54.45 - lr: 0.025000
2021-05-18 03:25:34,953 epoch 25 - iter 512/643 - loss 0.23690733 - samples/sec: 53.70 - lr: 0.025000
2021-05-18 03:25:53,872 epoch 25 - iter 576/643 - loss 0.22968647 - samples/sec: 54.13 - lr: 0.025000
2021-05-18 03:26:12,910 epoch 25 - iter 640/643 - loss 0.23339737 - samples/sec: 53.80 - lr: 0.025000
2021-05-18 03:26:13,748 ----------------------------------------------------------------------------------------------------
2021-05-18 03:26:13,749 EPOCH 25 done: loss 0.2332 - lr 0.0250000
2021-05-18 03:26:22,737 DEV : loss 0.20218080282211304 - score 0.9686
2021-05-18 03:26:22,884 BAD EPOCHS (no improvement): 1
2021-05-18 03:26:22,884 ----------------------------------------------------------------------------------------------------
2021-05-18 03:26:41,981 epoch 26 - iter 64/643 - loss 0.24514401 - samples/sec: 53.63 - lr: 0.025000
2021-05-18 03:27:00,969 epoch 26 - iter 128/643 - loss 0.23895484 - samples/sec: 53.94 - lr: 0.025000
2021-05-18 03:27:19,903 epoch 26 - iter 192/643 - loss 0.23304225 - samples/sec: 54.09 - lr: 0.025000
2021-05-18 03:27:39,023 epoch 26 - iter 256/643 - loss 0.21970166 - samples/sec: 53.56 - lr: 0.025000
2021-05-18 03:27:57,991 epoch 26 - iter 320/643 - loss 0.22185218 - samples/sec: 53.99 - lr: 0.025000
2021-05-18 03:28:17,022 epoch 26 - iter 384/643 - loss 0.22812213 - samples/sec: 53.81 - lr: 0.025000
2021-05-18 03:28:36,091 epoch 26 - iter 448/643 - loss 0.23189872 - samples/sec: 53.71 - lr: 0.025000
2021-05-18 03:28:55,072 epoch 26 - iter 512/643 - loss 0.23497468 - samples/sec: 53.96 - lr: 0.025000
2021-05-18 03:29:13,977 epoch 26 - iter 576/643 - loss 0.23438085 - samples/sec: 54.17 - lr: 0.025000
2021-05-18 03:29:33,027 epoch 26 - iter 640/643 - loss 0.23663663 - samples/sec: 53.76 - lr: 0.025000
2021-05-18 03:29:33,880 ----------------------------------------------------------------------------------------------------
2021-05-18 03:29:33,880 EPOCH 26 done: loss 0.2362 - lr 0.0250000
2021-05-18 03:29:42,845 DEV : loss 0.1939728856086731 - score 0.9679
2021-05-18 03:29:42,991 BAD EPOCHS (no improvement): 2
2021-05-18 03:29:42,991 ----------------------------------------------------------------------------------------------------
2021-05-18 03:30:01,971 epoch 27 - iter 64/643 - loss 0.24365046 - samples/sec: 53.96 - lr: 0.025000
2021-05-18 03:30:20,911 epoch 27 - iter 128/643 - loss 0.24325509 - samples/sec: 54.07 - lr: 0.025000
2021-05-18 03:30:40,022 epoch 27 - iter 192/643 - loss 0.24612658 - samples/sec: 53.59 - lr: 0.025000
2021-05-18 03:30:58,944 epoch 27 - iter 256/643 - loss 0.24531058 - samples/sec: 54.12 - lr: 0.025000
2021-05-18 03:31:18,003 epoch 27 - iter 320/643 - loss 0.24651883 - samples/sec: 53.73 - lr: 0.025000
2021-05-18 03:31:37,058 epoch 27 - iter 384/643 - loss 0.24088148 - samples/sec: 53.75 - lr: 0.025000
2021-05-18 03:31:56,068 epoch 27 - iter 448/643 - loss 0.23317886 - samples/sec: 53.87 - lr: 0.025000
2021-05-18 03:32:15,036 epoch 27 - iter 512/643 - loss 0.23356983 - samples/sec: 53.99 - lr: 0.025000
2021-05-18 03:32:33,862 epoch 27 - iter 576/643 - loss 0.23546396 - samples/sec: 54.40 - lr: 0.025000
2021-05-18 03:32:52,791 epoch 27 - iter 640/643 - loss 0.23252688 - samples/sec: 54.10 - lr: 0.025000
2021-05-18 03:32:53,648 ----------------------------------------------------------------------------------------------------
2021-05-18 03:32:53,648 EPOCH 27 done: loss 0.2326 - lr 0.0250000
2021-05-18 03:33:02,630 DEV : loss 0.1928500235080719 - score 0.9683
2021-05-18 03:33:02,777 BAD EPOCHS (no improvement): 3
2021-05-18 03:33:02,778 ----------------------------------------------------------------------------------------------------
2021-05-18 03:33:21,815 epoch 28 - iter 64/643 - loss 0.26817567 - samples/sec: 53.80 - lr: 0.025000
2021-05-18 03:33:40,687 epoch 28 - iter 128/643 - loss 0.25233919 - samples/sec: 54.27 - lr: 0.025000
2021-05-18 03:33:59,507 epoch 28 - iter 192/643 - loss 0.24601734 - samples/sec: 54.42 - lr: 0.025000
2021-05-18 03:34:18,499 epoch 28 - iter 256/643 - loss 0.23747135 - samples/sec: 53.93 - lr: 0.025000
2021-05-18 03:34:37,301 epoch 28 - iter 320/643 - loss 0.23612662 - samples/sec: 54.47 - lr: 0.025000
2021-05-18 03:34:56,186 epoch 28 - iter 384/643 - loss 0.23310005 - samples/sec: 54.23 - lr: 0.025000
2021-05-18 03:35:15,276 epoch 28 - iter 448/643 - loss 0.22921375 - samples/sec: 53.65 - lr: 0.025000
2021-05-18 03:35:34,245 epoch 28 - iter 512/643 - loss 0.23518975 - samples/sec: 53.99 - lr: 0.025000
2021-05-18 03:35:53,178 epoch 28 - iter 576/643 - loss 0.23356096 - samples/sec: 54.10 - lr: 0.025000
2021-05-18 03:36:12,286 epoch 28 - iter 640/643 - loss 0.23444307 - samples/sec: 53.60 - lr: 0.025000
2021-05-18 03:36:13,157 ----------------------------------------------------------------------------------------------------
2021-05-18 03:36:13,157 EPOCH 28 done: loss 0.2343 - lr 0.0250000
2021-05-18 03:36:22,133 DEV : loss 0.23268727958202362 - score 0.9638
Epoch    28: reducing learning rate of group 0 to 1.2500e-02.
2021-05-18 03:36:22,280 BAD EPOCHS (no improvement): 4
2021-05-18 03:36:22,281 ----------------------------------------------------------------------------------------------------
2021-05-18 03:36:41,184 epoch 29 - iter 64/643 - loss 0.21730988 - samples/sec: 54.18 - lr: 0.012500
2021-05-18 03:37:01,052 epoch 29 - iter 128/643 - loss 0.22884633 - samples/sec: 51.55 - lr: 0.012500
2021-05-18 03:37:20,024 epoch 29 - iter 192/643 - loss 0.23184179 - samples/sec: 53.98 - lr: 0.012500
2021-05-18 03:37:39,041 epoch 29 - iter 256/643 - loss 0.23388439 - samples/sec: 53.85 - lr: 0.012500
2021-05-18 03:37:57,990 epoch 29 - iter 320/643 - loss 0.23797667 - samples/sec: 54.05 - lr: 0.012500
2021-05-18 03:38:16,812 epoch 29 - iter 384/643 - loss 0.23770682 - samples/sec: 54.41 - lr: 0.012500
2021-05-18 03:38:35,819 epoch 29 - iter 448/643 - loss 0.23681348 - samples/sec: 53.88 - lr: 0.012500
2021-05-18 03:38:54,875 epoch 29 - iter 512/643 - loss 0.23212479 - samples/sec: 53.74 - lr: 0.012500
2021-05-18 03:39:13,842 epoch 29 - iter 576/643 - loss 0.23060323 - samples/sec: 54.00 - lr: 0.012500
2021-05-18 03:39:32,819 epoch 29 - iter 640/643 - loss 0.22887727 - samples/sec: 53.97 - lr: 0.012500
2021-05-18 03:39:33,686 ----------------------------------------------------------------------------------------------------
2021-05-18 03:39:33,686 EPOCH 29 done: loss 0.2304 - lr 0.0125000
2021-05-18 03:39:42,654 DEV : loss 0.18674269318580627 - score 0.9668
2021-05-18 03:39:42,801 BAD EPOCHS (no improvement): 1
2021-05-18 03:39:42,802 ----------------------------------------------------------------------------------------------------
2021-05-18 03:40:01,719 epoch 30 - iter 64/643 - loss 0.22116172 - samples/sec: 54.14 - lr: 0.012500
2021-05-18 03:40:20,712 epoch 30 - iter 128/643 - loss 0.20508761 - samples/sec: 53.92 - lr: 0.012500
2021-05-18 03:40:39,601 epoch 30 - iter 192/643 - loss 0.19704464 - samples/sec: 54.22 - lr: 0.012500
2021-05-18 03:40:58,509 epoch 30 - iter 256/643 - loss 0.20292163 - samples/sec: 54.16 - lr: 0.012500
2021-05-18 03:41:17,530 epoch 30 - iter 320/643 - loss 0.21192887 - samples/sec: 53.84 - lr: 0.012500
2021-05-18 03:41:36,479 epoch 30 - iter 384/643 - loss 0.21371906 - samples/sec: 54.05 - lr: 0.012500
2021-05-18 03:41:55,371 epoch 30 - iter 448/643 - loss 0.20981399 - samples/sec: 54.21 - lr: 0.012500
2021-05-18 03:42:14,345 epoch 30 - iter 512/643 - loss 0.21319266 - samples/sec: 53.98 - lr: 0.012500
2021-05-18 03:42:33,307 epoch 30 - iter 576/643 - loss 0.21839568 - samples/sec: 54.01 - lr: 0.012500
2021-05-18 03:42:52,174 epoch 30 - iter 640/643 - loss 0.22237142 - samples/sec: 54.28 - lr: 0.012500
2021-05-18 03:42:53,031 ----------------------------------------------------------------------------------------------------
2021-05-18 03:42:53,031 EPOCH 30 done: loss 0.2220 - lr 0.0125000
2021-05-18 03:43:01,955 DEV : loss 0.1928671896457672 - score 0.9687
2021-05-18 03:43:02,110 BAD EPOCHS (no improvement): 2
2021-05-18 03:43:12,702 ----------------------------------------------------------------------------------------------------
2021-05-18 03:43:12,702 Testing using best model ...
2021-05-18 03:43:12,702 loading file /home/shabnam/data/codes/data/DISRPT2019-output_data/rus.rst.rrt/best-model.pt
2021-05-18 03:43:56,798 0.9588	0.9646	0.9617
2021-05-18 03:43:56,798 
Results:
- F1-score (micro) 0.9617
- F1-score (macro) 0.9617

By class:
SENT       tp: 1280 - fp: 55 - fn: 47 - precision: 0.9588 - recall: 0.9646 - f1-score: 0.9617
2021-05-18 03:43:56,798 ----------------------------------------------------------------------------------------------------
