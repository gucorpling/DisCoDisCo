/home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb/
2021-05-24 12:50:30,508 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb
2021-05-24 12:50:30,508 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb/sent_train.txt
2021-05-24 12:50:30,508 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb/sent_dev.txt
2021-05-24 12:50:30,508 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb/sent_test.txt
Corpus: 2538 train + 541 dev + 490 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 12:50:54,323 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,326 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 12:50:54,326 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,326 Corpus: "Corpus: 2538 train + 541 dev + 490 test sentences"
2021-05-24 12:50:54,326 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,326 Parameters:
2021-05-24 12:50:54,326  - learning_rate: "0.1"
2021-05-24 12:50:54,326  - mini_batch_size: "16"
2021-05-24 12:50:54,326  - patience: "3"
2021-05-24 12:50:54,326  - anneal_factor: "0.5"
2021-05-24 12:50:54,326  - max_epochs: "30"
2021-05-24 12:50:54,326  - shuffle: "True"
2021-05-24 12:50:54,326  - train_with_dev: "False"
2021-05-24 12:50:54,327  - batch_growth_annealing: "False"
2021-05-24 12:50:54,327 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,327 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb"
2021-05-24 12:50:54,327 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,327 Device: cuda:0
2021-05-24 12:50:54,327 ----------------------------------------------------------------------------------------------------
2021-05-24 12:50:54,327 Embeddings storage mode: cpu
2021-05-24 12:50:54,328 ----------------------------------------------------------------------------------------------------
2021-05-24 12:51:05,134 epoch 1 - iter 15/159 - loss 10.49642200 - samples/sec: 22.23 - lr: 0.100000
2021-05-24 12:51:15,331 epoch 1 - iter 30/159 - loss 7.44138133 - samples/sec: 23.54 - lr: 0.100000
2021-05-24 12:51:25,502 epoch 1 - iter 45/159 - loss 6.39283209 - samples/sec: 23.60 - lr: 0.100000
2021-05-24 12:51:35,815 epoch 1 - iter 60/159 - loss 5.77098041 - samples/sec: 23.27 - lr: 0.100000
2021-05-24 12:51:46,109 epoch 1 - iter 75/159 - loss 5.30963749 - samples/sec: 23.31 - lr: 0.100000
2021-05-24 12:51:56,423 epoch 1 - iter 90/159 - loss 4.89396122 - samples/sec: 23.27 - lr: 0.100000
2021-05-24 12:52:06,765 epoch 1 - iter 105/159 - loss 4.53997762 - samples/sec: 23.21 - lr: 0.100000
2021-05-24 12:52:17,068 epoch 1 - iter 120/159 - loss 4.34051415 - samples/sec: 23.30 - lr: 0.100000
2021-05-24 12:52:27,681 epoch 1 - iter 135/159 - loss 4.07739878 - samples/sec: 22.61 - lr: 0.100000
2021-05-24 12:52:38,004 epoch 1 - iter 150/159 - loss 3.84488547 - samples/sec: 23.25 - lr: 0.100000
2021-05-24 12:52:43,978 ----------------------------------------------------------------------------------------------------
2021-05-24 12:52:43,978 EPOCH 1 done: loss 3.7639 - lr 0.1000000
2021-05-24 12:53:00,236 DEV : loss 0.6962494254112244 - score 0.872
2021-05-24 12:53:00,289 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 12:53:01,463 ----------------------------------------------------------------------------------------------------
2021-05-24 12:53:05,161 epoch 2 - iter 15/159 - loss 1.76352828 - samples/sec: 64.91 - lr: 0.100000
2021-05-24 12:53:08,824 epoch 2 - iter 30/159 - loss 1.81248850 - samples/sec: 65.54 - lr: 0.100000
2021-05-24 12:53:12,519 epoch 2 - iter 45/159 - loss 1.74340978 - samples/sec: 64.97 - lr: 0.100000
2021-05-24 12:53:16,187 epoch 2 - iter 60/159 - loss 1.63894734 - samples/sec: 65.44 - lr: 0.100000
2021-05-24 12:53:19,813 epoch 2 - iter 75/159 - loss 1.55164505 - samples/sec: 66.20 - lr: 0.100000
2021-05-24 12:53:23,436 epoch 2 - iter 90/159 - loss 1.50917587 - samples/sec: 66.26 - lr: 0.100000
2021-05-24 12:53:27,026 epoch 2 - iter 105/159 - loss 1.47193811 - samples/sec: 66.85 - lr: 0.100000
2021-05-24 12:53:30,701 epoch 2 - iter 120/159 - loss 1.45242184 - samples/sec: 65.32 - lr: 0.100000
2021-05-24 12:53:34,369 epoch 2 - iter 135/159 - loss 1.43127704 - samples/sec: 65.45 - lr: 0.100000
2021-05-24 12:53:38,036 epoch 2 - iter 150/159 - loss 1.42506120 - samples/sec: 65.46 - lr: 0.100000
2021-05-24 12:53:40,143 ----------------------------------------------------------------------------------------------------
2021-05-24 12:53:40,143 EPOCH 2 done: loss 1.4036 - lr 0.1000000
2021-05-24 12:53:43,091 DEV : loss 0.5420125722885132 - score 0.9026
2021-05-24 12:53:43,144 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 12:53:53,192 ----------------------------------------------------------------------------------------------------
2021-05-24 12:53:56,844 epoch 3 - iter 15/159 - loss 1.23485945 - samples/sec: 65.75 - lr: 0.100000
2021-05-24 12:54:00,486 epoch 3 - iter 30/159 - loss 1.15671722 - samples/sec: 65.90 - lr: 0.100000
2021-05-24 12:54:04,136 epoch 3 - iter 45/159 - loss 1.12860865 - samples/sec: 65.76 - lr: 0.100000
2021-05-24 12:54:07,803 epoch 3 - iter 60/159 - loss 1.12899397 - samples/sec: 65.46 - lr: 0.100000
2021-05-24 12:54:11,466 epoch 3 - iter 75/159 - loss 1.08652360 - samples/sec: 65.53 - lr: 0.100000
2021-05-24 12:54:15,176 epoch 3 - iter 90/159 - loss 1.06647319 - samples/sec: 64.71 - lr: 0.100000
2021-05-24 12:54:18,844 epoch 3 - iter 105/159 - loss 1.04623872 - samples/sec: 65.44 - lr: 0.100000
2021-05-24 12:54:22,516 epoch 3 - iter 120/159 - loss 1.04714475 - samples/sec: 65.37 - lr: 0.100000
2021-05-24 12:54:26,178 epoch 3 - iter 135/159 - loss 1.02749708 - samples/sec: 65.56 - lr: 0.100000
2021-05-24 12:54:29,773 epoch 3 - iter 150/159 - loss 1.02184366 - samples/sec: 66.76 - lr: 0.100000
2021-05-24 12:54:31,900 ----------------------------------------------------------------------------------------------------
2021-05-24 12:54:31,901 EPOCH 3 done: loss 1.0206 - lr 0.1000000
2021-05-24 12:54:34,842 DEV : loss 0.47016260027885437 - score 0.9031
2021-05-24 12:54:34,894 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 12:54:44,137 ----------------------------------------------------------------------------------------------------
2021-05-24 12:54:47,809 epoch 4 - iter 15/159 - loss 0.82454323 - samples/sec: 65.38 - lr: 0.100000
2021-05-24 12:54:51,405 epoch 4 - iter 30/159 - loss 0.72952367 - samples/sec: 66.75 - lr: 0.100000
2021-05-24 12:54:55,044 epoch 4 - iter 45/159 - loss 0.81009303 - samples/sec: 65.96 - lr: 0.100000
2021-05-24 12:54:58,681 epoch 4 - iter 60/159 - loss 0.79988735 - samples/sec: 66.00 - lr: 0.100000
2021-05-24 12:55:02,329 epoch 4 - iter 75/159 - loss 0.86186463 - samples/sec: 65.81 - lr: 0.100000
2021-05-24 12:55:05,948 epoch 4 - iter 90/159 - loss 0.84919790 - samples/sec: 66.34 - lr: 0.100000
2021-05-24 12:55:09,579 epoch 4 - iter 105/159 - loss 0.88497886 - samples/sec: 66.11 - lr: 0.100000
2021-05-24 12:55:13,229 epoch 4 - iter 120/159 - loss 0.90441607 - samples/sec: 65.77 - lr: 0.100000
2021-05-24 12:55:16,901 epoch 4 - iter 135/159 - loss 0.90614997 - samples/sec: 65.37 - lr: 0.100000
2021-05-24 12:55:20,576 epoch 4 - iter 150/159 - loss 0.87758316 - samples/sec: 65.32 - lr: 0.100000
2021-05-24 12:55:22,700 ----------------------------------------------------------------------------------------------------
2021-05-24 12:55:22,701 EPOCH 4 done: loss 0.8842 - lr 0.1000000
2021-05-24 12:55:25,645 DEV : loss 0.835846483707428 - score 0.859
2021-05-24 12:55:25,697 BAD EPOCHS (no improvement): 1
2021-05-24 12:55:25,698 ----------------------------------------------------------------------------------------------------
2021-05-24 12:55:29,366 epoch 5 - iter 15/159 - loss 1.16747853 - samples/sec: 65.44 - lr: 0.100000
2021-05-24 12:55:33,018 epoch 5 - iter 30/159 - loss 0.87645856 - samples/sec: 65.73 - lr: 0.100000
2021-05-24 12:55:36,711 epoch 5 - iter 45/159 - loss 0.89377133 - samples/sec: 65.00 - lr: 0.100000
2021-05-24 12:55:40,428 epoch 5 - iter 60/159 - loss 0.96853303 - samples/sec: 64.58 - lr: 0.100000
2021-05-24 12:55:44,111 epoch 5 - iter 75/159 - loss 0.90384142 - samples/sec: 65.18 - lr: 0.100000
2021-05-24 12:55:47,844 epoch 5 - iter 90/159 - loss 0.85704400 - samples/sec: 64.31 - lr: 0.100000
2021-05-24 12:55:51,503 epoch 5 - iter 105/159 - loss 0.82028601 - samples/sec: 65.60 - lr: 0.100000
2021-05-24 12:55:55,172 epoch 5 - iter 120/159 - loss 0.84315662 - samples/sec: 65.44 - lr: 0.100000
2021-05-24 12:55:58,818 epoch 5 - iter 135/159 - loss 0.84063272 - samples/sec: 65.84 - lr: 0.100000
2021-05-24 12:56:02,496 epoch 5 - iter 150/159 - loss 0.83175126 - samples/sec: 65.27 - lr: 0.100000
2021-05-24 12:56:04,626 ----------------------------------------------------------------------------------------------------
2021-05-24 12:56:04,626 EPOCH 5 done: loss 0.8176 - lr 0.1000000
2021-05-24 12:56:07,571 DEV : loss 0.8818099498748779 - score 0.8081
2021-05-24 12:56:07,623 BAD EPOCHS (no improvement): 2
2021-05-24 12:56:07,623 ----------------------------------------------------------------------------------------------------
2021-05-24 12:56:11,276 epoch 6 - iter 15/159 - loss 0.70361918 - samples/sec: 65.72 - lr: 0.100000
2021-05-24 12:56:14,965 epoch 6 - iter 30/159 - loss 0.75961552 - samples/sec: 65.06 - lr: 0.100000
2021-05-24 12:56:18,620 epoch 6 - iter 45/159 - loss 0.78635231 - samples/sec: 65.69 - lr: 0.100000
2021-05-24 12:56:22,291 epoch 6 - iter 60/159 - loss 0.79309181 - samples/sec: 65.39 - lr: 0.100000
2021-05-24 12:56:25,947 epoch 6 - iter 75/159 - loss 0.77596452 - samples/sec: 65.65 - lr: 0.100000
2021-05-24 12:56:29,662 epoch 6 - iter 90/159 - loss 0.76101275 - samples/sec: 64.63 - lr: 0.100000
2021-05-24 12:56:33,340 epoch 6 - iter 105/159 - loss 0.73690495 - samples/sec: 65.25 - lr: 0.100000
2021-05-24 12:56:37,037 epoch 6 - iter 120/159 - loss 0.73856090 - samples/sec: 64.94 - lr: 0.100000
2021-05-24 12:56:40,733 epoch 6 - iter 135/159 - loss 0.77298804 - samples/sec: 64.95 - lr: 0.100000
2021-05-24 12:56:44,427 epoch 6 - iter 150/159 - loss 0.77599001 - samples/sec: 64.99 - lr: 0.100000
2021-05-24 12:56:46,559 ----------------------------------------------------------------------------------------------------
2021-05-24 12:56:46,560 EPOCH 6 done: loss 0.7690 - lr 0.1000000
2021-05-24 12:56:49,832 DEV : loss 0.5357003808021545 - score 0.9004
2021-05-24 12:56:49,884 BAD EPOCHS (no improvement): 3
2021-05-24 12:56:49,885 ----------------------------------------------------------------------------------------------------
2021-05-24 12:56:53,562 epoch 7 - iter 15/159 - loss 0.75300746 - samples/sec: 65.28 - lr: 0.100000
2021-05-24 12:56:57,227 epoch 7 - iter 30/159 - loss 0.77597705 - samples/sec: 65.50 - lr: 0.100000
2021-05-24 12:57:00,924 epoch 7 - iter 45/159 - loss 0.72687331 - samples/sec: 64.92 - lr: 0.100000
2021-05-24 12:57:04,591 epoch 7 - iter 60/159 - loss 0.72394551 - samples/sec: 65.47 - lr: 0.100000
2021-05-24 12:57:08,274 epoch 7 - iter 75/159 - loss 0.72168673 - samples/sec: 65.18 - lr: 0.100000
2021-05-24 12:57:11,999 epoch 7 - iter 90/159 - loss 0.74236798 - samples/sec: 64.44 - lr: 0.100000
2021-05-24 12:57:15,675 epoch 7 - iter 105/159 - loss 0.76054102 - samples/sec: 65.30 - lr: 0.100000
2021-05-24 12:57:19,404 epoch 7 - iter 120/159 - loss 0.72854453 - samples/sec: 64.38 - lr: 0.100000
2021-05-24 12:57:23,131 epoch 7 - iter 135/159 - loss 0.71908823 - samples/sec: 64.41 - lr: 0.100000
2021-05-24 12:57:26,825 epoch 7 - iter 150/159 - loss 0.73316115 - samples/sec: 64.98 - lr: 0.100000
2021-05-24 12:57:28,967 ----------------------------------------------------------------------------------------------------
2021-05-24 12:57:28,967 EPOCH 7 done: loss 0.7304 - lr 0.1000000
2021-05-24 12:57:31,921 DEV : loss 0.48562297224998474 - score 0.886
Epoch     7: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 12:57:31,974 BAD EPOCHS (no improvement): 4
2021-05-24 12:57:31,974 ----------------------------------------------------------------------------------------------------
2021-05-24 12:57:35,670 epoch 8 - iter 15/159 - loss 0.65609729 - samples/sec: 64.96 - lr: 0.050000
2021-05-24 12:57:39,385 epoch 8 - iter 30/159 - loss 0.63113460 - samples/sec: 64.62 - lr: 0.050000
2021-05-24 12:57:43,121 epoch 8 - iter 45/159 - loss 0.60450709 - samples/sec: 64.24 - lr: 0.050000
2021-05-24 12:57:46,838 epoch 8 - iter 60/159 - loss 0.60545229 - samples/sec: 64.58 - lr: 0.050000
2021-05-24 12:57:50,523 epoch 8 - iter 75/159 - loss 0.56967196 - samples/sec: 65.15 - lr: 0.050000
2021-05-24 12:57:54,185 epoch 8 - iter 90/159 - loss 0.57877455 - samples/sec: 65.56 - lr: 0.050000
2021-05-24 12:57:57,855 epoch 8 - iter 105/159 - loss 0.58754094 - samples/sec: 65.40 - lr: 0.050000
2021-05-24 12:58:01,487 epoch 8 - iter 120/159 - loss 0.59305743 - samples/sec: 66.10 - lr: 0.050000
2021-05-24 12:58:05,145 epoch 8 - iter 135/159 - loss 0.59448592 - samples/sec: 65.63 - lr: 0.050000
2021-05-24 12:58:08,823 epoch 8 - iter 150/159 - loss 0.58815794 - samples/sec: 65.26 - lr: 0.050000
2021-05-24 12:58:10,975 ----------------------------------------------------------------------------------------------------
2021-05-24 12:58:10,975 EPOCH 8 done: loss 0.5834 - lr 0.0500000
2021-05-24 12:58:13,931 DEV : loss 0.3931913673877716 - score 0.9085
2021-05-24 12:58:13,984 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 12:58:24,276 ----------------------------------------------------------------------------------------------------
2021-05-24 12:58:27,970 epoch 9 - iter 15/159 - loss 0.68978318 - samples/sec: 64.99 - lr: 0.050000
2021-05-24 12:58:31,653 epoch 9 - iter 30/159 - loss 0.57192670 - samples/sec: 65.18 - lr: 0.050000
2021-05-24 12:58:35,324 epoch 9 - iter 45/159 - loss 0.55547952 - samples/sec: 65.39 - lr: 0.050000
2021-05-24 12:58:38,994 epoch 9 - iter 60/159 - loss 0.51283009 - samples/sec: 65.40 - lr: 0.050000
2021-05-24 12:58:42,681 epoch 9 - iter 75/159 - loss 0.52737158 - samples/sec: 65.11 - lr: 0.050000
2021-05-24 12:58:46,389 epoch 9 - iter 90/159 - loss 0.51537174 - samples/sec: 64.74 - lr: 0.050000
2021-05-24 12:58:50,085 epoch 9 - iter 105/159 - loss 0.51110091 - samples/sec: 64.94 - lr: 0.050000
2021-05-24 12:58:53,761 epoch 9 - iter 120/159 - loss 0.53941527 - samples/sec: 65.30 - lr: 0.050000
2021-05-24 12:58:57,418 epoch 9 - iter 135/159 - loss 0.53223375 - samples/sec: 65.65 - lr: 0.050000
2021-05-24 12:59:01,072 epoch 9 - iter 150/159 - loss 0.53308258 - samples/sec: 65.68 - lr: 0.050000
2021-05-24 12:59:03,213 ----------------------------------------------------------------------------------------------------
2021-05-24 12:59:03,213 EPOCH 9 done: loss 0.5284 - lr 0.0500000
2021-05-24 12:59:06,175 DEV : loss 0.37319889664649963 - score 0.9131
2021-05-24 12:59:06,227 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 12:59:15,621 ----------------------------------------------------------------------------------------------------
2021-05-24 12:59:19,341 epoch 10 - iter 15/159 - loss 0.48777722 - samples/sec: 64.54 - lr: 0.050000
2021-05-24 12:59:23,004 epoch 10 - iter 30/159 - loss 0.47143792 - samples/sec: 65.52 - lr: 0.050000
2021-05-24 12:59:26,668 epoch 10 - iter 45/159 - loss 0.45896930 - samples/sec: 65.52 - lr: 0.050000
2021-05-24 12:59:30,273 epoch 10 - iter 60/159 - loss 0.51009417 - samples/sec: 66.59 - lr: 0.050000
2021-05-24 12:59:33,940 epoch 10 - iter 75/159 - loss 0.50228289 - samples/sec: 65.47 - lr: 0.050000
2021-05-24 12:59:37,560 epoch 10 - iter 90/159 - loss 0.48162793 - samples/sec: 66.31 - lr: 0.050000
2021-05-24 12:59:41,183 epoch 10 - iter 105/159 - loss 0.49415989 - samples/sec: 66.25 - lr: 0.050000
2021-05-24 12:59:44,832 epoch 10 - iter 120/159 - loss 0.47575209 - samples/sec: 65.79 - lr: 0.050000
2021-05-24 12:59:48,505 epoch 10 - iter 135/159 - loss 0.49020956 - samples/sec: 65.36 - lr: 0.050000
2021-05-24 12:59:52,161 epoch 10 - iter 150/159 - loss 0.49496309 - samples/sec: 65.66 - lr: 0.050000
2021-05-24 12:59:54,307 ----------------------------------------------------------------------------------------------------
2021-05-24 12:59:54,307 EPOCH 10 done: loss 0.4886 - lr 0.0500000
2021-05-24 12:59:57,265 DEV : loss 0.3753516972064972 - score 0.9181
2021-05-24 12:59:57,317 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:00:07,590 ----------------------------------------------------------------------------------------------------
2021-05-24 13:00:11,287 epoch 11 - iter 15/159 - loss 0.55804051 - samples/sec: 64.96 - lr: 0.050000
2021-05-24 13:00:14,948 epoch 11 - iter 30/159 - loss 0.52950584 - samples/sec: 65.56 - lr: 0.050000
2021-05-24 13:00:18,618 epoch 11 - iter 45/159 - loss 0.48163163 - samples/sec: 65.42 - lr: 0.050000
2021-05-24 13:00:22,281 epoch 11 - iter 60/159 - loss 0.46659577 - samples/sec: 65.53 - lr: 0.050000
2021-05-24 13:00:25,939 epoch 11 - iter 75/159 - loss 0.49240632 - samples/sec: 65.63 - lr: 0.050000
2021-05-24 13:00:29,621 epoch 11 - iter 90/159 - loss 0.49770937 - samples/sec: 65.19 - lr: 0.050000
2021-05-24 13:00:33,251 epoch 11 - iter 105/159 - loss 0.51053389 - samples/sec: 66.13 - lr: 0.050000
2021-05-24 13:00:36,890 epoch 11 - iter 120/159 - loss 0.49941338 - samples/sec: 65.97 - lr: 0.050000
2021-05-24 13:00:40,549 epoch 11 - iter 135/159 - loss 0.49453053 - samples/sec: 65.61 - lr: 0.050000
2021-05-24 13:00:44,176 epoch 11 - iter 150/159 - loss 0.48364168 - samples/sec: 66.18 - lr: 0.050000
2021-05-24 13:00:46,283 ----------------------------------------------------------------------------------------------------
2021-05-24 13:00:46,284 EPOCH 11 done: loss 0.4740 - lr 0.0500000
2021-05-24 13:00:49,239 DEV : loss 0.36302754282951355 - score 0.9234
2021-05-24 13:00:49,291 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:00:59,164 ----------------------------------------------------------------------------------------------------
2021-05-24 13:01:02,808 epoch 12 - iter 15/159 - loss 0.40075420 - samples/sec: 65.88 - lr: 0.050000
2021-05-24 13:01:06,477 epoch 12 - iter 30/159 - loss 0.40856891 - samples/sec: 65.43 - lr: 0.050000
2021-05-24 13:01:10,137 epoch 12 - iter 45/159 - loss 0.40279322 - samples/sec: 65.58 - lr: 0.050000
2021-05-24 13:01:13,825 epoch 12 - iter 60/159 - loss 0.42062629 - samples/sec: 65.09 - lr: 0.050000
2021-05-24 13:01:17,523 epoch 12 - iter 75/159 - loss 0.43011706 - samples/sec: 64.92 - lr: 0.050000
2021-05-24 13:01:21,180 epoch 12 - iter 90/159 - loss 0.46289330 - samples/sec: 65.65 - lr: 0.050000
2021-05-24 13:01:24,867 epoch 12 - iter 105/159 - loss 0.44917238 - samples/sec: 65.11 - lr: 0.050000
2021-05-24 13:01:28,562 epoch 12 - iter 120/159 - loss 0.46165702 - samples/sec: 64.96 - lr: 0.050000
2021-05-24 13:01:32,215 epoch 12 - iter 135/159 - loss 0.45576047 - samples/sec: 65.72 - lr: 0.050000
2021-05-24 13:01:35,897 epoch 12 - iter 150/159 - loss 0.46930280 - samples/sec: 65.20 - lr: 0.050000
2021-05-24 13:01:38,063 ----------------------------------------------------------------------------------------------------
2021-05-24 13:01:38,063 EPOCH 12 done: loss 0.4747 - lr 0.0500000
2021-05-24 13:01:41,020 DEV : loss 0.38455888628959656 - score 0.9153
2021-05-24 13:01:41,072 BAD EPOCHS (no improvement): 1
2021-05-24 13:01:41,073 ----------------------------------------------------------------------------------------------------
2021-05-24 13:01:44,759 epoch 13 - iter 15/159 - loss 0.44081671 - samples/sec: 65.12 - lr: 0.050000
2021-05-24 13:01:48,413 epoch 13 - iter 30/159 - loss 0.36104082 - samples/sec: 65.70 - lr: 0.050000
2021-05-24 13:01:52,115 epoch 13 - iter 45/159 - loss 0.42300807 - samples/sec: 64.83 - lr: 0.050000
2021-05-24 13:01:55,806 epoch 13 - iter 60/159 - loss 0.39918981 - samples/sec: 65.04 - lr: 0.050000
2021-05-24 13:01:59,523 epoch 13 - iter 75/159 - loss 0.39575673 - samples/sec: 64.59 - lr: 0.050000
2021-05-24 13:02:03,221 epoch 13 - iter 90/159 - loss 0.38992623 - samples/sec: 64.90 - lr: 0.050000
2021-05-24 13:02:06,916 epoch 13 - iter 105/159 - loss 0.39947470 - samples/sec: 64.97 - lr: 0.050000
2021-05-24 13:02:10,556 epoch 13 - iter 120/159 - loss 0.41630611 - samples/sec: 65.95 - lr: 0.050000
2021-05-24 13:02:14,216 epoch 13 - iter 135/159 - loss 0.42823082 - samples/sec: 65.59 - lr: 0.050000
2021-05-24 13:02:17,901 epoch 13 - iter 150/159 - loss 0.43584060 - samples/sec: 65.13 - lr: 0.050000
2021-05-24 13:02:20,048 ----------------------------------------------------------------------------------------------------
2021-05-24 13:02:20,048 EPOCH 13 done: loss 0.4399 - lr 0.0500000
2021-05-24 13:02:23,002 DEV : loss 0.3658473789691925 - score 0.9168
2021-05-24 13:02:23,055 BAD EPOCHS (no improvement): 2
2021-05-24 13:02:23,055 ----------------------------------------------------------------------------------------------------
2021-05-24 13:02:26,754 epoch 14 - iter 15/159 - loss 0.52998568 - samples/sec: 64.90 - lr: 0.050000
2021-05-24 13:02:30,431 epoch 14 - iter 30/159 - loss 0.53740886 - samples/sec: 65.29 - lr: 0.050000
2021-05-24 13:02:34,156 epoch 14 - iter 45/159 - loss 0.46491248 - samples/sec: 64.43 - lr: 0.050000
2021-05-24 13:02:37,841 epoch 14 - iter 60/159 - loss 0.42661720 - samples/sec: 65.15 - lr: 0.050000
2021-05-24 13:02:41,563 epoch 14 - iter 75/159 - loss 0.44198024 - samples/sec: 64.49 - lr: 0.050000
2021-05-24 13:02:45,301 epoch 14 - iter 90/159 - loss 0.43517443 - samples/sec: 64.21 - lr: 0.050000
2021-05-24 13:02:49,008 epoch 14 - iter 105/159 - loss 0.42263003 - samples/sec: 64.75 - lr: 0.050000
2021-05-24 13:02:52,727 epoch 14 - iter 120/159 - loss 0.44268924 - samples/sec: 64.55 - lr: 0.050000
2021-05-24 13:02:56,467 epoch 14 - iter 135/159 - loss 0.44392687 - samples/sec: 64.19 - lr: 0.050000
2021-05-24 13:03:00,139 epoch 14 - iter 150/159 - loss 0.45573696 - samples/sec: 65.37 - lr: 0.050000
2021-05-24 13:03:02,276 ----------------------------------------------------------------------------------------------------
2021-05-24 13:03:02,276 EPOCH 14 done: loss 0.4499 - lr 0.0500000
2021-05-24 13:03:05,564 DEV : loss 0.3828198313713074 - score 0.9288
2021-05-24 13:03:05,617 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:03:15,224 ----------------------------------------------------------------------------------------------------
2021-05-24 13:03:18,952 epoch 15 - iter 15/159 - loss 0.46590362 - samples/sec: 64.40 - lr: 0.050000
2021-05-24 13:03:22,656 epoch 15 - iter 30/159 - loss 0.43058039 - samples/sec: 64.81 - lr: 0.050000
2021-05-24 13:03:26,334 epoch 15 - iter 45/159 - loss 0.41567804 - samples/sec: 65.26 - lr: 0.050000
2021-05-24 13:03:29,985 epoch 15 - iter 60/159 - loss 0.44596971 - samples/sec: 65.75 - lr: 0.050000
2021-05-24 13:03:33,645 epoch 15 - iter 75/159 - loss 0.44483864 - samples/sec: 65.58 - lr: 0.050000
2021-05-24 13:03:37,333 epoch 15 - iter 90/159 - loss 0.45388111 - samples/sec: 65.09 - lr: 0.050000
2021-05-24 13:03:41,014 epoch 15 - iter 105/159 - loss 0.45815609 - samples/sec: 65.22 - lr: 0.050000
2021-05-24 13:03:44,717 epoch 15 - iter 120/159 - loss 0.45198971 - samples/sec: 64.82 - lr: 0.050000
2021-05-24 13:03:48,409 epoch 15 - iter 135/159 - loss 0.47360756 - samples/sec: 65.02 - lr: 0.050000
2021-05-24 13:03:52,131 epoch 15 - iter 150/159 - loss 0.47035439 - samples/sec: 64.49 - lr: 0.050000
2021-05-24 13:03:54,263 ----------------------------------------------------------------------------------------------------
2021-05-24 13:03:54,263 EPOCH 15 done: loss 0.4650 - lr 0.0500000
2021-05-24 13:03:57,222 DEV : loss 0.33743950724601746 - score 0.9172
2021-05-24 13:03:57,274 BAD EPOCHS (no improvement): 1
2021-05-24 13:03:57,274 ----------------------------------------------------------------------------------------------------
2021-05-24 13:04:00,933 epoch 16 - iter 15/159 - loss 0.51768255 - samples/sec: 65.61 - lr: 0.050000
2021-05-24 13:04:04,664 epoch 16 - iter 30/159 - loss 0.44233515 - samples/sec: 64.35 - lr: 0.050000
2021-05-24 13:04:08,350 epoch 16 - iter 45/159 - loss 0.44981251 - samples/sec: 65.12 - lr: 0.050000
2021-05-24 13:04:12,040 epoch 16 - iter 60/159 - loss 0.46120064 - samples/sec: 65.06 - lr: 0.050000
2021-05-24 13:04:15,777 epoch 16 - iter 75/159 - loss 0.44142344 - samples/sec: 64.24 - lr: 0.050000
2021-05-24 13:04:19,498 epoch 16 - iter 90/159 - loss 0.43518232 - samples/sec: 64.50 - lr: 0.050000
2021-05-24 13:04:23,229 epoch 16 - iter 105/159 - loss 0.43640814 - samples/sec: 64.34 - lr: 0.050000
2021-05-24 13:04:26,892 epoch 16 - iter 120/159 - loss 0.42896066 - samples/sec: 65.53 - lr: 0.050000
2021-05-24 13:04:30,538 epoch 16 - iter 135/159 - loss 0.43225006 - samples/sec: 65.85 - lr: 0.050000
2021-05-24 13:04:34,190 epoch 16 - iter 150/159 - loss 0.42906208 - samples/sec: 65.73 - lr: 0.050000
2021-05-24 13:04:36,315 ----------------------------------------------------------------------------------------------------
2021-05-24 13:04:36,315 EPOCH 16 done: loss 0.4355 - lr 0.0500000
2021-05-24 13:04:39,274 DEV : loss 0.3623471260070801 - score 0.9308
2021-05-24 13:04:39,327 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:04:48,780 ----------------------------------------------------------------------------------------------------
2021-05-24 13:04:52,484 epoch 17 - iter 15/159 - loss 0.48407561 - samples/sec: 64.82 - lr: 0.050000
2021-05-24 13:04:56,159 epoch 17 - iter 30/159 - loss 0.53471371 - samples/sec: 65.32 - lr: 0.050000
2021-05-24 13:04:59,834 epoch 17 - iter 45/159 - loss 0.49783586 - samples/sec: 65.31 - lr: 0.050000
2021-05-24 13:05:03,476 epoch 17 - iter 60/159 - loss 0.48698718 - samples/sec: 65.92 - lr: 0.050000
2021-05-24 13:05:07,135 epoch 17 - iter 75/159 - loss 0.49318704 - samples/sec: 65.60 - lr: 0.050000
2021-05-24 13:05:10,804 epoch 17 - iter 90/159 - loss 0.48022817 - samples/sec: 65.43 - lr: 0.050000
2021-05-24 13:05:14,478 epoch 17 - iter 105/159 - loss 0.47371942 - samples/sec: 65.33 - lr: 0.050000
2021-05-24 13:05:18,131 epoch 17 - iter 120/159 - loss 0.46943086 - samples/sec: 65.71 - lr: 0.050000
2021-05-24 13:05:21,762 epoch 17 - iter 135/159 - loss 0.46452602 - samples/sec: 66.11 - lr: 0.050000
2021-05-24 13:05:25,433 epoch 17 - iter 150/159 - loss 0.46128076 - samples/sec: 65.39 - lr: 0.050000
2021-05-24 13:05:27,558 ----------------------------------------------------------------------------------------------------
2021-05-24 13:05:27,558 EPOCH 17 done: loss 0.4485 - lr 0.0500000
2021-05-24 13:05:30,525 DEV : loss 0.3556843101978302 - score 0.9261
2021-05-24 13:05:30,579 BAD EPOCHS (no improvement): 1
2021-05-24 13:05:30,579 ----------------------------------------------------------------------------------------------------
2021-05-24 13:05:34,252 epoch 18 - iter 15/159 - loss 0.46129065 - samples/sec: 65.37 - lr: 0.050000
2021-05-24 13:05:37,936 epoch 18 - iter 30/159 - loss 0.40155342 - samples/sec: 65.15 - lr: 0.050000
2021-05-24 13:05:41,588 epoch 18 - iter 45/159 - loss 0.41905734 - samples/sec: 65.75 - lr: 0.050000
2021-05-24 13:05:45,290 epoch 18 - iter 60/159 - loss 0.47211616 - samples/sec: 64.83 - lr: 0.050000
2021-05-24 13:05:48,968 epoch 18 - iter 75/159 - loss 0.45711739 - samples/sec: 65.27 - lr: 0.050000
2021-05-24 13:05:52,635 epoch 18 - iter 90/159 - loss 0.44441681 - samples/sec: 65.46 - lr: 0.050000
2021-05-24 13:05:56,347 epoch 18 - iter 105/159 - loss 0.43127991 - samples/sec: 64.66 - lr: 0.050000
2021-05-24 13:05:59,996 epoch 18 - iter 120/159 - loss 0.41377345 - samples/sec: 65.79 - lr: 0.050000
2021-05-24 13:06:03,680 epoch 18 - iter 135/159 - loss 0.41554285 - samples/sec: 65.16 - lr: 0.050000
2021-05-24 13:06:07,409 epoch 18 - iter 150/159 - loss 0.41891553 - samples/sec: 64.38 - lr: 0.050000
2021-05-24 13:06:09,560 ----------------------------------------------------------------------------------------------------
2021-05-24 13:06:09,560 EPOCH 18 done: loss 0.4198 - lr 0.0500000
2021-05-24 13:06:12,846 DEV : loss 0.34917953610420227 - score 0.9252
2021-05-24 13:06:12,899 BAD EPOCHS (no improvement): 2
2021-05-24 13:06:12,899 ----------------------------------------------------------------------------------------------------
2021-05-24 13:06:16,566 epoch 19 - iter 15/159 - loss 0.39704139 - samples/sec: 65.47 - lr: 0.050000
2021-05-24 13:06:20,282 epoch 19 - iter 30/159 - loss 0.39681248 - samples/sec: 64.59 - lr: 0.050000
2021-05-24 13:06:23,975 epoch 19 - iter 45/159 - loss 0.39694275 - samples/sec: 65.02 - lr: 0.050000
2021-05-24 13:06:27,637 epoch 19 - iter 60/159 - loss 0.40266672 - samples/sec: 65.55 - lr: 0.050000
2021-05-24 13:06:31,345 epoch 19 - iter 75/159 - loss 0.39838009 - samples/sec: 64.73 - lr: 0.050000
2021-05-24 13:06:35,029 epoch 19 - iter 90/159 - loss 0.39544457 - samples/sec: 65.16 - lr: 0.050000
2021-05-24 13:06:38,757 epoch 19 - iter 105/159 - loss 0.42150098 - samples/sec: 64.39 - lr: 0.050000
2021-05-24 13:06:42,433 epoch 19 - iter 120/159 - loss 0.42670489 - samples/sec: 65.31 - lr: 0.050000
2021-05-24 13:06:46,090 epoch 19 - iter 135/159 - loss 0.42535525 - samples/sec: 65.64 - lr: 0.050000
2021-05-24 13:06:49,759 epoch 19 - iter 150/159 - loss 0.41915469 - samples/sec: 65.42 - lr: 0.050000
2021-05-24 13:06:51,904 ----------------------------------------------------------------------------------------------------
2021-05-24 13:06:51,904 EPOCH 19 done: loss 0.4091 - lr 0.0500000
2021-05-24 13:06:54,859 DEV : loss 0.33825206756591797 - score 0.9284
2021-05-24 13:06:54,912 BAD EPOCHS (no improvement): 3
2021-05-24 13:06:54,912 ----------------------------------------------------------------------------------------------------
2021-05-24 13:06:58,584 epoch 20 - iter 15/159 - loss 0.35404720 - samples/sec: 65.39 - lr: 0.050000
2021-05-24 13:07:02,271 epoch 20 - iter 30/159 - loss 0.39678331 - samples/sec: 65.10 - lr: 0.050000
2021-05-24 13:07:05,956 epoch 20 - iter 45/159 - loss 0.35866227 - samples/sec: 65.14 - lr: 0.050000
2021-05-24 13:07:09,656 epoch 20 - iter 60/159 - loss 0.38329369 - samples/sec: 64.88 - lr: 0.050000
2021-05-24 13:07:13,334 epoch 20 - iter 75/159 - loss 0.39464715 - samples/sec: 65.25 - lr: 0.050000
2021-05-24 13:07:17,025 epoch 20 - iter 90/159 - loss 0.40848327 - samples/sec: 65.04 - lr: 0.050000
2021-05-24 13:07:20,701 epoch 20 - iter 105/159 - loss 0.41993434 - samples/sec: 65.31 - lr: 0.050000
2021-05-24 13:07:24,385 epoch 20 - iter 120/159 - loss 0.41258153 - samples/sec: 65.16 - lr: 0.050000
2021-05-24 13:07:28,094 epoch 20 - iter 135/159 - loss 0.40310092 - samples/sec: 64.73 - lr: 0.050000
2021-05-24 13:07:31,739 epoch 20 - iter 150/159 - loss 0.40618017 - samples/sec: 65.86 - lr: 0.050000
2021-05-24 13:07:33,883 ----------------------------------------------------------------------------------------------------
2021-05-24 13:07:33,883 EPOCH 20 done: loss 0.4104 - lr 0.0500000
2021-05-24 13:07:36,845 DEV : loss 0.31963682174682617 - score 0.9292
Epoch    20: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 13:07:36,898 BAD EPOCHS (no improvement): 4
2021-05-24 13:07:36,898 ----------------------------------------------------------------------------------------------------
2021-05-24 13:07:40,576 epoch 21 - iter 15/159 - loss 0.42193463 - samples/sec: 65.28 - lr: 0.025000
2021-05-24 13:07:44,254 epoch 21 - iter 30/159 - loss 0.36239985 - samples/sec: 65.26 - lr: 0.025000
2021-05-24 13:07:47,954 epoch 21 - iter 45/159 - loss 0.37889326 - samples/sec: 64.88 - lr: 0.025000
2021-05-24 13:07:51,620 epoch 21 - iter 60/159 - loss 0.37696127 - samples/sec: 65.47 - lr: 0.025000
2021-05-24 13:07:55,284 epoch 21 - iter 75/159 - loss 0.36331003 - samples/sec: 65.52 - lr: 0.025000
2021-05-24 13:07:58,956 epoch 21 - iter 90/159 - loss 0.34896961 - samples/sec: 65.38 - lr: 0.025000
2021-05-24 13:08:02,625 epoch 21 - iter 105/159 - loss 0.34872181 - samples/sec: 65.43 - lr: 0.025000
2021-05-24 13:08:06,298 epoch 21 - iter 120/159 - loss 0.36635667 - samples/sec: 65.35 - lr: 0.025000
2021-05-24 13:08:09,993 epoch 21 - iter 135/159 - loss 0.36219940 - samples/sec: 64.97 - lr: 0.025000
2021-05-24 13:08:13,701 epoch 21 - iter 150/159 - loss 0.36367036 - samples/sec: 64.73 - lr: 0.025000
2021-05-24 13:08:15,859 ----------------------------------------------------------------------------------------------------
2021-05-24 13:08:15,860 EPOCH 21 done: loss 0.3674 - lr 0.0250000
2021-05-24 13:08:18,815 DEV : loss 0.365509957075119 - score 0.9085
2021-05-24 13:08:18,868 BAD EPOCHS (no improvement): 1
2021-05-24 13:08:18,868 ----------------------------------------------------------------------------------------------------
2021-05-24 13:08:22,546 epoch 22 - iter 15/159 - loss 0.46456168 - samples/sec: 65.27 - lr: 0.025000
2021-05-24 13:08:26,285 epoch 22 - iter 30/159 - loss 0.40554744 - samples/sec: 64.20 - lr: 0.025000
2021-05-24 13:08:29,921 epoch 22 - iter 45/159 - loss 0.38627059 - samples/sec: 66.02 - lr: 0.025000
2021-05-24 13:08:33,545 epoch 22 - iter 60/159 - loss 0.35683713 - samples/sec: 66.24 - lr: 0.025000
2021-05-24 13:08:37,192 epoch 22 - iter 75/159 - loss 0.38353328 - samples/sec: 65.82 - lr: 0.025000
2021-05-24 13:08:40,840 epoch 22 - iter 90/159 - loss 0.36262354 - samples/sec: 65.80 - lr: 0.025000
2021-05-24 13:08:44,461 epoch 22 - iter 105/159 - loss 0.35947481 - samples/sec: 66.29 - lr: 0.025000
2021-05-24 13:08:48,154 epoch 22 - iter 120/159 - loss 0.37418350 - samples/sec: 65.00 - lr: 0.025000
2021-05-24 13:08:51,807 epoch 22 - iter 135/159 - loss 0.36540491 - samples/sec: 65.71 - lr: 0.025000
2021-05-24 13:08:55,481 epoch 22 - iter 150/159 - loss 0.36594464 - samples/sec: 65.34 - lr: 0.025000
2021-05-24 13:08:57,615 ----------------------------------------------------------------------------------------------------
2021-05-24 13:08:57,615 EPOCH 22 done: loss 0.3685 - lr 0.0250000
2021-05-24 13:09:00,893 DEV : loss 0.33613720536231995 - score 0.931
2021-05-24 13:09:00,946 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:09:10,400 ----------------------------------------------------------------------------------------------------
2021-05-24 13:09:14,044 epoch 23 - iter 15/159 - loss 0.48973696 - samples/sec: 65.88 - lr: 0.025000
2021-05-24 13:09:17,710 epoch 23 - iter 30/159 - loss 0.43276643 - samples/sec: 65.48 - lr: 0.025000
2021-05-24 13:09:21,355 epoch 23 - iter 45/159 - loss 0.37797967 - samples/sec: 65.86 - lr: 0.025000
2021-05-24 13:09:25,007 epoch 23 - iter 60/159 - loss 0.37534560 - samples/sec: 65.73 - lr: 0.025000
2021-05-24 13:09:28,671 epoch 23 - iter 75/159 - loss 0.36141371 - samples/sec: 65.51 - lr: 0.025000
2021-05-24 13:09:32,321 epoch 23 - iter 90/159 - loss 0.35156061 - samples/sec: 65.77 - lr: 0.025000
2021-05-24 13:09:35,974 epoch 23 - iter 105/159 - loss 0.37249685 - samples/sec: 65.71 - lr: 0.025000
2021-05-24 13:09:39,609 epoch 23 - iter 120/159 - loss 0.37357699 - samples/sec: 66.04 - lr: 0.025000
2021-05-24 13:09:43,241 epoch 23 - iter 135/159 - loss 0.37294443 - samples/sec: 66.09 - lr: 0.025000
2021-05-24 13:09:46,898 epoch 23 - iter 150/159 - loss 0.36858794 - samples/sec: 65.64 - lr: 0.025000
2021-05-24 13:09:48,996 ----------------------------------------------------------------------------------------------------
2021-05-24 13:09:48,996 EPOCH 23 done: loss 0.3681 - lr 0.0250000
2021-05-24 13:09:51,952 DEV : loss 0.3389587998390198 - score 0.9261
2021-05-24 13:09:52,005 BAD EPOCHS (no improvement): 1
2021-05-24 13:09:52,005 ----------------------------------------------------------------------------------------------------
2021-05-24 13:09:55,692 epoch 24 - iter 15/159 - loss 0.38323729 - samples/sec: 65.11 - lr: 0.025000
2021-05-24 13:09:59,340 epoch 24 - iter 30/159 - loss 0.35020411 - samples/sec: 65.80 - lr: 0.025000
2021-05-24 13:10:02,966 epoch 24 - iter 45/159 - loss 0.32567793 - samples/sec: 66.20 - lr: 0.025000
2021-05-24 13:10:06,638 epoch 24 - iter 60/159 - loss 0.37545750 - samples/sec: 65.37 - lr: 0.025000
2021-05-24 13:10:10,261 epoch 24 - iter 75/159 - loss 0.37059520 - samples/sec: 66.27 - lr: 0.025000
2021-05-24 13:10:13,884 epoch 24 - iter 90/159 - loss 0.35895319 - samples/sec: 66.26 - lr: 0.025000
2021-05-24 13:10:17,544 epoch 24 - iter 105/159 - loss 0.36566511 - samples/sec: 65.58 - lr: 0.025000
2021-05-24 13:10:21,134 epoch 24 - iter 120/159 - loss 0.36277720 - samples/sec: 66.86 - lr: 0.025000
2021-05-24 13:10:24,789 epoch 24 - iter 135/159 - loss 0.35934009 - samples/sec: 65.68 - lr: 0.025000
2021-05-24 13:10:28,404 epoch 24 - iter 150/159 - loss 0.36103442 - samples/sec: 66.41 - lr: 0.025000
2021-05-24 13:10:30,478 ----------------------------------------------------------------------------------------------------
2021-05-24 13:10:30,478 EPOCH 24 done: loss 0.3532 - lr 0.0250000
2021-05-24 13:10:33,444 DEV : loss 0.33195289969444275 - score 0.9292
2021-05-24 13:10:33,497 BAD EPOCHS (no improvement): 2
2021-05-24 13:10:33,497 ----------------------------------------------------------------------------------------------------
2021-05-24 13:10:37,117 epoch 25 - iter 15/159 - loss 0.30429969 - samples/sec: 66.32 - lr: 0.025000
2021-05-24 13:10:40,728 epoch 25 - iter 30/159 - loss 0.28002255 - samples/sec: 66.48 - lr: 0.025000
2021-05-24 13:10:44,365 epoch 25 - iter 45/159 - loss 0.30499942 - samples/sec: 66.00 - lr: 0.025000
2021-05-24 13:10:47,984 epoch 25 - iter 60/159 - loss 0.31970779 - samples/sec: 66.33 - lr: 0.025000
2021-05-24 13:10:51,598 epoch 25 - iter 75/159 - loss 0.31673588 - samples/sec: 66.43 - lr: 0.025000
2021-05-24 13:10:55,264 epoch 25 - iter 90/159 - loss 0.31166343 - samples/sec: 65.48 - lr: 0.025000
2021-05-24 13:10:58,888 epoch 25 - iter 105/159 - loss 0.33116926 - samples/sec: 66.24 - lr: 0.025000
2021-05-24 13:11:02,548 epoch 25 - iter 120/159 - loss 0.33904120 - samples/sec: 65.59 - lr: 0.025000
2021-05-24 13:11:06,233 epoch 25 - iter 135/159 - loss 0.34327055 - samples/sec: 65.13 - lr: 0.025000
2021-05-24 13:11:09,937 epoch 25 - iter 150/159 - loss 0.34993565 - samples/sec: 64.81 - lr: 0.025000
2021-05-24 13:11:12,064 ----------------------------------------------------------------------------------------------------
2021-05-24 13:11:12,064 EPOCH 25 done: loss 0.3471 - lr 0.0250000
2021-05-24 13:11:15,019 DEV : loss 0.33599114418029785 - score 0.9206
2021-05-24 13:11:15,071 BAD EPOCHS (no improvement): 3
2021-05-24 13:11:15,072 ----------------------------------------------------------------------------------------------------
2021-05-24 13:11:18,757 epoch 26 - iter 15/159 - loss 0.36980323 - samples/sec: 65.14 - lr: 0.025000
2021-05-24 13:11:22,437 epoch 26 - iter 30/159 - loss 0.34065891 - samples/sec: 65.23 - lr: 0.025000
2021-05-24 13:11:26,133 epoch 26 - iter 45/159 - loss 0.35757435 - samples/sec: 64.95 - lr: 0.025000
2021-05-24 13:11:29,836 epoch 26 - iter 60/159 - loss 0.35859649 - samples/sec: 64.84 - lr: 0.025000
2021-05-24 13:11:33,536 epoch 26 - iter 75/159 - loss 0.36175374 - samples/sec: 64.88 - lr: 0.025000
2021-05-24 13:11:37,214 epoch 26 - iter 90/159 - loss 0.35027480 - samples/sec: 65.26 - lr: 0.025000
2021-05-24 13:11:40,923 epoch 26 - iter 105/159 - loss 0.34976805 - samples/sec: 64.72 - lr: 0.025000
2021-05-24 13:11:44,601 epoch 26 - iter 120/159 - loss 0.34020262 - samples/sec: 65.26 - lr: 0.025000
2021-05-24 13:11:48,241 epoch 26 - iter 135/159 - loss 0.32915558 - samples/sec: 65.95 - lr: 0.025000
2021-05-24 13:11:51,918 epoch 26 - iter 150/159 - loss 0.32428516 - samples/sec: 65.28 - lr: 0.025000
2021-05-24 13:11:54,021 ----------------------------------------------------------------------------------------------------
2021-05-24 13:11:54,021 EPOCH 26 done: loss 0.3263 - lr 0.0250000
2021-05-24 13:11:57,302 DEV : loss 0.34887462854385376 - score 0.9156
Epoch    26: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 13:11:57,354 BAD EPOCHS (no improvement): 4
2021-05-24 13:11:57,354 ----------------------------------------------------------------------------------------------------
2021-05-24 13:12:01,016 epoch 27 - iter 15/159 - loss 0.29197727 - samples/sec: 65.56 - lr: 0.012500
2021-05-24 13:12:04,676 epoch 27 - iter 30/159 - loss 0.27308905 - samples/sec: 65.60 - lr: 0.012500
2021-05-24 13:12:08,411 epoch 27 - iter 45/159 - loss 0.30899675 - samples/sec: 64.26 - lr: 0.012500
2021-05-24 13:12:12,091 epoch 27 - iter 60/159 - loss 0.30116871 - samples/sec: 65.24 - lr: 0.012500
2021-05-24 13:12:15,782 epoch 27 - iter 75/159 - loss 0.31125746 - samples/sec: 65.03 - lr: 0.012500
2021-05-24 13:12:19,476 epoch 27 - iter 90/159 - loss 0.32090349 - samples/sec: 64.98 - lr: 0.012500
2021-05-24 13:12:23,157 epoch 27 - iter 105/159 - loss 0.31773953 - samples/sec: 65.22 - lr: 0.012500
2021-05-24 13:12:26,830 epoch 27 - iter 120/159 - loss 0.33040736 - samples/sec: 65.35 - lr: 0.012500
2021-05-24 13:12:30,496 epoch 27 - iter 135/159 - loss 0.32094670 - samples/sec: 65.48 - lr: 0.012500
2021-05-24 13:12:34,201 epoch 27 - iter 150/159 - loss 0.32285803 - samples/sec: 64.79 - lr: 0.012500
2021-05-24 13:12:36,339 ----------------------------------------------------------------------------------------------------
2021-05-24 13:12:36,339 EPOCH 27 done: loss 0.3262 - lr 0.0125000
2021-05-24 13:12:39,295 DEV : loss 0.3539225459098816 - score 0.92
2021-05-24 13:12:39,348 BAD EPOCHS (no improvement): 1
2021-05-24 13:12:39,348 ----------------------------------------------------------------------------------------------------
2021-05-24 13:12:43,009 epoch 28 - iter 15/159 - loss 0.34403897 - samples/sec: 65.57 - lr: 0.012500
2021-05-24 13:12:46,700 epoch 28 - iter 30/159 - loss 0.30390826 - samples/sec: 65.04 - lr: 0.012500
2021-05-24 13:12:50,417 epoch 28 - iter 45/159 - loss 0.32300631 - samples/sec: 64.57 - lr: 0.012500
2021-05-24 13:12:54,094 epoch 28 - iter 60/159 - loss 0.32950596 - samples/sec: 65.30 - lr: 0.012500
2021-05-24 13:12:57,709 epoch 28 - iter 75/159 - loss 0.32070659 - samples/sec: 66.39 - lr: 0.012500
2021-05-24 13:13:01,360 epoch 28 - iter 90/159 - loss 0.34765509 - samples/sec: 65.76 - lr: 0.012500
2021-05-24 13:13:05,011 epoch 28 - iter 105/159 - loss 0.33648556 - samples/sec: 65.75 - lr: 0.012500
2021-05-24 13:13:08,675 epoch 28 - iter 120/159 - loss 0.33149629 - samples/sec: 65.50 - lr: 0.012500
2021-05-24 13:13:12,348 epoch 28 - iter 135/159 - loss 0.32317864 - samples/sec: 65.36 - lr: 0.012500
2021-05-24 13:13:16,045 epoch 28 - iter 150/159 - loss 0.32183910 - samples/sec: 64.94 - lr: 0.012500
2021-05-24 13:13:18,197 ----------------------------------------------------------------------------------------------------
2021-05-24 13:13:18,197 EPOCH 28 done: loss 0.3301 - lr 0.0125000
2021-05-24 13:13:21,155 DEV : loss 0.32646164298057556 - score 0.9277
2021-05-24 13:13:21,208 BAD EPOCHS (no improvement): 2
2021-05-24 13:13:21,208 ----------------------------------------------------------------------------------------------------
2021-05-24 13:13:24,887 epoch 29 - iter 15/159 - loss 0.34179986 - samples/sec: 65.24 - lr: 0.012500
2021-05-24 13:13:28,565 epoch 29 - iter 30/159 - loss 0.34958194 - samples/sec: 65.28 - lr: 0.012500
2021-05-24 13:13:32,214 epoch 29 - iter 45/159 - loss 0.30320517 - samples/sec: 65.78 - lr: 0.012500
2021-05-24 13:13:35,836 epoch 29 - iter 60/159 - loss 0.31149417 - samples/sec: 66.27 - lr: 0.012500
2021-05-24 13:13:39,533 epoch 29 - iter 75/159 - loss 0.32720589 - samples/sec: 64.93 - lr: 0.012500
2021-05-24 13:13:43,210 epoch 29 - iter 90/159 - loss 0.33205655 - samples/sec: 65.29 - lr: 0.012500
2021-05-24 13:13:46,881 epoch 29 - iter 105/159 - loss 0.32764184 - samples/sec: 65.38 - lr: 0.012500
2021-05-24 13:13:50,543 epoch 29 - iter 120/159 - loss 0.33212466 - samples/sec: 65.55 - lr: 0.012500
2021-05-24 13:13:54,227 epoch 29 - iter 135/159 - loss 0.33204742 - samples/sec: 65.18 - lr: 0.012500
2021-05-24 13:13:57,897 epoch 29 - iter 150/159 - loss 0.33727739 - samples/sec: 65.40 - lr: 0.012500
2021-05-24 13:14:00,038 ----------------------------------------------------------------------------------------------------
2021-05-24 13:14:00,038 EPOCH 29 done: loss 0.3301 - lr 0.0125000
2021-05-24 13:14:03,000 DEV : loss 0.3290228843688965 - score 0.9298
2021-05-24 13:14:03,053 BAD EPOCHS (no improvement): 3
2021-05-24 13:14:03,053 ----------------------------------------------------------------------------------------------------
2021-05-24 13:14:06,766 epoch 30 - iter 15/159 - loss 0.27510538 - samples/sec: 64.66 - lr: 0.012500
2021-05-24 13:14:10,441 epoch 30 - iter 30/159 - loss 0.25329162 - samples/sec: 65.32 - lr: 0.012500
2021-05-24 13:14:14,090 epoch 30 - iter 45/159 - loss 0.28366530 - samples/sec: 65.78 - lr: 0.012500
2021-05-24 13:14:17,832 epoch 30 - iter 60/159 - loss 0.27900092 - samples/sec: 64.16 - lr: 0.012500
2021-05-24 13:14:21,501 epoch 30 - iter 75/159 - loss 0.28119512 - samples/sec: 65.43 - lr: 0.012500
2021-05-24 13:14:25,173 epoch 30 - iter 90/159 - loss 0.28043152 - samples/sec: 65.36 - lr: 0.012500
2021-05-24 13:14:28,856 epoch 30 - iter 105/159 - loss 0.28818824 - samples/sec: 65.18 - lr: 0.012500
2021-05-24 13:14:32,526 epoch 30 - iter 120/159 - loss 0.30068913 - samples/sec: 65.41 - lr: 0.012500
2021-05-24 13:14:36,203 epoch 30 - iter 135/159 - loss 0.29525131 - samples/sec: 65.28 - lr: 0.012500
2021-05-24 13:14:39,895 epoch 30 - iter 150/159 - loss 0.29257002 - samples/sec: 65.03 - lr: 0.012500
2021-05-24 13:14:42,054 ----------------------------------------------------------------------------------------------------
2021-05-24 13:14:42,055 EPOCH 30 done: loss 0.2981 - lr 0.0125000
2021-05-24 13:14:45,339 DEV : loss 0.35775795578956604 - score 0.915
Epoch    30: reducing learning rate of group 0 to 6.2500e-03.
2021-05-24 13:14:45,392 BAD EPOCHS (no improvement): 4
2021-05-24 13:14:46,509 ----------------------------------------------------------------------------------------------------
2021-05-24 13:14:46,509 Testing using best model ...
2021-05-24 13:14:46,509 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/zho.pdtb.cdtb/best-model.pt
2021-05-24 13:15:01,474 0.9302	0.8911	0.9102
2021-05-24 13:15:01,474 
Results:
- F1-score (micro) 0.9102
- F1-score (macro) 0.9102

By class:
SENT       tp: 360 - fp: 27 - fn: 44 - precision: 0.9302 - recall: 0.8911 - f1-score: 0.9102
2021-05-24 13:15:01,474 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb/
2021-05-24 13:15:01,500 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb
2021-05-24 13:15:01,500 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb/sent_train.txt
2021-05-24 13:15:01,501 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb/sent_dev.txt
2021-05-24 13:15:01,501 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb/sent_test.txt
Corpus: 2146 train + 371 dev + 396 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 13:15:08,932 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 13:15:08,935 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Corpus: "Corpus: 2146 train + 371 dev + 396 test sentences"
2021-05-24 13:15:08,935 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Parameters:
2021-05-24 13:15:08,935  - learning_rate: "0.1"
2021-05-24 13:15:08,935  - mini_batch_size: "16"
2021-05-24 13:15:08,935  - patience: "3"
2021-05-24 13:15:08,935  - anneal_factor: "0.5"
2021-05-24 13:15:08,935  - max_epochs: "30"
2021-05-24 13:15:08,935  - shuffle: "True"
2021-05-24 13:15:08,935  - train_with_dev: "False"
2021-05-24 13:15:08,935  - batch_growth_annealing: "False"
2021-05-24 13:15:08,935 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb"
2021-05-24 13:15:08,935 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Device: cuda:0
2021-05-24 13:15:08,935 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:08,935 Embeddings storage mode: cpu
2021-05-24 13:15:08,937 ----------------------------------------------------------------------------------------------------
2021-05-24 13:15:18,777 epoch 1 - iter 13/135 - loss 4.72210023 - samples/sec: 21.14 - lr: 0.100000
2021-05-24 13:15:28,716 epoch 1 - iter 26/135 - loss 4.16457886 - samples/sec: 20.93 - lr: 0.100000
2021-05-24 13:15:38,632 epoch 1 - iter 39/135 - loss 3.65270946 - samples/sec: 20.98 - lr: 0.100000
2021-05-24 13:15:48,312 epoch 1 - iter 52/135 - loss 3.34590717 - samples/sec: 21.49 - lr: 0.100000
2021-05-24 13:15:57,931 epoch 1 - iter 65/135 - loss 3.12461026 - samples/sec: 21.62 - lr: 0.100000
2021-05-24 13:16:07,875 epoch 1 - iter 78/135 - loss 2.89671319 - samples/sec: 20.92 - lr: 0.100000
2021-05-24 13:16:17,752 epoch 1 - iter 91/135 - loss 2.68967156 - samples/sec: 21.06 - lr: 0.100000
2021-05-24 13:16:27,620 epoch 1 - iter 104/135 - loss 2.51353606 - samples/sec: 21.08 - lr: 0.100000
2021-05-24 13:16:37,628 epoch 1 - iter 117/135 - loss 2.37997498 - samples/sec: 20.78 - lr: 0.100000
2021-05-24 13:16:47,545 epoch 1 - iter 130/135 - loss 2.27859579 - samples/sec: 20.98 - lr: 0.100000
2021-05-24 13:16:50,680 ----------------------------------------------------------------------------------------------------
2021-05-24 13:16:50,680 EPOCH 1 done: loss 2.2246 - lr 0.1000000
2021-05-24 13:17:01,984 DEV : loss 0.4880015552043915 - score 0.8138
2021-05-24 13:17:02,019 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:17:03,192 ----------------------------------------------------------------------------------------------------
2021-05-24 13:17:06,756 epoch 2 - iter 13/135 - loss 1.16502867 - samples/sec: 58.38 - lr: 0.100000
2021-05-24 13:17:10,220 epoch 2 - iter 26/135 - loss 1.08404150 - samples/sec: 60.07 - lr: 0.100000
2021-05-24 13:17:13,962 epoch 2 - iter 39/135 - loss 1.11114745 - samples/sec: 55.60 - lr: 0.100000
2021-05-24 13:17:17,468 epoch 2 - iter 52/135 - loss 1.02464067 - samples/sec: 59.32 - lr: 0.100000
2021-05-24 13:17:20,946 epoch 2 - iter 65/135 - loss 0.99839460 - samples/sec: 59.83 - lr: 0.100000
2021-05-24 13:17:24,410 epoch 2 - iter 78/135 - loss 0.99018159 - samples/sec: 60.05 - lr: 0.100000
2021-05-24 13:17:27,897 epoch 2 - iter 91/135 - loss 1.00902428 - samples/sec: 59.66 - lr: 0.100000
2021-05-24 13:17:31,360 epoch 2 - iter 104/135 - loss 0.97487054 - samples/sec: 60.08 - lr: 0.100000
2021-05-24 13:17:34,903 epoch 2 - iter 117/135 - loss 0.96242386 - samples/sec: 58.72 - lr: 0.100000
2021-05-24 13:17:38,451 epoch 2 - iter 130/135 - loss 0.94718049 - samples/sec: 58.64 - lr: 0.100000
2021-05-24 13:17:39,619 ----------------------------------------------------------------------------------------------------
2021-05-24 13:17:39,619 EPOCH 2 done: loss 0.9384 - lr 0.1000000
2021-05-24 13:17:41,838 DEV : loss 0.18002748489379883 - score 0.9542
2021-05-24 13:17:41,874 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:17:51,383 ----------------------------------------------------------------------------------------------------
2021-05-24 13:17:54,948 epoch 3 - iter 13/135 - loss 0.81387632 - samples/sec: 58.38 - lr: 0.100000
2021-05-24 13:17:58,475 epoch 3 - iter 26/135 - loss 0.80159622 - samples/sec: 58.98 - lr: 0.100000
2021-05-24 13:18:01,991 epoch 3 - iter 39/135 - loss 0.85618631 - samples/sec: 59.16 - lr: 0.100000
2021-05-24 13:18:05,460 epoch 3 - iter 52/135 - loss 0.80505663 - samples/sec: 59.97 - lr: 0.100000
2021-05-24 13:18:08,964 epoch 3 - iter 65/135 - loss 0.80612274 - samples/sec: 59.38 - lr: 0.100000
2021-05-24 13:18:12,410 epoch 3 - iter 78/135 - loss 0.83235217 - samples/sec: 60.38 - lr: 0.100000
2021-05-24 13:18:15,927 epoch 3 - iter 91/135 - loss 0.80344690 - samples/sec: 59.14 - lr: 0.100000
2021-05-24 13:18:19,372 epoch 3 - iter 104/135 - loss 0.77403993 - samples/sec: 60.40 - lr: 0.100000
2021-05-24 13:18:22,878 epoch 3 - iter 117/135 - loss 0.74949098 - samples/sec: 59.33 - lr: 0.100000
2021-05-24 13:18:26,353 epoch 3 - iter 130/135 - loss 0.74479997 - samples/sec: 59.87 - lr: 0.100000
2021-05-24 13:18:27,537 ----------------------------------------------------------------------------------------------------
2021-05-24 13:18:27,537 EPOCH 3 done: loss 0.7413 - lr 0.1000000
2021-05-24 13:18:29,744 DEV : loss 0.13789454102516174 - score 0.9637
2021-05-24 13:18:29,780 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:18:39,079 ----------------------------------------------------------------------------------------------------
2021-05-24 13:18:42,588 epoch 4 - iter 13/135 - loss 0.60415868 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 13:18:46,077 epoch 4 - iter 26/135 - loss 0.50663566 - samples/sec: 59.62 - lr: 0.100000
2021-05-24 13:18:49,583 epoch 4 - iter 39/135 - loss 0.57222572 - samples/sec: 59.34 - lr: 0.100000
2021-05-24 13:18:53,090 epoch 4 - iter 52/135 - loss 0.57109659 - samples/sec: 59.32 - lr: 0.100000
2021-05-24 13:18:56,573 epoch 4 - iter 65/135 - loss 0.53849756 - samples/sec: 59.73 - lr: 0.100000
2021-05-24 13:19:00,103 epoch 4 - iter 78/135 - loss 0.58190753 - samples/sec: 58.94 - lr: 0.100000
2021-05-24 13:19:03,596 epoch 4 - iter 91/135 - loss 0.60464550 - samples/sec: 59.56 - lr: 0.100000
2021-05-24 13:19:07,072 epoch 4 - iter 104/135 - loss 0.60253049 - samples/sec: 59.85 - lr: 0.100000
2021-05-24 13:19:10,561 epoch 4 - iter 117/135 - loss 0.59417442 - samples/sec: 59.62 - lr: 0.100000
2021-05-24 13:19:14,022 epoch 4 - iter 130/135 - loss 0.58587635 - samples/sec: 60.12 - lr: 0.100000
2021-05-24 13:19:15,194 ----------------------------------------------------------------------------------------------------
2021-05-24 13:19:15,194 EPOCH 4 done: loss 0.5837 - lr 0.1000000
2021-05-24 13:19:17,395 DEV : loss 0.11773910373449326 - score 0.969
2021-05-24 13:19:17,430 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:19:27,218 ----------------------------------------------------------------------------------------------------
2021-05-24 13:19:30,683 epoch 5 - iter 13/135 - loss 0.63548800 - samples/sec: 60.06 - lr: 0.100000
2021-05-24 13:19:34,125 epoch 5 - iter 26/135 - loss 0.63617257 - samples/sec: 60.45 - lr: 0.100000
2021-05-24 13:19:37,544 epoch 5 - iter 39/135 - loss 0.63064860 - samples/sec: 60.85 - lr: 0.100000
2021-05-24 13:19:41,041 epoch 5 - iter 52/135 - loss 0.63351611 - samples/sec: 59.49 - lr: 0.100000
2021-05-24 13:19:44,461 epoch 5 - iter 65/135 - loss 0.65371018 - samples/sec: 60.83 - lr: 0.100000
2021-05-24 13:19:47,945 epoch 5 - iter 78/135 - loss 0.63606889 - samples/sec: 59.72 - lr: 0.100000
2021-05-24 13:19:51,405 epoch 5 - iter 91/135 - loss 0.63533769 - samples/sec: 60.12 - lr: 0.100000
2021-05-24 13:19:54,890 epoch 5 - iter 104/135 - loss 0.66650951 - samples/sec: 59.70 - lr: 0.100000
2021-05-24 13:19:58,361 epoch 5 - iter 117/135 - loss 0.65976579 - samples/sec: 59.95 - lr: 0.100000
2021-05-24 13:20:01,849 epoch 5 - iter 130/135 - loss 0.68180218 - samples/sec: 59.64 - lr: 0.100000
2021-05-24 13:20:02,954 ----------------------------------------------------------------------------------------------------
2021-05-24 13:20:02,954 EPOCH 5 done: loss 0.6718 - lr 0.1000000
2021-05-24 13:20:05,156 DEV : loss 0.1261678785085678 - score 0.9766
2021-05-24 13:20:05,192 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:20:14,771 ----------------------------------------------------------------------------------------------------
2021-05-24 13:20:18,274 epoch 6 - iter 13/135 - loss 0.62704266 - samples/sec: 59.40 - lr: 0.100000
2021-05-24 13:20:21,752 epoch 6 - iter 26/135 - loss 0.66188058 - samples/sec: 59.82 - lr: 0.100000
2021-05-24 13:20:25,125 epoch 6 - iter 39/135 - loss 0.58936100 - samples/sec: 61.67 - lr: 0.100000
2021-05-24 13:20:28,572 epoch 6 - iter 52/135 - loss 0.59783964 - samples/sec: 60.36 - lr: 0.100000
2021-05-24 13:20:32,060 epoch 6 - iter 65/135 - loss 0.57059636 - samples/sec: 59.64 - lr: 0.100000
2021-05-24 13:20:35,546 epoch 6 - iter 78/135 - loss 0.57696898 - samples/sec: 59.68 - lr: 0.100000
2021-05-24 13:20:39,007 epoch 6 - iter 91/135 - loss 0.59924674 - samples/sec: 60.11 - lr: 0.100000
2021-05-24 13:20:42,492 epoch 6 - iter 104/135 - loss 0.60738238 - samples/sec: 59.70 - lr: 0.100000
2021-05-24 13:20:45,947 epoch 6 - iter 117/135 - loss 0.59887683 - samples/sec: 60.20 - lr: 0.100000
2021-05-24 13:20:49,408 epoch 6 - iter 130/135 - loss 0.60741746 - samples/sec: 60.11 - lr: 0.100000
2021-05-24 13:20:50,520 ----------------------------------------------------------------------------------------------------
2021-05-24 13:20:50,521 EPOCH 6 done: loss 0.5930 - lr 0.1000000
2021-05-24 13:20:52,963 DEV : loss 0.10310685634613037 - score 0.9729
2021-05-24 13:20:52,999 BAD EPOCHS (no improvement): 1
2021-05-24 13:20:52,999 ----------------------------------------------------------------------------------------------------
2021-05-24 13:20:56,459 epoch 7 - iter 13/135 - loss 0.33785459 - samples/sec: 60.14 - lr: 0.100000
2021-05-24 13:20:59,925 epoch 7 - iter 26/135 - loss 0.43818560 - samples/sec: 60.02 - lr: 0.100000
2021-05-24 13:21:03,389 epoch 7 - iter 39/135 - loss 0.55251076 - samples/sec: 60.05 - lr: 0.100000
2021-05-24 13:21:06,853 epoch 7 - iter 52/135 - loss 0.52105671 - samples/sec: 60.06 - lr: 0.100000
2021-05-24 13:21:10,370 epoch 7 - iter 65/135 - loss 0.53987942 - samples/sec: 59.15 - lr: 0.100000
2021-05-24 13:21:13,819 epoch 7 - iter 78/135 - loss 0.51673008 - samples/sec: 60.31 - lr: 0.100000
2021-05-24 13:21:17,327 epoch 7 - iter 91/135 - loss 0.52404520 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 13:21:20,808 epoch 7 - iter 104/135 - loss 0.51742107 - samples/sec: 59.77 - lr: 0.100000
2021-05-24 13:21:24,285 epoch 7 - iter 117/135 - loss 0.51693961 - samples/sec: 59.84 - lr: 0.100000
2021-05-24 13:21:27,755 epoch 7 - iter 130/135 - loss 0.52971132 - samples/sec: 59.94 - lr: 0.100000
2021-05-24 13:21:28,905 ----------------------------------------------------------------------------------------------------
2021-05-24 13:21:28,905 EPOCH 7 done: loss 0.5247 - lr 0.1000000
2021-05-24 13:21:31,109 DEV : loss 0.16713492572307587 - score 0.9539
2021-05-24 13:21:31,144 BAD EPOCHS (no improvement): 2
2021-05-24 13:21:31,144 ----------------------------------------------------------------------------------------------------
2021-05-24 13:21:34,578 epoch 8 - iter 13/135 - loss 0.53106031 - samples/sec: 60.59 - lr: 0.100000
2021-05-24 13:21:38,057 epoch 8 - iter 26/135 - loss 0.47329472 - samples/sec: 59.81 - lr: 0.100000
2021-05-24 13:21:41,535 epoch 8 - iter 39/135 - loss 0.51031872 - samples/sec: 59.81 - lr: 0.100000
2021-05-24 13:21:45,009 epoch 8 - iter 52/135 - loss 0.51829856 - samples/sec: 59.88 - lr: 0.100000
2021-05-24 13:21:48,448 epoch 8 - iter 65/135 - loss 0.50116527 - samples/sec: 60.50 - lr: 0.100000
2021-05-24 13:21:51,852 epoch 8 - iter 78/135 - loss 0.50441339 - samples/sec: 61.11 - lr: 0.100000
2021-05-24 13:21:55,309 epoch 8 - iter 91/135 - loss 0.51330573 - samples/sec: 60.19 - lr: 0.100000
2021-05-24 13:21:58,678 epoch 8 - iter 104/135 - loss 0.52061441 - samples/sec: 61.74 - lr: 0.100000
2021-05-24 13:22:02,127 epoch 8 - iter 117/135 - loss 0.49432764 - samples/sec: 60.32 - lr: 0.100000
2021-05-24 13:22:05,594 epoch 8 - iter 130/135 - loss 0.50222791 - samples/sec: 60.01 - lr: 0.100000
2021-05-24 13:22:06,740 ----------------------------------------------------------------------------------------------------
2021-05-24 13:22:06,740 EPOCH 8 done: loss 0.4951 - lr 0.1000000
2021-05-24 13:22:08,938 DEV : loss 0.1089860126376152 - score 0.9747
2021-05-24 13:22:08,973 BAD EPOCHS (no improvement): 3
2021-05-24 13:22:08,974 ----------------------------------------------------------------------------------------------------
2021-05-24 13:22:12,448 epoch 9 - iter 13/135 - loss 0.46188526 - samples/sec: 59.89 - lr: 0.100000
2021-05-24 13:22:15,957 epoch 9 - iter 26/135 - loss 0.50506567 - samples/sec: 59.27 - lr: 0.100000
2021-05-24 13:22:19,485 epoch 9 - iter 39/135 - loss 0.46108929 - samples/sec: 58.97 - lr: 0.100000
2021-05-24 13:22:22,974 epoch 9 - iter 52/135 - loss 0.46997761 - samples/sec: 59.63 - lr: 0.100000
2021-05-24 13:22:26,443 epoch 9 - iter 65/135 - loss 0.51037515 - samples/sec: 59.98 - lr: 0.100000
2021-05-24 13:22:29,945 epoch 9 - iter 78/135 - loss 0.52306963 - samples/sec: 59.40 - lr: 0.100000
2021-05-24 13:22:33,445 epoch 9 - iter 91/135 - loss 0.53056432 - samples/sec: 59.45 - lr: 0.100000
2021-05-24 13:22:36,966 epoch 9 - iter 104/135 - loss 0.52721019 - samples/sec: 59.08 - lr: 0.100000
2021-05-24 13:22:40,470 epoch 9 - iter 117/135 - loss 0.55155052 - samples/sec: 59.38 - lr: 0.100000
2021-05-24 13:22:43,988 epoch 9 - iter 130/135 - loss 0.53263838 - samples/sec: 59.12 - lr: 0.100000
2021-05-24 13:22:45,112 ----------------------------------------------------------------------------------------------------
2021-05-24 13:22:45,113 EPOCH 9 done: loss 0.5480 - lr 0.1000000
2021-05-24 13:22:47,311 DEV : loss 0.10558748245239258 - score 0.9692
Epoch     9: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 13:22:47,347 BAD EPOCHS (no improvement): 4
2021-05-24 13:22:47,347 ----------------------------------------------------------------------------------------------------
2021-05-24 13:22:50,889 epoch 10 - iter 13/135 - loss 0.42767876 - samples/sec: 58.74 - lr: 0.050000
2021-05-24 13:22:54,389 epoch 10 - iter 26/135 - loss 0.45922252 - samples/sec: 59.44 - lr: 0.050000
2021-05-24 13:22:57,912 epoch 10 - iter 39/135 - loss 0.43566867 - samples/sec: 59.06 - lr: 0.050000
2021-05-24 13:23:01,428 epoch 10 - iter 52/135 - loss 0.44196542 - samples/sec: 59.16 - lr: 0.050000
2021-05-24 13:23:04,921 epoch 10 - iter 65/135 - loss 0.41890199 - samples/sec: 59.57 - lr: 0.050000
2021-05-24 13:23:08,445 epoch 10 - iter 78/135 - loss 0.41830163 - samples/sec: 59.02 - lr: 0.050000
2021-05-24 13:23:11,965 epoch 10 - iter 91/135 - loss 0.41778508 - samples/sec: 59.10 - lr: 0.050000
2021-05-24 13:23:15,462 epoch 10 - iter 104/135 - loss 0.40787035 - samples/sec: 59.49 - lr: 0.050000
2021-05-24 13:23:18,958 epoch 10 - iter 117/135 - loss 0.42019987 - samples/sec: 59.51 - lr: 0.050000
2021-05-24 13:23:22,516 epoch 10 - iter 130/135 - loss 0.40239329 - samples/sec: 58.48 - lr: 0.050000
2021-05-24 13:23:23,662 ----------------------------------------------------------------------------------------------------
2021-05-24 13:23:23,662 EPOCH 10 done: loss 0.4072 - lr 0.0500000
2021-05-24 13:23:25,862 DEV : loss 0.11197756975889206 - score 0.9728
2021-05-24 13:23:25,898 BAD EPOCHS (no improvement): 1
2021-05-24 13:23:25,898 ----------------------------------------------------------------------------------------------------
2021-05-24 13:23:29,431 epoch 11 - iter 13/135 - loss 0.27501774 - samples/sec: 58.89 - lr: 0.050000
2021-05-24 13:23:32,956 epoch 11 - iter 26/135 - loss 0.32240758 - samples/sec: 59.03 - lr: 0.050000
2021-05-24 13:23:36,441 epoch 11 - iter 39/135 - loss 0.31512546 - samples/sec: 59.68 - lr: 0.050000
2021-05-24 13:23:39,929 epoch 11 - iter 52/135 - loss 0.34730434 - samples/sec: 59.65 - lr: 0.050000
2021-05-24 13:23:43,409 epoch 11 - iter 65/135 - loss 0.34737597 - samples/sec: 59.78 - lr: 0.050000
2021-05-24 13:23:47,183 epoch 11 - iter 78/135 - loss 0.36988128 - samples/sec: 55.13 - lr: 0.050000
2021-05-24 13:23:50,648 epoch 11 - iter 91/135 - loss 0.37022376 - samples/sec: 60.04 - lr: 0.050000
2021-05-24 13:23:54,183 epoch 11 - iter 104/135 - loss 0.36560685 - samples/sec: 58.85 - lr: 0.050000
2021-05-24 13:23:57,692 epoch 11 - iter 117/135 - loss 0.35218800 - samples/sec: 59.28 - lr: 0.050000
2021-05-24 13:24:01,192 epoch 11 - iter 130/135 - loss 0.35151721 - samples/sec: 59.44 - lr: 0.050000
2021-05-24 13:24:02,356 ----------------------------------------------------------------------------------------------------
2021-05-24 13:24:02,356 EPOCH 11 done: loss 0.3497 - lr 0.0500000
2021-05-24 13:24:04,557 DEV : loss 0.10945119708776474 - score 0.9711
2021-05-24 13:24:04,592 BAD EPOCHS (no improvement): 2
2021-05-24 13:24:04,592 ----------------------------------------------------------------------------------------------------
2021-05-24 13:24:08,105 epoch 12 - iter 13/135 - loss 0.39983536 - samples/sec: 59.22 - lr: 0.050000
2021-05-24 13:24:11,678 epoch 12 - iter 26/135 - loss 0.43347850 - samples/sec: 58.24 - lr: 0.050000
2021-05-24 13:24:15,210 epoch 12 - iter 39/135 - loss 0.41053793 - samples/sec: 58.90 - lr: 0.050000
2021-05-24 13:24:18,681 epoch 12 - iter 52/135 - loss 0.39837042 - samples/sec: 59.92 - lr: 0.050000
2021-05-24 13:24:22,166 epoch 12 - iter 65/135 - loss 0.38032742 - samples/sec: 59.70 - lr: 0.050000
2021-05-24 13:24:25,663 epoch 12 - iter 78/135 - loss 0.36261971 - samples/sec: 59.50 - lr: 0.050000
2021-05-24 13:24:29,170 epoch 12 - iter 91/135 - loss 0.38194559 - samples/sec: 59.32 - lr: 0.050000
2021-05-24 13:24:32,622 epoch 12 - iter 104/135 - loss 0.38803301 - samples/sec: 60.26 - lr: 0.050000
2021-05-24 13:24:36,135 epoch 12 - iter 117/135 - loss 0.38279257 - samples/sec: 59.23 - lr: 0.050000
2021-05-24 13:24:39,687 epoch 12 - iter 130/135 - loss 0.37928964 - samples/sec: 58.57 - lr: 0.050000
2021-05-24 13:24:40,816 ----------------------------------------------------------------------------------------------------
2021-05-24 13:24:40,816 EPOCH 12 done: loss 0.3724 - lr 0.0500000
2021-05-24 13:24:43,024 DEV : loss 0.09763847291469574 - score 0.9749
2021-05-24 13:24:43,059 BAD EPOCHS (no improvement): 3
2021-05-24 13:24:43,059 ----------------------------------------------------------------------------------------------------
2021-05-24 13:24:46,513 epoch 13 - iter 13/135 - loss 0.34674277 - samples/sec: 60.24 - lr: 0.050000
2021-05-24 13:24:50,050 epoch 13 - iter 26/135 - loss 0.35969753 - samples/sec: 58.81 - lr: 0.050000
2021-05-24 13:24:53,533 epoch 13 - iter 39/135 - loss 0.32193358 - samples/sec: 59.74 - lr: 0.050000
2021-05-24 13:24:56,994 epoch 13 - iter 52/135 - loss 0.31777154 - samples/sec: 60.11 - lr: 0.050000
2021-05-24 13:25:00,469 epoch 13 - iter 65/135 - loss 0.33057641 - samples/sec: 59.87 - lr: 0.050000
2021-05-24 13:25:03,940 epoch 13 - iter 78/135 - loss 0.33079467 - samples/sec: 59.94 - lr: 0.050000
2021-05-24 13:25:07,463 epoch 13 - iter 91/135 - loss 0.34590881 - samples/sec: 59.05 - lr: 0.050000
2021-05-24 13:25:10,971 epoch 13 - iter 104/135 - loss 0.34382657 - samples/sec: 59.31 - lr: 0.050000
2021-05-24 13:25:14,475 epoch 13 - iter 117/135 - loss 0.34564846 - samples/sec: 59.37 - lr: 0.050000
2021-05-24 13:25:17,960 epoch 13 - iter 130/135 - loss 0.34232112 - samples/sec: 59.70 - lr: 0.050000
2021-05-24 13:25:19,129 ----------------------------------------------------------------------------------------------------
2021-05-24 13:25:19,130 EPOCH 13 done: loss 0.3610 - lr 0.0500000
2021-05-24 13:25:21,336 DEV : loss 0.10248895734548569 - score 0.9712
Epoch    13: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 13:25:21,372 BAD EPOCHS (no improvement): 4
2021-05-24 13:25:21,372 ----------------------------------------------------------------------------------------------------
2021-05-24 13:25:24,863 epoch 14 - iter 13/135 - loss 0.31855282 - samples/sec: 59.59 - lr: 0.025000
2021-05-24 13:25:28,279 epoch 14 - iter 26/135 - loss 0.34815915 - samples/sec: 60.92 - lr: 0.025000
2021-05-24 13:25:31,753 epoch 14 - iter 39/135 - loss 0.37804531 - samples/sec: 59.88 - lr: 0.025000
2021-05-24 13:25:35,245 epoch 14 - iter 52/135 - loss 0.34669704 - samples/sec: 59.58 - lr: 0.025000
2021-05-24 13:25:38,643 epoch 14 - iter 65/135 - loss 0.36985285 - samples/sec: 61.23 - lr: 0.025000
2021-05-24 13:25:42,088 epoch 14 - iter 78/135 - loss 0.36423469 - samples/sec: 60.38 - lr: 0.025000
2021-05-24 13:25:45,567 epoch 14 - iter 91/135 - loss 0.35327626 - samples/sec: 59.81 - lr: 0.025000
2021-05-24 13:25:48,988 epoch 14 - iter 104/135 - loss 0.34488089 - samples/sec: 60.81 - lr: 0.025000
2021-05-24 13:25:52,424 epoch 14 - iter 117/135 - loss 0.33640174 - samples/sec: 60.55 - lr: 0.025000
2021-05-24 13:25:55,868 epoch 14 - iter 130/135 - loss 0.32332696 - samples/sec: 60.40 - lr: 0.025000
2021-05-24 13:25:57,017 ----------------------------------------------------------------------------------------------------
2021-05-24 13:25:57,017 EPOCH 14 done: loss 0.3220 - lr 0.0250000
2021-05-24 13:25:59,219 DEV : loss 0.11422185599803925 - score 0.9713
2021-05-24 13:25:59,254 BAD EPOCHS (no improvement): 1
2021-05-24 13:25:59,255 ----------------------------------------------------------------------------------------------------
2021-05-24 13:26:02,768 epoch 15 - iter 13/135 - loss 0.28026260 - samples/sec: 59.22 - lr: 0.025000
2021-05-24 13:26:06,289 epoch 15 - iter 26/135 - loss 0.35498388 - samples/sec: 59.09 - lr: 0.025000
2021-05-24 13:26:09,788 epoch 15 - iter 39/135 - loss 0.37721448 - samples/sec: 59.45 - lr: 0.025000
2021-05-24 13:26:13,233 epoch 15 - iter 52/135 - loss 0.36563599 - samples/sec: 60.40 - lr: 0.025000
2021-05-24 13:26:16,698 epoch 15 - iter 65/135 - loss 0.32504347 - samples/sec: 60.03 - lr: 0.025000
2021-05-24 13:26:20,174 epoch 15 - iter 78/135 - loss 0.32669406 - samples/sec: 59.85 - lr: 0.025000
2021-05-24 13:26:23,624 epoch 15 - iter 91/135 - loss 0.32129036 - samples/sec: 60.30 - lr: 0.025000
2021-05-24 13:26:27,081 epoch 15 - iter 104/135 - loss 0.31704119 - samples/sec: 60.19 - lr: 0.025000
2021-05-24 13:26:30,505 epoch 15 - iter 117/135 - loss 0.29799396 - samples/sec: 60.75 - lr: 0.025000
2021-05-24 13:26:33,972 epoch 15 - iter 130/135 - loss 0.30536702 - samples/sec: 60.02 - lr: 0.025000
2021-05-24 13:26:35,092 ----------------------------------------------------------------------------------------------------
2021-05-24 13:26:35,093 EPOCH 15 done: loss 0.3004 - lr 0.0250000
2021-05-24 13:26:37,292 DEV : loss 0.09737684577703476 - score 0.9692
2021-05-24 13:26:37,328 BAD EPOCHS (no improvement): 2
2021-05-24 13:26:37,328 ----------------------------------------------------------------------------------------------------
2021-05-24 13:26:41,041 epoch 16 - iter 13/135 - loss 0.31161637 - samples/sec: 56.03 - lr: 0.025000
2021-05-24 13:26:44,452 epoch 16 - iter 26/135 - loss 0.33827734 - samples/sec: 61.01 - lr: 0.025000
2021-05-24 13:26:47,909 epoch 16 - iter 39/135 - loss 0.31844983 - samples/sec: 60.17 - lr: 0.025000
2021-05-24 13:26:51,352 epoch 16 - iter 52/135 - loss 0.30617427 - samples/sec: 60.42 - lr: 0.025000
2021-05-24 13:26:54,856 epoch 16 - iter 65/135 - loss 0.30202569 - samples/sec: 59.38 - lr: 0.025000
2021-05-24 13:26:58,364 epoch 16 - iter 78/135 - loss 0.30934654 - samples/sec: 59.31 - lr: 0.025000
2021-05-24 13:27:01,846 epoch 16 - iter 91/135 - loss 0.31072548 - samples/sec: 59.74 - lr: 0.025000
2021-05-24 13:27:05,289 epoch 16 - iter 104/135 - loss 0.30509294 - samples/sec: 60.42 - lr: 0.025000
2021-05-24 13:27:08,786 epoch 16 - iter 117/135 - loss 0.30603770 - samples/sec: 59.50 - lr: 0.025000
2021-05-24 13:27:12,277 epoch 16 - iter 130/135 - loss 0.29720565 - samples/sec: 59.58 - lr: 0.025000
2021-05-24 13:27:13,400 ----------------------------------------------------------------------------------------------------
2021-05-24 13:27:13,401 EPOCH 16 done: loss 0.3004 - lr 0.0250000
2021-05-24 13:27:15,599 DEV : loss 0.1016509085893631 - score 0.9732
2021-05-24 13:27:15,634 BAD EPOCHS (no improvement): 3
2021-05-24 13:27:15,635 ----------------------------------------------------------------------------------------------------
2021-05-24 13:27:19,028 epoch 17 - iter 13/135 - loss 0.42735561 - samples/sec: 61.32 - lr: 0.025000
2021-05-24 13:27:22,508 epoch 17 - iter 26/135 - loss 0.36107834 - samples/sec: 59.78 - lr: 0.025000
2021-05-24 13:27:26,015 epoch 17 - iter 39/135 - loss 0.32906873 - samples/sec: 59.32 - lr: 0.025000
2021-05-24 13:27:29,543 epoch 17 - iter 52/135 - loss 0.29754757 - samples/sec: 58.96 - lr: 0.025000
2021-05-24 13:27:33,007 epoch 17 - iter 65/135 - loss 0.30072865 - samples/sec: 60.06 - lr: 0.025000
2021-05-24 13:27:36,486 epoch 17 - iter 78/135 - loss 0.31153828 - samples/sec: 59.80 - lr: 0.025000
2021-05-24 13:27:39,912 epoch 17 - iter 91/135 - loss 0.30789793 - samples/sec: 60.72 - lr: 0.025000
2021-05-24 13:27:43,425 epoch 17 - iter 104/135 - loss 0.30616172 - samples/sec: 59.23 - lr: 0.025000
2021-05-24 13:27:46,935 epoch 17 - iter 117/135 - loss 0.31407746 - samples/sec: 59.28 - lr: 0.025000
2021-05-24 13:27:50,413 epoch 17 - iter 130/135 - loss 0.31203213 - samples/sec: 59.81 - lr: 0.025000
2021-05-24 13:27:51,535 ----------------------------------------------------------------------------------------------------
2021-05-24 13:27:51,536 EPOCH 17 done: loss 0.3098 - lr 0.0250000
2021-05-24 13:27:53,740 DEV : loss 0.10020242631435394 - score 0.9712
Epoch    17: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 13:27:53,775 BAD EPOCHS (no improvement): 4
2021-05-24 13:27:53,776 ----------------------------------------------------------------------------------------------------
2021-05-24 13:27:57,311 epoch 18 - iter 13/135 - loss 0.31195379 - samples/sec: 58.86 - lr: 0.012500
2021-05-24 13:28:00,802 epoch 18 - iter 26/135 - loss 0.30073811 - samples/sec: 59.59 - lr: 0.012500
2021-05-24 13:28:04,296 epoch 18 - iter 39/135 - loss 0.29134032 - samples/sec: 59.54 - lr: 0.012500
2021-05-24 13:28:07,783 epoch 18 - iter 52/135 - loss 0.27761262 - samples/sec: 59.66 - lr: 0.012500
2021-05-24 13:28:11,227 epoch 18 - iter 65/135 - loss 0.26893105 - samples/sec: 60.41 - lr: 0.012500
2021-05-24 13:28:14,652 epoch 18 - iter 78/135 - loss 0.27027244 - samples/sec: 60.74 - lr: 0.012500
2021-05-24 13:28:18,096 epoch 18 - iter 91/135 - loss 0.25002874 - samples/sec: 60.40 - lr: 0.012500
2021-05-24 13:28:21,552 epoch 18 - iter 104/135 - loss 0.26475497 - samples/sec: 60.19 - lr: 0.012500
2021-05-24 13:28:24,990 epoch 18 - iter 117/135 - loss 0.26341062 - samples/sec: 60.51 - lr: 0.012500
2021-05-24 13:28:28,457 epoch 18 - iter 130/135 - loss 0.27806068 - samples/sec: 60.01 - lr: 0.012500
2021-05-24 13:28:29,606 ----------------------------------------------------------------------------------------------------
2021-05-24 13:28:29,606 EPOCH 18 done: loss 0.2759 - lr 0.0125000
2021-05-24 13:28:31,808 DEV : loss 0.09305843710899353 - score 0.9731
2021-05-24 13:28:31,843 BAD EPOCHS (no improvement): 1
2021-05-24 13:28:31,843 ----------------------------------------------------------------------------------------------------
2021-05-24 13:28:35,276 epoch 19 - iter 13/135 - loss 0.19804981 - samples/sec: 60.62 - lr: 0.012500
2021-05-24 13:28:38,744 epoch 19 - iter 26/135 - loss 0.22350939 - samples/sec: 59.99 - lr: 0.012500
2021-05-24 13:28:42,157 epoch 19 - iter 39/135 - loss 0.23585630 - samples/sec: 60.95 - lr: 0.012500
2021-05-24 13:28:45,630 epoch 19 - iter 52/135 - loss 0.24137135 - samples/sec: 59.89 - lr: 0.012500
2021-05-24 13:28:49,011 epoch 19 - iter 65/135 - loss 0.25384127 - samples/sec: 61.53 - lr: 0.012500
2021-05-24 13:28:52,473 epoch 19 - iter 78/135 - loss 0.24829048 - samples/sec: 60.10 - lr: 0.012500
2021-05-24 13:28:55,881 epoch 19 - iter 91/135 - loss 0.25710124 - samples/sec: 61.05 - lr: 0.012500
2021-05-24 13:28:59,319 epoch 19 - iter 104/135 - loss 0.26539683 - samples/sec: 60.50 - lr: 0.012500
2021-05-24 13:29:02,784 epoch 19 - iter 117/135 - loss 0.27173607 - samples/sec: 60.05 - lr: 0.012500
2021-05-24 13:29:06,221 epoch 19 - iter 130/135 - loss 0.26489620 - samples/sec: 60.54 - lr: 0.012500
2021-05-24 13:29:07,380 ----------------------------------------------------------------------------------------------------
2021-05-24 13:29:07,380 EPOCH 19 done: loss 0.2612 - lr 0.0125000
2021-05-24 13:29:09,583 DEV : loss 0.09339668601751328 - score 0.9712
2021-05-24 13:29:09,618 BAD EPOCHS (no improvement): 2
2021-05-24 13:29:09,619 ----------------------------------------------------------------------------------------------------
2021-05-24 13:29:13,118 epoch 20 - iter 13/135 - loss 0.15558585 - samples/sec: 59.45 - lr: 0.012500
2021-05-24 13:29:16,542 epoch 20 - iter 26/135 - loss 0.29246413 - samples/sec: 60.77 - lr: 0.012500
2021-05-24 13:29:19,981 epoch 20 - iter 39/135 - loss 0.26968941 - samples/sec: 60.50 - lr: 0.012500
2021-05-24 13:29:23,408 epoch 20 - iter 52/135 - loss 0.25376789 - samples/sec: 60.70 - lr: 0.012500
2021-05-24 13:29:26,919 epoch 20 - iter 65/135 - loss 0.27204922 - samples/sec: 59.25 - lr: 0.012500
2021-05-24 13:29:30,381 epoch 20 - iter 78/135 - loss 0.26172651 - samples/sec: 60.10 - lr: 0.012500
2021-05-24 13:29:33,848 epoch 20 - iter 91/135 - loss 0.26423603 - samples/sec: 60.01 - lr: 0.012500
2021-05-24 13:29:37,267 epoch 20 - iter 104/135 - loss 0.25887381 - samples/sec: 60.84 - lr: 0.012500
2021-05-24 13:29:40,746 epoch 20 - iter 117/135 - loss 0.25891632 - samples/sec: 59.80 - lr: 0.012500
2021-05-24 13:29:44,262 epoch 20 - iter 130/135 - loss 0.25806759 - samples/sec: 59.18 - lr: 0.012500
2021-05-24 13:29:45,415 ----------------------------------------------------------------------------------------------------
2021-05-24 13:29:45,415 EPOCH 20 done: loss 0.2595 - lr 0.0125000
2021-05-24 13:29:47,856 DEV : loss 0.08569633960723877 - score 0.9711
2021-05-24 13:29:47,891 BAD EPOCHS (no improvement): 3
2021-05-24 13:29:47,891 ----------------------------------------------------------------------------------------------------
2021-05-24 13:29:51,408 epoch 21 - iter 13/135 - loss 0.33725755 - samples/sec: 59.16 - lr: 0.012500
2021-05-24 13:29:54,845 epoch 21 - iter 26/135 - loss 0.29717988 - samples/sec: 60.53 - lr: 0.012500
2021-05-24 13:29:58,376 epoch 21 - iter 39/135 - loss 0.32087100 - samples/sec: 58.92 - lr: 0.012500
2021-05-24 13:30:01,915 epoch 21 - iter 52/135 - loss 0.31507343 - samples/sec: 58.79 - lr: 0.012500
2021-05-24 13:30:05,376 epoch 21 - iter 65/135 - loss 0.30018876 - samples/sec: 60.11 - lr: 0.012500
2021-05-24 13:30:08,859 epoch 21 - iter 78/135 - loss 0.28229219 - samples/sec: 59.73 - lr: 0.012500
2021-05-24 13:30:12,325 epoch 21 - iter 91/135 - loss 0.26670080 - samples/sec: 60.01 - lr: 0.012500
2021-05-24 13:30:15,822 epoch 21 - iter 104/135 - loss 0.26514435 - samples/sec: 59.51 - lr: 0.012500
2021-05-24 13:30:19,276 epoch 21 - iter 117/135 - loss 0.26588902 - samples/sec: 60.22 - lr: 0.012500
2021-05-24 13:30:22,726 epoch 21 - iter 130/135 - loss 0.25999058 - samples/sec: 60.31 - lr: 0.012500
2021-05-24 13:30:23,848 ----------------------------------------------------------------------------------------------------
2021-05-24 13:30:23,848 EPOCH 21 done: loss 0.2609 - lr 0.0125000
2021-05-24 13:30:26,051 DEV : loss 0.08023826032876968 - score 0.9767
2021-05-24 13:30:26,087 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:30:35,756 ----------------------------------------------------------------------------------------------------
2021-05-24 13:30:39,241 epoch 22 - iter 13/135 - loss 0.31049080 - samples/sec: 59.70 - lr: 0.012500
2021-05-24 13:30:42,681 epoch 22 - iter 26/135 - loss 0.28828008 - samples/sec: 60.48 - lr: 0.012500
2021-05-24 13:30:46,122 epoch 22 - iter 39/135 - loss 0.28671847 - samples/sec: 60.47 - lr: 0.012500
2021-05-24 13:30:49,589 epoch 22 - iter 52/135 - loss 0.27080182 - samples/sec: 60.00 - lr: 0.012500
2021-05-24 13:30:52,989 epoch 22 - iter 65/135 - loss 0.26748665 - samples/sec: 61.19 - lr: 0.012500
2021-05-24 13:30:56,366 epoch 22 - iter 78/135 - loss 0.27317077 - samples/sec: 61.62 - lr: 0.012500
2021-05-24 13:30:59,807 epoch 22 - iter 91/135 - loss 0.26312256 - samples/sec: 60.45 - lr: 0.012500
2021-05-24 13:31:03,252 epoch 22 - iter 104/135 - loss 0.26923229 - samples/sec: 60.40 - lr: 0.012500
2021-05-24 13:31:06,698 epoch 22 - iter 117/135 - loss 0.26586040 - samples/sec: 60.37 - lr: 0.012500
2021-05-24 13:31:10,086 epoch 22 - iter 130/135 - loss 0.26797633 - samples/sec: 61.40 - lr: 0.012500
2021-05-24 13:31:11,245 ----------------------------------------------------------------------------------------------------
2021-05-24 13:31:11,245 EPOCH 22 done: loss 0.2695 - lr 0.0125000
2021-05-24 13:31:13,445 DEV : loss 0.08068329840898514 - score 0.9787
2021-05-24 13:31:13,480 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:31:23,244 ----------------------------------------------------------------------------------------------------
2021-05-24 13:31:26,672 epoch 23 - iter 13/135 - loss 0.33699485 - samples/sec: 60.69 - lr: 0.012500
2021-05-24 13:31:30,086 epoch 23 - iter 26/135 - loss 0.25958375 - samples/sec: 60.94 - lr: 0.012500
2021-05-24 13:31:33,520 epoch 23 - iter 39/135 - loss 0.26835074 - samples/sec: 60.57 - lr: 0.012500
2021-05-24 13:31:36,974 epoch 23 - iter 52/135 - loss 0.24896746 - samples/sec: 60.25 - lr: 0.012500
2021-05-24 13:31:40,465 epoch 23 - iter 65/135 - loss 0.24373000 - samples/sec: 59.58 - lr: 0.012500
2021-05-24 13:31:43,893 epoch 23 - iter 78/135 - loss 0.25591930 - samples/sec: 60.70 - lr: 0.012500
2021-05-24 13:31:47,314 epoch 23 - iter 91/135 - loss 0.24222157 - samples/sec: 60.80 - lr: 0.012500
2021-05-24 13:31:50,792 epoch 23 - iter 104/135 - loss 0.25489569 - samples/sec: 59.83 - lr: 0.012500
2021-05-24 13:31:54,190 epoch 23 - iter 117/135 - loss 0.26505043 - samples/sec: 61.22 - lr: 0.012500
2021-05-24 13:31:57,647 epoch 23 - iter 130/135 - loss 0.26917937 - samples/sec: 60.17 - lr: 0.012500
2021-05-24 13:31:58,811 ----------------------------------------------------------------------------------------------------
2021-05-24 13:31:58,812 EPOCH 23 done: loss 0.2659 - lr 0.0125000
2021-05-24 13:32:01,007 DEV : loss 0.07988385856151581 - score 0.9768
2021-05-24 13:32:01,043 BAD EPOCHS (no improvement): 1
2021-05-24 13:32:01,043 ----------------------------------------------------------------------------------------------------
2021-05-24 13:32:04,496 epoch 24 - iter 13/135 - loss 0.25512399 - samples/sec: 60.25 - lr: 0.012500
2021-05-24 13:32:07,968 epoch 24 - iter 26/135 - loss 0.22803782 - samples/sec: 59.92 - lr: 0.012500
2021-05-24 13:32:11,442 epoch 24 - iter 39/135 - loss 0.25395086 - samples/sec: 59.88 - lr: 0.012500
2021-05-24 13:32:14,933 epoch 24 - iter 52/135 - loss 0.26577883 - samples/sec: 59.60 - lr: 0.012500
2021-05-24 13:32:18,394 epoch 24 - iter 65/135 - loss 0.25632743 - samples/sec: 60.10 - lr: 0.012500
2021-05-24 13:32:21,866 epoch 24 - iter 78/135 - loss 0.24830159 - samples/sec: 59.92 - lr: 0.012500
2021-05-24 13:32:25,258 epoch 24 - iter 91/135 - loss 0.25137507 - samples/sec: 61.34 - lr: 0.012500
2021-05-24 13:32:28,756 epoch 24 - iter 104/135 - loss 0.24262427 - samples/sec: 59.48 - lr: 0.012500
2021-05-24 13:32:32,190 epoch 24 - iter 117/135 - loss 0.24979057 - samples/sec: 60.57 - lr: 0.012500
2021-05-24 13:32:35,718 epoch 24 - iter 130/135 - loss 0.25149004 - samples/sec: 58.97 - lr: 0.012500
2021-05-24 13:32:36,873 ----------------------------------------------------------------------------------------------------
2021-05-24 13:32:36,873 EPOCH 24 done: loss 0.2471 - lr 0.0125000
2021-05-24 13:32:39,071 DEV : loss 0.08002708852291107 - score 0.9768
2021-05-24 13:32:39,107 BAD EPOCHS (no improvement): 2
2021-05-24 13:32:39,107 ----------------------------------------------------------------------------------------------------
2021-05-24 13:32:42,613 epoch 25 - iter 13/135 - loss 0.28600679 - samples/sec: 59.33 - lr: 0.012500
2021-05-24 13:32:46,042 epoch 25 - iter 26/135 - loss 0.24594306 - samples/sec: 60.67 - lr: 0.012500
2021-05-24 13:32:49,517 epoch 25 - iter 39/135 - loss 0.25862189 - samples/sec: 59.88 - lr: 0.012500
2021-05-24 13:32:53,000 epoch 25 - iter 52/135 - loss 0.28324656 - samples/sec: 59.73 - lr: 0.012500
2021-05-24 13:32:56,501 epoch 25 - iter 65/135 - loss 0.29092455 - samples/sec: 59.42 - lr: 0.012500
2021-05-24 13:32:59,999 epoch 25 - iter 78/135 - loss 0.28561099 - samples/sec: 59.47 - lr: 0.012500
2021-05-24 13:33:03,476 epoch 25 - iter 91/135 - loss 0.28735199 - samples/sec: 59.84 - lr: 0.012500
2021-05-24 13:33:06,873 epoch 25 - iter 104/135 - loss 0.29217366 - samples/sec: 61.23 - lr: 0.012500
2021-05-24 13:33:10,314 epoch 25 - iter 117/135 - loss 0.28590853 - samples/sec: 60.46 - lr: 0.012500
2021-05-24 13:33:13,752 epoch 25 - iter 130/135 - loss 0.28424709 - samples/sec: 60.51 - lr: 0.012500
2021-05-24 13:33:14,917 ----------------------------------------------------------------------------------------------------
2021-05-24 13:33:14,917 EPOCH 25 done: loss 0.2797 - lr 0.0125000
2021-05-24 13:33:17,352 DEV : loss 0.09597302973270416 - score 0.9771
2021-05-24 13:33:17,388 BAD EPOCHS (no improvement): 3
2021-05-24 13:33:17,388 ----------------------------------------------------------------------------------------------------
2021-05-24 13:33:20,869 epoch 26 - iter 13/135 - loss 0.38486503 - samples/sec: 59.77 - lr: 0.012500
2021-05-24 13:33:24,301 epoch 26 - iter 26/135 - loss 0.29271790 - samples/sec: 60.61 - lr: 0.012500
2021-05-24 13:33:27,780 epoch 26 - iter 39/135 - loss 0.26951237 - samples/sec: 59.81 - lr: 0.012500
2021-05-24 13:33:31,185 epoch 26 - iter 52/135 - loss 0.24096233 - samples/sec: 61.10 - lr: 0.012500
2021-05-24 13:33:34,658 epoch 26 - iter 65/135 - loss 0.24433211 - samples/sec: 59.89 - lr: 0.012500
2021-05-24 13:33:38,141 epoch 26 - iter 78/135 - loss 0.22900003 - samples/sec: 59.74 - lr: 0.012500
2021-05-24 13:33:41,577 epoch 26 - iter 91/135 - loss 0.23227962 - samples/sec: 60.55 - lr: 0.012500
2021-05-24 13:33:45,038 epoch 26 - iter 104/135 - loss 0.22569362 - samples/sec: 60.12 - lr: 0.012500
2021-05-24 13:33:48,486 epoch 26 - iter 117/135 - loss 0.22674665 - samples/sec: 60.33 - lr: 0.012500
2021-05-24 13:33:51,994 epoch 26 - iter 130/135 - loss 0.23486771 - samples/sec: 59.30 - lr: 0.012500
2021-05-24 13:33:53,131 ----------------------------------------------------------------------------------------------------
2021-05-24 13:33:53,131 EPOCH 26 done: loss 0.2357 - lr 0.0125000
2021-05-24 13:33:55,330 DEV : loss 0.09330688416957855 - score 0.9732
Epoch    26: reducing learning rate of group 0 to 6.2500e-03.
2021-05-24 13:33:55,365 BAD EPOCHS (no improvement): 4
2021-05-24 13:33:55,366 ----------------------------------------------------------------------------------------------------
2021-05-24 13:33:58,821 epoch 27 - iter 13/135 - loss 0.26718543 - samples/sec: 60.21 - lr: 0.006250
2021-05-24 13:34:02,235 epoch 27 - iter 26/135 - loss 0.26927191 - samples/sec: 60.95 - lr: 0.006250
2021-05-24 13:34:05,656 epoch 27 - iter 39/135 - loss 0.26393799 - samples/sec: 60.80 - lr: 0.006250
2021-05-24 13:34:09,097 epoch 27 - iter 52/135 - loss 0.28249881 - samples/sec: 60.46 - lr: 0.006250
2021-05-24 13:34:12,529 epoch 27 - iter 65/135 - loss 0.25907826 - samples/sec: 60.61 - lr: 0.006250
2021-05-24 13:34:15,978 epoch 27 - iter 78/135 - loss 0.26125592 - samples/sec: 60.32 - lr: 0.006250
2021-05-24 13:34:19,452 epoch 27 - iter 91/135 - loss 0.25816660 - samples/sec: 59.89 - lr: 0.006250
2021-05-24 13:34:22,901 epoch 27 - iter 104/135 - loss 0.25457765 - samples/sec: 60.32 - lr: 0.006250
2021-05-24 13:34:26,372 epoch 27 - iter 117/135 - loss 0.25379541 - samples/sec: 59.92 - lr: 0.006250
2021-05-24 13:34:29,827 epoch 27 - iter 130/135 - loss 0.24065114 - samples/sec: 60.23 - lr: 0.006250
2021-05-24 13:34:30,952 ----------------------------------------------------------------------------------------------------
2021-05-24 13:34:30,952 EPOCH 27 done: loss 0.2419 - lr 0.0062500
2021-05-24 13:34:33,156 DEV : loss 0.08914300799369812 - score 0.9751
2021-05-24 13:34:33,192 BAD EPOCHS (no improvement): 1
2021-05-24 13:34:33,192 ----------------------------------------------------------------------------------------------------
2021-05-24 13:34:36,684 epoch 28 - iter 13/135 - loss 0.27082027 - samples/sec: 59.58 - lr: 0.006250
2021-05-24 13:34:40,132 epoch 28 - iter 26/135 - loss 0.24005798 - samples/sec: 60.33 - lr: 0.006250
2021-05-24 13:34:43,606 epoch 28 - iter 39/135 - loss 0.22030942 - samples/sec: 59.89 - lr: 0.006250
2021-05-24 13:34:47,015 epoch 28 - iter 52/135 - loss 0.21641848 - samples/sec: 61.03 - lr: 0.006250
2021-05-24 13:34:50,445 epoch 28 - iter 65/135 - loss 0.21486308 - samples/sec: 60.64 - lr: 0.006250
2021-05-24 13:34:53,903 epoch 28 - iter 78/135 - loss 0.23143964 - samples/sec: 60.16 - lr: 0.006250
2021-05-24 13:34:57,393 epoch 28 - iter 91/135 - loss 0.23752128 - samples/sec: 59.62 - lr: 0.006250
2021-05-24 13:35:00,820 epoch 28 - iter 104/135 - loss 0.23731483 - samples/sec: 60.69 - lr: 0.006250
2021-05-24 13:35:04,328 epoch 28 - iter 117/135 - loss 0.23687602 - samples/sec: 59.30 - lr: 0.006250
2021-05-24 13:35:07,760 epoch 28 - iter 130/135 - loss 0.23918289 - samples/sec: 60.62 - lr: 0.006250
2021-05-24 13:35:08,891 ----------------------------------------------------------------------------------------------------
2021-05-24 13:35:08,891 EPOCH 28 done: loss 0.2410 - lr 0.0062500
2021-05-24 13:35:11,092 DEV : loss 0.0859774649143219 - score 0.9731
2021-05-24 13:35:11,128 BAD EPOCHS (no improvement): 2
2021-05-24 13:35:11,128 ----------------------------------------------------------------------------------------------------
2021-05-24 13:35:14,612 epoch 29 - iter 13/135 - loss 0.26453682 - samples/sec: 59.72 - lr: 0.006250
2021-05-24 13:35:18,110 epoch 29 - iter 26/135 - loss 0.25961513 - samples/sec: 59.47 - lr: 0.006250
2021-05-24 13:35:21,527 epoch 29 - iter 39/135 - loss 0.27812148 - samples/sec: 60.89 - lr: 0.006250
2021-05-24 13:35:24,971 epoch 29 - iter 52/135 - loss 0.26820748 - samples/sec: 60.40 - lr: 0.006250
2021-05-24 13:35:28,340 epoch 29 - iter 65/135 - loss 0.27443557 - samples/sec: 61.75 - lr: 0.006250
2021-05-24 13:35:31,790 epoch 29 - iter 78/135 - loss 0.27356717 - samples/sec: 60.30 - lr: 0.006250
2021-05-24 13:35:35,217 epoch 29 - iter 91/135 - loss 0.26278687 - samples/sec: 60.71 - lr: 0.006250
2021-05-24 13:35:38,725 epoch 29 - iter 104/135 - loss 0.26064529 - samples/sec: 59.31 - lr: 0.006250
2021-05-24 13:35:42,230 epoch 29 - iter 117/135 - loss 0.25815519 - samples/sec: 59.35 - lr: 0.006250
2021-05-24 13:35:45,705 epoch 29 - iter 130/135 - loss 0.26217049 - samples/sec: 59.88 - lr: 0.006250
2021-05-24 13:35:46,833 ----------------------------------------------------------------------------------------------------
2021-05-24 13:35:46,833 EPOCH 29 done: loss 0.2570 - lr 0.0062500
2021-05-24 13:35:49,032 DEV : loss 0.0915968045592308 - score 0.9751
2021-05-24 13:35:49,067 BAD EPOCHS (no improvement): 3
2021-05-24 13:35:49,068 ----------------------------------------------------------------------------------------------------
2021-05-24 13:35:52,615 epoch 30 - iter 13/135 - loss 0.18665120 - samples/sec: 58.64 - lr: 0.006250
2021-05-24 13:35:56,062 epoch 30 - iter 26/135 - loss 0.24844979 - samples/sec: 60.35 - lr: 0.006250
2021-05-24 13:35:59,576 epoch 30 - iter 39/135 - loss 0.21672489 - samples/sec: 59.22 - lr: 0.006250
2021-05-24 13:36:03,083 epoch 30 - iter 52/135 - loss 0.21144925 - samples/sec: 59.31 - lr: 0.006250
2021-05-24 13:36:06,558 epoch 30 - iter 65/135 - loss 0.20763628 - samples/sec: 59.87 - lr: 0.006250
2021-05-24 13:36:10,106 epoch 30 - iter 78/135 - loss 0.21954925 - samples/sec: 58.64 - lr: 0.006250
2021-05-24 13:36:13,604 epoch 30 - iter 91/135 - loss 0.21925186 - samples/sec: 59.46 - lr: 0.006250
2021-05-24 13:36:17,073 epoch 30 - iter 104/135 - loss 0.21392079 - samples/sec: 59.98 - lr: 0.006250
2021-05-24 13:36:20,513 epoch 30 - iter 117/135 - loss 0.21609751 - samples/sec: 60.48 - lr: 0.006250
2021-05-24 13:36:24,053 epoch 30 - iter 130/135 - loss 0.21570250 - samples/sec: 58.77 - lr: 0.006250
2021-05-24 13:36:25,406 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:25,406 EPOCH 30 done: loss 0.2160 - lr 0.0062500
2021-05-24 13:36:27,605 DEV : loss 0.08742430061101913 - score 0.9732
Epoch    30: reducing learning rate of group 0 to 3.1250e-03.
2021-05-24 13:36:27,640 BAD EPOCHS (no improvement): 4
2021-05-24 13:36:28,796 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:28,796 Testing using best model ...
2021-05-24 13:36:28,796 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.rststb/best-model.pt
2021-05-24 13:36:41,918 0.9577	0.9703	0.9639
2021-05-24 13:36:41,918 
Results:
- F1-score (micro) 0.9639
- F1-score (macro) 0.9639

By class:
SENT       tp: 294 - fp: 13 - fn: 9 - precision: 0.9577 - recall: 0.9703 - f1-score: 0.9639
2021-05-24 13:36:41,918 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb/
2021-05-24 13:36:41,941 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb
2021-05-24 13:36:41,942 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb/sent_train.txt
2021-05-24 13:36:41,942 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb/sent_dev.txt
2021-05-24 13:36:41,942 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb/sent_test.txt
Corpus: 441 train + 104 dev + 165 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 13:36:47,688 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 13:36:47,691 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Corpus: "Corpus: 441 train + 104 dev + 165 test sentences"
2021-05-24 13:36:47,691 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Parameters:
2021-05-24 13:36:47,691  - learning_rate: "0.1"
2021-05-24 13:36:47,691  - mini_batch_size: "16"
2021-05-24 13:36:47,691  - patience: "3"
2021-05-24 13:36:47,691  - anneal_factor: "0.5"
2021-05-24 13:36:47,691  - max_epochs: "30"
2021-05-24 13:36:47,691  - shuffle: "True"
2021-05-24 13:36:47,691  - train_with_dev: "False"
2021-05-24 13:36:47,691  - batch_growth_annealing: "False"
2021-05-24 13:36:47,691 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb"
2021-05-24 13:36:47,691 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Device: cuda:0
2021-05-24 13:36:47,691 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:47,691 Embeddings storage mode: cpu
2021-05-24 13:36:47,695 ----------------------------------------------------------------------------------------------------
2021-05-24 13:36:49,105 epoch 1 - iter 2/28 - loss 10.78760791 - samples/sec: 22.69 - lr: 0.100000
2021-05-24 13:36:50,548 epoch 1 - iter 4/28 - loss 7.25033867 - samples/sec: 22.19 - lr: 0.100000
2021-05-24 13:36:52,724 epoch 1 - iter 6/28 - loss 6.84327849 - samples/sec: 14.70 - lr: 0.100000
2021-05-24 13:36:54,198 epoch 1 - iter 8/28 - loss 6.52080560 - samples/sec: 21.72 - lr: 0.100000
2021-05-24 13:36:55,675 epoch 1 - iter 10/28 - loss 6.31032562 - samples/sec: 21.67 - lr: 0.100000
2021-05-24 13:36:57,149 epoch 1 - iter 12/28 - loss 5.97565814 - samples/sec: 21.71 - lr: 0.100000
2021-05-24 13:36:58,632 epoch 1 - iter 14/28 - loss 5.80826419 - samples/sec: 21.59 - lr: 0.100000
2021-05-24 13:37:00,122 epoch 1 - iter 16/28 - loss 5.73910043 - samples/sec: 21.47 - lr: 0.100000
2021-05-24 13:37:01,574 epoch 1 - iter 18/28 - loss 5.51674326 - samples/sec: 22.06 - lr: 0.100000
2021-05-24 13:37:03,051 epoch 1 - iter 20/28 - loss 5.32575575 - samples/sec: 21.67 - lr: 0.100000
2021-05-24 13:37:04,522 epoch 1 - iter 22/28 - loss 5.15798208 - samples/sec: 21.76 - lr: 0.100000
2021-05-24 13:37:05,987 epoch 1 - iter 24/28 - loss 4.95203928 - samples/sec: 21.85 - lr: 0.100000
2021-05-24 13:37:07,424 epoch 1 - iter 26/28 - loss 4.80805016 - samples/sec: 22.26 - lr: 0.100000
2021-05-24 13:37:08,592 epoch 1 - iter 28/28 - loss 4.71476607 - samples/sec: 27.41 - lr: 0.100000
2021-05-24 13:37:08,592 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:08,592 EPOCH 1 done: loss 4.7148 - lr 0.1000000
2021-05-24 13:37:11,654 DEV : loss 3.272414207458496 - score 0.0
2021-05-24 13:37:11,664 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:37:12,759 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:13,265 epoch 2 - iter 2/28 - loss 3.99622142 - samples/sec: 63.28 - lr: 0.100000
2021-05-24 13:37:13,771 epoch 2 - iter 4/28 - loss 4.20788771 - samples/sec: 63.31 - lr: 0.100000
2021-05-24 13:37:14,281 epoch 2 - iter 6/28 - loss 4.27691825 - samples/sec: 62.76 - lr: 0.100000
2021-05-24 13:37:14,764 epoch 2 - iter 8/28 - loss 3.89680144 - samples/sec: 66.34 - lr: 0.100000
2021-05-24 13:37:15,256 epoch 2 - iter 10/28 - loss 3.75731795 - samples/sec: 65.01 - lr: 0.100000
2021-05-24 13:37:15,751 epoch 2 - iter 12/28 - loss 3.68615286 - samples/sec: 64.68 - lr: 0.100000
2021-05-24 13:37:16,247 epoch 2 - iter 14/28 - loss 3.70170038 - samples/sec: 64.53 - lr: 0.100000
2021-05-24 13:37:16,737 epoch 2 - iter 16/28 - loss 3.79274964 - samples/sec: 65.35 - lr: 0.100000
2021-05-24 13:37:17,226 epoch 2 - iter 18/28 - loss 3.86416984 - samples/sec: 65.60 - lr: 0.100000
2021-05-24 13:37:17,721 epoch 2 - iter 20/28 - loss 3.82756333 - samples/sec: 64.58 - lr: 0.100000
2021-05-24 13:37:18,217 epoch 2 - iter 22/28 - loss 3.79073915 - samples/sec: 64.61 - lr: 0.100000
2021-05-24 13:37:18,694 epoch 2 - iter 24/28 - loss 3.76435951 - samples/sec: 67.18 - lr: 0.100000
2021-05-24 13:37:19,187 epoch 2 - iter 26/28 - loss 3.72234238 - samples/sec: 64.96 - lr: 0.100000
2021-05-24 13:37:19,590 epoch 2 - iter 28/28 - loss 3.63877193 - samples/sec: 79.50 - lr: 0.100000
2021-05-24 13:37:19,590 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:19,590 EPOCH 2 done: loss 3.6388 - lr 0.1000000
2021-05-24 13:37:20,159 DEV : loss 3.922236919403076 - score 0.0
2021-05-24 13:37:20,184 BAD EPOCHS (no improvement): 1
2021-05-24 13:37:20,184 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:20,686 epoch 3 - iter 2/28 - loss 2.11620641 - samples/sec: 63.83 - lr: 0.100000
2021-05-24 13:37:21,187 epoch 3 - iter 4/28 - loss 2.78534299 - samples/sec: 63.83 - lr: 0.100000
2021-05-24 13:37:21,688 epoch 3 - iter 6/28 - loss 3.03430931 - samples/sec: 64.01 - lr: 0.100000
2021-05-24 13:37:22,189 epoch 3 - iter 8/28 - loss 3.19719979 - samples/sec: 63.84 - lr: 0.100000
2021-05-24 13:37:22,665 epoch 3 - iter 10/28 - loss 3.28403285 - samples/sec: 67.26 - lr: 0.100000
2021-05-24 13:37:23,162 epoch 3 - iter 12/28 - loss 3.05975346 - samples/sec: 64.46 - lr: 0.100000
2021-05-24 13:37:23,661 epoch 3 - iter 14/28 - loss 3.21746513 - samples/sec: 64.13 - lr: 0.100000
2021-05-24 13:37:24,153 epoch 3 - iter 16/28 - loss 3.23658377 - samples/sec: 65.13 - lr: 0.100000
2021-05-24 13:37:24,631 epoch 3 - iter 18/28 - loss 3.15547132 - samples/sec: 66.94 - lr: 0.100000
2021-05-24 13:37:25,123 epoch 3 - iter 20/28 - loss 3.09257470 - samples/sec: 65.20 - lr: 0.100000
2021-05-24 13:37:25,607 epoch 3 - iter 22/28 - loss 3.02022879 - samples/sec: 66.15 - lr: 0.100000
2021-05-24 13:37:26,106 epoch 3 - iter 24/28 - loss 3.02619301 - samples/sec: 64.17 - lr: 0.100000
2021-05-24 13:37:26,585 epoch 3 - iter 26/28 - loss 3.04072449 - samples/sec: 66.86 - lr: 0.100000
2021-05-24 13:37:26,986 epoch 3 - iter 28/28 - loss 3.03009140 - samples/sec: 79.72 - lr: 0.100000
2021-05-24 13:37:26,987 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:26,987 EPOCH 3 done: loss 3.0301 - lr 0.1000000
2021-05-24 13:37:27,558 DEV : loss 3.728315830230713 - score 0.0
2021-05-24 13:37:27,568 BAD EPOCHS (no improvement): 2
2021-05-24 13:37:27,568 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:28,068 epoch 4 - iter 2/28 - loss 2.44928503 - samples/sec: 64.03 - lr: 0.100000
2021-05-24 13:37:28,545 epoch 4 - iter 4/28 - loss 2.46083993 - samples/sec: 67.26 - lr: 0.100000
2021-05-24 13:37:29,052 epoch 4 - iter 6/28 - loss 2.47995778 - samples/sec: 63.09 - lr: 0.100000
2021-05-24 13:37:29,652 epoch 4 - iter 8/28 - loss 2.23821725 - samples/sec: 53.36 - lr: 0.100000
2021-05-24 13:37:30,171 epoch 4 - iter 10/28 - loss 2.68000914 - samples/sec: 61.70 - lr: 0.100000
2021-05-24 13:37:30,669 epoch 4 - iter 12/28 - loss 2.68597640 - samples/sec: 64.39 - lr: 0.100000
2021-05-24 13:37:31,167 epoch 4 - iter 14/28 - loss 2.83176740 - samples/sec: 64.24 - lr: 0.100000
2021-05-24 13:37:31,660 epoch 4 - iter 16/28 - loss 2.78336317 - samples/sec: 65.01 - lr: 0.100000
2021-05-24 13:37:32,161 epoch 4 - iter 18/28 - loss 2.95838322 - samples/sec: 63.91 - lr: 0.100000
2021-05-24 13:37:32,665 epoch 4 - iter 20/28 - loss 2.94535353 - samples/sec: 63.54 - lr: 0.100000
2021-05-24 13:37:33,168 epoch 4 - iter 22/28 - loss 2.93456436 - samples/sec: 63.56 - lr: 0.100000
2021-05-24 13:37:33,669 epoch 4 - iter 24/28 - loss 2.92065978 - samples/sec: 63.91 - lr: 0.100000
2021-05-24 13:37:34,169 epoch 4 - iter 26/28 - loss 2.90741816 - samples/sec: 64.04 - lr: 0.100000
2021-05-24 13:37:34,577 epoch 4 - iter 28/28 - loss 2.79996995 - samples/sec: 78.58 - lr: 0.100000
2021-05-24 13:37:34,577 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:34,577 EPOCH 4 done: loss 2.8000 - lr 0.1000000
2021-05-24 13:37:35,149 DEV : loss 1.9310104846954346 - score 0.0
2021-05-24 13:37:35,159 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:37:44,585 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:45,081 epoch 5 - iter 2/28 - loss 1.99270916 - samples/sec: 64.63 - lr: 0.100000
2021-05-24 13:37:45,575 epoch 5 - iter 4/28 - loss 2.44250834 - samples/sec: 64.81 - lr: 0.100000
2021-05-24 13:37:46,076 epoch 5 - iter 6/28 - loss 2.44641685 - samples/sec: 63.94 - lr: 0.100000
2021-05-24 13:37:46,575 epoch 5 - iter 8/28 - loss 2.47238690 - samples/sec: 64.16 - lr: 0.100000
2021-05-24 13:37:47,066 epoch 5 - iter 10/28 - loss 2.37903328 - samples/sec: 65.15 - lr: 0.100000
2021-05-24 13:37:47,553 epoch 5 - iter 12/28 - loss 2.42097652 - samples/sec: 65.72 - lr: 0.100000
2021-05-24 13:37:48,048 epoch 5 - iter 14/28 - loss 2.36291790 - samples/sec: 64.68 - lr: 0.100000
2021-05-24 13:37:48,547 epoch 5 - iter 16/28 - loss 2.41921370 - samples/sec: 64.16 - lr: 0.100000
2021-05-24 13:37:49,052 epoch 5 - iter 18/28 - loss 2.41318440 - samples/sec: 63.43 - lr: 0.100000
2021-05-24 13:37:49,554 epoch 5 - iter 20/28 - loss 2.37185748 - samples/sec: 63.82 - lr: 0.100000
2021-05-24 13:37:50,053 epoch 5 - iter 22/28 - loss 2.40807211 - samples/sec: 64.22 - lr: 0.100000
2021-05-24 13:37:50,555 epoch 5 - iter 24/28 - loss 2.41946738 - samples/sec: 63.71 - lr: 0.100000
2021-05-24 13:37:51,057 epoch 5 - iter 26/28 - loss 2.46195290 - samples/sec: 63.77 - lr: 0.100000
2021-05-24 13:37:51,470 epoch 5 - iter 28/28 - loss 2.44615895 - samples/sec: 77.60 - lr: 0.100000
2021-05-24 13:37:51,470 ----------------------------------------------------------------------------------------------------
2021-05-24 13:37:51,470 EPOCH 5 done: loss 2.4462 - lr 0.1000000
2021-05-24 13:37:52,041 DEV : loss 1.3787262439727783 - score 0.6567
2021-05-24 13:37:52,051 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:38:01,527 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:02,024 epoch 6 - iter 2/28 - loss 3.36239719 - samples/sec: 64.57 - lr: 0.100000
2021-05-24 13:38:02,509 epoch 6 - iter 4/28 - loss 2.84518450 - samples/sec: 65.95 - lr: 0.100000
2021-05-24 13:38:03,011 epoch 6 - iter 6/28 - loss 2.44631271 - samples/sec: 63.82 - lr: 0.100000
2021-05-24 13:38:03,505 epoch 6 - iter 8/28 - loss 2.54944989 - samples/sec: 64.80 - lr: 0.100000
2021-05-24 13:38:04,012 epoch 6 - iter 10/28 - loss 2.39044943 - samples/sec: 63.24 - lr: 0.100000
2021-05-24 13:38:04,491 epoch 6 - iter 12/28 - loss 2.23111563 - samples/sec: 66.87 - lr: 0.100000
2021-05-24 13:38:04,996 epoch 6 - iter 14/28 - loss 2.09011968 - samples/sec: 63.40 - lr: 0.100000
2021-05-24 13:38:05,502 epoch 6 - iter 16/28 - loss 2.02846765 - samples/sec: 63.31 - lr: 0.100000
2021-05-24 13:38:05,993 epoch 6 - iter 18/28 - loss 2.03421460 - samples/sec: 65.16 - lr: 0.100000
2021-05-24 13:38:06,481 epoch 6 - iter 20/28 - loss 2.02660171 - samples/sec: 65.63 - lr: 0.100000
2021-05-24 13:38:06,982 epoch 6 - iter 22/28 - loss 2.09987331 - samples/sec: 63.88 - lr: 0.100000
2021-05-24 13:38:07,472 epoch 6 - iter 24/28 - loss 2.07119216 - samples/sec: 65.33 - lr: 0.100000
2021-05-24 13:38:07,973 epoch 6 - iter 26/28 - loss 2.01929660 - samples/sec: 63.99 - lr: 0.100000
2021-05-24 13:38:08,361 epoch 6 - iter 28/28 - loss 1.98710118 - samples/sec: 82.41 - lr: 0.100000
2021-05-24 13:38:08,362 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:08,362 EPOCH 6 done: loss 1.9871 - lr 0.1000000
2021-05-24 13:38:08,933 DEV : loss 1.3275871276855469 - score 0.6406
2021-05-24 13:38:08,943 BAD EPOCHS (no improvement): 1
2021-05-24 13:38:08,943 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:09,447 epoch 7 - iter 2/28 - loss 1.38224244 - samples/sec: 63.55 - lr: 0.100000
2021-05-24 13:38:09,939 epoch 7 - iter 4/28 - loss 1.49646521 - samples/sec: 65.14 - lr: 0.100000
2021-05-24 13:38:10,434 epoch 7 - iter 6/28 - loss 1.90841838 - samples/sec: 64.64 - lr: 0.100000
2021-05-24 13:38:10,928 epoch 7 - iter 8/28 - loss 1.87012699 - samples/sec: 64.77 - lr: 0.100000
2021-05-24 13:38:11,419 epoch 7 - iter 10/28 - loss 1.74999127 - samples/sec: 65.23 - lr: 0.100000
2021-05-24 13:38:11,898 epoch 7 - iter 12/28 - loss 1.63239515 - samples/sec: 66.84 - lr: 0.100000
2021-05-24 13:38:12,398 epoch 7 - iter 14/28 - loss 1.72968735 - samples/sec: 64.04 - lr: 0.100000
2021-05-24 13:38:12,889 epoch 7 - iter 16/28 - loss 1.73164182 - samples/sec: 65.32 - lr: 0.100000
2021-05-24 13:38:13,387 epoch 7 - iter 18/28 - loss 1.85795291 - samples/sec: 64.21 - lr: 0.100000
2021-05-24 13:38:13,883 epoch 7 - iter 20/28 - loss 1.79094170 - samples/sec: 64.57 - lr: 0.100000
2021-05-24 13:38:14,377 epoch 7 - iter 22/28 - loss 1.79842853 - samples/sec: 64.80 - lr: 0.100000
2021-05-24 13:38:14,876 epoch 7 - iter 24/28 - loss 1.78663791 - samples/sec: 64.21 - lr: 0.100000
2021-05-24 13:38:15,380 epoch 7 - iter 26/28 - loss 1.82772943 - samples/sec: 63.58 - lr: 0.100000
2021-05-24 13:38:15,795 epoch 7 - iter 28/28 - loss 1.89848872 - samples/sec: 77.10 - lr: 0.100000
2021-05-24 13:38:15,795 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:15,795 EPOCH 7 done: loss 1.8985 - lr 0.1000000
2021-05-24 13:38:16,367 DEV : loss 1.237295150756836 - score 0.7571
2021-05-24 13:38:16,378 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:38:26,286 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:26,780 epoch 8 - iter 2/28 - loss 2.19023782 - samples/sec: 64.90 - lr: 0.100000
2021-05-24 13:38:27,293 epoch 8 - iter 4/28 - loss 1.78442207 - samples/sec: 62.48 - lr: 0.100000
2021-05-24 13:38:27,798 epoch 8 - iter 6/28 - loss 2.42134053 - samples/sec: 63.45 - lr: 0.100000
2021-05-24 13:38:28,279 epoch 8 - iter 8/28 - loss 2.41376694 - samples/sec: 66.48 - lr: 0.100000
2021-05-24 13:38:28,778 epoch 8 - iter 10/28 - loss 2.27021707 - samples/sec: 64.21 - lr: 0.100000
2021-05-24 13:38:29,270 epoch 8 - iter 12/28 - loss 2.18441977 - samples/sec: 65.16 - lr: 0.100000
2021-05-24 13:38:29,776 epoch 8 - iter 14/28 - loss 2.08494282 - samples/sec: 63.29 - lr: 0.100000
2021-05-24 13:38:30,264 epoch 8 - iter 16/28 - loss 1.99374656 - samples/sec: 65.63 - lr: 0.100000
2021-05-24 13:38:30,760 epoch 8 - iter 18/28 - loss 1.99893900 - samples/sec: 64.57 - lr: 0.100000
2021-05-24 13:38:31,251 epoch 8 - iter 20/28 - loss 1.96489502 - samples/sec: 65.16 - lr: 0.100000
2021-05-24 13:38:31,747 epoch 8 - iter 22/28 - loss 1.94365648 - samples/sec: 64.54 - lr: 0.100000
2021-05-24 13:38:32,240 epoch 8 - iter 24/28 - loss 1.93981472 - samples/sec: 64.97 - lr: 0.100000
2021-05-24 13:38:32,727 epoch 8 - iter 26/28 - loss 1.92123761 - samples/sec: 65.83 - lr: 0.100000
2021-05-24 13:38:33,145 epoch 8 - iter 28/28 - loss 2.00656086 - samples/sec: 76.57 - lr: 0.100000
2021-05-24 13:38:33,145 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:33,145 EPOCH 8 done: loss 2.0066 - lr 0.1000000
2021-05-24 13:38:33,714 DEV : loss 2.123497486114502 - score 0.2424
2021-05-24 13:38:33,724 BAD EPOCHS (no improvement): 1
2021-05-24 13:38:33,725 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:34,210 epoch 9 - iter 2/28 - loss 1.82380325 - samples/sec: 65.96 - lr: 0.100000
2021-05-24 13:38:34,710 epoch 9 - iter 4/28 - loss 1.87620375 - samples/sec: 64.10 - lr: 0.100000
2021-05-24 13:38:35,205 epoch 9 - iter 6/28 - loss 2.04283847 - samples/sec: 64.70 - lr: 0.100000
2021-05-24 13:38:35,692 epoch 9 - iter 8/28 - loss 1.85034309 - samples/sec: 65.65 - lr: 0.100000
2021-05-24 13:38:36,195 epoch 9 - iter 10/28 - loss 1.94628638 - samples/sec: 63.76 - lr: 0.100000
2021-05-24 13:38:36,687 epoch 9 - iter 12/28 - loss 1.86413464 - samples/sec: 65.04 - lr: 0.100000
2021-05-24 13:38:37,195 epoch 9 - iter 14/28 - loss 1.92512993 - samples/sec: 63.00 - lr: 0.100000
2021-05-24 13:38:37,703 epoch 9 - iter 16/28 - loss 1.88387627 - samples/sec: 63.08 - lr: 0.100000
2021-05-24 13:38:38,204 epoch 9 - iter 18/28 - loss 1.97820570 - samples/sec: 63.83 - lr: 0.100000
2021-05-24 13:38:38,679 epoch 9 - iter 20/28 - loss 1.86729659 - samples/sec: 67.45 - lr: 0.100000
2021-05-24 13:38:39,182 epoch 9 - iter 22/28 - loss 1.89920618 - samples/sec: 63.70 - lr: 0.100000
2021-05-24 13:38:39,688 epoch 9 - iter 24/28 - loss 1.97297264 - samples/sec: 63.20 - lr: 0.100000
2021-05-24 13:38:40,191 epoch 9 - iter 26/28 - loss 1.90755077 - samples/sec: 63.77 - lr: 0.100000
2021-05-24 13:38:40,591 epoch 9 - iter 28/28 - loss 1.88315039 - samples/sec: 79.97 - lr: 0.100000
2021-05-24 13:38:40,591 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:40,591 EPOCH 9 done: loss 1.8832 - lr 0.1000000
2021-05-24 13:38:41,163 DEV : loss 1.1576592922210693 - score 0.8075
2021-05-24 13:38:41,173 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:38:50,812 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:51,310 epoch 10 - iter 2/28 - loss 1.83446801 - samples/sec: 64.27 - lr: 0.100000
2021-05-24 13:38:51,797 epoch 10 - iter 4/28 - loss 1.58502132 - samples/sec: 65.78 - lr: 0.100000
2021-05-24 13:38:52,305 epoch 10 - iter 6/28 - loss 1.51452800 - samples/sec: 63.01 - lr: 0.100000
2021-05-24 13:38:52,777 epoch 10 - iter 8/28 - loss 1.44769458 - samples/sec: 67.86 - lr: 0.100000
2021-05-24 13:38:53,285 epoch 10 - iter 10/28 - loss 1.74938436 - samples/sec: 63.04 - lr: 0.100000
2021-05-24 13:38:53,800 epoch 10 - iter 12/28 - loss 1.72824463 - samples/sec: 62.19 - lr: 0.100000
2021-05-24 13:38:54,289 epoch 10 - iter 14/28 - loss 1.62047849 - samples/sec: 65.43 - lr: 0.100000
2021-05-24 13:38:54,796 epoch 10 - iter 16/28 - loss 1.73934707 - samples/sec: 63.25 - lr: 0.100000
2021-05-24 13:38:55,314 epoch 10 - iter 18/28 - loss 1.79383958 - samples/sec: 61.80 - lr: 0.100000
2021-05-24 13:38:55,815 epoch 10 - iter 20/28 - loss 1.77797227 - samples/sec: 63.92 - lr: 0.100000
2021-05-24 13:38:56,320 epoch 10 - iter 22/28 - loss 1.79035258 - samples/sec: 63.31 - lr: 0.100000
2021-05-24 13:38:56,823 epoch 10 - iter 24/28 - loss 1.92870609 - samples/sec: 63.70 - lr: 0.100000
2021-05-24 13:38:57,330 epoch 10 - iter 26/28 - loss 1.86658745 - samples/sec: 63.16 - lr: 0.100000
2021-05-24 13:38:57,727 epoch 10 - iter 28/28 - loss 1.84372829 - samples/sec: 80.70 - lr: 0.100000
2021-05-24 13:38:57,727 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:57,727 EPOCH 10 done: loss 1.8437 - lr 0.1000000
2021-05-24 13:38:58,300 DEV : loss 1.592225193977356 - score 0.6818
2021-05-24 13:38:58,310 BAD EPOCHS (no improvement): 1
2021-05-24 13:38:58,310 ----------------------------------------------------------------------------------------------------
2021-05-24 13:38:58,820 epoch 11 - iter 2/28 - loss 3.09444976 - samples/sec: 62.85 - lr: 0.100000
2021-05-24 13:38:59,320 epoch 11 - iter 4/28 - loss 2.79294172 - samples/sec: 63.99 - lr: 0.100000
2021-05-24 13:38:59,930 epoch 11 - iter 6/28 - loss 2.78579360 - samples/sec: 52.50 - lr: 0.100000
2021-05-24 13:39:00,432 epoch 11 - iter 8/28 - loss 2.41894151 - samples/sec: 63.75 - lr: 0.100000
2021-05-24 13:39:00,935 epoch 11 - iter 10/28 - loss 2.34247218 - samples/sec: 63.69 - lr: 0.100000
2021-05-24 13:39:01,437 epoch 11 - iter 12/28 - loss 2.10046545 - samples/sec: 63.82 - lr: 0.100000
2021-05-24 13:39:01,941 epoch 11 - iter 14/28 - loss 1.92077874 - samples/sec: 63.53 - lr: 0.100000
2021-05-24 13:39:02,428 epoch 11 - iter 16/28 - loss 1.84084562 - samples/sec: 65.69 - lr: 0.100000
2021-05-24 13:39:02,938 epoch 11 - iter 18/28 - loss 1.72722082 - samples/sec: 62.86 - lr: 0.100000
2021-05-24 13:39:03,442 epoch 11 - iter 20/28 - loss 1.65311719 - samples/sec: 63.48 - lr: 0.100000
2021-05-24 13:39:03,959 epoch 11 - iter 22/28 - loss 1.66862070 - samples/sec: 62.02 - lr: 0.100000
2021-05-24 13:39:04,461 epoch 11 - iter 24/28 - loss 1.69662618 - samples/sec: 63.74 - lr: 0.100000
2021-05-24 13:39:04,964 epoch 11 - iter 26/28 - loss 1.71411196 - samples/sec: 63.68 - lr: 0.100000
2021-05-24 13:39:05,370 epoch 11 - iter 28/28 - loss 1.70597491 - samples/sec: 78.93 - lr: 0.100000
2021-05-24 13:39:05,370 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:05,370 EPOCH 11 done: loss 1.7060 - lr 0.1000000
2021-05-24 13:39:05,942 DEV : loss 0.984950065612793 - score 0.7724
2021-05-24 13:39:05,952 BAD EPOCHS (no improvement): 2
2021-05-24 13:39:05,952 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:06,444 epoch 12 - iter 2/28 - loss 0.93741122 - samples/sec: 65.16 - lr: 0.100000
2021-05-24 13:39:06,948 epoch 12 - iter 4/28 - loss 1.74275382 - samples/sec: 63.58 - lr: 0.100000
2021-05-24 13:39:07,448 epoch 12 - iter 6/28 - loss 1.68373693 - samples/sec: 63.98 - lr: 0.100000
2021-05-24 13:39:07,944 epoch 12 - iter 8/28 - loss 1.63513053 - samples/sec: 64.54 - lr: 0.100000
2021-05-24 13:39:08,426 epoch 12 - iter 10/28 - loss 1.75388095 - samples/sec: 66.50 - lr: 0.100000
2021-05-24 13:39:08,910 epoch 12 - iter 12/28 - loss 1.65911630 - samples/sec: 66.07 - lr: 0.100000
2021-05-24 13:39:09,405 epoch 12 - iter 14/28 - loss 1.66747519 - samples/sec: 64.69 - lr: 0.100000
2021-05-24 13:39:09,904 epoch 12 - iter 16/28 - loss 1.58669889 - samples/sec: 64.27 - lr: 0.100000
2021-05-24 13:39:10,409 epoch 12 - iter 18/28 - loss 1.60923922 - samples/sec: 63.31 - lr: 0.100000
2021-05-24 13:39:10,900 epoch 12 - iter 20/28 - loss 1.59000997 - samples/sec: 65.23 - lr: 0.100000
2021-05-24 13:39:11,408 epoch 12 - iter 22/28 - loss 1.63323781 - samples/sec: 63.03 - lr: 0.100000
2021-05-24 13:39:11,918 epoch 12 - iter 24/28 - loss 1.63622353 - samples/sec: 62.83 - lr: 0.100000
2021-05-24 13:39:12,421 epoch 12 - iter 26/28 - loss 1.63513998 - samples/sec: 63.64 - lr: 0.100000
2021-05-24 13:39:12,824 epoch 12 - iter 28/28 - loss 1.55766580 - samples/sec: 79.49 - lr: 0.100000
2021-05-24 13:39:12,824 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:12,824 EPOCH 12 done: loss 1.5577 - lr 0.1000000
2021-05-24 13:39:13,398 DEV : loss 0.919570803642273 - score 0.8079
2021-05-24 13:39:13,409 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:39:23,297 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:23,804 epoch 13 - iter 2/28 - loss 1.16929770 - samples/sec: 63.15 - lr: 0.100000
2021-05-24 13:39:24,286 epoch 13 - iter 4/28 - loss 1.05719864 - samples/sec: 66.47 - lr: 0.100000
2021-05-24 13:39:24,781 epoch 13 - iter 6/28 - loss 1.19378523 - samples/sec: 64.75 - lr: 0.100000
2021-05-24 13:39:25,275 epoch 13 - iter 8/28 - loss 1.28101298 - samples/sec: 64.83 - lr: 0.100000
2021-05-24 13:39:25,776 epoch 13 - iter 10/28 - loss 1.24425275 - samples/sec: 63.85 - lr: 0.100000
2021-05-24 13:39:26,273 epoch 13 - iter 12/28 - loss 1.30204346 - samples/sec: 64.44 - lr: 0.100000
2021-05-24 13:39:26,749 epoch 13 - iter 14/28 - loss 1.27193617 - samples/sec: 67.31 - lr: 0.100000
2021-05-24 13:39:27,233 epoch 13 - iter 16/28 - loss 1.29282986 - samples/sec: 66.06 - lr: 0.100000
2021-05-24 13:39:27,713 epoch 13 - iter 18/28 - loss 1.28915628 - samples/sec: 66.76 - lr: 0.100000
2021-05-24 13:39:28,207 epoch 13 - iter 20/28 - loss 1.25014885 - samples/sec: 64.86 - lr: 0.100000
2021-05-24 13:39:28,706 epoch 13 - iter 22/28 - loss 1.32367481 - samples/sec: 64.21 - lr: 0.100000
2021-05-24 13:39:29,203 epoch 13 - iter 24/28 - loss 1.30227878 - samples/sec: 64.40 - lr: 0.100000
2021-05-24 13:39:29,697 epoch 13 - iter 26/28 - loss 1.31085835 - samples/sec: 64.82 - lr: 0.100000
2021-05-24 13:39:30,112 epoch 13 - iter 28/28 - loss 1.45828173 - samples/sec: 77.17 - lr: 0.100000
2021-05-24 13:39:30,112 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:30,112 EPOCH 13 done: loss 1.4583 - lr 0.1000000
2021-05-24 13:39:30,679 DEV : loss 1.3252438306808472 - score 0.6917
2021-05-24 13:39:30,689 BAD EPOCHS (no improvement): 1
2021-05-24 13:39:30,689 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:31,173 epoch 14 - iter 2/28 - loss 2.23917258 - samples/sec: 66.18 - lr: 0.100000
2021-05-24 13:39:31,676 epoch 14 - iter 4/28 - loss 1.57771832 - samples/sec: 63.72 - lr: 0.100000
2021-05-24 13:39:32,172 epoch 14 - iter 6/28 - loss 1.99741745 - samples/sec: 64.53 - lr: 0.100000
2021-05-24 13:39:32,667 epoch 14 - iter 8/28 - loss 1.76208043 - samples/sec: 64.70 - lr: 0.100000
2021-05-24 13:39:33,159 epoch 14 - iter 10/28 - loss 1.68331792 - samples/sec: 65.07 - lr: 0.100000
2021-05-24 13:39:33,650 epoch 14 - iter 12/28 - loss 1.58210638 - samples/sec: 65.20 - lr: 0.100000
2021-05-24 13:39:34,132 epoch 14 - iter 14/28 - loss 1.55552075 - samples/sec: 66.47 - lr: 0.100000
2021-05-24 13:39:34,637 epoch 14 - iter 16/28 - loss 1.49405304 - samples/sec: 63.44 - lr: 0.100000
2021-05-24 13:39:35,113 epoch 14 - iter 18/28 - loss 1.48557929 - samples/sec: 67.25 - lr: 0.100000
2021-05-24 13:39:35,614 epoch 14 - iter 20/28 - loss 1.44096086 - samples/sec: 63.91 - lr: 0.100000
2021-05-24 13:39:36,123 epoch 14 - iter 22/28 - loss 1.45460003 - samples/sec: 62.87 - lr: 0.100000
2021-05-24 13:39:36,609 epoch 14 - iter 24/28 - loss 1.40514138 - samples/sec: 65.82 - lr: 0.100000
2021-05-24 13:39:37,111 epoch 14 - iter 26/28 - loss 1.41786147 - samples/sec: 63.85 - lr: 0.100000
2021-05-24 13:39:37,520 epoch 14 - iter 28/28 - loss 1.38526624 - samples/sec: 78.28 - lr: 0.100000
2021-05-24 13:39:37,520 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:37,520 EPOCH 14 done: loss 1.3853 - lr 0.1000000
2021-05-24 13:39:38,086 DEV : loss 1.2260822057724 - score 0.7571
2021-05-24 13:39:38,096 BAD EPOCHS (no improvement): 2
2021-05-24 13:39:38,096 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:38,602 epoch 15 - iter 2/28 - loss 0.56009614 - samples/sec: 63.31 - lr: 0.100000
2021-05-24 13:39:39,102 epoch 15 - iter 4/28 - loss 1.18377802 - samples/sec: 64.06 - lr: 0.100000
2021-05-24 13:39:39,617 epoch 15 - iter 6/28 - loss 1.19139928 - samples/sec: 62.19 - lr: 0.100000
2021-05-24 13:39:40,123 epoch 15 - iter 8/28 - loss 1.11808766 - samples/sec: 63.23 - lr: 0.100000
2021-05-24 13:39:40,624 epoch 15 - iter 10/28 - loss 1.34780368 - samples/sec: 64.00 - lr: 0.100000
2021-05-24 13:39:41,111 epoch 15 - iter 12/28 - loss 1.26061977 - samples/sec: 65.68 - lr: 0.100000
2021-05-24 13:39:41,596 epoch 15 - iter 14/28 - loss 1.21174392 - samples/sec: 65.98 - lr: 0.100000
2021-05-24 13:39:42,103 epoch 15 - iter 16/28 - loss 1.21541764 - samples/sec: 63.19 - lr: 0.100000
2021-05-24 13:39:42,599 epoch 15 - iter 18/28 - loss 1.30541903 - samples/sec: 64.54 - lr: 0.100000
2021-05-24 13:39:43,089 epoch 15 - iter 20/28 - loss 1.22966801 - samples/sec: 65.47 - lr: 0.100000
2021-05-24 13:39:43,592 epoch 15 - iter 22/28 - loss 1.16278903 - samples/sec: 63.63 - lr: 0.100000
2021-05-24 13:39:44,063 epoch 15 - iter 24/28 - loss 1.13370065 - samples/sec: 68.01 - lr: 0.100000
2021-05-24 13:39:44,559 epoch 15 - iter 26/28 - loss 1.21141328 - samples/sec: 64.57 - lr: 0.100000
2021-05-24 13:39:44,971 epoch 15 - iter 28/28 - loss 1.18482436 - samples/sec: 77.70 - lr: 0.100000
2021-05-24 13:39:44,971 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:44,971 EPOCH 15 done: loss 1.1848 - lr 0.1000000
2021-05-24 13:39:45,543 DEV : loss 1.4779047966003418 - score 0.6512
2021-05-24 13:39:45,553 BAD EPOCHS (no improvement): 3
2021-05-24 13:39:45,553 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:46,043 epoch 16 - iter 2/28 - loss 1.47179127 - samples/sec: 65.36 - lr: 0.100000
2021-05-24 13:39:46,528 epoch 16 - iter 4/28 - loss 1.70640761 - samples/sec: 66.10 - lr: 0.100000
2021-05-24 13:39:47,027 epoch 16 - iter 6/28 - loss 1.50932844 - samples/sec: 64.17 - lr: 0.100000
2021-05-24 13:39:47,518 epoch 16 - iter 8/28 - loss 1.42568877 - samples/sec: 65.19 - lr: 0.100000
2021-05-24 13:39:48,020 epoch 16 - iter 10/28 - loss 1.53276336 - samples/sec: 63.79 - lr: 0.100000
2021-05-24 13:39:48,531 epoch 16 - iter 12/28 - loss 1.46914830 - samples/sec: 62.65 - lr: 0.100000
2021-05-24 13:39:49,027 epoch 16 - iter 14/28 - loss 1.47611716 - samples/sec: 64.55 - lr: 0.100000
2021-05-24 13:39:49,532 epoch 16 - iter 16/28 - loss 1.44408336 - samples/sec: 63.42 - lr: 0.100000
2021-05-24 13:39:50,037 epoch 16 - iter 18/28 - loss 1.39561646 - samples/sec: 63.44 - lr: 0.100000
2021-05-24 13:39:50,525 epoch 16 - iter 20/28 - loss 1.33074427 - samples/sec: 65.62 - lr: 0.100000
2021-05-24 13:39:51,023 epoch 16 - iter 22/28 - loss 1.28520114 - samples/sec: 64.27 - lr: 0.100000
2021-05-24 13:39:51,528 epoch 16 - iter 24/28 - loss 1.26260294 - samples/sec: 63.49 - lr: 0.100000
2021-05-24 13:39:52,031 epoch 16 - iter 26/28 - loss 1.28163015 - samples/sec: 63.63 - lr: 0.100000
2021-05-24 13:39:52,436 epoch 16 - iter 28/28 - loss 1.25739333 - samples/sec: 78.99 - lr: 0.100000
2021-05-24 13:39:52,436 ----------------------------------------------------------------------------------------------------
2021-05-24 13:39:52,437 EPOCH 16 done: loss 1.2574 - lr 0.1000000
2021-05-24 13:39:53,008 DEV : loss 1.1679037809371948 - score 0.8082
2021-05-24 13:39:53,018 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:40:02,389 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:02,998 epoch 17 - iter 2/28 - loss 1.64546919 - samples/sec: 52.61 - lr: 0.100000
2021-05-24 13:40:03,492 epoch 17 - iter 4/28 - loss 1.64117244 - samples/sec: 64.83 - lr: 0.100000
2021-05-24 13:40:03,989 epoch 17 - iter 6/28 - loss 1.53929156 - samples/sec: 64.40 - lr: 0.100000
2021-05-24 13:40:04,474 epoch 17 - iter 8/28 - loss 1.50747789 - samples/sec: 65.97 - lr: 0.100000
2021-05-24 13:40:04,972 epoch 17 - iter 10/28 - loss 1.43478538 - samples/sec: 64.41 - lr: 0.100000
2021-05-24 13:40:05,450 epoch 17 - iter 12/28 - loss 1.34171347 - samples/sec: 67.00 - lr: 0.100000
2021-05-24 13:40:05,939 epoch 17 - iter 14/28 - loss 1.26317593 - samples/sec: 65.36 - lr: 0.100000
2021-05-24 13:40:06,439 epoch 17 - iter 16/28 - loss 1.26997572 - samples/sec: 64.07 - lr: 0.100000
2021-05-24 13:40:06,921 epoch 17 - iter 18/28 - loss 1.19595653 - samples/sec: 66.55 - lr: 0.100000
2021-05-24 13:40:07,421 epoch 17 - iter 20/28 - loss 1.15700094 - samples/sec: 63.98 - lr: 0.100000
2021-05-24 13:40:07,924 epoch 17 - iter 22/28 - loss 1.14251411 - samples/sec: 63.72 - lr: 0.100000
2021-05-24 13:40:08,410 epoch 17 - iter 24/28 - loss 1.19942414 - samples/sec: 65.85 - lr: 0.100000
2021-05-24 13:40:08,896 epoch 17 - iter 26/28 - loss 1.16794447 - samples/sec: 65.84 - lr: 0.100000
2021-05-24 13:40:09,308 epoch 17 - iter 28/28 - loss 1.12349309 - samples/sec: 77.83 - lr: 0.100000
2021-05-24 13:40:09,308 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:09,308 EPOCH 17 done: loss 1.1235 - lr 0.1000000
2021-05-24 13:40:09,874 DEV : loss 1.0457667112350464 - score 0.7513
2021-05-24 13:40:09,884 BAD EPOCHS (no improvement): 1
2021-05-24 13:40:09,884 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:10,373 epoch 18 - iter 2/28 - loss 1.13823807 - samples/sec: 65.47 - lr: 0.100000
2021-05-24 13:40:10,872 epoch 18 - iter 4/28 - loss 0.98400491 - samples/sec: 64.20 - lr: 0.100000
2021-05-24 13:40:11,336 epoch 18 - iter 6/28 - loss 0.92474373 - samples/sec: 69.01 - lr: 0.100000
2021-05-24 13:40:11,828 epoch 18 - iter 8/28 - loss 0.94942707 - samples/sec: 65.03 - lr: 0.100000
2021-05-24 13:40:12,325 epoch 18 - iter 10/28 - loss 1.05767353 - samples/sec: 64.47 - lr: 0.100000
2021-05-24 13:40:12,810 epoch 18 - iter 12/28 - loss 0.99306850 - samples/sec: 65.97 - lr: 0.100000
2021-05-24 13:40:13,295 epoch 18 - iter 14/28 - loss 1.00087161 - samples/sec: 66.08 - lr: 0.100000
2021-05-24 13:40:13,788 epoch 18 - iter 16/28 - loss 1.05043684 - samples/sec: 64.89 - lr: 0.100000
2021-05-24 13:40:14,267 epoch 18 - iter 18/28 - loss 1.03900536 - samples/sec: 66.95 - lr: 0.100000
2021-05-24 13:40:14,755 epoch 18 - iter 20/28 - loss 1.05913703 - samples/sec: 65.57 - lr: 0.100000
2021-05-24 13:40:15,248 epoch 18 - iter 22/28 - loss 1.09937195 - samples/sec: 64.96 - lr: 0.100000
2021-05-24 13:40:15,746 epoch 18 - iter 24/28 - loss 1.13684307 - samples/sec: 64.23 - lr: 0.100000
2021-05-24 13:40:16,244 epoch 18 - iter 26/28 - loss 1.09649928 - samples/sec: 64.34 - lr: 0.100000
2021-05-24 13:40:16,643 epoch 18 - iter 28/28 - loss 1.09893276 - samples/sec: 80.33 - lr: 0.100000
2021-05-24 13:40:16,643 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:16,643 EPOCH 18 done: loss 1.0989 - lr 0.1000000
2021-05-24 13:40:17,208 DEV : loss 0.9513649940490723 - score 0.8133
2021-05-24 13:40:17,218 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:40:26,704 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:27,213 epoch 19 - iter 2/28 - loss 0.86954594 - samples/sec: 62.98 - lr: 0.100000
2021-05-24 13:40:27,717 epoch 19 - iter 4/28 - loss 0.95759523 - samples/sec: 63.49 - lr: 0.100000
2021-05-24 13:40:28,217 epoch 19 - iter 6/28 - loss 1.19324370 - samples/sec: 64.04 - lr: 0.100000
2021-05-24 13:40:28,708 epoch 19 - iter 8/28 - loss 1.28361583 - samples/sec: 65.28 - lr: 0.100000
2021-05-24 13:40:29,216 epoch 19 - iter 10/28 - loss 1.34803233 - samples/sec: 63.04 - lr: 0.100000
2021-05-24 13:40:29,717 epoch 19 - iter 12/28 - loss 1.31200876 - samples/sec: 63.92 - lr: 0.100000
2021-05-24 13:40:30,220 epoch 19 - iter 14/28 - loss 1.24071436 - samples/sec: 63.65 - lr: 0.100000
2021-05-24 13:40:30,703 epoch 19 - iter 16/28 - loss 1.17221741 - samples/sec: 66.23 - lr: 0.100000
2021-05-24 13:40:31,192 epoch 19 - iter 18/28 - loss 1.18751559 - samples/sec: 65.58 - lr: 0.100000
2021-05-24 13:40:31,673 epoch 19 - iter 20/28 - loss 1.19439745 - samples/sec: 66.49 - lr: 0.100000
2021-05-24 13:40:32,165 epoch 19 - iter 22/28 - loss 1.22818733 - samples/sec: 65.08 - lr: 0.100000
2021-05-24 13:40:32,663 epoch 19 - iter 24/28 - loss 1.19644557 - samples/sec: 64.37 - lr: 0.100000
2021-05-24 13:40:33,169 epoch 19 - iter 26/28 - loss 1.26274958 - samples/sec: 63.19 - lr: 0.100000
2021-05-24 13:40:33,577 epoch 19 - iter 28/28 - loss 1.23514356 - samples/sec: 78.49 - lr: 0.100000
2021-05-24 13:40:33,578 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:33,578 EPOCH 19 done: loss 1.2351 - lr 0.1000000
2021-05-24 13:40:34,150 DEV : loss 1.1581981182098389 - score 0.8
2021-05-24 13:40:34,160 BAD EPOCHS (no improvement): 1
2021-05-24 13:40:34,160 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:34,652 epoch 20 - iter 2/28 - loss 0.69235742 - samples/sec: 65.16 - lr: 0.100000
2021-05-24 13:40:35,143 epoch 20 - iter 4/28 - loss 1.16319007 - samples/sec: 65.15 - lr: 0.100000
2021-05-24 13:40:35,630 epoch 20 - iter 6/28 - loss 1.07291687 - samples/sec: 65.76 - lr: 0.100000
2021-05-24 13:40:36,115 epoch 20 - iter 8/28 - loss 1.27818896 - samples/sec: 66.08 - lr: 0.100000
2021-05-24 13:40:36,612 epoch 20 - iter 10/28 - loss 1.09847575 - samples/sec: 64.46 - lr: 0.100000
2021-05-24 13:40:37,115 epoch 20 - iter 12/28 - loss 1.04031131 - samples/sec: 63.61 - lr: 0.100000
2021-05-24 13:40:37,604 epoch 20 - iter 14/28 - loss 1.07521059 - samples/sec: 65.48 - lr: 0.100000
2021-05-24 13:40:38,110 epoch 20 - iter 16/28 - loss 1.10446699 - samples/sec: 63.33 - lr: 0.100000
2021-05-24 13:40:38,618 epoch 20 - iter 18/28 - loss 1.14224149 - samples/sec: 62.94 - lr: 0.100000
2021-05-24 13:40:39,123 epoch 20 - iter 20/28 - loss 1.23903355 - samples/sec: 63.43 - lr: 0.100000
2021-05-24 13:40:39,625 epoch 20 - iter 22/28 - loss 1.25685841 - samples/sec: 63.84 - lr: 0.100000
2021-05-24 13:40:40,114 epoch 20 - iter 24/28 - loss 1.26125028 - samples/sec: 65.45 - lr: 0.100000
2021-05-24 13:40:40,628 epoch 20 - iter 26/28 - loss 1.24779498 - samples/sec: 62.27 - lr: 0.100000
2021-05-24 13:40:41,042 epoch 20 - iter 28/28 - loss 1.26531223 - samples/sec: 77.29 - lr: 0.100000
2021-05-24 13:40:41,043 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:41,043 EPOCH 20 done: loss 1.2653 - lr 0.1000000
2021-05-24 13:40:41,615 DEV : loss 1.369341254234314 - score 0.7832
2021-05-24 13:40:41,625 BAD EPOCHS (no improvement): 2
2021-05-24 13:40:41,625 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:42,105 epoch 21 - iter 2/28 - loss 1.68180138 - samples/sec: 66.66 - lr: 0.100000
2021-05-24 13:40:42,611 epoch 21 - iter 4/28 - loss 1.56905451 - samples/sec: 63.26 - lr: 0.100000
2021-05-24 13:40:43,104 epoch 21 - iter 6/28 - loss 1.35786664 - samples/sec: 64.99 - lr: 0.100000
2021-05-24 13:40:43,613 epoch 21 - iter 8/28 - loss 1.37122449 - samples/sec: 62.91 - lr: 0.100000
2021-05-24 13:40:44,105 epoch 21 - iter 10/28 - loss 1.23391294 - samples/sec: 65.06 - lr: 0.100000
2021-05-24 13:40:44,620 epoch 21 - iter 12/28 - loss 1.33642562 - samples/sec: 62.20 - lr: 0.100000
2021-05-24 13:40:45,120 epoch 21 - iter 14/28 - loss 1.27183759 - samples/sec: 64.06 - lr: 0.100000
2021-05-24 13:40:45,617 epoch 21 - iter 16/28 - loss 1.28820721 - samples/sec: 64.39 - lr: 0.100000
2021-05-24 13:40:46,123 epoch 21 - iter 18/28 - loss 1.20569836 - samples/sec: 63.30 - lr: 0.100000
2021-05-24 13:40:46,633 epoch 21 - iter 20/28 - loss 1.20563784 - samples/sec: 62.85 - lr: 0.100000
2021-05-24 13:40:47,139 epoch 21 - iter 22/28 - loss 1.22581267 - samples/sec: 63.25 - lr: 0.100000
2021-05-24 13:40:47,636 epoch 21 - iter 24/28 - loss 1.18665166 - samples/sec: 64.43 - lr: 0.100000
2021-05-24 13:40:48,120 epoch 21 - iter 26/28 - loss 1.20533708 - samples/sec: 66.10 - lr: 0.100000
2021-05-24 13:40:48,536 epoch 21 - iter 28/28 - loss 1.16923502 - samples/sec: 76.96 - lr: 0.100000
2021-05-24 13:40:48,537 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:48,537 EPOCH 21 done: loss 1.1692 - lr 0.1000000
2021-05-24 13:40:49,109 DEV : loss 0.9400323033332825 - score 0.8481
2021-05-24 13:40:49,120 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:40:58,543 ----------------------------------------------------------------------------------------------------
2021-05-24 13:40:59,035 epoch 22 - iter 2/28 - loss 0.90641314 - samples/sec: 65.21 - lr: 0.100000
2021-05-24 13:40:59,535 epoch 22 - iter 4/28 - loss 0.80823222 - samples/sec: 64.09 - lr: 0.100000
2021-05-24 13:41:00,032 epoch 22 - iter 6/28 - loss 0.82470264 - samples/sec: 64.42 - lr: 0.100000
2021-05-24 13:41:00,534 epoch 22 - iter 8/28 - loss 0.68258129 - samples/sec: 63.79 - lr: 0.100000
2021-05-24 13:41:01,013 epoch 22 - iter 10/28 - loss 0.70050082 - samples/sec: 66.86 - lr: 0.100000
2021-05-24 13:41:01,511 epoch 22 - iter 12/28 - loss 0.73287682 - samples/sec: 64.21 - lr: 0.100000
2021-05-24 13:41:02,011 epoch 22 - iter 14/28 - loss 0.77928414 - samples/sec: 64.09 - lr: 0.100000
2021-05-24 13:41:02,496 epoch 22 - iter 16/28 - loss 0.90462641 - samples/sec: 65.97 - lr: 0.100000
2021-05-24 13:41:02,993 epoch 22 - iter 18/28 - loss 0.95401935 - samples/sec: 64.45 - lr: 0.100000
2021-05-24 13:41:03,497 epoch 22 - iter 20/28 - loss 0.96663762 - samples/sec: 63.61 - lr: 0.100000
2021-05-24 13:41:03,990 epoch 22 - iter 22/28 - loss 1.08840719 - samples/sec: 64.93 - lr: 0.100000
2021-05-24 13:41:04,483 epoch 22 - iter 24/28 - loss 1.09063346 - samples/sec: 64.89 - lr: 0.100000
2021-05-24 13:41:04,975 epoch 22 - iter 26/28 - loss 1.10146214 - samples/sec: 65.05 - lr: 0.100000
2021-05-24 13:41:05,388 epoch 22 - iter 28/28 - loss 1.12912895 - samples/sec: 77.63 - lr: 0.100000
2021-05-24 13:41:05,388 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:05,405 EPOCH 22 done: loss 1.1291 - lr 0.1000000
2021-05-24 13:41:05,976 DEV : loss 0.9574224948883057 - score 0.8258
2021-05-24 13:41:05,986 BAD EPOCHS (no improvement): 1
2021-05-24 13:41:05,986 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:06,486 epoch 23 - iter 2/28 - loss 1.16990590 - samples/sec: 64.07 - lr: 0.100000
2021-05-24 13:41:06,966 epoch 23 - iter 4/28 - loss 1.02662936 - samples/sec: 66.73 - lr: 0.100000
2021-05-24 13:41:07,464 epoch 23 - iter 6/28 - loss 0.95190519 - samples/sec: 64.28 - lr: 0.100000
2021-05-24 13:41:07,961 epoch 23 - iter 8/28 - loss 1.07109235 - samples/sec: 64.43 - lr: 0.100000
2021-05-24 13:41:08,458 epoch 23 - iter 10/28 - loss 1.04575771 - samples/sec: 64.42 - lr: 0.100000
2021-05-24 13:41:08,953 epoch 23 - iter 12/28 - loss 1.06614752 - samples/sec: 64.66 - lr: 0.100000
2021-05-24 13:41:09,446 epoch 23 - iter 14/28 - loss 0.96294920 - samples/sec: 64.99 - lr: 0.100000
2021-05-24 13:41:09,940 epoch 23 - iter 16/28 - loss 0.92883230 - samples/sec: 64.84 - lr: 0.100000
2021-05-24 13:41:10,425 epoch 23 - iter 18/28 - loss 0.95949574 - samples/sec: 66.01 - lr: 0.100000
2021-05-24 13:41:10,927 epoch 23 - iter 20/28 - loss 0.97861391 - samples/sec: 63.76 - lr: 0.100000
2021-05-24 13:41:11,427 epoch 23 - iter 22/28 - loss 1.06427413 - samples/sec: 63.99 - lr: 0.100000
2021-05-24 13:41:11,922 epoch 23 - iter 24/28 - loss 1.05513600 - samples/sec: 64.72 - lr: 0.100000
2021-05-24 13:41:12,402 epoch 23 - iter 26/28 - loss 1.05920109 - samples/sec: 66.73 - lr: 0.100000
2021-05-24 13:41:12,817 epoch 23 - iter 28/28 - loss 1.03760426 - samples/sec: 77.13 - lr: 0.100000
2021-05-24 13:41:12,817 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:12,817 EPOCH 23 done: loss 1.0376 - lr 0.1000000
2021-05-24 13:41:13,390 DEV : loss 1.112237811088562 - score 0.8289
2021-05-24 13:41:13,400 BAD EPOCHS (no improvement): 2
2021-05-24 13:41:13,400 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:13,903 epoch 24 - iter 2/28 - loss 1.08177257 - samples/sec: 63.69 - lr: 0.100000
2021-05-24 13:41:14,387 epoch 24 - iter 4/28 - loss 0.91225857 - samples/sec: 66.13 - lr: 0.100000
2021-05-24 13:41:14,878 epoch 24 - iter 6/28 - loss 0.78752995 - samples/sec: 65.26 - lr: 0.100000
2021-05-24 13:41:15,362 epoch 24 - iter 8/28 - loss 0.72108413 - samples/sec: 66.15 - lr: 0.100000
2021-05-24 13:41:15,865 epoch 24 - iter 10/28 - loss 0.76098076 - samples/sec: 63.64 - lr: 0.100000
2021-05-24 13:41:16,467 epoch 24 - iter 12/28 - loss 0.79908459 - samples/sec: 53.18 - lr: 0.100000
2021-05-24 13:41:16,964 epoch 24 - iter 14/28 - loss 0.83628921 - samples/sec: 64.50 - lr: 0.100000
2021-05-24 13:41:17,468 epoch 24 - iter 16/28 - loss 0.88890917 - samples/sec: 63.46 - lr: 0.100000
2021-05-24 13:41:17,976 epoch 24 - iter 18/28 - loss 0.88888090 - samples/sec: 63.05 - lr: 0.100000
2021-05-24 13:41:18,463 epoch 24 - iter 20/28 - loss 0.94014459 - samples/sec: 65.74 - lr: 0.100000
2021-05-24 13:41:18,962 epoch 24 - iter 22/28 - loss 1.02343549 - samples/sec: 64.14 - lr: 0.100000
2021-05-24 13:41:19,452 epoch 24 - iter 24/28 - loss 0.99935604 - samples/sec: 65.36 - lr: 0.100000
2021-05-24 13:41:19,952 epoch 24 - iter 26/28 - loss 0.97328865 - samples/sec: 64.02 - lr: 0.100000
2021-05-24 13:41:20,359 epoch 24 - iter 28/28 - loss 0.99933254 - samples/sec: 78.83 - lr: 0.100000
2021-05-24 13:41:20,359 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:20,359 EPOCH 24 done: loss 0.9993 - lr 0.1000000
2021-05-24 13:41:20,932 DEV : loss 1.073596715927124 - score 0.8054
2021-05-24 13:41:20,942 BAD EPOCHS (no improvement): 3
2021-05-24 13:41:20,942 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:21,428 epoch 25 - iter 2/28 - loss 1.52150685 - samples/sec: 65.96 - lr: 0.100000
2021-05-24 13:41:21,919 epoch 25 - iter 4/28 - loss 1.62839675 - samples/sec: 65.22 - lr: 0.100000
2021-05-24 13:41:22,416 epoch 25 - iter 6/28 - loss 1.49537408 - samples/sec: 64.43 - lr: 0.100000
2021-05-24 13:41:22,901 epoch 25 - iter 8/28 - loss 1.23808077 - samples/sec: 65.94 - lr: 0.100000
2021-05-24 13:41:23,409 epoch 25 - iter 10/28 - loss 1.13469269 - samples/sec: 63.10 - lr: 0.100000
2021-05-24 13:41:23,906 epoch 25 - iter 12/28 - loss 1.18815617 - samples/sec: 64.39 - lr: 0.100000
2021-05-24 13:41:24,411 epoch 25 - iter 14/28 - loss 1.07785964 - samples/sec: 63.39 - lr: 0.100000
2021-05-24 13:41:24,898 epoch 25 - iter 16/28 - loss 1.11496566 - samples/sec: 65.83 - lr: 0.100000
2021-05-24 13:41:25,397 epoch 25 - iter 18/28 - loss 1.12235494 - samples/sec: 64.11 - lr: 0.100000
2021-05-24 13:41:25,881 epoch 25 - iter 20/28 - loss 1.11011338 - samples/sec: 66.16 - lr: 0.100000
2021-05-24 13:41:26,385 epoch 25 - iter 22/28 - loss 1.10593295 - samples/sec: 63.52 - lr: 0.100000
2021-05-24 13:41:26,876 epoch 25 - iter 24/28 - loss 1.12119428 - samples/sec: 65.18 - lr: 0.100000
2021-05-24 13:41:27,372 epoch 25 - iter 26/28 - loss 1.07512940 - samples/sec: 64.56 - lr: 0.100000
2021-05-24 13:41:27,789 epoch 25 - iter 28/28 - loss 1.16121820 - samples/sec: 76.84 - lr: 0.100000
2021-05-24 13:41:27,789 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:27,789 EPOCH 25 done: loss 1.1612 - lr 0.1000000
2021-05-24 13:41:28,363 DEV : loss 1.6055306196212769 - score 0.6441
Epoch    25: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 13:41:28,373 BAD EPOCHS (no improvement): 4
2021-05-24 13:41:28,373 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:28,870 epoch 26 - iter 2/28 - loss 1.05912286 - samples/sec: 64.47 - lr: 0.050000
2021-05-24 13:41:29,364 epoch 26 - iter 4/28 - loss 0.86522716 - samples/sec: 64.83 - lr: 0.050000
2021-05-24 13:41:29,864 epoch 26 - iter 6/28 - loss 0.83844020 - samples/sec: 63.93 - lr: 0.050000
2021-05-24 13:41:30,367 epoch 26 - iter 8/28 - loss 0.75993212 - samples/sec: 63.73 - lr: 0.050000
2021-05-24 13:41:30,869 epoch 26 - iter 10/28 - loss 0.86367120 - samples/sec: 63.83 - lr: 0.050000
2021-05-24 13:41:31,371 epoch 26 - iter 12/28 - loss 0.92914137 - samples/sec: 63.71 - lr: 0.050000
2021-05-24 13:41:31,876 epoch 26 - iter 14/28 - loss 0.85510381 - samples/sec: 63.39 - lr: 0.050000
2021-05-24 13:41:32,382 epoch 26 - iter 16/28 - loss 0.84263612 - samples/sec: 63.34 - lr: 0.050000
2021-05-24 13:41:32,872 epoch 26 - iter 18/28 - loss 0.83115046 - samples/sec: 65.28 - lr: 0.050000
2021-05-24 13:41:33,376 epoch 26 - iter 20/28 - loss 0.83856583 - samples/sec: 63.64 - lr: 0.050000
2021-05-24 13:41:33,881 epoch 26 - iter 22/28 - loss 0.82012517 - samples/sec: 63.35 - lr: 0.050000
2021-05-24 13:41:34,383 epoch 26 - iter 24/28 - loss 0.79695063 - samples/sec: 63.78 - lr: 0.050000
2021-05-24 13:41:34,889 epoch 26 - iter 26/28 - loss 0.79546680 - samples/sec: 63.24 - lr: 0.050000
2021-05-24 13:41:35,311 epoch 26 - iter 28/28 - loss 0.77599377 - samples/sec: 75.99 - lr: 0.050000
2021-05-24 13:41:35,311 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:35,311 EPOCH 26 done: loss 0.7760 - lr 0.0500000
2021-05-24 13:41:35,884 DEV : loss 0.9103930592536926 - score 0.7931
2021-05-24 13:41:35,894 BAD EPOCHS (no improvement): 1
2021-05-24 13:41:35,894 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:36,402 epoch 27 - iter 2/28 - loss 0.29498196 - samples/sec: 63.05 - lr: 0.050000
2021-05-24 13:41:36,900 epoch 27 - iter 4/28 - loss 0.46781981 - samples/sec: 64.23 - lr: 0.050000
2021-05-24 13:41:37,410 epoch 27 - iter 6/28 - loss 0.71567496 - samples/sec: 62.82 - lr: 0.050000
2021-05-24 13:41:37,920 epoch 27 - iter 8/28 - loss 0.72618383 - samples/sec: 62.79 - lr: 0.050000
2021-05-24 13:41:38,425 epoch 27 - iter 10/28 - loss 0.81126637 - samples/sec: 63.41 - lr: 0.050000
2021-05-24 13:41:38,925 epoch 27 - iter 12/28 - loss 0.82038258 - samples/sec: 64.00 - lr: 0.050000
2021-05-24 13:41:39,433 epoch 27 - iter 14/28 - loss 0.78198159 - samples/sec: 63.10 - lr: 0.050000
2021-05-24 13:41:39,926 epoch 27 - iter 16/28 - loss 0.71732994 - samples/sec: 64.91 - lr: 0.050000
2021-05-24 13:41:40,419 epoch 27 - iter 18/28 - loss 0.72381405 - samples/sec: 65.00 - lr: 0.050000
2021-05-24 13:41:40,915 epoch 27 - iter 20/28 - loss 0.70437569 - samples/sec: 64.57 - lr: 0.050000
2021-05-24 13:41:41,398 epoch 27 - iter 22/28 - loss 0.69987588 - samples/sec: 66.22 - lr: 0.050000
2021-05-24 13:41:41,897 epoch 27 - iter 24/28 - loss 0.69187824 - samples/sec: 64.14 - lr: 0.050000
2021-05-24 13:41:42,394 epoch 27 - iter 26/28 - loss 0.69635038 - samples/sec: 64.43 - lr: 0.050000
2021-05-24 13:41:42,790 epoch 27 - iter 28/28 - loss 0.70573145 - samples/sec: 81.06 - lr: 0.050000
2021-05-24 13:41:42,790 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:42,790 EPOCH 27 done: loss 0.7057 - lr 0.0500000
2021-05-24 13:41:43,364 DEV : loss 0.9328110218048096 - score 0.8302
2021-05-24 13:41:43,374 BAD EPOCHS (no improvement): 2
2021-05-24 13:41:43,374 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:43,877 epoch 28 - iter 2/28 - loss 0.73715663 - samples/sec: 63.66 - lr: 0.050000
2021-05-24 13:41:44,366 epoch 28 - iter 4/28 - loss 1.13117045 - samples/sec: 65.44 - lr: 0.050000
2021-05-24 13:41:44,859 epoch 28 - iter 6/28 - loss 0.93609297 - samples/sec: 65.00 - lr: 0.050000
2021-05-24 13:41:45,359 epoch 28 - iter 8/28 - loss 0.78654778 - samples/sec: 64.05 - lr: 0.050000
2021-05-24 13:41:45,858 epoch 28 - iter 10/28 - loss 0.78696953 - samples/sec: 64.11 - lr: 0.050000
2021-05-24 13:41:46,348 epoch 28 - iter 12/28 - loss 0.81331453 - samples/sec: 65.35 - lr: 0.050000
2021-05-24 13:41:46,849 epoch 28 - iter 14/28 - loss 0.86242733 - samples/sec: 63.94 - lr: 0.050000
2021-05-24 13:41:47,350 epoch 28 - iter 16/28 - loss 0.89402840 - samples/sec: 63.99 - lr: 0.050000
2021-05-24 13:41:47,858 epoch 28 - iter 18/28 - loss 0.85948341 - samples/sec: 62.97 - lr: 0.050000
2021-05-24 13:41:48,356 epoch 28 - iter 20/28 - loss 0.86737030 - samples/sec: 64.36 - lr: 0.050000
2021-05-24 13:41:48,864 epoch 28 - iter 22/28 - loss 0.84627613 - samples/sec: 62.95 - lr: 0.050000
2021-05-24 13:41:49,362 epoch 28 - iter 24/28 - loss 0.82796937 - samples/sec: 64.33 - lr: 0.050000
2021-05-24 13:41:49,868 epoch 28 - iter 26/28 - loss 0.80230329 - samples/sec: 63.26 - lr: 0.050000
2021-05-24 13:41:50,274 epoch 28 - iter 28/28 - loss 0.84421353 - samples/sec: 78.97 - lr: 0.050000
2021-05-24 13:41:50,274 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:50,274 EPOCH 28 done: loss 0.8442 - lr 0.0500000
2021-05-24 13:41:50,847 DEV : loss 0.8569745421409607 - score 0.8095
2021-05-24 13:41:50,857 BAD EPOCHS (no improvement): 3
2021-05-24 13:41:50,857 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:51,372 epoch 29 - iter 2/28 - loss 0.50701237 - samples/sec: 62.26 - lr: 0.050000
2021-05-24 13:41:51,863 epoch 29 - iter 4/28 - loss 0.73144355 - samples/sec: 65.22 - lr: 0.050000
2021-05-24 13:41:52,371 epoch 29 - iter 6/28 - loss 0.75931154 - samples/sec: 62.97 - lr: 0.050000
2021-05-24 13:41:52,869 epoch 29 - iter 8/28 - loss 0.85791852 - samples/sec: 64.29 - lr: 0.050000
2021-05-24 13:41:53,342 epoch 29 - iter 10/28 - loss 0.79660090 - samples/sec: 67.72 - lr: 0.050000
2021-05-24 13:41:53,841 epoch 29 - iter 12/28 - loss 0.78224070 - samples/sec: 64.16 - lr: 0.050000
2021-05-24 13:41:54,335 epoch 29 - iter 14/28 - loss 0.75496674 - samples/sec: 64.83 - lr: 0.050000
2021-05-24 13:41:54,804 epoch 29 - iter 16/28 - loss 0.71971266 - samples/sec: 68.31 - lr: 0.050000
2021-05-24 13:41:55,302 epoch 29 - iter 18/28 - loss 0.74434912 - samples/sec: 64.29 - lr: 0.050000
2021-05-24 13:41:55,800 epoch 29 - iter 20/28 - loss 0.71648961 - samples/sec: 64.24 - lr: 0.050000
2021-05-24 13:41:56,293 epoch 29 - iter 22/28 - loss 0.74065245 - samples/sec: 64.94 - lr: 0.050000
2021-05-24 13:41:56,799 epoch 29 - iter 24/28 - loss 0.71888832 - samples/sec: 63.30 - lr: 0.050000
2021-05-24 13:41:57,301 epoch 29 - iter 26/28 - loss 0.71738380 - samples/sec: 63.76 - lr: 0.050000
2021-05-24 13:41:57,717 epoch 29 - iter 28/28 - loss 0.70579828 - samples/sec: 77.16 - lr: 0.050000
2021-05-24 13:41:57,717 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:57,717 EPOCH 29 done: loss 0.7058 - lr 0.0500000
2021-05-24 13:41:58,292 DEV : loss 1.1128782033920288 - score 0.7677
Epoch    29: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 13:41:58,302 BAD EPOCHS (no improvement): 4
2021-05-24 13:41:58,302 ----------------------------------------------------------------------------------------------------
2021-05-24 13:41:58,777 epoch 30 - iter 2/28 - loss 0.42196639 - samples/sec: 67.38 - lr: 0.025000
2021-05-24 13:41:59,271 epoch 30 - iter 4/28 - loss 0.40224648 - samples/sec: 64.90 - lr: 0.025000
2021-05-24 13:41:59,773 epoch 30 - iter 6/28 - loss 0.58791511 - samples/sec: 63.76 - lr: 0.025000
2021-05-24 13:42:00,288 epoch 30 - iter 8/28 - loss 0.79752529 - samples/sec: 62.12 - lr: 0.025000
2021-05-24 13:42:00,784 epoch 30 - iter 10/28 - loss 0.74068938 - samples/sec: 64.57 - lr: 0.025000
2021-05-24 13:42:01,295 epoch 30 - iter 12/28 - loss 0.70236020 - samples/sec: 62.75 - lr: 0.025000
2021-05-24 13:42:01,793 epoch 30 - iter 14/28 - loss 0.70949132 - samples/sec: 64.19 - lr: 0.025000
2021-05-24 13:42:02,280 epoch 30 - iter 16/28 - loss 0.68131247 - samples/sec: 65.78 - lr: 0.025000
2021-05-24 13:42:02,792 epoch 30 - iter 18/28 - loss 0.68721514 - samples/sec: 62.59 - lr: 0.025000
2021-05-24 13:42:03,292 epoch 30 - iter 20/28 - loss 0.65806426 - samples/sec: 64.07 - lr: 0.025000
2021-05-24 13:42:03,801 epoch 30 - iter 22/28 - loss 0.64618397 - samples/sec: 62.88 - lr: 0.025000
2021-05-24 13:42:04,301 epoch 30 - iter 24/28 - loss 0.64290436 - samples/sec: 63.99 - lr: 0.025000
2021-05-24 13:42:04,790 epoch 30 - iter 26/28 - loss 0.65056161 - samples/sec: 65.45 - lr: 0.025000
2021-05-24 13:42:05,213 epoch 30 - iter 28/28 - loss 0.64512811 - samples/sec: 75.71 - lr: 0.025000
2021-05-24 13:42:05,214 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:05,214 EPOCH 30 done: loss 0.6451 - lr 0.0250000
2021-05-24 13:42:05,787 DEV : loss 0.9084153175354004 - score 0.8046
2021-05-24 13:42:05,797 BAD EPOCHS (no improvement): 1
2021-05-24 13:42:06,862 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:06,863 Testing using best model ...
2021-05-24 13:42:06,863 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/zho.rst.sctb/best-model.pt
2021-05-24 13:42:13,219 0.9204	0.7879	0.8490
2021-05-24 13:42:13,219 
Results:
- F1-score (micro) 0.8490
- F1-score (macro) 0.8490

By class:
SENT       tp: 104 - fp: 9 - fn: 28 - precision: 0.9204 - recall: 0.7879 - f1-score: 0.8490
2021-05-24 13:42:13,219 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis/
2021-05-24 13:42:13,244 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis
2021-05-24 13:42:13,245 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis/sent_train.txt
2021-05-24 13:42:13,247 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis/sent_dev.txt
2021-05-24 13:42:13,247 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis/sent_test.txt
Corpus: 1092 train + 243 dev + 249 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 13:42:19,278 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,281 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 13:42:19,281 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,281 Corpus: "Corpus: 1092 train + 243 dev + 249 test sentences"
2021-05-24 13:42:19,281 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,281 Parameters:
2021-05-24 13:42:19,281  - learning_rate: "0.1"
2021-05-24 13:42:19,281  - mini_batch_size: "16"
2021-05-24 13:42:19,281  - patience: "3"
2021-05-24 13:42:19,281  - anneal_factor: "0.5"
2021-05-24 13:42:19,281  - max_epochs: "30"
2021-05-24 13:42:19,282  - shuffle: "True"
2021-05-24 13:42:19,282  - train_with_dev: "False"
2021-05-24 13:42:19,282  - batch_growth_annealing: "False"
2021-05-24 13:42:19,282 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,282 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis"
2021-05-24 13:42:19,282 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,282 Device: cuda:0
2021-05-24 13:42:19,282 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:19,282 Embeddings storage mode: cpu
2021-05-24 13:42:19,283 ----------------------------------------------------------------------------------------------------
2021-05-24 13:42:23,920 epoch 1 - iter 6/69 - loss 17.69180592 - samples/sec: 20.71 - lr: 0.100000
2021-05-24 13:42:28,551 epoch 1 - iter 12/69 - loss 11.94601270 - samples/sec: 20.73 - lr: 0.100000
2021-05-24 13:42:33,178 epoch 1 - iter 18/69 - loss 9.52171856 - samples/sec: 20.75 - lr: 0.100000
2021-05-24 13:42:37,793 epoch 1 - iter 24/69 - loss 8.20703105 - samples/sec: 20.81 - lr: 0.100000
2021-05-24 13:42:42,439 epoch 1 - iter 30/69 - loss 7.38202742 - samples/sec: 20.66 - lr: 0.100000
2021-05-24 13:42:47,074 epoch 1 - iter 36/69 - loss 6.75570145 - samples/sec: 20.71 - lr: 0.100000
2021-05-24 13:42:51,614 epoch 1 - iter 42/69 - loss 6.27231247 - samples/sec: 21.15 - lr: 0.100000
2021-05-24 13:42:56,048 epoch 1 - iter 48/69 - loss 5.81919091 - samples/sec: 21.65 - lr: 0.100000
2021-05-24 13:43:00,539 epoch 1 - iter 54/69 - loss 5.49147653 - samples/sec: 21.38 - lr: 0.100000
2021-05-24 13:43:05,111 epoch 1 - iter 60/69 - loss 5.17778806 - samples/sec: 21.00 - lr: 0.100000
2021-05-24 13:43:09,879 epoch 1 - iter 66/69 - loss 4.86293404 - samples/sec: 20.14 - lr: 0.100000
2021-05-24 13:43:11,601 ----------------------------------------------------------------------------------------------------
2021-05-24 13:43:11,601 EPOCH 1 done: loss 4.7091 - lr 0.1000000
2021-05-24 13:43:19,087 DEV : loss 0.7360409498214722 - score 0.8154
2021-05-24 13:43:19,110 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:43:20,196 ----------------------------------------------------------------------------------------------------
2021-05-24 13:43:21,829 epoch 2 - iter 6/69 - loss 2.09054999 - samples/sec: 58.80 - lr: 0.100000
2021-05-24 13:43:23,450 epoch 2 - iter 12/69 - loss 1.70714890 - samples/sec: 59.26 - lr: 0.100000
2021-05-24 13:43:25,098 epoch 2 - iter 18/69 - loss 1.74000289 - samples/sec: 58.24 - lr: 0.100000
2021-05-24 13:43:26,742 epoch 2 - iter 24/69 - loss 1.62557729 - samples/sec: 58.42 - lr: 0.100000
2021-05-24 13:43:28,366 epoch 2 - iter 30/69 - loss 1.46451532 - samples/sec: 59.15 - lr: 0.100000
2021-05-24 13:43:29,992 epoch 2 - iter 36/69 - loss 1.39333298 - samples/sec: 59.03 - lr: 0.100000
2021-05-24 13:43:31,611 epoch 2 - iter 42/69 - loss 1.38525625 - samples/sec: 59.32 - lr: 0.100000
2021-05-24 13:43:33,205 epoch 2 - iter 48/69 - loss 1.36618173 - samples/sec: 60.26 - lr: 0.100000
2021-05-24 13:43:34,810 epoch 2 - iter 54/69 - loss 1.32809743 - samples/sec: 59.82 - lr: 0.100000
2021-05-24 13:43:36,439 epoch 2 - iter 60/69 - loss 1.33179721 - samples/sec: 58.94 - lr: 0.100000
2021-05-24 13:43:38,088 epoch 2 - iter 66/69 - loss 1.32043777 - samples/sec: 58.23 - lr: 0.100000
2021-05-24 13:43:38,723 ----------------------------------------------------------------------------------------------------
2021-05-24 13:43:38,723 EPOCH 2 done: loss 1.2963 - lr 0.1000000
2021-05-24 13:43:40,182 DEV : loss 0.3456994593143463 - score 0.9238
2021-05-24 13:43:40,205 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:43:50,218 ----------------------------------------------------------------------------------------------------
2021-05-24 13:43:51,848 epoch 3 - iter 6/69 - loss 1.37489053 - samples/sec: 58.94 - lr: 0.100000
2021-05-24 13:43:53,459 epoch 3 - iter 12/69 - loss 1.77345574 - samples/sec: 59.61 - lr: 0.100000
2021-05-24 13:43:55,074 epoch 3 - iter 18/69 - loss 1.41972158 - samples/sec: 59.47 - lr: 0.100000
2021-05-24 13:43:56,677 epoch 3 - iter 24/69 - loss 1.26600788 - samples/sec: 59.91 - lr: 0.100000
2021-05-24 13:43:58,291 epoch 3 - iter 30/69 - loss 1.25343031 - samples/sec: 59.48 - lr: 0.100000
2021-05-24 13:43:59,888 epoch 3 - iter 36/69 - loss 1.24191557 - samples/sec: 60.14 - lr: 0.100000
2021-05-24 13:44:01,507 epoch 3 - iter 42/69 - loss 1.19578792 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 13:44:03,119 epoch 3 - iter 48/69 - loss 1.14571803 - samples/sec: 59.57 - lr: 0.100000
2021-05-24 13:44:04,753 epoch 3 - iter 54/69 - loss 1.14440700 - samples/sec: 58.78 - lr: 0.100000
2021-05-24 13:44:06,349 epoch 3 - iter 60/69 - loss 1.12923022 - samples/sec: 60.14 - lr: 0.100000
2021-05-24 13:44:07,940 epoch 3 - iter 66/69 - loss 1.11906857 - samples/sec: 60.35 - lr: 0.100000
2021-05-24 13:44:08,602 ----------------------------------------------------------------------------------------------------
2021-05-24 13:44:08,602 EPOCH 3 done: loss 1.0888 - lr 0.1000000
2021-05-24 13:44:10,052 DEV : loss 0.3540061414241791 - score 0.9514
2021-05-24 13:44:10,076 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:44:20,550 ----------------------------------------------------------------------------------------------------
2021-05-24 13:44:22,172 epoch 4 - iter 6/69 - loss 0.90207505 - samples/sec: 59.23 - lr: 0.100000
2021-05-24 13:44:23,789 epoch 4 - iter 12/69 - loss 0.90280270 - samples/sec: 59.39 - lr: 0.100000
2021-05-24 13:44:25,404 epoch 4 - iter 18/69 - loss 0.82028619 - samples/sec: 59.46 - lr: 0.100000
2021-05-24 13:44:26,984 epoch 4 - iter 24/69 - loss 0.92834614 - samples/sec: 60.77 - lr: 0.100000
2021-05-24 13:44:28,585 epoch 4 - iter 30/69 - loss 0.91901817 - samples/sec: 59.96 - lr: 0.100000
2021-05-24 13:44:30,389 epoch 4 - iter 36/69 - loss 0.93701194 - samples/sec: 53.24 - lr: 0.100000
2021-05-24 13:44:31,989 epoch 4 - iter 42/69 - loss 0.95289155 - samples/sec: 60.04 - lr: 0.100000
2021-05-24 13:44:33,586 epoch 4 - iter 48/69 - loss 0.92951040 - samples/sec: 60.11 - lr: 0.100000
2021-05-24 13:44:35,224 epoch 4 - iter 54/69 - loss 0.89428546 - samples/sec: 58.65 - lr: 0.100000
2021-05-24 13:44:36,852 epoch 4 - iter 60/69 - loss 0.88167275 - samples/sec: 59.00 - lr: 0.100000
2021-05-24 13:44:38,480 epoch 4 - iter 66/69 - loss 0.84980450 - samples/sec: 58.96 - lr: 0.100000
2021-05-24 13:44:39,127 ----------------------------------------------------------------------------------------------------
2021-05-24 13:44:39,128 EPOCH 4 done: loss 0.8331 - lr 0.1000000
2021-05-24 13:44:40,585 DEV : loss 0.2340250313282013 - score 0.9672
2021-05-24 13:44:40,608 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:44:50,147 ----------------------------------------------------------------------------------------------------
2021-05-24 13:44:51,758 epoch 5 - iter 6/69 - loss 0.79538878 - samples/sec: 59.64 - lr: 0.100000
2021-05-24 13:44:53,367 epoch 5 - iter 12/69 - loss 0.72952378 - samples/sec: 59.70 - lr: 0.100000
2021-05-24 13:44:54,916 epoch 5 - iter 18/69 - loss 0.63594298 - samples/sec: 61.99 - lr: 0.100000
2021-05-24 13:44:56,518 epoch 5 - iter 24/69 - loss 0.61771002 - samples/sec: 59.93 - lr: 0.100000
2021-05-24 13:44:58,132 epoch 5 - iter 30/69 - loss 0.67124861 - samples/sec: 59.49 - lr: 0.100000
2021-05-24 13:44:59,725 epoch 5 - iter 36/69 - loss 0.65459973 - samples/sec: 60.30 - lr: 0.100000
2021-05-24 13:45:01,375 epoch 5 - iter 42/69 - loss 0.70148070 - samples/sec: 58.20 - lr: 0.100000
2021-05-24 13:45:02,999 epoch 5 - iter 48/69 - loss 0.73076236 - samples/sec: 59.13 - lr: 0.100000
2021-05-24 13:45:04,641 epoch 5 - iter 54/69 - loss 0.74215404 - samples/sec: 58.46 - lr: 0.100000
2021-05-24 13:45:06,310 epoch 5 - iter 60/69 - loss 0.77023422 - samples/sec: 57.54 - lr: 0.100000
2021-05-24 13:45:07,961 epoch 5 - iter 66/69 - loss 0.77199846 - samples/sec: 58.16 - lr: 0.100000
2021-05-24 13:45:08,609 ----------------------------------------------------------------------------------------------------
2021-05-24 13:45:08,609 EPOCH 5 done: loss 0.7628 - lr 0.1000000
2021-05-24 13:45:10,064 DEV : loss 0.2198888659477234 - score 0.9598
2021-05-24 13:45:10,087 BAD EPOCHS (no improvement): 1
2021-05-24 13:45:10,088 ----------------------------------------------------------------------------------------------------
2021-05-24 13:45:11,722 epoch 6 - iter 6/69 - loss 0.59093777 - samples/sec: 58.76 - lr: 0.100000
2021-05-24 13:45:13,326 epoch 6 - iter 12/69 - loss 0.67466985 - samples/sec: 59.87 - lr: 0.100000
2021-05-24 13:45:14,930 epoch 6 - iter 18/69 - loss 0.72620079 - samples/sec: 59.89 - lr: 0.100000
2021-05-24 13:45:16,506 epoch 6 - iter 24/69 - loss 0.69901557 - samples/sec: 60.93 - lr: 0.100000
2021-05-24 13:45:18,140 epoch 6 - iter 30/69 - loss 0.74572021 - samples/sec: 58.75 - lr: 0.100000
2021-05-24 13:45:19,742 epoch 6 - iter 36/69 - loss 0.70420336 - samples/sec: 59.96 - lr: 0.100000
2021-05-24 13:45:21,345 epoch 6 - iter 42/69 - loss 0.76333721 - samples/sec: 59.89 - lr: 0.100000
2021-05-24 13:45:22,960 epoch 6 - iter 48/69 - loss 0.73775991 - samples/sec: 59.48 - lr: 0.100000
2021-05-24 13:45:24,568 epoch 6 - iter 54/69 - loss 0.75193249 - samples/sec: 59.74 - lr: 0.100000
2021-05-24 13:45:26,193 epoch 6 - iter 60/69 - loss 0.74367030 - samples/sec: 59.09 - lr: 0.100000
2021-05-24 13:45:27,813 epoch 6 - iter 66/69 - loss 0.75389843 - samples/sec: 59.26 - lr: 0.100000
2021-05-24 13:45:28,463 ----------------------------------------------------------------------------------------------------
2021-05-24 13:45:28,463 EPOCH 6 done: loss 0.7417 - lr 0.1000000
2021-05-24 13:45:29,921 DEV : loss 0.20369869470596313 - score 0.9736
2021-05-24 13:45:29,945 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:45:40,837 ----------------------------------------------------------------------------------------------------
2021-05-24 13:45:42,464 epoch 7 - iter 6/69 - loss 1.00193759 - samples/sec: 59.05 - lr: 0.100000
2021-05-24 13:45:44,089 epoch 7 - iter 12/69 - loss 0.84372956 - samples/sec: 59.08 - lr: 0.100000
2021-05-24 13:45:45,720 epoch 7 - iter 18/69 - loss 0.69688755 - samples/sec: 58.90 - lr: 0.100000
2021-05-24 13:45:47,345 epoch 7 - iter 24/69 - loss 0.79053273 - samples/sec: 59.08 - lr: 0.100000
2021-05-24 13:45:48,964 epoch 7 - iter 30/69 - loss 0.76046504 - samples/sec: 59.30 - lr: 0.100000
2021-05-24 13:45:50,570 epoch 7 - iter 36/69 - loss 0.75724747 - samples/sec: 59.82 - lr: 0.100000
2021-05-24 13:45:52,186 epoch 7 - iter 42/69 - loss 0.75668465 - samples/sec: 59.42 - lr: 0.100000
2021-05-24 13:45:53,774 epoch 7 - iter 48/69 - loss 0.72407661 - samples/sec: 60.49 - lr: 0.100000
2021-05-24 13:45:55,391 epoch 7 - iter 54/69 - loss 0.72877590 - samples/sec: 59.37 - lr: 0.100000
2021-05-24 13:45:56,996 epoch 7 - iter 60/69 - loss 0.76008339 - samples/sec: 59.82 - lr: 0.100000
2021-05-24 13:45:58,598 epoch 7 - iter 66/69 - loss 0.74322752 - samples/sec: 59.94 - lr: 0.100000
2021-05-24 13:45:59,225 ----------------------------------------------------------------------------------------------------
2021-05-24 13:45:59,225 EPOCH 7 done: loss 0.7287 - lr 0.1000000
2021-05-24 13:46:00,680 DEV : loss 0.21030749380588531 - score 0.9598
2021-05-24 13:46:00,703 BAD EPOCHS (no improvement): 1
2021-05-24 13:46:00,703 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:02,322 epoch 8 - iter 6/69 - loss 0.43639072 - samples/sec: 59.34 - lr: 0.100000
2021-05-24 13:46:03,905 epoch 8 - iter 12/69 - loss 0.40839065 - samples/sec: 60.65 - lr: 0.100000
2021-05-24 13:46:05,521 epoch 8 - iter 18/69 - loss 0.52057363 - samples/sec: 59.43 - lr: 0.100000
2021-05-24 13:46:07,151 epoch 8 - iter 24/69 - loss 0.55947455 - samples/sec: 58.92 - lr: 0.100000
2021-05-24 13:46:08,763 epoch 8 - iter 30/69 - loss 0.55507635 - samples/sec: 59.58 - lr: 0.100000
2021-05-24 13:46:10,409 epoch 8 - iter 36/69 - loss 0.55113053 - samples/sec: 58.34 - lr: 0.100000
2021-05-24 13:46:12,031 epoch 8 - iter 42/69 - loss 0.61640331 - samples/sec: 59.17 - lr: 0.100000
2021-05-24 13:46:13,639 epoch 8 - iter 48/69 - loss 0.60526100 - samples/sec: 59.72 - lr: 0.100000
2021-05-24 13:46:15,261 epoch 8 - iter 54/69 - loss 0.62732273 - samples/sec: 59.22 - lr: 0.100000
2021-05-24 13:46:16,896 epoch 8 - iter 60/69 - loss 0.63072136 - samples/sec: 58.73 - lr: 0.100000
2021-05-24 13:46:18,538 epoch 8 - iter 66/69 - loss 0.63942481 - samples/sec: 58.49 - lr: 0.100000
2021-05-24 13:46:19,172 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:19,172 EPOCH 8 done: loss 0.6257 - lr 0.1000000
2021-05-24 13:46:20,777 DEV : loss 0.2682439088821411 - score 0.9526
2021-05-24 13:46:20,800 BAD EPOCHS (no improvement): 2
2021-05-24 13:46:20,801 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:22,417 epoch 9 - iter 6/69 - loss 0.50413751 - samples/sec: 59.42 - lr: 0.100000
2021-05-24 13:46:24,035 epoch 9 - iter 12/69 - loss 0.57846920 - samples/sec: 59.34 - lr: 0.100000
2021-05-24 13:46:25,658 epoch 9 - iter 18/69 - loss 0.54349890 - samples/sec: 59.16 - lr: 0.100000
2021-05-24 13:46:27,284 epoch 9 - iter 24/69 - loss 0.59870620 - samples/sec: 59.06 - lr: 0.100000
2021-05-24 13:46:28,942 epoch 9 - iter 30/69 - loss 0.56486606 - samples/sec: 57.92 - lr: 0.100000
2021-05-24 13:46:30,568 epoch 9 - iter 36/69 - loss 0.55791493 - samples/sec: 59.07 - lr: 0.100000
2021-05-24 13:46:32,196 epoch 9 - iter 42/69 - loss 0.55380265 - samples/sec: 58.97 - lr: 0.100000
2021-05-24 13:46:33,812 epoch 9 - iter 48/69 - loss 0.59196274 - samples/sec: 59.42 - lr: 0.100000
2021-05-24 13:46:35,424 epoch 9 - iter 54/69 - loss 0.57754170 - samples/sec: 59.56 - lr: 0.100000
2021-05-24 13:46:37,049 epoch 9 - iter 60/69 - loss 0.60024585 - samples/sec: 59.12 - lr: 0.100000
2021-05-24 13:46:38,664 epoch 9 - iter 66/69 - loss 0.59322705 - samples/sec: 59.44 - lr: 0.100000
2021-05-24 13:46:39,330 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:39,330 EPOCH 9 done: loss 0.6026 - lr 0.1000000
2021-05-24 13:46:40,789 DEV : loss 0.18879295885562897 - score 0.9669
2021-05-24 13:46:40,812 BAD EPOCHS (no improvement): 3
2021-05-24 13:46:40,813 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:42,452 epoch 10 - iter 6/69 - loss 0.80404758 - samples/sec: 58.59 - lr: 0.100000
2021-05-24 13:46:44,057 epoch 10 - iter 12/69 - loss 0.60541590 - samples/sec: 59.81 - lr: 0.100000
2021-05-24 13:46:45,684 epoch 10 - iter 18/69 - loss 0.57250247 - samples/sec: 59.02 - lr: 0.100000
2021-05-24 13:46:47,318 epoch 10 - iter 24/69 - loss 0.55129616 - samples/sec: 58.79 - lr: 0.100000
2021-05-24 13:46:48,925 epoch 10 - iter 30/69 - loss 0.59392591 - samples/sec: 59.75 - lr: 0.100000
2021-05-24 13:46:50,560 epoch 10 - iter 36/69 - loss 0.64163390 - samples/sec: 58.74 - lr: 0.100000
2021-05-24 13:46:52,174 epoch 10 - iter 42/69 - loss 0.63826772 - samples/sec: 59.51 - lr: 0.100000
2021-05-24 13:46:53,817 epoch 10 - iter 48/69 - loss 0.63058511 - samples/sec: 58.43 - lr: 0.100000
2021-05-24 13:46:55,449 epoch 10 - iter 54/69 - loss 0.62186096 - samples/sec: 58.83 - lr: 0.100000
2021-05-24 13:46:57,084 epoch 10 - iter 60/69 - loss 0.61760239 - samples/sec: 58.74 - lr: 0.100000
2021-05-24 13:46:58,721 epoch 10 - iter 66/69 - loss 0.60611217 - samples/sec: 58.65 - lr: 0.100000
2021-05-24 13:46:59,364 ----------------------------------------------------------------------------------------------------
2021-05-24 13:46:59,364 EPOCH 10 done: loss 0.5985 - lr 0.1000000
2021-05-24 13:47:00,818 DEV : loss 0.30334314703941345 - score 0.9358
Epoch    10: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 13:47:00,841 BAD EPOCHS (no improvement): 4
2021-05-24 13:47:00,841 ----------------------------------------------------------------------------------------------------
2021-05-24 13:47:02,458 epoch 11 - iter 6/69 - loss 0.43855584 - samples/sec: 59.39 - lr: 0.050000
2021-05-24 13:47:04,108 epoch 11 - iter 12/69 - loss 0.44592466 - samples/sec: 58.22 - lr: 0.050000
2021-05-24 13:47:05,724 epoch 11 - iter 18/69 - loss 0.49616522 - samples/sec: 59.41 - lr: 0.050000
2021-05-24 13:47:07,384 epoch 11 - iter 24/69 - loss 0.47858095 - samples/sec: 57.85 - lr: 0.050000
2021-05-24 13:47:09,017 epoch 11 - iter 30/69 - loss 0.51421228 - samples/sec: 58.81 - lr: 0.050000
2021-05-24 13:47:10,642 epoch 11 - iter 36/69 - loss 0.49995834 - samples/sec: 59.09 - lr: 0.050000
2021-05-24 13:47:12,262 epoch 11 - iter 42/69 - loss 0.47848224 - samples/sec: 59.28 - lr: 0.050000
2021-05-24 13:47:13,915 epoch 11 - iter 48/69 - loss 0.47772108 - samples/sec: 58.08 - lr: 0.050000
2021-05-24 13:47:15,534 epoch 11 - iter 54/69 - loss 0.47316093 - samples/sec: 59.33 - lr: 0.050000
2021-05-24 13:47:17,147 epoch 11 - iter 60/69 - loss 0.47015846 - samples/sec: 59.53 - lr: 0.050000
2021-05-24 13:47:18,772 epoch 11 - iter 66/69 - loss 0.46780760 - samples/sec: 59.08 - lr: 0.050000
2021-05-24 13:47:19,407 ----------------------------------------------------------------------------------------------------
2021-05-24 13:47:19,407 EPOCH 11 done: loss 0.4568 - lr 0.0500000
2021-05-24 13:47:20,859 DEV : loss 0.3783295452594757 - score 0.933
2021-05-24 13:47:20,883 BAD EPOCHS (no improvement): 1
2021-05-24 13:47:20,883 ----------------------------------------------------------------------------------------------------
2021-05-24 13:47:22,484 epoch 12 - iter 6/69 - loss 0.45822724 - samples/sec: 60.00 - lr: 0.050000
2021-05-24 13:47:24,125 epoch 12 - iter 12/69 - loss 0.50440045 - samples/sec: 58.50 - lr: 0.050000
2021-05-24 13:47:25,715 epoch 12 - iter 18/69 - loss 0.46004713 - samples/sec: 60.39 - lr: 0.050000
2021-05-24 13:47:27,297 epoch 12 - iter 24/69 - loss 0.42144641 - samples/sec: 60.72 - lr: 0.050000
2021-05-24 13:47:28,878 epoch 12 - iter 30/69 - loss 0.42092030 - samples/sec: 60.74 - lr: 0.050000
2021-05-24 13:47:30,458 epoch 12 - iter 36/69 - loss 0.39208655 - samples/sec: 60.80 - lr: 0.050000
2021-05-24 13:47:32,069 epoch 12 - iter 42/69 - loss 0.41754726 - samples/sec: 59.60 - lr: 0.050000
2021-05-24 13:47:33,652 epoch 12 - iter 48/69 - loss 0.41013582 - samples/sec: 60.64 - lr: 0.050000
2021-05-24 13:47:35,255 epoch 12 - iter 54/69 - loss 0.41584540 - samples/sec: 59.90 - lr: 0.050000
2021-05-24 13:47:36,843 epoch 12 - iter 60/69 - loss 0.41436574 - samples/sec: 60.49 - lr: 0.050000
2021-05-24 13:47:38,461 epoch 12 - iter 66/69 - loss 0.41169980 - samples/sec: 59.35 - lr: 0.050000
2021-05-24 13:47:39,090 ----------------------------------------------------------------------------------------------------
2021-05-24 13:47:39,091 EPOCH 12 done: loss 0.4070 - lr 0.0500000
2021-05-24 13:47:40,547 DEV : loss 0.1683865785598755 - score 0.9736
2021-05-24 13:47:40,571 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:47:50,560 ----------------------------------------------------------------------------------------------------
2021-05-24 13:47:52,165 epoch 13 - iter 6/69 - loss 0.42922807 - samples/sec: 59.92 - lr: 0.050000
2021-05-24 13:47:53,770 epoch 13 - iter 12/69 - loss 0.41357677 - samples/sec: 59.84 - lr: 0.050000
2021-05-24 13:47:55,366 epoch 13 - iter 18/69 - loss 0.33255830 - samples/sec: 60.19 - lr: 0.050000
2021-05-24 13:47:56,946 epoch 13 - iter 24/69 - loss 0.35822159 - samples/sec: 60.75 - lr: 0.050000
2021-05-24 13:47:58,549 epoch 13 - iter 30/69 - loss 0.35497956 - samples/sec: 59.92 - lr: 0.050000
2021-05-24 13:48:00,169 epoch 13 - iter 36/69 - loss 0.36225855 - samples/sec: 59.28 - lr: 0.050000
2021-05-24 13:48:01,797 epoch 13 - iter 42/69 - loss 0.34696220 - samples/sec: 58.97 - lr: 0.050000
2021-05-24 13:48:03,406 epoch 13 - iter 48/69 - loss 0.37219003 - samples/sec: 59.67 - lr: 0.050000
2021-05-24 13:48:05,009 epoch 13 - iter 54/69 - loss 0.36806838 - samples/sec: 59.94 - lr: 0.050000
2021-05-24 13:48:06,621 epoch 13 - iter 60/69 - loss 0.37394967 - samples/sec: 59.56 - lr: 0.050000
2021-05-24 13:48:08,237 epoch 13 - iter 66/69 - loss 0.36587421 - samples/sec: 59.42 - lr: 0.050000
2021-05-24 13:48:08,881 ----------------------------------------------------------------------------------------------------
2021-05-24 13:48:08,881 EPOCH 13 done: loss 0.3644 - lr 0.0500000
2021-05-24 13:48:10,333 DEV : loss 0.2965477406978607 - score 0.9284
2021-05-24 13:48:10,356 BAD EPOCHS (no improvement): 1
2021-05-24 13:48:10,357 ----------------------------------------------------------------------------------------------------
2021-05-24 13:48:12,132 epoch 14 - iter 6/69 - loss 0.30978819 - samples/sec: 54.08 - lr: 0.050000
2021-05-24 13:48:13,737 epoch 14 - iter 12/69 - loss 0.40667812 - samples/sec: 59.83 - lr: 0.050000
2021-05-24 13:48:15,356 epoch 14 - iter 18/69 - loss 0.42395985 - samples/sec: 59.31 - lr: 0.050000
2021-05-24 13:48:16,941 epoch 14 - iter 24/69 - loss 0.37703824 - samples/sec: 60.59 - lr: 0.050000
2021-05-24 13:48:18,561 epoch 14 - iter 30/69 - loss 0.35134812 - samples/sec: 59.29 - lr: 0.050000
2021-05-24 13:48:20,171 epoch 14 - iter 36/69 - loss 0.33452449 - samples/sec: 59.66 - lr: 0.050000
2021-05-24 13:48:21,801 epoch 14 - iter 42/69 - loss 0.34498873 - samples/sec: 58.91 - lr: 0.050000
2021-05-24 13:48:23,390 epoch 14 - iter 48/69 - loss 0.33733965 - samples/sec: 60.43 - lr: 0.050000
2021-05-24 13:48:24,969 epoch 14 - iter 54/69 - loss 0.32778110 - samples/sec: 60.81 - lr: 0.050000
2021-05-24 13:48:26,563 epoch 14 - iter 60/69 - loss 0.34544096 - samples/sec: 60.27 - lr: 0.050000
2021-05-24 13:48:28,179 epoch 14 - iter 66/69 - loss 0.36562474 - samples/sec: 59.40 - lr: 0.050000
2021-05-24 13:48:28,794 ----------------------------------------------------------------------------------------------------
2021-05-24 13:48:28,794 EPOCH 14 done: loss 0.3684 - lr 0.0500000
2021-05-24 13:48:30,247 DEV : loss 0.1673942655324936 - score 0.9714
2021-05-24 13:48:30,270 BAD EPOCHS (no improvement): 2
2021-05-24 13:48:30,270 ----------------------------------------------------------------------------------------------------
2021-05-24 13:48:31,880 epoch 15 - iter 6/69 - loss 0.41137763 - samples/sec: 59.66 - lr: 0.050000
2021-05-24 13:48:33,477 epoch 15 - iter 12/69 - loss 0.36379663 - samples/sec: 60.13 - lr: 0.050000
2021-05-24 13:48:35,092 epoch 15 - iter 18/69 - loss 0.35794540 - samples/sec: 59.45 - lr: 0.050000
2021-05-24 13:48:36,712 epoch 15 - iter 24/69 - loss 0.35431626 - samples/sec: 59.29 - lr: 0.050000
2021-05-24 13:48:38,341 epoch 15 - iter 30/69 - loss 0.37159936 - samples/sec: 58.95 - lr: 0.050000
2021-05-24 13:48:39,933 epoch 15 - iter 36/69 - loss 0.35828757 - samples/sec: 60.32 - lr: 0.050000
2021-05-24 13:48:41,537 epoch 15 - iter 42/69 - loss 0.37524094 - samples/sec: 59.88 - lr: 0.050000
2021-05-24 13:48:43,147 epoch 15 - iter 48/69 - loss 0.37429435 - samples/sec: 59.63 - lr: 0.050000
2021-05-24 13:48:44,765 epoch 15 - iter 54/69 - loss 0.36766359 - samples/sec: 59.38 - lr: 0.050000
2021-05-24 13:48:46,405 epoch 15 - iter 60/69 - loss 0.37617199 - samples/sec: 58.52 - lr: 0.050000
2021-05-24 13:48:48,039 epoch 15 - iter 66/69 - loss 0.36966159 - samples/sec: 58.78 - lr: 0.050000
2021-05-24 13:48:48,689 ----------------------------------------------------------------------------------------------------
2021-05-24 13:48:48,689 EPOCH 15 done: loss 0.3615 - lr 0.0500000
2021-05-24 13:48:50,142 DEV : loss 0.2001819908618927 - score 0.9741
2021-05-24 13:48:50,166 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:49:00,457 ----------------------------------------------------------------------------------------------------
2021-05-24 13:49:02,101 epoch 16 - iter 6/69 - loss 0.20710262 - samples/sec: 58.43 - lr: 0.050000
2021-05-24 13:49:03,746 epoch 16 - iter 12/69 - loss 0.24436206 - samples/sec: 58.39 - lr: 0.050000
2021-05-24 13:49:05,320 epoch 16 - iter 18/69 - loss 0.35576352 - samples/sec: 60.98 - lr: 0.050000
2021-05-24 13:49:06,956 epoch 16 - iter 24/69 - loss 0.36185380 - samples/sec: 58.69 - lr: 0.050000
2021-05-24 13:49:08,592 epoch 16 - iter 30/69 - loss 0.35541828 - samples/sec: 58.71 - lr: 0.050000
2021-05-24 13:49:10,230 epoch 16 - iter 36/69 - loss 0.35247571 - samples/sec: 58.63 - lr: 0.050000
2021-05-24 13:49:11,816 epoch 16 - iter 42/69 - loss 0.37006999 - samples/sec: 60.54 - lr: 0.050000
2021-05-24 13:49:13,431 epoch 16 - iter 48/69 - loss 0.37313541 - samples/sec: 59.48 - lr: 0.050000
2021-05-24 13:49:15,048 epoch 16 - iter 54/69 - loss 0.35812296 - samples/sec: 59.35 - lr: 0.050000
2021-05-24 13:49:16,678 epoch 16 - iter 60/69 - loss 0.38006309 - samples/sec: 58.94 - lr: 0.050000
2021-05-24 13:49:18,324 epoch 16 - iter 66/69 - loss 0.36239811 - samples/sec: 58.33 - lr: 0.050000
2021-05-24 13:49:18,984 ----------------------------------------------------------------------------------------------------
2021-05-24 13:49:18,985 EPOCH 16 done: loss 0.3528 - lr 0.0500000
2021-05-24 13:49:20,438 DEV : loss 0.23739288747310638 - score 0.9596
2021-05-24 13:49:20,461 BAD EPOCHS (no improvement): 1
2021-05-24 13:49:20,462 ----------------------------------------------------------------------------------------------------
2021-05-24 13:49:22,092 epoch 17 - iter 6/69 - loss 0.36366900 - samples/sec: 58.90 - lr: 0.050000
2021-05-24 13:49:23,724 epoch 17 - iter 12/69 - loss 0.35879989 - samples/sec: 58.83 - lr: 0.050000
2021-05-24 13:49:25,351 epoch 17 - iter 18/69 - loss 0.29382951 - samples/sec: 59.05 - lr: 0.050000
2021-05-24 13:49:26,986 epoch 17 - iter 24/69 - loss 0.34055003 - samples/sec: 58.72 - lr: 0.050000
2021-05-24 13:49:28,636 epoch 17 - iter 30/69 - loss 0.34210270 - samples/sec: 58.20 - lr: 0.050000
2021-05-24 13:49:30,240 epoch 17 - iter 36/69 - loss 0.33950433 - samples/sec: 59.87 - lr: 0.050000
2021-05-24 13:49:31,845 epoch 17 - iter 42/69 - loss 0.35416567 - samples/sec: 59.81 - lr: 0.050000
2021-05-24 13:49:33,479 epoch 17 - iter 48/69 - loss 0.34836365 - samples/sec: 58.77 - lr: 0.050000
2021-05-24 13:49:35,093 epoch 17 - iter 54/69 - loss 0.35011763 - samples/sec: 59.51 - lr: 0.050000
2021-05-24 13:49:36,693 epoch 17 - iter 60/69 - loss 0.34591853 - samples/sec: 60.00 - lr: 0.050000
2021-05-24 13:49:38,309 epoch 17 - iter 66/69 - loss 0.34930738 - samples/sec: 59.43 - lr: 0.050000
2021-05-24 13:49:38,950 ----------------------------------------------------------------------------------------------------
2021-05-24 13:49:38,950 EPOCH 17 done: loss 0.3395 - lr 0.0500000
2021-05-24 13:49:40,405 DEV : loss 0.16865062713623047 - score 0.9783
2021-05-24 13:49:40,429 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:49:50,159 ----------------------------------------------------------------------------------------------------
2021-05-24 13:49:51,784 epoch 18 - iter 6/69 - loss 0.25599138 - samples/sec: 59.13 - lr: 0.050000
2021-05-24 13:49:53,417 epoch 18 - iter 12/69 - loss 0.32070504 - samples/sec: 58.81 - lr: 0.050000
2021-05-24 13:49:55,041 epoch 18 - iter 18/69 - loss 0.26740019 - samples/sec: 59.12 - lr: 0.050000
2021-05-24 13:49:56,657 epoch 18 - iter 24/69 - loss 0.27672382 - samples/sec: 59.42 - lr: 0.050000
2021-05-24 13:49:58,272 epoch 18 - iter 30/69 - loss 0.27274698 - samples/sec: 59.49 - lr: 0.050000
2021-05-24 13:49:59,861 epoch 18 - iter 36/69 - loss 0.27330880 - samples/sec: 60.43 - lr: 0.050000
2021-05-24 13:50:01,481 epoch 18 - iter 42/69 - loss 0.28867365 - samples/sec: 59.26 - lr: 0.050000
2021-05-24 13:50:03,080 epoch 18 - iter 48/69 - loss 0.26980647 - samples/sec: 60.07 - lr: 0.050000
2021-05-24 13:50:04,707 epoch 18 - iter 54/69 - loss 0.30746386 - samples/sec: 59.03 - lr: 0.050000
2021-05-24 13:50:06,341 epoch 18 - iter 60/69 - loss 0.31733458 - samples/sec: 58.74 - lr: 0.050000
2021-05-24 13:50:07,962 epoch 18 - iter 66/69 - loss 0.33332333 - samples/sec: 59.25 - lr: 0.050000
2021-05-24 13:50:08,609 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:08,610 EPOCH 18 done: loss 0.3372 - lr 0.0500000
2021-05-24 13:50:10,066 DEV : loss 0.16436588764190674 - score 0.969
2021-05-24 13:50:10,090 BAD EPOCHS (no improvement): 1
2021-05-24 13:50:10,090 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:11,869 epoch 19 - iter 6/69 - loss 0.62512747 - samples/sec: 53.98 - lr: 0.050000
2021-05-24 13:50:13,485 epoch 19 - iter 12/69 - loss 0.48109156 - samples/sec: 59.43 - lr: 0.050000
2021-05-24 13:50:15,097 epoch 19 - iter 18/69 - loss 0.44105767 - samples/sec: 59.58 - lr: 0.050000
2021-05-24 13:50:16,695 epoch 19 - iter 24/69 - loss 0.38651666 - samples/sec: 60.09 - lr: 0.050000
2021-05-24 13:50:18,320 epoch 19 - iter 30/69 - loss 0.39232709 - samples/sec: 59.10 - lr: 0.050000
2021-05-24 13:50:19,925 epoch 19 - iter 36/69 - loss 0.38855085 - samples/sec: 59.84 - lr: 0.050000
2021-05-24 13:50:21,538 epoch 19 - iter 42/69 - loss 0.36159441 - samples/sec: 59.51 - lr: 0.050000
2021-05-24 13:50:23,153 epoch 19 - iter 48/69 - loss 0.36345962 - samples/sec: 59.46 - lr: 0.050000
2021-05-24 13:50:24,747 epoch 19 - iter 54/69 - loss 0.38043869 - samples/sec: 60.26 - lr: 0.050000
2021-05-24 13:50:26,358 epoch 19 - iter 60/69 - loss 0.37608661 - samples/sec: 59.60 - lr: 0.050000
2021-05-24 13:50:27,966 epoch 19 - iter 66/69 - loss 0.37721298 - samples/sec: 59.71 - lr: 0.050000
2021-05-24 13:50:28,584 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:28,584 EPOCH 19 done: loss 0.3682 - lr 0.0500000
2021-05-24 13:50:30,038 DEV : loss 0.1522209346294403 - score 0.9717
2021-05-24 13:50:30,062 BAD EPOCHS (no improvement): 2
2021-05-24 13:50:30,062 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:31,679 epoch 20 - iter 6/69 - loss 0.37603664 - samples/sec: 59.37 - lr: 0.050000
2021-05-24 13:50:33,291 epoch 20 - iter 12/69 - loss 0.36750552 - samples/sec: 59.60 - lr: 0.050000
2021-05-24 13:50:34,904 epoch 20 - iter 18/69 - loss 0.34505426 - samples/sec: 59.51 - lr: 0.050000
2021-05-24 13:50:36,526 epoch 20 - iter 24/69 - loss 0.29674989 - samples/sec: 59.21 - lr: 0.050000
2021-05-24 13:50:38,124 epoch 20 - iter 30/69 - loss 0.29418427 - samples/sec: 60.09 - lr: 0.050000
2021-05-24 13:50:39,722 epoch 20 - iter 36/69 - loss 0.31517926 - samples/sec: 60.08 - lr: 0.050000
2021-05-24 13:50:41,319 epoch 20 - iter 42/69 - loss 0.31091256 - samples/sec: 60.16 - lr: 0.050000
2021-05-24 13:50:42,875 epoch 20 - iter 48/69 - loss 0.29861333 - samples/sec: 61.69 - lr: 0.050000
2021-05-24 13:50:44,476 epoch 20 - iter 54/69 - loss 0.29206803 - samples/sec: 60.00 - lr: 0.050000
2021-05-24 13:50:46,087 epoch 20 - iter 60/69 - loss 0.29132671 - samples/sec: 59.61 - lr: 0.050000
2021-05-24 13:50:47,693 epoch 20 - iter 66/69 - loss 0.29741953 - samples/sec: 59.79 - lr: 0.050000
2021-05-24 13:50:48,340 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:48,340 EPOCH 20 done: loss 0.2940 - lr 0.0500000
2021-05-24 13:50:49,794 DEV : loss 0.1799042969942093 - score 0.9713
2021-05-24 13:50:49,818 BAD EPOCHS (no improvement): 3
2021-05-24 13:50:49,818 ----------------------------------------------------------------------------------------------------
2021-05-24 13:50:51,421 epoch 21 - iter 6/69 - loss 0.35351670 - samples/sec: 59.93 - lr: 0.050000
2021-05-24 13:50:53,034 epoch 21 - iter 12/69 - loss 0.32909002 - samples/sec: 59.52 - lr: 0.050000
2021-05-24 13:50:54,644 epoch 21 - iter 18/69 - loss 0.35916771 - samples/sec: 59.66 - lr: 0.050000
2021-05-24 13:50:56,259 epoch 21 - iter 24/69 - loss 0.37386885 - samples/sec: 59.46 - lr: 0.050000
2021-05-24 13:50:57,856 epoch 21 - iter 30/69 - loss 0.34948552 - samples/sec: 60.13 - lr: 0.050000
2021-05-24 13:50:59,466 epoch 21 - iter 36/69 - loss 0.33628435 - samples/sec: 59.63 - lr: 0.050000
2021-05-24 13:51:01,060 epoch 21 - iter 42/69 - loss 0.33045370 - samples/sec: 60.25 - lr: 0.050000
2021-05-24 13:51:02,651 epoch 21 - iter 48/69 - loss 0.32823250 - samples/sec: 60.35 - lr: 0.050000
2021-05-24 13:51:04,275 epoch 21 - iter 54/69 - loss 0.31766474 - samples/sec: 59.15 - lr: 0.050000
2021-05-24 13:51:05,877 epoch 21 - iter 60/69 - loss 0.33783645 - samples/sec: 59.91 - lr: 0.050000
2021-05-24 13:51:07,458 epoch 21 - iter 66/69 - loss 0.32333575 - samples/sec: 60.75 - lr: 0.050000
2021-05-24 13:51:08,102 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:08,103 EPOCH 21 done: loss 0.3160 - lr 0.0500000
2021-05-24 13:51:09,556 DEV : loss 0.1532195657491684 - score 0.9736
Epoch    21: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 13:51:09,580 BAD EPOCHS (no improvement): 4
2021-05-24 13:51:09,580 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:11,179 epoch 22 - iter 6/69 - loss 0.37519125 - samples/sec: 60.08 - lr: 0.025000
2021-05-24 13:51:12,767 epoch 22 - iter 12/69 - loss 0.31936652 - samples/sec: 60.47 - lr: 0.025000
2021-05-24 13:51:14,372 epoch 22 - iter 18/69 - loss 0.35194396 - samples/sec: 59.85 - lr: 0.025000
2021-05-24 13:51:15,965 epoch 22 - iter 24/69 - loss 0.30872662 - samples/sec: 60.28 - lr: 0.025000
2021-05-24 13:51:17,570 epoch 22 - iter 30/69 - loss 0.30297001 - samples/sec: 59.83 - lr: 0.025000
2021-05-24 13:51:19,175 epoch 22 - iter 36/69 - loss 0.28574384 - samples/sec: 59.82 - lr: 0.025000
2021-05-24 13:51:20,772 epoch 22 - iter 42/69 - loss 0.29205009 - samples/sec: 60.12 - lr: 0.025000
2021-05-24 13:51:22,384 epoch 22 - iter 48/69 - loss 0.29644543 - samples/sec: 59.57 - lr: 0.025000
2021-05-24 13:51:24,007 epoch 22 - iter 54/69 - loss 0.27888049 - samples/sec: 59.15 - lr: 0.025000
2021-05-24 13:51:25,645 epoch 22 - iter 60/69 - loss 0.27584041 - samples/sec: 58.64 - lr: 0.025000
2021-05-24 13:51:27,244 epoch 22 - iter 66/69 - loss 0.27476466 - samples/sec: 60.05 - lr: 0.025000
2021-05-24 13:51:27,890 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:27,891 EPOCH 22 done: loss 0.2704 - lr 0.0250000
2021-05-24 13:51:29,346 DEV : loss 0.19661377370357513 - score 0.9666
2021-05-24 13:51:29,369 BAD EPOCHS (no improvement): 1
2021-05-24 13:51:29,370 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:31,012 epoch 23 - iter 6/69 - loss 0.31753278 - samples/sec: 58.47 - lr: 0.025000
2021-05-24 13:51:32,638 epoch 23 - iter 12/69 - loss 0.20481233 - samples/sec: 59.07 - lr: 0.025000
2021-05-24 13:51:34,268 epoch 23 - iter 18/69 - loss 0.19188776 - samples/sec: 58.91 - lr: 0.025000
2021-05-24 13:51:35,898 epoch 23 - iter 24/69 - loss 0.21573114 - samples/sec: 58.92 - lr: 0.025000
2021-05-24 13:51:37,552 epoch 23 - iter 30/69 - loss 0.26530884 - samples/sec: 58.05 - lr: 0.025000
2021-05-24 13:51:39,171 epoch 23 - iter 36/69 - loss 0.26531766 - samples/sec: 59.30 - lr: 0.025000
2021-05-24 13:51:40,793 epoch 23 - iter 42/69 - loss 0.26874995 - samples/sec: 59.23 - lr: 0.025000
2021-05-24 13:51:42,384 epoch 23 - iter 48/69 - loss 0.26156136 - samples/sec: 60.34 - lr: 0.025000
2021-05-24 13:51:44,016 epoch 23 - iter 54/69 - loss 0.26929239 - samples/sec: 58.84 - lr: 0.025000
2021-05-24 13:51:45,644 epoch 23 - iter 60/69 - loss 0.26791073 - samples/sec: 58.99 - lr: 0.025000
2021-05-24 13:51:47,293 epoch 23 - iter 66/69 - loss 0.26620457 - samples/sec: 58.22 - lr: 0.025000
2021-05-24 13:51:47,954 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:47,954 EPOCH 23 done: loss 0.2765 - lr 0.0250000
2021-05-24 13:51:49,564 DEV : loss 0.16934768855571747 - score 0.9783
2021-05-24 13:51:49,587 BAD EPOCHS (no improvement): 2
2021-05-24 13:51:49,588 ----------------------------------------------------------------------------------------------------
2021-05-24 13:51:51,199 epoch 24 - iter 6/69 - loss 0.16716224 - samples/sec: 59.61 - lr: 0.025000
2021-05-24 13:51:52,824 epoch 24 - iter 12/69 - loss 0.15699614 - samples/sec: 59.07 - lr: 0.025000
2021-05-24 13:51:54,472 epoch 24 - iter 18/69 - loss 0.17020263 - samples/sec: 58.27 - lr: 0.025000
2021-05-24 13:51:56,088 epoch 24 - iter 24/69 - loss 0.23475184 - samples/sec: 59.45 - lr: 0.025000
2021-05-24 13:51:57,720 epoch 24 - iter 30/69 - loss 0.22175653 - samples/sec: 58.83 - lr: 0.025000
2021-05-24 13:51:59,309 epoch 24 - iter 36/69 - loss 0.25000003 - samples/sec: 60.42 - lr: 0.025000
2021-05-24 13:52:00,920 epoch 24 - iter 42/69 - loss 0.24113945 - samples/sec: 59.62 - lr: 0.025000
2021-05-24 13:52:02,560 epoch 24 - iter 48/69 - loss 0.24813647 - samples/sec: 58.53 - lr: 0.025000
2021-05-24 13:52:04,172 epoch 24 - iter 54/69 - loss 0.24449477 - samples/sec: 59.60 - lr: 0.025000
2021-05-24 13:52:05,805 epoch 24 - iter 60/69 - loss 0.24892274 - samples/sec: 58.79 - lr: 0.025000
2021-05-24 13:52:07,428 epoch 24 - iter 66/69 - loss 0.24610587 - samples/sec: 59.16 - lr: 0.025000
2021-05-24 13:52:08,075 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:08,075 EPOCH 24 done: loss 0.2376 - lr 0.0250000
2021-05-24 13:52:09,533 DEV : loss 0.1557433158159256 - score 0.9759
2021-05-24 13:52:09,557 BAD EPOCHS (no improvement): 3
2021-05-24 13:52:09,557 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:11,183 epoch 25 - iter 6/69 - loss 0.23263002 - samples/sec: 59.07 - lr: 0.025000
2021-05-24 13:52:12,825 epoch 25 - iter 12/69 - loss 0.22932319 - samples/sec: 58.48 - lr: 0.025000
2021-05-24 13:52:14,429 epoch 25 - iter 18/69 - loss 0.23811221 - samples/sec: 59.88 - lr: 0.025000
2021-05-24 13:52:16,043 epoch 25 - iter 24/69 - loss 0.20407472 - samples/sec: 59.47 - lr: 0.025000
2021-05-24 13:52:17,664 epoch 25 - iter 30/69 - loss 0.22733611 - samples/sec: 59.27 - lr: 0.025000
2021-05-24 13:52:19,296 epoch 25 - iter 36/69 - loss 0.21720248 - samples/sec: 58.83 - lr: 0.025000
2021-05-24 13:52:20,890 epoch 25 - iter 42/69 - loss 0.23154907 - samples/sec: 60.22 - lr: 0.025000
2021-05-24 13:52:22,521 epoch 25 - iter 48/69 - loss 0.24039617 - samples/sec: 58.89 - lr: 0.025000
2021-05-24 13:52:24,142 epoch 25 - iter 54/69 - loss 0.24424152 - samples/sec: 59.25 - lr: 0.025000
2021-05-24 13:52:25,776 epoch 25 - iter 60/69 - loss 0.25285942 - samples/sec: 58.76 - lr: 0.025000
2021-05-24 13:52:27,389 epoch 25 - iter 66/69 - loss 0.26691980 - samples/sec: 59.52 - lr: 0.025000
2021-05-24 13:52:28,047 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:28,048 EPOCH 25 done: loss 0.2637 - lr 0.0250000
2021-05-24 13:52:29,503 DEV : loss 0.1441795974969864 - score 0.9782
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 13:52:29,526 BAD EPOCHS (no improvement): 4
2021-05-24 13:52:29,526 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:31,153 epoch 26 - iter 6/69 - loss 0.20668638 - samples/sec: 59.05 - lr: 0.012500
2021-05-24 13:52:32,766 epoch 26 - iter 12/69 - loss 0.19013120 - samples/sec: 59.51 - lr: 0.012500
2021-05-24 13:52:34,356 epoch 26 - iter 18/69 - loss 0.16214035 - samples/sec: 60.41 - lr: 0.012500
2021-05-24 13:52:35,960 epoch 26 - iter 24/69 - loss 0.16691539 - samples/sec: 59.84 - lr: 0.012500
2021-05-24 13:52:37,602 epoch 26 - iter 30/69 - loss 0.19152582 - samples/sec: 58.49 - lr: 0.012500
2021-05-24 13:52:39,209 epoch 26 - iter 36/69 - loss 0.19885615 - samples/sec: 59.74 - lr: 0.012500
2021-05-24 13:52:40,843 epoch 26 - iter 42/69 - loss 0.20997283 - samples/sec: 58.80 - lr: 0.012500
2021-05-24 13:52:42,469 epoch 26 - iter 48/69 - loss 0.21998440 - samples/sec: 59.03 - lr: 0.012500
2021-05-24 13:52:44,103 epoch 26 - iter 54/69 - loss 0.20441285 - samples/sec: 58.77 - lr: 0.012500
2021-05-24 13:52:45,730 epoch 26 - iter 60/69 - loss 0.22004776 - samples/sec: 59.04 - lr: 0.012500
2021-05-24 13:52:47,354 epoch 26 - iter 66/69 - loss 0.20989616 - samples/sec: 59.13 - lr: 0.012500
2021-05-24 13:52:47,999 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:48,000 EPOCH 26 done: loss 0.2252 - lr 0.0125000
2021-05-24 13:52:49,462 DEV : loss 0.14436353743076324 - score 0.9714
2021-05-24 13:52:49,486 BAD EPOCHS (no improvement): 1
2021-05-24 13:52:49,486 ----------------------------------------------------------------------------------------------------
2021-05-24 13:52:51,116 epoch 27 - iter 6/69 - loss 0.20250841 - samples/sec: 58.91 - lr: 0.012500
2021-05-24 13:52:52,717 epoch 27 - iter 12/69 - loss 0.28057430 - samples/sec: 60.00 - lr: 0.012500
2021-05-24 13:52:54,343 epoch 27 - iter 18/69 - loss 0.29962030 - samples/sec: 59.06 - lr: 0.012500
2021-05-24 13:52:55,961 epoch 27 - iter 24/69 - loss 0.24969836 - samples/sec: 59.32 - lr: 0.012500
2021-05-24 13:52:57,595 epoch 27 - iter 30/69 - loss 0.29762682 - samples/sec: 58.78 - lr: 0.012500
2021-05-24 13:52:59,229 epoch 27 - iter 36/69 - loss 0.25927526 - samples/sec: 58.77 - lr: 0.012500
2021-05-24 13:53:00,867 epoch 27 - iter 42/69 - loss 0.26275068 - samples/sec: 58.62 - lr: 0.012500
2021-05-24 13:53:02,480 epoch 27 - iter 48/69 - loss 0.25569747 - samples/sec: 59.52 - lr: 0.012500
2021-05-24 13:53:04,096 epoch 27 - iter 54/69 - loss 0.25016254 - samples/sec: 59.44 - lr: 0.012500
2021-05-24 13:53:05,711 epoch 27 - iter 60/69 - loss 0.24189224 - samples/sec: 59.45 - lr: 0.012500
2021-05-24 13:53:07,346 epoch 27 - iter 66/69 - loss 0.24775340 - samples/sec: 58.75 - lr: 0.012500
2021-05-24 13:53:07,986 ----------------------------------------------------------------------------------------------------
2021-05-24 13:53:07,986 EPOCH 27 done: loss 0.2741 - lr 0.0125000
2021-05-24 13:53:09,439 DEV : loss 0.14546509087085724 - score 0.9783
2021-05-24 13:53:09,462 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:53:19,575 ----------------------------------------------------------------------------------------------------
2021-05-24 13:53:21,206 epoch 28 - iter 6/69 - loss 0.27061268 - samples/sec: 58.91 - lr: 0.012500
2021-05-24 13:53:22,809 epoch 28 - iter 12/69 - loss 0.23718743 - samples/sec: 59.92 - lr: 0.012500
2021-05-24 13:53:24,423 epoch 28 - iter 18/69 - loss 0.19185183 - samples/sec: 59.49 - lr: 0.012500
2021-05-24 13:53:26,068 epoch 28 - iter 24/69 - loss 0.20739315 - samples/sec: 58.38 - lr: 0.012500
2021-05-24 13:53:27,673 epoch 28 - iter 30/69 - loss 0.24680953 - samples/sec: 59.83 - lr: 0.012500
2021-05-24 13:53:29,307 epoch 28 - iter 36/69 - loss 0.24359673 - samples/sec: 58.76 - lr: 0.012500
2021-05-24 13:53:30,937 epoch 28 - iter 42/69 - loss 0.25248212 - samples/sec: 58.90 - lr: 0.012500
2021-05-24 13:53:32,564 epoch 28 - iter 48/69 - loss 0.25907969 - samples/sec: 59.04 - lr: 0.012500
2021-05-24 13:53:34,191 epoch 28 - iter 54/69 - loss 0.24196916 - samples/sec: 58.99 - lr: 0.012500
2021-05-24 13:53:35,818 epoch 28 - iter 60/69 - loss 0.23262417 - samples/sec: 59.04 - lr: 0.012500
2021-05-24 13:53:37,451 epoch 28 - iter 66/69 - loss 0.22895052 - samples/sec: 58.82 - lr: 0.012500
2021-05-24 13:53:38,097 ----------------------------------------------------------------------------------------------------
2021-05-24 13:53:38,097 EPOCH 28 done: loss 0.2338 - lr 0.0125000
2021-05-24 13:53:39,705 DEV : loss 0.1377052366733551 - score 0.9782
2021-05-24 13:53:39,728 BAD EPOCHS (no improvement): 1
2021-05-24 13:53:39,728 ----------------------------------------------------------------------------------------------------
2021-05-24 13:53:41,332 epoch 29 - iter 6/69 - loss 0.18444512 - samples/sec: 59.89 - lr: 0.012500
2021-05-24 13:53:42,958 epoch 29 - iter 12/69 - loss 0.24273446 - samples/sec: 59.04 - lr: 0.012500
2021-05-24 13:53:44,575 epoch 29 - iter 18/69 - loss 0.22834731 - samples/sec: 59.40 - lr: 0.012500
2021-05-24 13:53:46,223 epoch 29 - iter 24/69 - loss 0.23840945 - samples/sec: 58.26 - lr: 0.012500
2021-05-24 13:53:47,826 epoch 29 - iter 30/69 - loss 0.23129508 - samples/sec: 59.90 - lr: 0.012500
2021-05-24 13:53:49,482 epoch 29 - iter 36/69 - loss 0.22811416 - samples/sec: 58.00 - lr: 0.012500
2021-05-24 13:53:51,112 epoch 29 - iter 42/69 - loss 0.24309406 - samples/sec: 58.92 - lr: 0.012500
2021-05-24 13:53:52,727 epoch 29 - iter 48/69 - loss 0.23984910 - samples/sec: 59.47 - lr: 0.012500
2021-05-24 13:53:54,358 epoch 29 - iter 54/69 - loss 0.24277996 - samples/sec: 58.88 - lr: 0.012500
2021-05-24 13:53:55,986 epoch 29 - iter 60/69 - loss 0.24039383 - samples/sec: 59.00 - lr: 0.012500
2021-05-24 13:53:57,627 epoch 29 - iter 66/69 - loss 0.22874052 - samples/sec: 58.49 - lr: 0.012500
2021-05-24 13:53:58,275 ----------------------------------------------------------------------------------------------------
2021-05-24 13:53:58,275 EPOCH 29 done: loss 0.2213 - lr 0.0125000
2021-05-24 13:53:59,732 DEV : loss 0.14849714934825897 - score 0.974
2021-05-24 13:53:59,756 BAD EPOCHS (no improvement): 2
2021-05-24 13:53:59,756 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:01,358 epoch 30 - iter 6/69 - loss 0.17137890 - samples/sec: 59.93 - lr: 0.012500
2021-05-24 13:54:02,979 epoch 30 - iter 12/69 - loss 0.18852403 - samples/sec: 59.23 - lr: 0.012500
2021-05-24 13:54:04,621 epoch 30 - iter 18/69 - loss 0.19108271 - samples/sec: 58.48 - lr: 0.012500
2021-05-24 13:54:06,252 epoch 30 - iter 24/69 - loss 0.21849810 - samples/sec: 58.88 - lr: 0.012500
2021-05-24 13:54:07,904 epoch 30 - iter 30/69 - loss 0.22554619 - samples/sec: 58.12 - lr: 0.012500
2021-05-24 13:54:09,524 epoch 30 - iter 36/69 - loss 0.21702716 - samples/sec: 59.29 - lr: 0.012500
2021-05-24 13:54:11,172 epoch 30 - iter 42/69 - loss 0.20834307 - samples/sec: 58.25 - lr: 0.012500
2021-05-24 13:54:12,809 epoch 30 - iter 48/69 - loss 0.20473785 - samples/sec: 58.68 - lr: 0.012500
2021-05-24 13:54:14,431 epoch 30 - iter 54/69 - loss 0.20684594 - samples/sec: 59.20 - lr: 0.012500
2021-05-24 13:54:16,025 epoch 30 - iter 60/69 - loss 0.19256013 - samples/sec: 60.24 - lr: 0.012500
2021-05-24 13:54:17,658 epoch 30 - iter 66/69 - loss 0.20087006 - samples/sec: 58.79 - lr: 0.012500
2021-05-24 13:54:18,309 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:18,309 EPOCH 30 done: loss 0.1970 - lr 0.0125000
2021-05-24 13:54:19,768 DEV : loss 0.1419055312871933 - score 0.9718
2021-05-24 13:54:19,792 BAD EPOCHS (no improvement): 3
2021-05-24 13:54:20,868 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:20,868 Testing using best model ...
2021-05-24 13:54:20,868 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/fra.sdrt.annodis/best-model.pt
2021-05-24 13:54:29,891 0.8814	0.9858	0.9306
2021-05-24 13:54:29,891 
Results:
- F1-score (micro) 0.9306
- F1-score (macro) 0.9306

By class:
SENT       tp: 208 - fp: 28 - fn: 3 - precision: 0.8814 - recall: 0.9858 - f1-score: 0.9306
2021-05-24 13:54:29,891 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert/
2021-05-24 13:54:29,916 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert
2021-05-24 13:54:29,918 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert/sent_train.txt
2021-05-24 13:54:29,918 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert/sent_dev.txt
2021-05-24 13:54:29,918 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert/sent_test.txt
Corpus: 1046 train + 371 dev + 330 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 13:54:36,318 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 13:54:36,321 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Corpus: "Corpus: 1046 train + 371 dev + 330 test sentences"
2021-05-24 13:54:36,321 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Parameters:
2021-05-24 13:54:36,321  - learning_rate: "0.1"
2021-05-24 13:54:36,321  - mini_batch_size: "16"
2021-05-24 13:54:36,321  - patience: "3"
2021-05-24 13:54:36,321  - anneal_factor: "0.5"
2021-05-24 13:54:36,321  - max_epochs: "30"
2021-05-24 13:54:36,321  - shuffle: "True"
2021-05-24 13:54:36,321  - train_with_dev: "False"
2021-05-24 13:54:36,321  - batch_growth_annealing: "False"
2021-05-24 13:54:36,321 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert"
2021-05-24 13:54:36,321 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Device: cuda:0
2021-05-24 13:54:36,321 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:36,321 Embeddings storage mode: cpu
2021-05-24 13:54:36,323 ----------------------------------------------------------------------------------------------------
2021-05-24 13:54:40,953 epoch 1 - iter 6/66 - loss 11.45166159 - samples/sec: 20.74 - lr: 0.100000
2021-05-24 13:54:45,656 epoch 1 - iter 12/66 - loss 9.42005014 - samples/sec: 20.41 - lr: 0.100000
2021-05-24 13:54:50,327 epoch 1 - iter 18/66 - loss 8.63603012 - samples/sec: 20.56 - lr: 0.100000
2021-05-24 13:54:55,133 epoch 1 - iter 24/66 - loss 8.19719106 - samples/sec: 19.98 - lr: 0.100000
2021-05-24 13:54:59,823 epoch 1 - iter 30/66 - loss 7.58733931 - samples/sec: 20.47 - lr: 0.100000
2021-05-24 13:55:04,481 epoch 1 - iter 36/66 - loss 7.23211724 - samples/sec: 20.61 - lr: 0.100000
2021-05-24 13:55:09,179 epoch 1 - iter 42/66 - loss 6.67900533 - samples/sec: 20.44 - lr: 0.100000
2021-05-24 13:55:13,850 epoch 1 - iter 48/66 - loss 6.29369000 - samples/sec: 20.55 - lr: 0.100000
2021-05-24 13:55:18,492 epoch 1 - iter 54/66 - loss 5.93846386 - samples/sec: 20.68 - lr: 0.100000
2021-05-24 13:55:23,012 epoch 1 - iter 60/66 - loss 5.64470521 - samples/sec: 21.24 - lr: 0.100000
2021-05-24 13:55:27,173 epoch 1 - iter 66/66 - loss 5.34278155 - samples/sec: 23.08 - lr: 0.100000
2021-05-24 13:55:27,173 ----------------------------------------------------------------------------------------------------
2021-05-24 13:55:27,173 EPOCH 1 done: loss 5.3428 - lr 0.1000000
2021-05-24 13:55:38,887 DEV : loss 1.023491621017456 - score 0.8435
2021-05-24 13:55:38,922 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:55:39,994 ----------------------------------------------------------------------------------------------------
2021-05-24 13:55:41,790 epoch 2 - iter 6/66 - loss 3.14426132 - samples/sec: 53.48 - lr: 0.100000
2021-05-24 13:55:43,428 epoch 2 - iter 12/66 - loss 2.62724673 - samples/sec: 58.60 - lr: 0.100000
2021-05-24 13:55:45,095 epoch 2 - iter 18/66 - loss 2.43687813 - samples/sec: 57.63 - lr: 0.100000
2021-05-24 13:55:46,748 epoch 2 - iter 24/66 - loss 2.22368653 - samples/sec: 58.09 - lr: 0.100000
2021-05-24 13:55:48,402 epoch 2 - iter 30/66 - loss 2.11515769 - samples/sec: 58.06 - lr: 0.100000
2021-05-24 13:55:50,054 epoch 2 - iter 36/66 - loss 2.00571920 - samples/sec: 58.12 - lr: 0.100000
2021-05-24 13:55:51,719 epoch 2 - iter 42/66 - loss 1.99510662 - samples/sec: 57.67 - lr: 0.100000
2021-05-24 13:55:53,365 epoch 2 - iter 48/66 - loss 2.07130587 - samples/sec: 58.33 - lr: 0.100000
2021-05-24 13:55:55,016 epoch 2 - iter 54/66 - loss 1.98991310 - samples/sec: 58.19 - lr: 0.100000
2021-05-24 13:55:56,643 epoch 2 - iter 60/66 - loss 1.96876962 - samples/sec: 59.03 - lr: 0.100000
2021-05-24 13:55:58,178 epoch 2 - iter 66/66 - loss 1.96366116 - samples/sec: 62.56 - lr: 0.100000
2021-05-24 13:55:58,178 ----------------------------------------------------------------------------------------------------
2021-05-24 13:55:58,178 EPOCH 2 done: loss 1.9637 - lr 0.1000000
2021-05-24 13:56:00,435 DEV : loss 0.5400625467300415 - score 0.8994
2021-05-24 13:56:00,470 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:56:10,520 ----------------------------------------------------------------------------------------------------
2021-05-24 13:56:12,174 epoch 3 - iter 6/66 - loss 1.67200116 - samples/sec: 58.07 - lr: 0.100000
2021-05-24 13:56:13,808 epoch 3 - iter 12/66 - loss 1.32420814 - samples/sec: 58.78 - lr: 0.100000
2021-05-24 13:56:15,432 epoch 3 - iter 18/66 - loss 1.48886983 - samples/sec: 59.15 - lr: 0.100000
2021-05-24 13:56:17,086 epoch 3 - iter 24/66 - loss 1.42503316 - samples/sec: 58.05 - lr: 0.100000
2021-05-24 13:56:18,687 epoch 3 - iter 30/66 - loss 1.43790917 - samples/sec: 59.96 - lr: 0.100000
2021-05-24 13:56:20,322 epoch 3 - iter 36/66 - loss 1.43776473 - samples/sec: 58.76 - lr: 0.100000
2021-05-24 13:56:21,963 epoch 3 - iter 42/66 - loss 1.37443406 - samples/sec: 58.50 - lr: 0.100000
2021-05-24 13:56:23,599 epoch 3 - iter 48/66 - loss 1.36958913 - samples/sec: 58.70 - lr: 0.100000
2021-05-24 13:56:25,231 epoch 3 - iter 54/66 - loss 1.29686195 - samples/sec: 58.84 - lr: 0.100000
2021-05-24 13:56:26,896 epoch 3 - iter 60/66 - loss 1.26677231 - samples/sec: 57.66 - lr: 0.100000
2021-05-24 13:56:28,407 epoch 3 - iter 66/66 - loss 1.30578145 - samples/sec: 63.55 - lr: 0.100000
2021-05-24 13:56:28,408 ----------------------------------------------------------------------------------------------------
2021-05-24 13:56:28,408 EPOCH 3 done: loss 1.3058 - lr 0.1000000
2021-05-24 13:56:30,665 DEV : loss 0.7795769572257996 - score 0.878
2021-05-24 13:56:30,700 BAD EPOCHS (no improvement): 1
2021-05-24 13:56:30,700 ----------------------------------------------------------------------------------------------------
2021-05-24 13:56:32,379 epoch 4 - iter 6/66 - loss 0.74304058 - samples/sec: 57.22 - lr: 0.100000
2021-05-24 13:56:34,045 epoch 4 - iter 12/66 - loss 1.03210180 - samples/sec: 57.62 - lr: 0.100000
2021-05-24 13:56:35,721 epoch 4 - iter 18/66 - loss 1.12081848 - samples/sec: 57.29 - lr: 0.100000
2021-05-24 13:56:37,355 epoch 4 - iter 24/66 - loss 1.09138140 - samples/sec: 58.78 - lr: 0.100000
2021-05-24 13:56:39,006 epoch 4 - iter 30/66 - loss 1.05564231 - samples/sec: 58.16 - lr: 0.100000
2021-05-24 13:56:40,662 epoch 4 - iter 36/66 - loss 1.04078642 - samples/sec: 57.98 - lr: 0.100000
2021-05-24 13:56:42,314 epoch 4 - iter 42/66 - loss 1.02135303 - samples/sec: 58.15 - lr: 0.100000
2021-05-24 13:56:43,956 epoch 4 - iter 48/66 - loss 1.04316272 - samples/sec: 58.46 - lr: 0.100000
2021-05-24 13:56:45,595 epoch 4 - iter 54/66 - loss 1.04678231 - samples/sec: 58.60 - lr: 0.100000
2021-05-24 13:56:47,274 epoch 4 - iter 60/66 - loss 1.06330181 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 13:56:48,771 epoch 4 - iter 66/66 - loss 1.04156536 - samples/sec: 64.15 - lr: 0.100000
2021-05-24 13:56:48,771 ----------------------------------------------------------------------------------------------------
2021-05-24 13:56:48,771 EPOCH 4 done: loss 1.0416 - lr 0.1000000
2021-05-24 13:56:51,030 DEV : loss 0.4320473074913025 - score 0.9141
2021-05-24 13:56:51,066 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:57:00,843 ----------------------------------------------------------------------------------------------------
2021-05-24 13:57:02,511 epoch 5 - iter 6/66 - loss 0.79501943 - samples/sec: 57.57 - lr: 0.100000
2021-05-24 13:57:04,179 epoch 5 - iter 12/66 - loss 0.78966635 - samples/sec: 57.59 - lr: 0.100000
2021-05-24 13:57:05,820 epoch 5 - iter 18/66 - loss 0.77717178 - samples/sec: 58.52 - lr: 0.100000
2021-05-24 13:57:07,493 epoch 5 - iter 24/66 - loss 0.81085470 - samples/sec: 57.40 - lr: 0.100000
2021-05-24 13:57:09,130 epoch 5 - iter 30/66 - loss 0.83374683 - samples/sec: 58.65 - lr: 0.100000
2021-05-24 13:57:10,748 epoch 5 - iter 36/66 - loss 0.83717915 - samples/sec: 59.36 - lr: 0.100000
2021-05-24 13:57:12,349 epoch 5 - iter 42/66 - loss 0.79200977 - samples/sec: 59.96 - lr: 0.100000
2021-05-24 13:57:14,007 epoch 5 - iter 48/66 - loss 0.76888474 - samples/sec: 57.92 - lr: 0.100000
2021-05-24 13:57:15,674 epoch 5 - iter 54/66 - loss 0.80128694 - samples/sec: 57.61 - lr: 0.100000
2021-05-24 13:57:17,344 epoch 5 - iter 60/66 - loss 0.80249365 - samples/sec: 57.50 - lr: 0.100000
2021-05-24 13:57:18,850 epoch 5 - iter 66/66 - loss 0.79791738 - samples/sec: 63.79 - lr: 0.100000
2021-05-24 13:57:18,850 ----------------------------------------------------------------------------------------------------
2021-05-24 13:57:18,850 EPOCH 5 done: loss 0.7979 - lr 0.1000000
2021-05-24 13:57:21,291 DEV : loss 1.2566356658935547 - score 0.8416
2021-05-24 13:57:21,326 BAD EPOCHS (no improvement): 1
2021-05-24 13:57:21,327 ----------------------------------------------------------------------------------------------------
2021-05-24 13:57:22,984 epoch 6 - iter 6/66 - loss 0.69239096 - samples/sec: 57.94 - lr: 0.100000
2021-05-24 13:57:24,633 epoch 6 - iter 12/66 - loss 0.95891257 - samples/sec: 58.25 - lr: 0.100000
2021-05-24 13:57:26,261 epoch 6 - iter 18/66 - loss 0.83299447 - samples/sec: 58.98 - lr: 0.100000
2021-05-24 13:57:27,921 epoch 6 - iter 24/66 - loss 0.87234074 - samples/sec: 57.83 - lr: 0.100000
2021-05-24 13:57:29,586 epoch 6 - iter 30/66 - loss 0.85627141 - samples/sec: 57.70 - lr: 0.100000
2021-05-24 13:57:31,250 epoch 6 - iter 36/66 - loss 0.89674651 - samples/sec: 57.69 - lr: 0.100000
2021-05-24 13:57:32,914 epoch 6 - iter 42/66 - loss 0.86564305 - samples/sec: 57.72 - lr: 0.100000
2021-05-24 13:57:34,596 epoch 6 - iter 48/66 - loss 0.85978830 - samples/sec: 57.07 - lr: 0.100000
2021-05-24 13:57:36,266 epoch 6 - iter 54/66 - loss 0.83806715 - samples/sec: 57.51 - lr: 0.100000
2021-05-24 13:57:37,914 epoch 6 - iter 60/66 - loss 0.80707959 - samples/sec: 58.27 - lr: 0.100000
2021-05-24 13:57:39,424 epoch 6 - iter 66/66 - loss 0.80320136 - samples/sec: 63.59 - lr: 0.100000
2021-05-24 13:57:39,424 ----------------------------------------------------------------------------------------------------
2021-05-24 13:57:39,424 EPOCH 6 done: loss 0.8032 - lr 0.1000000
2021-05-24 13:57:41,685 DEV : loss 0.4112832248210907 - score 0.9231
2021-05-24 13:57:41,721 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:57:51,280 ----------------------------------------------------------------------------------------------------
2021-05-24 13:57:52,949 epoch 7 - iter 6/66 - loss 0.75594204 - samples/sec: 57.55 - lr: 0.100000
2021-05-24 13:57:54,613 epoch 7 - iter 12/66 - loss 0.69999858 - samples/sec: 57.73 - lr: 0.100000
2021-05-24 13:57:56,275 epoch 7 - iter 18/66 - loss 0.75845564 - samples/sec: 57.74 - lr: 0.100000
2021-05-24 13:57:57,893 epoch 7 - iter 24/66 - loss 0.68591472 - samples/sec: 59.36 - lr: 0.100000
2021-05-24 13:57:59,541 epoch 7 - iter 30/66 - loss 0.68773587 - samples/sec: 58.27 - lr: 0.100000
2021-05-24 13:58:01,206 epoch 7 - iter 36/66 - loss 0.70276788 - samples/sec: 57.69 - lr: 0.100000
2021-05-24 13:58:02,866 epoch 7 - iter 42/66 - loss 0.71721543 - samples/sec: 57.86 - lr: 0.100000
2021-05-24 13:58:04,496 epoch 7 - iter 48/66 - loss 0.71551824 - samples/sec: 58.90 - lr: 0.100000
2021-05-24 13:58:06,144 epoch 7 - iter 54/66 - loss 0.72657254 - samples/sec: 58.29 - lr: 0.100000
2021-05-24 13:58:07,765 epoch 7 - iter 60/66 - loss 0.74072211 - samples/sec: 59.23 - lr: 0.100000
2021-05-24 13:58:09,242 epoch 7 - iter 66/66 - loss 0.73038687 - samples/sec: 65.01 - lr: 0.100000
2021-05-24 13:58:09,243 ----------------------------------------------------------------------------------------------------
2021-05-24 13:58:09,243 EPOCH 7 done: loss 0.7304 - lr 0.1000000
2021-05-24 13:58:11,503 DEV : loss 0.3827822804450989 - score 0.9324
2021-05-24 13:58:11,538 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 13:58:20,830 ----------------------------------------------------------------------------------------------------
2021-05-24 13:58:22,453 epoch 8 - iter 6/66 - loss 0.73437270 - samples/sec: 59.19 - lr: 0.100000
2021-05-24 13:58:24,108 epoch 8 - iter 12/66 - loss 0.73046919 - samples/sec: 58.03 - lr: 0.100000
2021-05-24 13:58:25,732 epoch 8 - iter 18/66 - loss 0.80539484 - samples/sec: 59.14 - lr: 0.100000
2021-05-24 13:58:27,356 epoch 8 - iter 24/66 - loss 0.80489840 - samples/sec: 59.13 - lr: 0.100000
2021-05-24 13:58:28,997 epoch 8 - iter 30/66 - loss 0.84471526 - samples/sec: 58.53 - lr: 0.100000
2021-05-24 13:58:30,617 epoch 8 - iter 36/66 - loss 0.81584679 - samples/sec: 59.28 - lr: 0.100000
2021-05-24 13:58:32,228 epoch 8 - iter 42/66 - loss 0.81322740 - samples/sec: 59.59 - lr: 0.100000
2021-05-24 13:58:33,856 epoch 8 - iter 48/66 - loss 0.81457036 - samples/sec: 58.98 - lr: 0.100000
2021-05-24 13:58:35,541 epoch 8 - iter 54/66 - loss 0.79181078 - samples/sec: 57.01 - lr: 0.100000
2021-05-24 13:58:37,154 epoch 8 - iter 60/66 - loss 0.77908463 - samples/sec: 59.51 - lr: 0.100000
2021-05-24 13:58:38,644 epoch 8 - iter 66/66 - loss 0.75541460 - samples/sec: 64.46 - lr: 0.100000
2021-05-24 13:58:38,644 ----------------------------------------------------------------------------------------------------
2021-05-24 13:58:38,645 EPOCH 8 done: loss 0.7554 - lr 0.1000000
2021-05-24 13:58:40,892 DEV : loss 0.34293481707572937 - score 0.9258
2021-05-24 13:58:40,927 BAD EPOCHS (no improvement): 1
2021-05-24 13:58:40,927 ----------------------------------------------------------------------------------------------------
2021-05-24 13:58:42,552 epoch 9 - iter 6/66 - loss 0.73917512 - samples/sec: 59.11 - lr: 0.100000
2021-05-24 13:58:44,177 epoch 9 - iter 12/66 - loss 0.65075795 - samples/sec: 59.10 - lr: 0.100000
2021-05-24 13:58:45,803 epoch 9 - iter 18/66 - loss 0.73770291 - samples/sec: 59.07 - lr: 0.100000
2021-05-24 13:58:47,454 epoch 9 - iter 24/66 - loss 0.70530021 - samples/sec: 58.17 - lr: 0.100000
2021-05-24 13:58:49,080 epoch 9 - iter 30/66 - loss 0.70510315 - samples/sec: 59.06 - lr: 0.100000
2021-05-24 13:58:50,716 epoch 9 - iter 36/66 - loss 0.71933552 - samples/sec: 58.69 - lr: 0.100000
2021-05-24 13:58:52,395 epoch 9 - iter 42/66 - loss 0.71685436 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 13:58:54,044 epoch 9 - iter 48/66 - loss 0.66921333 - samples/sec: 58.23 - lr: 0.100000
2021-05-24 13:58:55,706 epoch 9 - iter 54/66 - loss 0.65559640 - samples/sec: 57.76 - lr: 0.100000
2021-05-24 13:58:57,349 epoch 9 - iter 60/66 - loss 0.68114038 - samples/sec: 58.45 - lr: 0.100000
2021-05-24 13:58:58,868 epoch 9 - iter 66/66 - loss 0.69178734 - samples/sec: 63.23 - lr: 0.100000
2021-05-24 13:58:58,868 ----------------------------------------------------------------------------------------------------
2021-05-24 13:58:58,868 EPOCH 9 done: loss 0.6918 - lr 0.1000000
2021-05-24 13:59:01,300 DEV : loss 0.4883880317211151 - score 0.9237
2021-05-24 13:59:01,336 BAD EPOCHS (no improvement): 2
2021-05-24 13:59:01,336 ----------------------------------------------------------------------------------------------------
2021-05-24 13:59:02,994 epoch 10 - iter 6/66 - loss 0.70822612 - samples/sec: 57.93 - lr: 0.100000
2021-05-24 13:59:04,637 epoch 10 - iter 12/66 - loss 0.52195132 - samples/sec: 58.45 - lr: 0.100000
2021-05-24 13:59:06,299 epoch 10 - iter 18/66 - loss 0.49123897 - samples/sec: 57.79 - lr: 0.100000
2021-05-24 13:59:07,953 epoch 10 - iter 24/66 - loss 0.49840547 - samples/sec: 58.05 - lr: 0.100000
2021-05-24 13:59:09,559 epoch 10 - iter 30/66 - loss 0.54492615 - samples/sec: 59.78 - lr: 0.100000
2021-05-24 13:59:11,208 epoch 10 - iter 36/66 - loss 0.58270449 - samples/sec: 58.24 - lr: 0.100000
2021-05-24 13:59:12,877 epoch 10 - iter 42/66 - loss 0.59581525 - samples/sec: 57.52 - lr: 0.100000
2021-05-24 13:59:14,562 epoch 10 - iter 48/66 - loss 0.58066219 - samples/sec: 56.99 - lr: 0.100000
2021-05-24 13:59:16,236 epoch 10 - iter 54/66 - loss 0.57607046 - samples/sec: 57.37 - lr: 0.100000
2021-05-24 13:59:17,916 epoch 10 - iter 60/66 - loss 0.58610298 - samples/sec: 57.14 - lr: 0.100000
2021-05-24 13:59:19,431 epoch 10 - iter 66/66 - loss 0.57323484 - samples/sec: 63.39 - lr: 0.100000
2021-05-24 13:59:19,432 ----------------------------------------------------------------------------------------------------
2021-05-24 13:59:19,432 EPOCH 10 done: loss 0.5732 - lr 0.1000000
2021-05-24 13:59:21,683 DEV : loss 0.6669554114341736 - score 0.901
2021-05-24 13:59:21,719 BAD EPOCHS (no improvement): 3
2021-05-24 13:59:21,719 ----------------------------------------------------------------------------------------------------
2021-05-24 13:59:23,396 epoch 11 - iter 6/66 - loss 0.87424548 - samples/sec: 57.28 - lr: 0.100000
2021-05-24 13:59:25,021 epoch 11 - iter 12/66 - loss 0.67243952 - samples/sec: 59.09 - lr: 0.100000
2021-05-24 13:59:26,674 epoch 11 - iter 18/66 - loss 0.57193693 - samples/sec: 58.11 - lr: 0.100000
2021-05-24 13:59:28,369 epoch 11 - iter 24/66 - loss 0.55430693 - samples/sec: 56.65 - lr: 0.100000
2021-05-24 13:59:30,039 epoch 11 - iter 30/66 - loss 0.60323960 - samples/sec: 57.48 - lr: 0.100000
2021-05-24 13:59:31,694 epoch 11 - iter 36/66 - loss 0.59503317 - samples/sec: 58.02 - lr: 0.100000
2021-05-24 13:59:33,357 epoch 11 - iter 42/66 - loss 0.59987629 - samples/sec: 57.73 - lr: 0.100000
2021-05-24 13:59:34,994 epoch 11 - iter 48/66 - loss 0.58528516 - samples/sec: 58.69 - lr: 0.100000
2021-05-24 13:59:36,652 epoch 11 - iter 54/66 - loss 0.63507047 - samples/sec: 57.89 - lr: 0.100000
2021-05-24 13:59:38,319 epoch 11 - iter 60/66 - loss 0.63638536 - samples/sec: 57.61 - lr: 0.100000
2021-05-24 13:59:39,852 epoch 11 - iter 66/66 - loss 0.62012035 - samples/sec: 62.64 - lr: 0.100000
2021-05-24 13:59:39,852 ----------------------------------------------------------------------------------------------------
2021-05-24 13:59:39,852 EPOCH 11 done: loss 0.6201 - lr 0.1000000
2021-05-24 13:59:42,109 DEV : loss 0.9280904531478882 - score 0.8744
Epoch    11: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 13:59:42,145 BAD EPOCHS (no improvement): 4
2021-05-24 13:59:42,145 ----------------------------------------------------------------------------------------------------
2021-05-24 13:59:43,794 epoch 12 - iter 6/66 - loss 0.54731406 - samples/sec: 58.24 - lr: 0.050000
2021-05-24 13:59:45,433 epoch 12 - iter 12/66 - loss 0.41867556 - samples/sec: 58.58 - lr: 0.050000
2021-05-24 13:59:47,057 epoch 12 - iter 18/66 - loss 0.35322712 - samples/sec: 59.14 - lr: 0.050000
2021-05-24 13:59:48,717 epoch 12 - iter 24/66 - loss 0.35403798 - samples/sec: 57.86 - lr: 0.050000
2021-05-24 13:59:50,382 epoch 12 - iter 30/66 - loss 0.43787557 - samples/sec: 57.66 - lr: 0.050000
2021-05-24 13:59:52,087 epoch 12 - iter 36/66 - loss 0.43710529 - samples/sec: 56.32 - lr: 0.050000
2021-05-24 13:59:53,766 epoch 12 - iter 42/66 - loss 0.42380690 - samples/sec: 57.20 - lr: 0.050000
2021-05-24 13:59:55,430 epoch 12 - iter 48/66 - loss 0.42792905 - samples/sec: 57.69 - lr: 0.050000
2021-05-24 13:59:57,088 epoch 12 - iter 54/66 - loss 0.45823767 - samples/sec: 57.93 - lr: 0.050000
2021-05-24 13:59:58,747 epoch 12 - iter 60/66 - loss 0.45093959 - samples/sec: 57.87 - lr: 0.050000
2021-05-24 14:00:00,272 epoch 12 - iter 66/66 - loss 0.46168869 - samples/sec: 62.99 - lr: 0.050000
2021-05-24 14:00:00,272 ----------------------------------------------------------------------------------------------------
2021-05-24 14:00:00,272 EPOCH 12 done: loss 0.4617 - lr 0.0500000
2021-05-24 14:00:02,532 DEV : loss 0.32141968607902527 - score 0.9472
2021-05-24 14:00:02,568 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:00:11,923 ----------------------------------------------------------------------------------------------------
2021-05-24 14:00:13,581 epoch 13 - iter 6/66 - loss 0.41160167 - samples/sec: 57.94 - lr: 0.050000
2021-05-24 14:00:15,203 epoch 13 - iter 12/66 - loss 0.41202349 - samples/sec: 59.19 - lr: 0.050000
2021-05-24 14:00:16,854 epoch 13 - iter 18/66 - loss 0.36585979 - samples/sec: 58.16 - lr: 0.050000
2021-05-24 14:00:18,481 epoch 13 - iter 24/66 - loss 0.41812351 - samples/sec: 59.00 - lr: 0.050000
2021-05-24 14:00:20,145 epoch 13 - iter 30/66 - loss 0.41382860 - samples/sec: 57.73 - lr: 0.050000
2021-05-24 14:00:21,827 epoch 13 - iter 36/66 - loss 0.40407097 - samples/sec: 57.09 - lr: 0.050000
2021-05-24 14:00:23,487 epoch 13 - iter 42/66 - loss 0.39319772 - samples/sec: 57.85 - lr: 0.050000
2021-05-24 14:00:25,143 epoch 13 - iter 48/66 - loss 0.39672661 - samples/sec: 57.96 - lr: 0.050000
2021-05-24 14:00:26,789 epoch 13 - iter 54/66 - loss 0.38076204 - samples/sec: 58.35 - lr: 0.050000
2021-05-24 14:00:28,445 epoch 13 - iter 60/66 - loss 0.40144301 - samples/sec: 57.99 - lr: 0.050000
2021-05-24 14:00:29,948 epoch 13 - iter 66/66 - loss 0.41455368 - samples/sec: 63.90 - lr: 0.050000
2021-05-24 14:00:29,948 ----------------------------------------------------------------------------------------------------
2021-05-24 14:00:29,948 EPOCH 13 done: loss 0.4146 - lr 0.0500000
2021-05-24 14:00:32,375 DEV : loss 0.30701521039009094 - score 0.9481
2021-05-24 14:00:32,411 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:00:41,672 ----------------------------------------------------------------------------------------------------
2021-05-24 14:00:43,290 epoch 14 - iter 6/66 - loss 0.33358590 - samples/sec: 59.36 - lr: 0.050000
2021-05-24 14:00:44,928 epoch 14 - iter 12/66 - loss 0.33307286 - samples/sec: 58.66 - lr: 0.050000
2021-05-24 14:00:46,541 epoch 14 - iter 18/66 - loss 0.35774385 - samples/sec: 59.52 - lr: 0.050000
2021-05-24 14:00:48,159 epoch 14 - iter 24/66 - loss 0.37069314 - samples/sec: 59.35 - lr: 0.050000
2021-05-24 14:00:49,787 epoch 14 - iter 30/66 - loss 0.36390888 - samples/sec: 58.98 - lr: 0.050000
2021-05-24 14:00:51,420 epoch 14 - iter 36/66 - loss 0.36049643 - samples/sec: 58.78 - lr: 0.050000
2021-05-24 14:00:53,030 epoch 14 - iter 42/66 - loss 0.34648438 - samples/sec: 59.65 - lr: 0.050000
2021-05-24 14:00:54,677 epoch 14 - iter 48/66 - loss 0.36613335 - samples/sec: 58.32 - lr: 0.050000
2021-05-24 14:00:56,319 epoch 14 - iter 54/66 - loss 0.38012493 - samples/sec: 58.47 - lr: 0.050000
2021-05-24 14:00:57,982 epoch 14 - iter 60/66 - loss 0.37146928 - samples/sec: 57.74 - lr: 0.050000
2021-05-24 14:00:59,478 epoch 14 - iter 66/66 - loss 0.37264295 - samples/sec: 64.19 - lr: 0.050000
2021-05-24 14:00:59,478 ----------------------------------------------------------------------------------------------------
2021-05-24 14:00:59,478 EPOCH 14 done: loss 0.3726 - lr 0.0500000
2021-05-24 14:01:01,734 DEV : loss 0.28775474429130554 - score 0.9474
2021-05-24 14:01:01,770 BAD EPOCHS (no improvement): 1
2021-05-24 14:01:01,770 ----------------------------------------------------------------------------------------------------
2021-05-24 14:01:03,425 epoch 15 - iter 6/66 - loss 0.42385331 - samples/sec: 58.04 - lr: 0.050000
2021-05-24 14:01:05,082 epoch 15 - iter 12/66 - loss 0.41766648 - samples/sec: 57.96 - lr: 0.050000
2021-05-24 14:01:06,735 epoch 15 - iter 18/66 - loss 0.33552785 - samples/sec: 58.07 - lr: 0.050000
2021-05-24 14:01:08,389 epoch 15 - iter 24/66 - loss 0.32415484 - samples/sec: 58.08 - lr: 0.050000
2021-05-24 14:01:10,018 epoch 15 - iter 30/66 - loss 0.29891450 - samples/sec: 58.93 - lr: 0.050000
2021-05-24 14:01:11,651 epoch 15 - iter 36/66 - loss 0.30238391 - samples/sec: 58.80 - lr: 0.050000
2021-05-24 14:01:13,288 epoch 15 - iter 42/66 - loss 0.29466611 - samples/sec: 58.66 - lr: 0.050000
2021-05-24 14:01:14,941 epoch 15 - iter 48/66 - loss 0.28193568 - samples/sec: 58.10 - lr: 0.050000
2021-05-24 14:01:16,583 epoch 15 - iter 54/66 - loss 0.32855354 - samples/sec: 58.49 - lr: 0.050000
2021-05-24 14:01:18,227 epoch 15 - iter 60/66 - loss 0.33407083 - samples/sec: 58.39 - lr: 0.050000
2021-05-24 14:01:19,776 epoch 15 - iter 66/66 - loss 0.32413329 - samples/sec: 62.03 - lr: 0.050000
2021-05-24 14:01:19,776 ----------------------------------------------------------------------------------------------------
2021-05-24 14:01:19,776 EPOCH 15 done: loss 0.3241 - lr 0.0500000
2021-05-24 14:01:22,035 DEV : loss 0.3781345784664154 - score 0.9399
2021-05-24 14:01:22,070 BAD EPOCHS (no improvement): 2
2021-05-24 14:01:22,070 ----------------------------------------------------------------------------------------------------
2021-05-24 14:01:23,721 epoch 16 - iter 6/66 - loss 0.37497818 - samples/sec: 58.18 - lr: 0.050000
2021-05-24 14:01:25,340 epoch 16 - iter 12/66 - loss 0.33093634 - samples/sec: 59.30 - lr: 0.050000
2021-05-24 14:01:26,996 epoch 16 - iter 18/66 - loss 0.32868508 - samples/sec: 57.97 - lr: 0.050000
2021-05-24 14:01:28,638 epoch 16 - iter 24/66 - loss 0.33633322 - samples/sec: 58.51 - lr: 0.050000
2021-05-24 14:01:30,281 epoch 16 - iter 30/66 - loss 0.32348537 - samples/sec: 58.42 - lr: 0.050000
2021-05-24 14:01:31,951 epoch 16 - iter 36/66 - loss 0.33667284 - samples/sec: 57.52 - lr: 0.050000
2021-05-24 14:01:33,611 epoch 16 - iter 42/66 - loss 0.32656262 - samples/sec: 57.86 - lr: 0.050000
2021-05-24 14:01:35,249 epoch 16 - iter 48/66 - loss 0.31819526 - samples/sec: 58.60 - lr: 0.050000
2021-05-24 14:01:36,934 epoch 16 - iter 54/66 - loss 0.31900736 - samples/sec: 57.00 - lr: 0.050000
2021-05-24 14:01:38,593 epoch 16 - iter 60/66 - loss 0.30877368 - samples/sec: 57.88 - lr: 0.050000
2021-05-24 14:01:40,065 epoch 16 - iter 66/66 - loss 0.30480405 - samples/sec: 65.23 - lr: 0.050000
2021-05-24 14:01:40,065 ----------------------------------------------------------------------------------------------------
2021-05-24 14:01:40,065 EPOCH 16 done: loss 0.3048 - lr 0.0500000
2021-05-24 14:01:42,321 DEV : loss 0.5096579194068909 - score 0.9264
2021-05-24 14:01:42,357 BAD EPOCHS (no improvement): 3
2021-05-24 14:01:42,357 ----------------------------------------------------------------------------------------------------
2021-05-24 14:01:44,002 epoch 17 - iter 6/66 - loss 0.31011645 - samples/sec: 58.38 - lr: 0.050000
2021-05-24 14:01:45,640 epoch 17 - iter 12/66 - loss 0.25289866 - samples/sec: 58.64 - lr: 0.050000
2021-05-24 14:01:47,307 epoch 17 - iter 18/66 - loss 0.32719355 - samples/sec: 57.58 - lr: 0.050000
2021-05-24 14:01:48,962 epoch 17 - iter 24/66 - loss 0.32661772 - samples/sec: 58.05 - lr: 0.050000
2021-05-24 14:01:50,611 epoch 17 - iter 30/66 - loss 0.35589557 - samples/sec: 58.23 - lr: 0.050000
2021-05-24 14:01:52,259 epoch 17 - iter 36/66 - loss 0.37206966 - samples/sec: 58.25 - lr: 0.050000
2021-05-24 14:01:53,913 epoch 17 - iter 42/66 - loss 0.36335767 - samples/sec: 58.05 - lr: 0.050000
2021-05-24 14:01:55,568 epoch 17 - iter 48/66 - loss 0.34060496 - samples/sec: 58.03 - lr: 0.050000
2021-05-24 14:01:57,384 epoch 17 - iter 54/66 - loss 0.34723383 - samples/sec: 52.89 - lr: 0.050000
2021-05-24 14:01:59,017 epoch 17 - iter 60/66 - loss 0.33388018 - samples/sec: 58.80 - lr: 0.050000
2021-05-24 14:02:00,510 epoch 17 - iter 66/66 - loss 0.33007469 - samples/sec: 64.33 - lr: 0.050000
2021-05-24 14:02:00,510 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:00,510 EPOCH 17 done: loss 0.3301 - lr 0.0500000
2021-05-24 14:02:02,767 DEV : loss 0.4307940900325775 - score 0.9362
Epoch    17: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 14:02:02,803 BAD EPOCHS (no improvement): 4
2021-05-24 14:02:02,803 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:04,433 epoch 18 - iter 6/66 - loss 0.27437278 - samples/sec: 58.92 - lr: 0.025000
2021-05-24 14:02:06,064 epoch 18 - iter 12/66 - loss 0.33949945 - samples/sec: 58.87 - lr: 0.025000
2021-05-24 14:02:07,719 epoch 18 - iter 18/66 - loss 0.27608992 - samples/sec: 58.05 - lr: 0.025000
2021-05-24 14:02:09,375 epoch 18 - iter 24/66 - loss 0.28485672 - samples/sec: 57.99 - lr: 0.025000
2021-05-24 14:02:11,010 epoch 18 - iter 30/66 - loss 0.30193097 - samples/sec: 58.73 - lr: 0.025000
2021-05-24 14:02:12,677 epoch 18 - iter 36/66 - loss 0.30418103 - samples/sec: 57.59 - lr: 0.025000
2021-05-24 14:02:14,354 epoch 18 - iter 42/66 - loss 0.30548215 - samples/sec: 57.28 - lr: 0.025000
2021-05-24 14:02:15,951 epoch 18 - iter 48/66 - loss 0.30550408 - samples/sec: 60.14 - lr: 0.025000
2021-05-24 14:02:17,583 epoch 18 - iter 54/66 - loss 0.30837357 - samples/sec: 58.82 - lr: 0.025000
2021-05-24 14:02:19,229 epoch 18 - iter 60/66 - loss 0.29151978 - samples/sec: 58.34 - lr: 0.025000
2021-05-24 14:02:20,728 epoch 18 - iter 66/66 - loss 0.29225137 - samples/sec: 64.06 - lr: 0.025000
2021-05-24 14:02:20,728 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:20,729 EPOCH 18 done: loss 0.2923 - lr 0.0250000
2021-05-24 14:02:22,987 DEV : loss 0.34315383434295654 - score 0.9474
2021-05-24 14:02:23,022 BAD EPOCHS (no improvement): 1
2021-05-24 14:02:23,023 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:24,646 epoch 19 - iter 6/66 - loss 0.22031999 - samples/sec: 59.18 - lr: 0.025000
2021-05-24 14:02:26,300 epoch 19 - iter 12/66 - loss 0.25059978 - samples/sec: 58.03 - lr: 0.025000
2021-05-24 14:02:27,955 epoch 19 - iter 18/66 - loss 0.25340533 - samples/sec: 58.03 - lr: 0.025000
2021-05-24 14:02:29,605 epoch 19 - iter 24/66 - loss 0.27219486 - samples/sec: 58.21 - lr: 0.025000
2021-05-24 14:02:31,245 epoch 19 - iter 30/66 - loss 0.28192065 - samples/sec: 58.53 - lr: 0.025000
2021-05-24 14:02:32,883 epoch 19 - iter 36/66 - loss 0.25809081 - samples/sec: 58.66 - lr: 0.025000
2021-05-24 14:02:34,526 epoch 19 - iter 42/66 - loss 0.25081141 - samples/sec: 58.43 - lr: 0.025000
2021-05-24 14:02:36,142 epoch 19 - iter 48/66 - loss 0.23429212 - samples/sec: 59.41 - lr: 0.025000
2021-05-24 14:02:37,768 epoch 19 - iter 54/66 - loss 0.23334540 - samples/sec: 59.07 - lr: 0.025000
2021-05-24 14:02:39,401 epoch 19 - iter 60/66 - loss 0.24531211 - samples/sec: 58.81 - lr: 0.025000
2021-05-24 14:02:40,899 epoch 19 - iter 66/66 - loss 0.25116877 - samples/sec: 64.09 - lr: 0.025000
2021-05-24 14:02:40,900 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:40,900 EPOCH 19 done: loss 0.2512 - lr 0.0250000
2021-05-24 14:02:43,154 DEV : loss 0.3573038578033447 - score 0.9425
2021-05-24 14:02:43,189 BAD EPOCHS (no improvement): 2
2021-05-24 14:02:43,189 ----------------------------------------------------------------------------------------------------
2021-05-24 14:02:44,823 epoch 20 - iter 6/66 - loss 0.25527545 - samples/sec: 58.78 - lr: 0.025000
2021-05-24 14:02:46,477 epoch 20 - iter 12/66 - loss 0.23980890 - samples/sec: 58.08 - lr: 0.025000
2021-05-24 14:02:48,097 epoch 20 - iter 18/66 - loss 0.25523769 - samples/sec: 59.27 - lr: 0.025000
2021-05-24 14:02:49,731 epoch 20 - iter 24/66 - loss 0.24271711 - samples/sec: 58.79 - lr: 0.025000
2021-05-24 14:02:51,366 epoch 20 - iter 30/66 - loss 0.23264572 - samples/sec: 58.72 - lr: 0.025000
2021-05-24 14:02:53,048 epoch 20 - iter 36/66 - loss 0.22775021 - samples/sec: 57.08 - lr: 0.025000
2021-05-24 14:02:54,727 epoch 20 - iter 42/66 - loss 0.24585349 - samples/sec: 57.20 - lr: 0.025000
2021-05-24 14:02:56,407 epoch 20 - iter 48/66 - loss 0.24528598 - samples/sec: 57.17 - lr: 0.025000
2021-05-24 14:02:58,055 epoch 20 - iter 54/66 - loss 0.23628848 - samples/sec: 58.27 - lr: 0.025000
2021-05-24 14:02:59,724 epoch 20 - iter 60/66 - loss 0.24706059 - samples/sec: 57.51 - lr: 0.025000
2021-05-24 14:03:01,200 epoch 20 - iter 66/66 - loss 0.24129476 - samples/sec: 65.09 - lr: 0.025000
2021-05-24 14:03:01,200 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:01,200 EPOCH 20 done: loss 0.2413 - lr 0.0250000
2021-05-24 14:03:03,641 DEV : loss 0.34589189291000366 - score 0.9504
2021-05-24 14:03:03,677 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:03:13,132 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:14,803 epoch 21 - iter 6/66 - loss 0.33633995 - samples/sec: 57.49 - lr: 0.025000
2021-05-24 14:03:16,449 epoch 21 - iter 12/66 - loss 0.30161285 - samples/sec: 58.35 - lr: 0.025000
2021-05-24 14:03:18,112 epoch 21 - iter 18/66 - loss 0.29245561 - samples/sec: 57.72 - lr: 0.025000
2021-05-24 14:03:19,796 epoch 21 - iter 24/66 - loss 0.30179162 - samples/sec: 57.03 - lr: 0.025000
2021-05-24 14:03:21,431 epoch 21 - iter 30/66 - loss 0.28272850 - samples/sec: 58.76 - lr: 0.025000
2021-05-24 14:03:23,107 epoch 21 - iter 36/66 - loss 0.26973440 - samples/sec: 57.28 - lr: 0.025000
2021-05-24 14:03:24,740 epoch 21 - iter 42/66 - loss 0.24798078 - samples/sec: 58.81 - lr: 0.025000
2021-05-24 14:03:26,397 epoch 21 - iter 48/66 - loss 0.23760229 - samples/sec: 57.94 - lr: 0.025000
2021-05-24 14:03:28,046 epoch 21 - iter 54/66 - loss 0.22525792 - samples/sec: 58.25 - lr: 0.025000
2021-05-24 14:03:29,695 epoch 21 - iter 60/66 - loss 0.22805263 - samples/sec: 58.21 - lr: 0.025000
2021-05-24 14:03:31,194 epoch 21 - iter 66/66 - loss 0.22486762 - samples/sec: 64.08 - lr: 0.025000
2021-05-24 14:03:31,194 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:31,194 EPOCH 21 done: loss 0.2249 - lr 0.0250000
2021-05-24 14:03:33,457 DEV : loss 0.3834744393825531 - score 0.9405
2021-05-24 14:03:33,493 BAD EPOCHS (no improvement): 1
2021-05-24 14:03:33,493 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:35,167 epoch 22 - iter 6/66 - loss 0.15492713 - samples/sec: 57.40 - lr: 0.025000
2021-05-24 14:03:36,787 epoch 22 - iter 12/66 - loss 0.25706746 - samples/sec: 59.24 - lr: 0.025000
2021-05-24 14:03:38,447 epoch 22 - iter 18/66 - loss 0.25736692 - samples/sec: 57.86 - lr: 0.025000
2021-05-24 14:03:40,125 epoch 22 - iter 24/66 - loss 0.23299170 - samples/sec: 57.21 - lr: 0.025000
2021-05-24 14:03:41,779 epoch 22 - iter 30/66 - loss 0.24320530 - samples/sec: 58.06 - lr: 0.025000
2021-05-24 14:03:43,412 epoch 22 - iter 36/66 - loss 0.25201092 - samples/sec: 58.82 - lr: 0.025000
2021-05-24 14:03:45,045 epoch 22 - iter 42/66 - loss 0.24150767 - samples/sec: 58.80 - lr: 0.025000
2021-05-24 14:03:46,695 epoch 22 - iter 48/66 - loss 0.23714277 - samples/sec: 58.21 - lr: 0.025000
2021-05-24 14:03:48,317 epoch 22 - iter 54/66 - loss 0.25532409 - samples/sec: 59.20 - lr: 0.025000
2021-05-24 14:03:49,984 epoch 22 - iter 60/66 - loss 0.25277794 - samples/sec: 57.60 - lr: 0.025000
2021-05-24 14:03:51,494 epoch 22 - iter 66/66 - loss 0.24501635 - samples/sec: 63.60 - lr: 0.025000
2021-05-24 14:03:51,494 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:51,494 EPOCH 22 done: loss 0.2450 - lr 0.0250000
2021-05-24 14:03:53,758 DEV : loss 0.3015596866607666 - score 0.9493
2021-05-24 14:03:53,794 BAD EPOCHS (no improvement): 2
2021-05-24 14:03:53,794 ----------------------------------------------------------------------------------------------------
2021-05-24 14:03:55,457 epoch 23 - iter 6/66 - loss 0.17093968 - samples/sec: 57.75 - lr: 0.025000
2021-05-24 14:03:57,087 epoch 23 - iter 12/66 - loss 0.21535784 - samples/sec: 58.92 - lr: 0.025000
2021-05-24 14:03:58,746 epoch 23 - iter 18/66 - loss 0.20970298 - samples/sec: 57.87 - lr: 0.025000
2021-05-24 14:04:00,375 epoch 23 - iter 24/66 - loss 0.22880699 - samples/sec: 58.95 - lr: 0.025000
2021-05-24 14:04:02,029 epoch 23 - iter 30/66 - loss 0.25955440 - samples/sec: 58.07 - lr: 0.025000
2021-05-24 14:04:03,608 epoch 23 - iter 36/66 - loss 0.23872936 - samples/sec: 60.80 - lr: 0.025000
2021-05-24 14:04:05,247 epoch 23 - iter 42/66 - loss 0.23535158 - samples/sec: 58.61 - lr: 0.025000
2021-05-24 14:04:06,891 epoch 23 - iter 48/66 - loss 0.23287192 - samples/sec: 58.41 - lr: 0.025000
2021-05-24 14:04:08,547 epoch 23 - iter 54/66 - loss 0.24593047 - samples/sec: 57.99 - lr: 0.025000
2021-05-24 14:04:10,175 epoch 23 - iter 60/66 - loss 0.22840942 - samples/sec: 58.99 - lr: 0.025000
2021-05-24 14:04:11,698 epoch 23 - iter 66/66 - loss 0.23677498 - samples/sec: 63.05 - lr: 0.025000
2021-05-24 14:04:11,698 ----------------------------------------------------------------------------------------------------
2021-05-24 14:04:11,698 EPOCH 23 done: loss 0.2368 - lr 0.0250000
2021-05-24 14:04:13,966 DEV : loss 0.2846739888191223 - score 0.9508
2021-05-24 14:04:14,002 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:04:23,887 ----------------------------------------------------------------------------------------------------
2021-05-24 14:04:25,540 epoch 24 - iter 6/66 - loss 0.16345032 - samples/sec: 58.12 - lr: 0.025000
2021-05-24 14:04:27,223 epoch 24 - iter 12/66 - loss 0.21061691 - samples/sec: 57.05 - lr: 0.025000
2021-05-24 14:04:28,870 epoch 24 - iter 18/66 - loss 0.23124120 - samples/sec: 58.32 - lr: 0.025000
2021-05-24 14:04:30,520 epoch 24 - iter 24/66 - loss 0.22600089 - samples/sec: 58.19 - lr: 0.025000
2021-05-24 14:04:32,148 epoch 24 - iter 30/66 - loss 0.22974245 - samples/sec: 58.98 - lr: 0.025000
2021-05-24 14:04:33,811 epoch 24 - iter 36/66 - loss 0.24030157 - samples/sec: 57.73 - lr: 0.025000
2021-05-24 14:04:35,430 epoch 24 - iter 42/66 - loss 0.24258028 - samples/sec: 59.34 - lr: 0.025000
2021-05-24 14:04:37,016 epoch 24 - iter 48/66 - loss 0.22097663 - samples/sec: 60.54 - lr: 0.025000
2021-05-24 14:04:38,636 epoch 24 - iter 54/66 - loss 0.21607596 - samples/sec: 59.27 - lr: 0.025000
2021-05-24 14:04:40,287 epoch 24 - iter 60/66 - loss 0.21216573 - samples/sec: 58.17 - lr: 0.025000
2021-05-24 14:04:41,826 epoch 24 - iter 66/66 - loss 0.21179975 - samples/sec: 62.39 - lr: 0.025000
2021-05-24 14:04:41,826 ----------------------------------------------------------------------------------------------------
2021-05-24 14:04:41,827 EPOCH 24 done: loss 0.2118 - lr 0.0250000
2021-05-24 14:04:44,266 DEV : loss 0.4315478205680847 - score 0.9375
2021-05-24 14:04:44,302 BAD EPOCHS (no improvement): 1
2021-05-24 14:04:44,302 ----------------------------------------------------------------------------------------------------
2021-05-24 14:04:45,960 epoch 25 - iter 6/66 - loss 0.25096885 - samples/sec: 57.92 - lr: 0.025000
2021-05-24 14:04:47,628 epoch 25 - iter 12/66 - loss 0.16214152 - samples/sec: 57.57 - lr: 0.025000
2021-05-24 14:04:49,282 epoch 25 - iter 18/66 - loss 0.17090786 - samples/sec: 58.05 - lr: 0.025000
2021-05-24 14:04:50,940 epoch 25 - iter 24/66 - loss 0.14654614 - samples/sec: 57.91 - lr: 0.025000
2021-05-24 14:04:52,600 epoch 25 - iter 30/66 - loss 0.15110729 - samples/sec: 57.85 - lr: 0.025000
2021-05-24 14:04:54,243 epoch 25 - iter 36/66 - loss 0.15382300 - samples/sec: 58.47 - lr: 0.025000
2021-05-24 14:04:55,901 epoch 25 - iter 42/66 - loss 0.16408983 - samples/sec: 57.90 - lr: 0.025000
2021-05-24 14:04:57,530 epoch 25 - iter 48/66 - loss 0.17912179 - samples/sec: 58.97 - lr: 0.025000
2021-05-24 14:04:59,181 epoch 25 - iter 54/66 - loss 0.19176151 - samples/sec: 58.14 - lr: 0.025000
2021-05-24 14:05:00,833 epoch 25 - iter 60/66 - loss 0.19361145 - samples/sec: 58.15 - lr: 0.025000
2021-05-24 14:05:02,329 epoch 25 - iter 66/66 - loss 0.19415044 - samples/sec: 64.19 - lr: 0.025000
2021-05-24 14:05:02,329 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:02,329 EPOCH 25 done: loss 0.1942 - lr 0.0250000
2021-05-24 14:05:04,584 DEV : loss 0.40080398321151733 - score 0.9404
2021-05-24 14:05:04,620 BAD EPOCHS (no improvement): 2
2021-05-24 14:05:04,620 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:06,292 epoch 26 - iter 6/66 - loss 0.17584749 - samples/sec: 57.45 - lr: 0.025000
2021-05-24 14:05:07,912 epoch 26 - iter 12/66 - loss 0.24447483 - samples/sec: 59.26 - lr: 0.025000
2021-05-24 14:05:09,543 epoch 26 - iter 18/66 - loss 0.22511091 - samples/sec: 58.88 - lr: 0.025000
2021-05-24 14:05:11,202 epoch 26 - iter 24/66 - loss 0.21436752 - samples/sec: 57.90 - lr: 0.025000
2021-05-24 14:05:12,865 epoch 26 - iter 30/66 - loss 0.18713691 - samples/sec: 57.75 - lr: 0.025000
2021-05-24 14:05:14,487 epoch 26 - iter 36/66 - loss 0.17944304 - samples/sec: 59.19 - lr: 0.025000
2021-05-24 14:05:16,161 epoch 26 - iter 42/66 - loss 0.19040343 - samples/sec: 57.37 - lr: 0.025000
2021-05-24 14:05:17,773 epoch 26 - iter 48/66 - loss 0.20861990 - samples/sec: 59.57 - lr: 0.025000
2021-05-24 14:05:19,425 epoch 26 - iter 54/66 - loss 0.22312656 - samples/sec: 58.12 - lr: 0.025000
2021-05-24 14:05:21,105 epoch 26 - iter 60/66 - loss 0.22135459 - samples/sec: 57.17 - lr: 0.025000
2021-05-24 14:05:22,603 epoch 26 - iter 66/66 - loss 0.21763082 - samples/sec: 64.09 - lr: 0.025000
2021-05-24 14:05:22,603 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:22,626 EPOCH 26 done: loss 0.2176 - lr 0.0250000
2021-05-24 14:05:24,884 DEV : loss 0.2687566578388214 - score 0.9535
2021-05-24 14:05:24,920 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:05:34,536 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:36,173 epoch 27 - iter 6/66 - loss 0.24068042 - samples/sec: 58.67 - lr: 0.025000
2021-05-24 14:05:37,815 epoch 27 - iter 12/66 - loss 0.21490500 - samples/sec: 58.48 - lr: 0.025000
2021-05-24 14:05:39,457 epoch 27 - iter 18/66 - loss 0.23427073 - samples/sec: 58.49 - lr: 0.025000
2021-05-24 14:05:41,117 epoch 27 - iter 24/66 - loss 0.21807683 - samples/sec: 57.85 - lr: 0.025000
2021-05-24 14:05:42,767 epoch 27 - iter 30/66 - loss 0.20787918 - samples/sec: 58.19 - lr: 0.025000
2021-05-24 14:05:44,407 epoch 27 - iter 36/66 - loss 0.20938876 - samples/sec: 58.56 - lr: 0.025000
2021-05-24 14:05:46,039 epoch 27 - iter 42/66 - loss 0.20688568 - samples/sec: 58.82 - lr: 0.025000
2021-05-24 14:05:47,690 epoch 27 - iter 48/66 - loss 0.20593253 - samples/sec: 58.17 - lr: 0.025000
2021-05-24 14:05:49,314 epoch 27 - iter 54/66 - loss 0.19878943 - samples/sec: 59.14 - lr: 0.025000
2021-05-24 14:05:50,943 epoch 27 - iter 60/66 - loss 0.19729267 - samples/sec: 58.95 - lr: 0.025000
2021-05-24 14:05:52,446 epoch 27 - iter 66/66 - loss 0.20120160 - samples/sec: 63.89 - lr: 0.025000
2021-05-24 14:05:52,446 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:52,446 EPOCH 27 done: loss 0.2012 - lr 0.0250000
2021-05-24 14:05:54,703 DEV : loss 0.2422541230916977 - score 0.9506
2021-05-24 14:05:54,739 BAD EPOCHS (no improvement): 1
2021-05-24 14:05:54,739 ----------------------------------------------------------------------------------------------------
2021-05-24 14:05:56,377 epoch 28 - iter 6/66 - loss 0.24966061 - samples/sec: 58.63 - lr: 0.025000
2021-05-24 14:05:58,018 epoch 28 - iter 12/66 - loss 0.18251587 - samples/sec: 58.53 - lr: 0.025000
2021-05-24 14:05:59,652 epoch 28 - iter 18/66 - loss 0.19741859 - samples/sec: 58.78 - lr: 0.025000
2021-05-24 14:06:01,264 epoch 28 - iter 24/66 - loss 0.20399944 - samples/sec: 59.55 - lr: 0.025000
2021-05-24 14:06:02,901 epoch 28 - iter 30/66 - loss 0.21737015 - samples/sec: 58.67 - lr: 0.025000
2021-05-24 14:06:04,544 epoch 28 - iter 36/66 - loss 0.20941362 - samples/sec: 58.43 - lr: 0.025000
2021-05-24 14:06:06,206 epoch 28 - iter 42/66 - loss 0.22827473 - samples/sec: 57.78 - lr: 0.025000
2021-05-24 14:06:07,881 epoch 28 - iter 48/66 - loss 0.22409096 - samples/sec: 57.36 - lr: 0.025000
2021-05-24 14:06:09,527 epoch 28 - iter 54/66 - loss 0.22413679 - samples/sec: 58.32 - lr: 0.025000
2021-05-24 14:06:11,211 epoch 28 - iter 60/66 - loss 0.22766652 - samples/sec: 57.04 - lr: 0.025000
2021-05-24 14:06:12,900 epoch 28 - iter 66/66 - loss 0.23367362 - samples/sec: 56.84 - lr: 0.025000
2021-05-24 14:06:12,900 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:12,901 EPOCH 28 done: loss 0.2337 - lr 0.0250000
2021-05-24 14:06:15,161 DEV : loss 0.25440514087677 - score 0.9492
2021-05-24 14:06:15,197 BAD EPOCHS (no improvement): 2
2021-05-24 14:06:15,197 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:16,813 epoch 29 - iter 6/66 - loss 0.18276219 - samples/sec: 59.43 - lr: 0.025000
2021-05-24 14:06:18,484 epoch 29 - iter 12/66 - loss 0.19187440 - samples/sec: 57.47 - lr: 0.025000
2021-05-24 14:06:20,133 epoch 29 - iter 18/66 - loss 0.17738449 - samples/sec: 58.23 - lr: 0.025000
2021-05-24 14:06:21,787 epoch 29 - iter 24/66 - loss 0.19749866 - samples/sec: 58.04 - lr: 0.025000
2021-05-24 14:06:23,445 epoch 29 - iter 30/66 - loss 0.20763979 - samples/sec: 57.92 - lr: 0.025000
2021-05-24 14:06:25,094 epoch 29 - iter 36/66 - loss 0.20434527 - samples/sec: 58.26 - lr: 0.025000
2021-05-24 14:06:26,781 epoch 29 - iter 42/66 - loss 0.21370892 - samples/sec: 56.90 - lr: 0.025000
2021-05-24 14:06:28,440 epoch 29 - iter 48/66 - loss 0.20842746 - samples/sec: 57.89 - lr: 0.025000
2021-05-24 14:06:30,048 epoch 29 - iter 54/66 - loss 0.20311073 - samples/sec: 59.71 - lr: 0.025000
2021-05-24 14:06:31,719 epoch 29 - iter 60/66 - loss 0.19287072 - samples/sec: 57.48 - lr: 0.025000
2021-05-24 14:06:33,230 epoch 29 - iter 66/66 - loss 0.18882864 - samples/sec: 63.54 - lr: 0.025000
2021-05-24 14:06:33,231 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:33,231 EPOCH 29 done: loss 0.1888 - lr 0.0250000
2021-05-24 14:06:35,488 DEV : loss 0.3324491083621979 - score 0.9479
2021-05-24 14:06:35,523 BAD EPOCHS (no improvement): 3
2021-05-24 14:06:35,524 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:37,158 epoch 30 - iter 6/66 - loss 0.12614504 - samples/sec: 58.75 - lr: 0.025000
2021-05-24 14:06:38,829 epoch 30 - iter 12/66 - loss 0.15031823 - samples/sec: 57.49 - lr: 0.025000
2021-05-24 14:06:40,488 epoch 30 - iter 18/66 - loss 0.14820500 - samples/sec: 57.86 - lr: 0.025000
2021-05-24 14:06:42,135 epoch 30 - iter 24/66 - loss 0.17353560 - samples/sec: 58.33 - lr: 0.025000
2021-05-24 14:06:43,799 epoch 30 - iter 30/66 - loss 0.16540295 - samples/sec: 57.71 - lr: 0.025000
2021-05-24 14:06:45,407 epoch 30 - iter 36/66 - loss 0.16047222 - samples/sec: 59.71 - lr: 0.025000
2021-05-24 14:06:47,063 epoch 30 - iter 42/66 - loss 0.16297803 - samples/sec: 57.97 - lr: 0.025000
2021-05-24 14:06:48,722 epoch 30 - iter 48/66 - loss 0.16525095 - samples/sec: 57.90 - lr: 0.025000
2021-05-24 14:06:50,404 epoch 30 - iter 54/66 - loss 0.16070683 - samples/sec: 57.07 - lr: 0.025000
2021-05-24 14:06:52,079 epoch 30 - iter 60/66 - loss 0.15945232 - samples/sec: 57.36 - lr: 0.025000
2021-05-24 14:06:53,598 epoch 30 - iter 66/66 - loss 0.16756632 - samples/sec: 63.20 - lr: 0.025000
2021-05-24 14:06:53,598 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:53,598 EPOCH 30 done: loss 0.1676 - lr 0.0250000
2021-05-24 14:06:55,857 DEV : loss 0.40297457575798035 - score 0.9402
Epoch    30: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 14:06:55,892 BAD EPOCHS (no improvement): 4
2021-05-24 14:06:56,966 ----------------------------------------------------------------------------------------------------
2021-05-24 14:06:56,966 Testing using best model ...
2021-05-24 14:06:56,966 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/eus.rst.ert/best-model.pt
2021-05-24 14:07:08,469 0.9313	0.9750	0.9527
2021-05-24 14:07:08,469 
Results:
- F1-score (micro) 0.9527
- F1-score (macro) 0.9527

By class:
SENT       tp: 312 - fp: 23 - fn: 8 - precision: 0.9313 - recall: 0.9750 - f1-score: 0.9527
2021-05-24 14:07:08,469 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb/
2021-05-24 14:07:08,502 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb
2021-05-24 14:07:08,504 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb/sent_train.txt
2021-05-24 14:07:08,505 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb/sent_dev.txt
2021-05-24 14:07:08,505 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb/sent_test.txt
Corpus: 51481 train + 1929 dev + 2695 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 14:07:34,815 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 14:07:34,818 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Corpus: "Corpus: 51481 train + 1929 dev + 2695 test sentences"
2021-05-24 14:07:34,818 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Parameters:
2021-05-24 14:07:34,818  - learning_rate: "0.1"
2021-05-24 14:07:34,818  - mini_batch_size: "16"
2021-05-24 14:07:34,818  - patience: "3"
2021-05-24 14:07:34,818  - anneal_factor: "0.5"
2021-05-24 14:07:34,818  - max_epochs: "30"
2021-05-24 14:07:34,818  - shuffle: "True"
2021-05-24 14:07:34,818  - train_with_dev: "False"
2021-05-24 14:07:34,818  - batch_growth_annealing: "False"
2021-05-24 14:07:34,818 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb"
2021-05-24 14:07:34,818 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Device: cuda:0
2021-05-24 14:07:34,818 ----------------------------------------------------------------------------------------------------
2021-05-24 14:07:34,818 Embeddings storage mode: cpu
2021-05-24 14:07:34,820 ----------------------------------------------------------------------------------------------------
2021-05-24 14:11:36,223 epoch 1 - iter 321/3218 - loss 2.23096095 - samples/sec: 21.28 - lr: 0.100000
2021-05-24 14:15:27,517 epoch 1 - iter 642/3218 - loss 1.60429266 - samples/sec: 22.21 - lr: 0.100000
2021-05-24 14:19:19,382 epoch 1 - iter 963/3218 - loss 1.32033858 - samples/sec: 22.15 - lr: 0.100000
2021-05-24 14:23:11,815 epoch 1 - iter 1284/3218 - loss 1.16747713 - samples/sec: 22.10 - lr: 0.100000
2021-05-24 14:27:04,008 epoch 1 - iter 1605/3218 - loss 1.07509842 - samples/sec: 22.12 - lr: 0.100000
2021-05-24 14:30:58,601 epoch 1 - iter 1926/3218 - loss 1.00571912 - samples/sec: 21.89 - lr: 0.100000
2021-05-24 14:34:51,800 epoch 1 - iter 2247/3218 - loss 0.94210475 - samples/sec: 22.03 - lr: 0.100000
2021-05-24 14:38:46,438 epoch 1 - iter 2568/3218 - loss 0.89411998 - samples/sec: 21.89 - lr: 0.100000
2021-05-24 14:42:41,604 epoch 1 - iter 2889/3218 - loss 0.85141575 - samples/sec: 21.84 - lr: 0.100000
2021-05-24 14:46:35,764 epoch 1 - iter 3210/3218 - loss 0.82065763 - samples/sec: 21.93 - lr: 0.100000
2021-05-24 14:46:41,283 ----------------------------------------------------------------------------------------------------
2021-05-24 14:46:41,283 EPOCH 1 done: loss 0.8200 - lr 0.1000000
2021-05-24 14:47:40,798 DEV : loss 0.40876153111457825 - score 0.9241
2021-05-24 14:47:40,980 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 14:47:42,307 ----------------------------------------------------------------------------------------------------
2021-05-24 14:49:09,261 epoch 2 - iter 321/3218 - loss 0.57637321 - samples/sec: 59.08 - lr: 0.100000
2021-05-24 14:50:35,537 epoch 2 - iter 642/3218 - loss 0.55180592 - samples/sec: 59.54 - lr: 0.100000
2021-05-24 14:52:01,687 epoch 2 - iter 963/3218 - loss 0.55275534 - samples/sec: 59.63 - lr: 0.100000
2021-05-24 14:53:28,613 epoch 2 - iter 1284/3218 - loss 0.55059572 - samples/sec: 59.09 - lr: 0.100000
2021-05-24 14:54:54,781 epoch 2 - iter 1605/3218 - loss 0.53555638 - samples/sec: 59.61 - lr: 0.100000
2021-05-24 14:56:20,953 epoch 2 - iter 1926/3218 - loss 0.52856340 - samples/sec: 59.61 - lr: 0.100000
2021-05-24 14:57:46,817 epoch 2 - iter 2247/3218 - loss 0.52462178 - samples/sec: 59.82 - lr: 0.100000
2021-05-24 14:59:12,941 epoch 2 - iter 2568/3218 - loss 0.52050476 - samples/sec: 59.64 - lr: 0.100000
2021-05-24 15:00:39,494 epoch 2 - iter 2889/3218 - loss 0.51696783 - samples/sec: 59.35 - lr: 0.100000
2021-05-24 15:02:05,476 epoch 2 - iter 3210/3218 - loss 0.51190012 - samples/sec: 59.74 - lr: 0.100000
2021-05-24 15:02:07,479 ----------------------------------------------------------------------------------------------------
2021-05-24 15:02:07,479 EPOCH 2 done: loss 0.5123 - lr 0.1000000
2021-05-24 15:02:21,734 DEV : loss 0.32625895738601685 - score 0.9367
2021-05-24 15:02:21,915 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 15:02:32,085 ----------------------------------------------------------------------------------------------------
2021-05-24 15:03:59,310 epoch 3 - iter 321/3218 - loss 0.46196534 - samples/sec: 58.89 - lr: 0.100000
2021-05-24 15:05:26,838 epoch 3 - iter 642/3218 - loss 0.44593593 - samples/sec: 58.69 - lr: 0.100000
2021-05-24 15:06:54,589 epoch 3 - iter 963/3218 - loss 0.44393439 - samples/sec: 58.54 - lr: 0.100000
2021-05-24 15:08:22,573 epoch 3 - iter 1284/3218 - loss 0.44770619 - samples/sec: 58.38 - lr: 0.100000
2021-05-24 15:09:50,167 epoch 3 - iter 1605/3218 - loss 0.45036613 - samples/sec: 58.64 - lr: 0.100000
2021-05-24 15:11:17,526 epoch 3 - iter 1926/3218 - loss 0.44827829 - samples/sec: 58.80 - lr: 0.100000
2021-05-24 15:12:44,555 epoch 3 - iter 2247/3218 - loss 0.44124514 - samples/sec: 59.02 - lr: 0.100000
2021-05-24 15:14:10,967 epoch 3 - iter 2568/3218 - loss 0.43537020 - samples/sec: 59.44 - lr: 0.100000
2021-05-24 15:15:37,236 epoch 3 - iter 2889/3218 - loss 0.42996024 - samples/sec: 59.54 - lr: 0.100000
2021-05-24 15:17:03,847 epoch 3 - iter 3210/3218 - loss 0.42410560 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 15:17:05,898 ----------------------------------------------------------------------------------------------------
2021-05-24 15:17:05,898 EPOCH 3 done: loss 0.4245 - lr 0.1000000
2021-05-24 15:17:17,236 DEV : loss 0.27703040838241577 - score 0.9488
2021-05-24 15:17:17,419 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 15:17:27,640 ----------------------------------------------------------------------------------------------------
2021-05-24 15:18:54,662 epoch 4 - iter 321/3218 - loss 0.40036945 - samples/sec: 59.03 - lr: 0.100000
2021-05-24 15:20:21,658 epoch 4 - iter 642/3218 - loss 0.42839211 - samples/sec: 59.05 - lr: 0.100000
2021-05-24 15:21:48,741 epoch 4 - iter 963/3218 - loss 0.41214442 - samples/sec: 58.99 - lr: 0.100000
2021-05-24 15:23:16,556 epoch 4 - iter 1284/3218 - loss 0.41116815 - samples/sec: 58.49 - lr: 0.100000
2021-05-24 15:24:43,938 epoch 4 - iter 1605/3218 - loss 0.40933742 - samples/sec: 58.78 - lr: 0.100000
2021-05-24 15:26:11,388 epoch 4 - iter 1926/3218 - loss 0.40565358 - samples/sec: 58.74 - lr: 0.100000
2021-05-24 15:27:38,350 epoch 4 - iter 2247/3218 - loss 0.40534220 - samples/sec: 59.07 - lr: 0.100000
2021-05-24 15:29:05,133 epoch 4 - iter 2568/3218 - loss 0.40057902 - samples/sec: 59.19 - lr: 0.100000
2021-05-24 15:30:32,017 epoch 4 - iter 2889/3218 - loss 0.39773037 - samples/sec: 59.12 - lr: 0.100000
2021-05-24 15:31:59,026 epoch 4 - iter 3210/3218 - loss 0.39648355 - samples/sec: 59.04 - lr: 0.100000
2021-05-24 15:32:01,076 ----------------------------------------------------------------------------------------------------
2021-05-24 15:32:01,076 EPOCH 4 done: loss 0.3964 - lr 0.1000000
2021-05-24 15:32:12,465 DEV : loss 0.25118353962898254 - score 0.9577
2021-05-24 15:32:12,650 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 15:32:22,372 ----------------------------------------------------------------------------------------------------
2021-05-24 15:33:49,517 epoch 5 - iter 321/3218 - loss 0.36787312 - samples/sec: 58.95 - lr: 0.100000
2021-05-24 15:35:16,992 epoch 5 - iter 642/3218 - loss 0.38842010 - samples/sec: 58.72 - lr: 0.100000
2021-05-24 15:36:44,856 epoch 5 - iter 963/3218 - loss 0.37218955 - samples/sec: 58.46 - lr: 0.100000
2021-05-24 15:38:11,382 epoch 5 - iter 1284/3218 - loss 0.36897412 - samples/sec: 59.37 - lr: 0.100000
2021-05-24 15:39:39,109 epoch 5 - iter 1605/3218 - loss 0.36570171 - samples/sec: 58.55 - lr: 0.100000
2021-05-24 15:41:07,093 epoch 5 - iter 1926/3218 - loss 0.36684268 - samples/sec: 58.38 - lr: 0.100000
2021-05-24 15:42:35,096 epoch 5 - iter 2247/3218 - loss 0.36842794 - samples/sec: 58.37 - lr: 0.100000
2021-05-24 15:44:01,736 epoch 5 - iter 2568/3218 - loss 0.36459820 - samples/sec: 59.29 - lr: 0.100000
2021-05-24 15:45:28,404 epoch 5 - iter 2889/3218 - loss 0.36176358 - samples/sec: 59.27 - lr: 0.100000
2021-05-24 15:46:55,096 epoch 5 - iter 3210/3218 - loss 0.36334350 - samples/sec: 59.25 - lr: 0.100000
2021-05-24 15:46:57,156 ----------------------------------------------------------------------------------------------------
2021-05-24 15:46:57,157 EPOCH 5 done: loss 0.3630 - lr 0.1000000
2021-05-24 15:47:08,552 DEV : loss 0.23445174098014832 - score 0.9572
2021-05-24 15:47:08,735 BAD EPOCHS (no improvement): 1
2021-05-24 15:47:08,735 ----------------------------------------------------------------------------------------------------
2021-05-24 15:48:35,259 epoch 6 - iter 321/3218 - loss 0.35831739 - samples/sec: 59.37 - lr: 0.100000
2021-05-24 15:50:01,902 epoch 6 - iter 642/3218 - loss 0.35762551 - samples/sec: 59.29 - lr: 0.100000
2021-05-24 15:51:28,642 epoch 6 - iter 963/3218 - loss 0.36423306 - samples/sec: 59.22 - lr: 0.100000
2021-05-24 15:52:55,245 epoch 6 - iter 1284/3218 - loss 0.36888744 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 15:54:21,772 epoch 6 - iter 1605/3218 - loss 0.36183144 - samples/sec: 59.37 - lr: 0.100000
2021-05-24 15:55:48,398 epoch 6 - iter 1926/3218 - loss 0.35674901 - samples/sec: 59.30 - lr: 0.100000
2021-05-24 15:57:15,105 epoch 6 - iter 2247/3218 - loss 0.35224833 - samples/sec: 59.24 - lr: 0.100000
2021-05-24 15:58:41,895 epoch 6 - iter 2568/3218 - loss 0.34902056 - samples/sec: 59.19 - lr: 0.100000
2021-05-24 16:00:08,821 epoch 6 - iter 2889/3218 - loss 0.34778263 - samples/sec: 59.09 - lr: 0.100000
2021-05-24 16:01:36,221 epoch 6 - iter 3210/3218 - loss 0.34782496 - samples/sec: 58.77 - lr: 0.100000
2021-05-24 16:01:38,306 ----------------------------------------------------------------------------------------------------
2021-05-24 16:01:38,306 EPOCH 6 done: loss 0.3477 - lr 0.1000000
2021-05-24 16:01:49,708 DEV : loss 0.2171979546546936 - score 0.9589
2021-05-24 16:01:49,890 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 16:01:59,862 ----------------------------------------------------------------------------------------------------
2021-05-24 16:03:27,528 epoch 7 - iter 321/3218 - loss 0.28576964 - samples/sec: 58.60 - lr: 0.100000
2021-05-24 16:04:55,020 epoch 7 - iter 642/3218 - loss 0.32035542 - samples/sec: 58.71 - lr: 0.100000
2021-05-24 16:06:22,738 epoch 7 - iter 963/3218 - loss 0.32727721 - samples/sec: 58.56 - lr: 0.100000
2021-05-24 16:07:50,487 epoch 7 - iter 1284/3218 - loss 0.32487187 - samples/sec: 58.54 - lr: 0.100000
2021-05-24 16:09:17,389 epoch 7 - iter 1605/3218 - loss 0.32509382 - samples/sec: 59.11 - lr: 0.100000
2021-05-24 16:10:44,601 epoch 7 - iter 1926/3218 - loss 0.32358778 - samples/sec: 58.90 - lr: 0.100000
2021-05-24 16:12:11,310 epoch 7 - iter 2247/3218 - loss 0.32446110 - samples/sec: 59.24 - lr: 0.100000
2021-05-24 16:13:38,376 epoch 7 - iter 2568/3218 - loss 0.32407226 - samples/sec: 59.00 - lr: 0.100000
2021-05-24 16:15:05,452 epoch 7 - iter 2889/3218 - loss 0.32517863 - samples/sec: 58.99 - lr: 0.100000
2021-05-24 16:16:31,932 epoch 7 - iter 3210/3218 - loss 0.32651797 - samples/sec: 59.40 - lr: 0.100000
2021-05-24 16:16:33,993 ----------------------------------------------------------------------------------------------------
2021-05-24 16:16:33,994 EPOCH 7 done: loss 0.3262 - lr 0.1000000
2021-05-24 16:16:45,406 DEV : loss 0.22435320913791656 - score 0.9621
2021-05-24 16:16:45,591 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 16:16:55,624 ----------------------------------------------------------------------------------------------------
2021-05-24 16:18:23,391 epoch 8 - iter 321/3218 - loss 0.31457537 - samples/sec: 58.53 - lr: 0.100000
2021-05-24 16:19:51,086 epoch 8 - iter 642/3218 - loss 0.33689703 - samples/sec: 58.57 - lr: 0.100000
2021-05-24 16:21:18,783 epoch 8 - iter 963/3218 - loss 0.32856754 - samples/sec: 58.57 - lr: 0.100000
2021-05-24 16:22:46,223 epoch 8 - iter 1284/3218 - loss 0.33575168 - samples/sec: 58.74 - lr: 0.100000
2021-05-24 16:24:13,223 epoch 8 - iter 1605/3218 - loss 0.33229027 - samples/sec: 59.04 - lr: 0.100000
2021-05-24 16:25:40,360 epoch 8 - iter 1926/3218 - loss 0.32441737 - samples/sec: 58.95 - lr: 0.100000
2021-05-24 16:27:07,702 epoch 8 - iter 2247/3218 - loss 0.32443826 - samples/sec: 58.81 - lr: 0.100000
2021-05-24 16:28:34,958 epoch 8 - iter 2568/3218 - loss 0.32335641 - samples/sec: 58.87 - lr: 0.100000
2021-05-24 16:30:01,665 epoch 8 - iter 2889/3218 - loss 0.32042973 - samples/sec: 59.24 - lr: 0.100000
2021-05-24 16:31:28,528 epoch 8 - iter 3210/3218 - loss 0.32095234 - samples/sec: 59.13 - lr: 0.100000
2021-05-24 16:31:30,575 ----------------------------------------------------------------------------------------------------
2021-05-24 16:31:30,576 EPOCH 8 done: loss 0.3210 - lr 0.1000000
2021-05-24 16:31:41,977 DEV : loss 0.20807935297489166 - score 0.9653
2021-05-24 16:31:42,161 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 16:31:52,084 ----------------------------------------------------------------------------------------------------
2021-05-24 16:33:19,318 epoch 9 - iter 321/3218 - loss 0.31316422 - samples/sec: 58.89 - lr: 0.100000
2021-05-24 16:34:46,551 epoch 9 - iter 642/3218 - loss 0.31243663 - samples/sec: 58.88 - lr: 0.100000
2021-05-24 16:36:13,728 epoch 9 - iter 963/3218 - loss 0.29767561 - samples/sec: 58.92 - lr: 0.100000
2021-05-24 16:37:40,357 epoch 9 - iter 1284/3218 - loss 0.29545255 - samples/sec: 59.30 - lr: 0.100000
2021-05-24 16:39:07,152 epoch 9 - iter 1605/3218 - loss 0.29617193 - samples/sec: 59.18 - lr: 0.100000
2021-05-24 16:40:34,398 epoch 9 - iter 1926/3218 - loss 0.30075108 - samples/sec: 58.88 - lr: 0.100000
2021-05-24 16:42:01,454 epoch 9 - iter 2247/3218 - loss 0.30590640 - samples/sec: 59.00 - lr: 0.100000
2021-05-24 16:43:28,756 epoch 9 - iter 2568/3218 - loss 0.30623553 - samples/sec: 58.84 - lr: 0.100000
2021-05-24 16:44:56,060 epoch 9 - iter 2889/3218 - loss 0.30717948 - samples/sec: 58.84 - lr: 0.100000
2021-05-24 16:46:23,014 epoch 9 - iter 3210/3218 - loss 0.30725894 - samples/sec: 59.07 - lr: 0.100000
2021-05-24 16:46:25,082 ----------------------------------------------------------------------------------------------------
2021-05-24 16:46:25,082 EPOCH 9 done: loss 0.3079 - lr 0.1000000
2021-05-24 16:46:36,486 DEV : loss 0.22624708712100983 - score 0.9619
2021-05-24 16:46:36,670 BAD EPOCHS (no improvement): 1
2021-05-24 16:46:36,670 ----------------------------------------------------------------------------------------------------
2021-05-24 16:48:03,804 epoch 10 - iter 321/3218 - loss 0.34052321 - samples/sec: 58.95 - lr: 0.100000
2021-05-24 16:49:34,589 epoch 10 - iter 642/3218 - loss 0.33199750 - samples/sec: 56.58 - lr: 0.100000
2021-05-24 16:51:01,937 epoch 10 - iter 963/3218 - loss 0.32690325 - samples/sec: 58.81 - lr: 0.100000
2021-05-24 16:52:28,640 epoch 10 - iter 1284/3218 - loss 0.31928744 - samples/sec: 59.24 - lr: 0.100000
2021-05-24 16:53:55,450 epoch 10 - iter 1605/3218 - loss 0.31772362 - samples/sec: 59.17 - lr: 0.100000
2021-05-24 16:55:21,951 epoch 10 - iter 1926/3218 - loss 0.31908393 - samples/sec: 59.38 - lr: 0.100000
2021-05-24 16:56:49,209 epoch 10 - iter 2247/3218 - loss 0.31319206 - samples/sec: 58.87 - lr: 0.100000
2021-05-24 16:58:15,841 epoch 10 - iter 2568/3218 - loss 0.31279849 - samples/sec: 59.29 - lr: 0.100000
2021-05-24 16:59:42,382 epoch 10 - iter 2889/3218 - loss 0.31268608 - samples/sec: 59.36 - lr: 0.100000
2021-05-24 17:01:09,381 epoch 10 - iter 3210/3218 - loss 0.31120194 - samples/sec: 59.04 - lr: 0.100000
2021-05-24 17:01:11,470 ----------------------------------------------------------------------------------------------------
2021-05-24 17:01:11,470 EPOCH 10 done: loss 0.3111 - lr 0.1000000
2021-05-24 17:01:22,886 DEV : loss 0.2045580893754959 - score 0.958
2021-05-24 17:01:23,072 BAD EPOCHS (no improvement): 2
2021-05-24 17:01:23,073 ----------------------------------------------------------------------------------------------------
2021-05-24 17:02:49,750 epoch 11 - iter 321/3218 - loss 0.24640340 - samples/sec: 59.26 - lr: 0.100000
2021-05-24 17:04:16,671 epoch 11 - iter 642/3218 - loss 0.27044578 - samples/sec: 59.10 - lr: 0.100000
2021-05-24 17:05:43,065 epoch 11 - iter 963/3218 - loss 0.28510459 - samples/sec: 59.46 - lr: 0.100000
2021-05-24 17:07:10,863 epoch 11 - iter 1284/3218 - loss 0.28994382 - samples/sec: 58.51 - lr: 0.100000
2021-05-24 17:08:38,703 epoch 11 - iter 1605/3218 - loss 0.29704299 - samples/sec: 58.48 - lr: 0.100000
2021-05-24 17:10:05,757 epoch 11 - iter 1926/3218 - loss 0.29701570 - samples/sec: 59.01 - lr: 0.100000
2021-05-24 17:11:33,508 epoch 11 - iter 2247/3218 - loss 0.29394341 - samples/sec: 58.54 - lr: 0.100000
2021-05-24 17:13:00,798 epoch 11 - iter 2568/3218 - loss 0.29032592 - samples/sec: 58.85 - lr: 0.100000
2021-05-24 17:14:27,537 epoch 11 - iter 2889/3218 - loss 0.29286799 - samples/sec: 59.22 - lr: 0.100000
2021-05-24 17:15:54,270 epoch 11 - iter 3210/3218 - loss 0.29357740 - samples/sec: 59.22 - lr: 0.100000
2021-05-24 17:15:56,355 ----------------------------------------------------------------------------------------------------
2021-05-24 17:15:56,355 EPOCH 11 done: loss 0.2938 - lr 0.1000000
2021-05-24 17:16:07,774 DEV : loss 0.20128516852855682 - score 0.9589
2021-05-24 17:16:07,959 BAD EPOCHS (no improvement): 3
2021-05-24 17:16:07,960 ----------------------------------------------------------------------------------------------------
2021-05-24 17:17:35,313 epoch 12 - iter 321/3218 - loss 0.29098290 - samples/sec: 58.80 - lr: 0.100000
2021-05-24 17:19:02,189 epoch 12 - iter 642/3218 - loss 0.27713653 - samples/sec: 59.13 - lr: 0.100000
2021-05-24 17:20:29,046 epoch 12 - iter 963/3218 - loss 0.27804785 - samples/sec: 59.14 - lr: 0.100000
2021-05-24 17:21:56,293 epoch 12 - iter 1284/3218 - loss 0.28182911 - samples/sec: 58.88 - lr: 0.100000
2021-05-24 17:23:22,856 epoch 12 - iter 1605/3218 - loss 0.28280130 - samples/sec: 59.34 - lr: 0.100000
2021-05-24 17:24:50,032 epoch 12 - iter 1926/3218 - loss 0.28344111 - samples/sec: 58.92 - lr: 0.100000
2021-05-24 17:26:18,183 epoch 12 - iter 2247/3218 - loss 0.28641035 - samples/sec: 58.27 - lr: 0.100000
2021-05-24 17:27:45,849 epoch 12 - iter 2568/3218 - loss 0.28657643 - samples/sec: 58.59 - lr: 0.100000
2021-05-24 17:29:12,831 epoch 12 - iter 2889/3218 - loss 0.28396635 - samples/sec: 59.05 - lr: 0.100000
2021-05-24 17:30:39,435 epoch 12 - iter 3210/3218 - loss 0.28634062 - samples/sec: 59.31 - lr: 0.100000
2021-05-24 17:30:41,510 ----------------------------------------------------------------------------------------------------
2021-05-24 17:30:41,510 EPOCH 12 done: loss 0.2863 - lr 0.1000000
2021-05-24 17:30:52,928 DEV : loss 0.20694144070148468 - score 0.9607
Epoch    12: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 17:30:53,113 BAD EPOCHS (no improvement): 4
2021-05-24 17:30:53,113 ----------------------------------------------------------------------------------------------------
2021-05-24 17:32:19,932 epoch 13 - iter 321/3218 - loss 0.27940492 - samples/sec: 59.17 - lr: 0.050000
2021-05-24 17:33:46,481 epoch 13 - iter 642/3218 - loss 0.27124176 - samples/sec: 59.35 - lr: 0.050000
2021-05-24 17:35:13,162 epoch 13 - iter 963/3218 - loss 0.26314589 - samples/sec: 59.26 - lr: 0.050000
2021-05-24 17:36:39,753 epoch 13 - iter 1284/3218 - loss 0.26972450 - samples/sec: 59.32 - lr: 0.050000
2021-05-24 17:38:06,241 epoch 13 - iter 1605/3218 - loss 0.26505116 - samples/sec: 59.39 - lr: 0.050000
2021-05-24 17:39:33,012 epoch 13 - iter 1926/3218 - loss 0.26402263 - samples/sec: 59.20 - lr: 0.050000
2021-05-24 17:41:00,657 epoch 13 - iter 2247/3218 - loss 0.26181103 - samples/sec: 58.61 - lr: 0.050000
2021-05-24 17:42:28,154 epoch 13 - iter 2568/3218 - loss 0.26062587 - samples/sec: 58.71 - lr: 0.050000
2021-05-24 17:43:55,412 epoch 13 - iter 2889/3218 - loss 0.25732939 - samples/sec: 58.87 - lr: 0.050000
2021-05-24 17:45:22,452 epoch 13 - iter 3210/3218 - loss 0.25705618 - samples/sec: 59.01 - lr: 0.050000
2021-05-24 17:45:24,505 ----------------------------------------------------------------------------------------------------
2021-05-24 17:45:24,506 EPOCH 13 done: loss 0.2569 - lr 0.0500000
2021-05-24 17:45:35,918 DEV : loss 0.1835816502571106 - score 0.9608
2021-05-24 17:45:36,103 BAD EPOCHS (no improvement): 1
2021-05-24 17:45:36,103 ----------------------------------------------------------------------------------------------------
2021-05-24 17:47:02,991 epoch 14 - iter 321/3218 - loss 0.25057021 - samples/sec: 59.12 - lr: 0.050000
2021-05-24 17:48:29,674 epoch 14 - iter 642/3218 - loss 0.24177303 - samples/sec: 59.26 - lr: 0.050000
2021-05-24 17:49:57,341 epoch 14 - iter 963/3218 - loss 0.25094629 - samples/sec: 58.59 - lr: 0.050000
2021-05-24 17:51:24,269 epoch 14 - iter 1284/3218 - loss 0.25254742 - samples/sec: 59.09 - lr: 0.050000
2021-05-24 17:52:50,993 epoch 14 - iter 1605/3218 - loss 0.25321478 - samples/sec: 59.23 - lr: 0.050000
2021-05-24 17:54:17,931 epoch 14 - iter 1926/3218 - loss 0.25219162 - samples/sec: 59.08 - lr: 0.050000
2021-05-24 17:55:44,347 epoch 14 - iter 2247/3218 - loss 0.24812581 - samples/sec: 59.44 - lr: 0.050000
2021-05-24 17:57:11,513 epoch 14 - iter 2568/3218 - loss 0.25014572 - samples/sec: 58.93 - lr: 0.050000
2021-05-24 17:58:39,333 epoch 14 - iter 2889/3218 - loss 0.24910625 - samples/sec: 58.49 - lr: 0.050000
2021-05-24 18:00:06,813 epoch 14 - iter 3210/3218 - loss 0.24769470 - samples/sec: 58.72 - lr: 0.050000
2021-05-24 18:00:08,879 ----------------------------------------------------------------------------------------------------
2021-05-24 18:00:08,879 EPOCH 14 done: loss 0.2481 - lr 0.0500000
2021-05-24 18:00:20,313 DEV : loss 0.17557616531848907 - score 0.9638
2021-05-24 18:00:20,499 BAD EPOCHS (no improvement): 2
2021-05-24 18:00:20,499 ----------------------------------------------------------------------------------------------------
2021-05-24 18:01:48,173 epoch 15 - iter 321/3218 - loss 0.23796572 - samples/sec: 58.59 - lr: 0.050000
2021-05-24 18:03:15,065 epoch 15 - iter 642/3218 - loss 0.24095530 - samples/sec: 59.12 - lr: 0.050000
2021-05-24 18:04:41,614 epoch 15 - iter 963/3218 - loss 0.23603426 - samples/sec: 59.35 - lr: 0.050000
2021-05-24 18:06:08,261 epoch 15 - iter 1284/3218 - loss 0.23401499 - samples/sec: 59.28 - lr: 0.050000
2021-05-24 18:07:36,142 epoch 15 - iter 1605/3218 - loss 0.23645103 - samples/sec: 58.45 - lr: 0.050000
2021-05-24 18:09:03,442 epoch 15 - iter 1926/3218 - loss 0.24192685 - samples/sec: 58.84 - lr: 0.050000
2021-05-24 18:10:30,739 epoch 15 - iter 2247/3218 - loss 0.24136210 - samples/sec: 58.84 - lr: 0.050000
2021-05-24 18:11:58,369 epoch 15 - iter 2568/3218 - loss 0.24096347 - samples/sec: 58.62 - lr: 0.050000
2021-05-24 18:13:25,691 epoch 15 - iter 2889/3218 - loss 0.23976194 - samples/sec: 58.82 - lr: 0.050000
2021-05-24 18:14:52,552 epoch 15 - iter 3210/3218 - loss 0.24107788 - samples/sec: 59.14 - lr: 0.050000
2021-05-24 18:14:54,615 ----------------------------------------------------------------------------------------------------
2021-05-24 18:14:54,615 EPOCH 15 done: loss 0.2410 - lr 0.0500000
2021-05-24 18:15:06,051 DEV : loss 0.17983128130435944 - score 0.9625
2021-05-24 18:15:06,236 BAD EPOCHS (no improvement): 3
2021-05-24 18:15:06,236 ----------------------------------------------------------------------------------------------------
2021-05-24 18:16:33,631 epoch 16 - iter 321/3218 - loss 0.23222377 - samples/sec: 58.78 - lr: 0.050000
2021-05-24 18:18:00,881 epoch 16 - iter 642/3218 - loss 0.24631418 - samples/sec: 58.87 - lr: 0.050000
2021-05-24 18:19:27,691 epoch 16 - iter 963/3218 - loss 0.24139884 - samples/sec: 59.17 - lr: 0.050000
2021-05-24 18:20:54,615 epoch 16 - iter 1284/3218 - loss 0.24469782 - samples/sec: 59.09 - lr: 0.050000
2021-05-24 18:22:21,769 epoch 16 - iter 1605/3218 - loss 0.24339027 - samples/sec: 58.94 - lr: 0.050000
2021-05-24 18:23:48,420 epoch 16 - iter 1926/3218 - loss 0.24322443 - samples/sec: 59.28 - lr: 0.050000
2021-05-24 18:25:15,966 epoch 16 - iter 2247/3218 - loss 0.23835005 - samples/sec: 58.67 - lr: 0.050000
2021-05-24 18:26:43,156 epoch 16 - iter 2568/3218 - loss 0.23782197 - samples/sec: 58.91 - lr: 0.050000
2021-05-24 18:28:10,409 epoch 16 - iter 2889/3218 - loss 0.23630795 - samples/sec: 58.87 - lr: 0.050000
2021-05-24 18:29:37,851 epoch 16 - iter 3210/3218 - loss 0.23646894 - samples/sec: 58.74 - lr: 0.050000
2021-05-24 18:29:39,937 ----------------------------------------------------------------------------------------------------
2021-05-24 18:29:39,937 EPOCH 16 done: loss 0.2366 - lr 0.0500000
2021-05-24 18:29:51,383 DEV : loss 0.17467853426933289 - score 0.9587
Epoch    16: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 18:29:51,569 BAD EPOCHS (no improvement): 4
2021-05-24 18:29:51,570 ----------------------------------------------------------------------------------------------------
2021-05-24 18:31:19,117 epoch 17 - iter 321/3218 - loss 0.22307582 - samples/sec: 58.67 - lr: 0.025000
2021-05-24 18:32:45,882 epoch 17 - iter 642/3218 - loss 0.23908851 - samples/sec: 59.20 - lr: 0.025000
2021-05-24 18:34:12,628 epoch 17 - iter 963/3218 - loss 0.23411392 - samples/sec: 59.22 - lr: 0.025000
2021-05-24 18:35:39,225 epoch 17 - iter 1284/3218 - loss 0.23724158 - samples/sec: 59.32 - lr: 0.025000
2021-05-24 18:37:06,375 epoch 17 - iter 1605/3218 - loss 0.23570603 - samples/sec: 58.94 - lr: 0.025000
2021-05-24 18:38:33,787 epoch 17 - iter 1926/3218 - loss 0.23593716 - samples/sec: 58.76 - lr: 0.025000
2021-05-24 18:40:00,876 epoch 17 - iter 2247/3218 - loss 0.23455721 - samples/sec: 58.98 - lr: 0.025000
2021-05-24 18:41:28,439 epoch 17 - iter 2568/3218 - loss 0.22971546 - samples/sec: 58.66 - lr: 0.025000
2021-05-24 18:42:58,960 epoch 17 - iter 2889/3218 - loss 0.22658330 - samples/sec: 56.75 - lr: 0.025000
2021-05-24 18:44:26,375 epoch 17 - iter 3210/3218 - loss 0.22533655 - samples/sec: 58.76 - lr: 0.025000
2021-05-24 18:44:28,479 ----------------------------------------------------------------------------------------------------
2021-05-24 18:44:28,479 EPOCH 17 done: loss 0.2251 - lr 0.0250000
2021-05-24 18:44:39,913 DEV : loss 0.16596047580242157 - score 0.9603
2021-05-24 18:44:40,098 BAD EPOCHS (no improvement): 1
2021-05-24 18:44:40,098 ----------------------------------------------------------------------------------------------------
2021-05-24 18:46:06,952 epoch 18 - iter 321/3218 - loss 0.22630863 - samples/sec: 59.14 - lr: 0.025000
2021-05-24 18:47:33,811 epoch 18 - iter 642/3218 - loss 0.21679665 - samples/sec: 59.14 - lr: 0.025000
2021-05-24 18:49:00,725 epoch 18 - iter 963/3218 - loss 0.21699153 - samples/sec: 59.10 - lr: 0.025000
2021-05-24 18:50:27,620 epoch 18 - iter 1284/3218 - loss 0.22420471 - samples/sec: 59.11 - lr: 0.025000
2021-05-24 18:51:54,412 epoch 18 - iter 1605/3218 - loss 0.22236476 - samples/sec: 59.18 - lr: 0.025000
2021-05-24 18:53:21,194 epoch 18 - iter 1926/3218 - loss 0.22206769 - samples/sec: 59.19 - lr: 0.025000
2021-05-24 18:54:47,719 epoch 18 - iter 2247/3218 - loss 0.22053305 - samples/sec: 59.37 - lr: 0.025000
2021-05-24 18:56:14,537 epoch 18 - iter 2568/3218 - loss 0.22136668 - samples/sec: 59.17 - lr: 0.025000
2021-05-24 18:57:41,385 epoch 18 - iter 2889/3218 - loss 0.22073109 - samples/sec: 59.15 - lr: 0.025000
2021-05-24 18:59:08,212 epoch 18 - iter 3210/3218 - loss 0.21962854 - samples/sec: 59.16 - lr: 0.025000
2021-05-24 18:59:10,278 ----------------------------------------------------------------------------------------------------
2021-05-24 18:59:10,278 EPOCH 18 done: loss 0.2195 - lr 0.0250000
2021-05-24 18:59:21,736 DEV : loss 0.16782799363136292 - score 0.9576
2021-05-24 18:59:21,922 BAD EPOCHS (no improvement): 2
2021-05-24 18:59:21,923 ----------------------------------------------------------------------------------------------------
2021-05-24 19:00:49,180 epoch 19 - iter 321/3218 - loss 0.20592688 - samples/sec: 58.87 - lr: 0.025000
2021-05-24 19:02:16,524 epoch 19 - iter 642/3218 - loss 0.21168782 - samples/sec: 58.81 - lr: 0.025000
2021-05-24 19:03:43,933 epoch 19 - iter 963/3218 - loss 0.21663254 - samples/sec: 58.77 - lr: 0.025000
2021-05-24 19:05:11,106 epoch 19 - iter 1284/3218 - loss 0.21250394 - samples/sec: 58.92 - lr: 0.025000
2021-05-24 19:06:38,053 epoch 19 - iter 1605/3218 - loss 0.21362393 - samples/sec: 59.08 - lr: 0.025000
2021-05-24 19:08:04,932 epoch 19 - iter 1926/3218 - loss 0.21742184 - samples/sec: 59.12 - lr: 0.025000
2021-05-24 19:09:32,386 epoch 19 - iter 2247/3218 - loss 0.21771313 - samples/sec: 58.74 - lr: 0.025000
2021-05-24 19:10:59,118 epoch 19 - iter 2568/3218 - loss 0.21697311 - samples/sec: 59.22 - lr: 0.025000
2021-05-24 19:12:26,352 epoch 19 - iter 2889/3218 - loss 0.21711845 - samples/sec: 58.88 - lr: 0.025000
2021-05-24 19:13:53,531 epoch 19 - iter 3210/3218 - loss 0.21679637 - samples/sec: 58.92 - lr: 0.025000
2021-05-24 19:13:55,566 ----------------------------------------------------------------------------------------------------
2021-05-24 19:13:55,566 EPOCH 19 done: loss 0.2166 - lr 0.0250000
2021-05-24 19:14:07,015 DEV : loss 0.16679637134075165 - score 0.9576
2021-05-24 19:14:07,198 BAD EPOCHS (no improvement): 3
2021-05-24 19:14:07,198 ----------------------------------------------------------------------------------------------------
2021-05-24 19:15:34,559 epoch 20 - iter 321/3218 - loss 0.19361547 - samples/sec: 58.80 - lr: 0.025000
2021-05-24 19:17:01,518 epoch 20 - iter 642/3218 - loss 0.20593482 - samples/sec: 59.07 - lr: 0.025000
2021-05-24 19:18:27,985 epoch 20 - iter 963/3218 - loss 0.21444626 - samples/sec: 59.41 - lr: 0.025000
2021-05-24 19:19:54,568 epoch 20 - iter 1284/3218 - loss 0.21473144 - samples/sec: 59.33 - lr: 0.025000
2021-05-24 19:21:21,046 epoch 20 - iter 1605/3218 - loss 0.21607128 - samples/sec: 59.40 - lr: 0.025000
2021-05-24 19:22:47,652 epoch 20 - iter 1926/3218 - loss 0.21510589 - samples/sec: 59.31 - lr: 0.025000
2021-05-24 19:24:14,496 epoch 20 - iter 2247/3218 - loss 0.21487167 - samples/sec: 59.15 - lr: 0.025000
2021-05-24 19:25:41,709 epoch 20 - iter 2568/3218 - loss 0.21371170 - samples/sec: 58.90 - lr: 0.025000
2021-05-24 19:27:08,443 epoch 20 - iter 2889/3218 - loss 0.21384795 - samples/sec: 59.22 - lr: 0.025000
2021-05-24 19:28:35,376 epoch 20 - iter 3210/3218 - loss 0.21358491 - samples/sec: 59.09 - lr: 0.025000
2021-05-24 19:28:37,444 ----------------------------------------------------------------------------------------------------
2021-05-24 19:28:37,445 EPOCH 20 done: loss 0.2134 - lr 0.0250000
2021-05-24 19:28:48,882 DEV : loss 0.15901720523834229 - score 0.9619
Epoch    20: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 19:28:49,068 BAD EPOCHS (no improvement): 4
2021-05-24 19:28:49,068 ----------------------------------------------------------------------------------------------------
2021-05-24 19:30:15,652 epoch 21 - iter 321/3218 - loss 0.21157388 - samples/sec: 59.33 - lr: 0.012500
2021-05-24 19:31:42,185 epoch 21 - iter 642/3218 - loss 0.21034488 - samples/sec: 59.36 - lr: 0.012500
2021-05-24 19:33:09,245 epoch 21 - iter 963/3218 - loss 0.21083412 - samples/sec: 59.00 - lr: 0.012500
2021-05-24 19:34:36,125 epoch 21 - iter 1284/3218 - loss 0.20998997 - samples/sec: 59.12 - lr: 0.012500
2021-05-24 19:36:03,901 epoch 21 - iter 1605/3218 - loss 0.21055273 - samples/sec: 58.52 - lr: 0.012500
2021-05-24 19:37:31,921 epoch 21 - iter 1926/3218 - loss 0.20902616 - samples/sec: 58.36 - lr: 0.012500
2021-05-24 19:38:59,687 epoch 21 - iter 2247/3218 - loss 0.20790688 - samples/sec: 58.53 - lr: 0.012500
2021-05-24 19:40:27,375 epoch 21 - iter 2568/3218 - loss 0.20649132 - samples/sec: 58.58 - lr: 0.012500
2021-05-24 19:41:54,910 epoch 21 - iter 2889/3218 - loss 0.20788348 - samples/sec: 58.68 - lr: 0.012500
2021-05-24 19:43:21,342 epoch 21 - iter 3210/3218 - loss 0.20667489 - samples/sec: 59.43 - lr: 0.012500
2021-05-24 19:43:23,410 ----------------------------------------------------------------------------------------------------
2021-05-24 19:43:23,410 EPOCH 21 done: loss 0.2066 - lr 0.0125000
2021-05-24 19:43:34,853 DEV : loss 0.15415287017822266 - score 0.9621
2021-05-24 19:43:35,041 BAD EPOCHS (no improvement): 1
2021-05-24 19:43:35,041 ----------------------------------------------------------------------------------------------------
2021-05-24 19:45:02,348 epoch 22 - iter 321/3218 - loss 0.20036434 - samples/sec: 58.84 - lr: 0.012500
2021-05-24 19:46:29,528 epoch 22 - iter 642/3218 - loss 0.20844076 - samples/sec: 58.92 - lr: 0.012500
2021-05-24 19:47:56,006 epoch 22 - iter 963/3218 - loss 0.20250772 - samples/sec: 59.40 - lr: 0.012500
2021-05-24 19:49:23,711 epoch 22 - iter 1284/3218 - loss 0.19994959 - samples/sec: 58.57 - lr: 0.012500
2021-05-24 19:50:51,128 epoch 22 - iter 1605/3218 - loss 0.20560659 - samples/sec: 58.76 - lr: 0.012500
2021-05-24 19:52:17,779 epoch 22 - iter 1926/3218 - loss 0.20581704 - samples/sec: 59.28 - lr: 0.012500
2021-05-24 19:53:44,989 epoch 22 - iter 2247/3218 - loss 0.20633224 - samples/sec: 58.90 - lr: 0.012500
2021-05-24 19:55:13,049 epoch 22 - iter 2568/3218 - loss 0.20491845 - samples/sec: 58.33 - lr: 0.012500
2021-05-24 19:56:40,429 epoch 22 - iter 2889/3218 - loss 0.20611686 - samples/sec: 58.79 - lr: 0.012500
2021-05-24 19:58:08,151 epoch 22 - iter 3210/3218 - loss 0.20590092 - samples/sec: 58.56 - lr: 0.012500
2021-05-24 19:58:10,237 ----------------------------------------------------------------------------------------------------
2021-05-24 19:58:10,238 EPOCH 22 done: loss 0.2059 - lr 0.0125000
2021-05-24 19:58:21,686 DEV : loss 0.1581682413816452 - score 0.9604
2021-05-24 19:58:21,872 BAD EPOCHS (no improvement): 2
2021-05-24 19:58:21,872 ----------------------------------------------------------------------------------------------------
2021-05-24 19:59:49,713 epoch 23 - iter 321/3218 - loss 0.20149377 - samples/sec: 58.48 - lr: 0.012500
2021-05-24 20:01:17,030 epoch 23 - iter 642/3218 - loss 0.20353903 - samples/sec: 58.83 - lr: 0.012500
2021-05-24 20:02:44,157 epoch 23 - iter 963/3218 - loss 0.19877102 - samples/sec: 58.96 - lr: 0.012500
2021-05-24 20:04:11,272 epoch 23 - iter 1284/3218 - loss 0.19941733 - samples/sec: 58.96 - lr: 0.012500
2021-05-24 20:05:39,057 epoch 23 - iter 1605/3218 - loss 0.20075558 - samples/sec: 58.51 - lr: 0.012500
2021-05-24 20:07:06,535 epoch 23 - iter 1926/3218 - loss 0.20144360 - samples/sec: 58.72 - lr: 0.012500
2021-05-24 20:08:33,812 epoch 23 - iter 2247/3218 - loss 0.20406456 - samples/sec: 58.86 - lr: 0.012500
2021-05-24 20:10:01,243 epoch 23 - iter 2568/3218 - loss 0.20555466 - samples/sec: 58.75 - lr: 0.012500
2021-05-24 20:11:28,255 epoch 23 - iter 2889/3218 - loss 0.20499283 - samples/sec: 59.03 - lr: 0.012500
2021-05-24 20:12:56,094 epoch 23 - iter 3210/3218 - loss 0.20420920 - samples/sec: 58.48 - lr: 0.012500
2021-05-24 20:12:58,203 ----------------------------------------------------------------------------------------------------
2021-05-24 20:12:58,203 EPOCH 23 done: loss 0.2042 - lr 0.0125000
2021-05-24 20:13:09,648 DEV : loss 0.15550225973129272 - score 0.9634
2021-05-24 20:13:09,833 BAD EPOCHS (no improvement): 3
2021-05-24 20:13:09,834 ----------------------------------------------------------------------------------------------------
2021-05-24 20:14:37,528 epoch 24 - iter 321/3218 - loss 0.17193652 - samples/sec: 58.58 - lr: 0.012500
2021-05-24 20:16:05,720 epoch 24 - iter 642/3218 - loss 0.19109902 - samples/sec: 58.24 - lr: 0.012500
2021-05-24 20:17:33,901 epoch 24 - iter 963/3218 - loss 0.19073282 - samples/sec: 58.25 - lr: 0.012500
2021-05-24 20:19:01,931 epoch 24 - iter 1284/3218 - loss 0.20143981 - samples/sec: 58.35 - lr: 0.012500
2021-05-24 20:20:29,320 epoch 24 - iter 1605/3218 - loss 0.20264463 - samples/sec: 58.78 - lr: 0.012500
2021-05-24 20:21:56,215 epoch 24 - iter 1926/3218 - loss 0.20524623 - samples/sec: 59.11 - lr: 0.012500
2021-05-24 20:23:24,061 epoch 24 - iter 2247/3218 - loss 0.20448087 - samples/sec: 58.47 - lr: 0.012500
2021-05-24 20:24:51,927 epoch 24 - iter 2568/3218 - loss 0.20150479 - samples/sec: 58.46 - lr: 0.012500
2021-05-24 20:26:18,685 epoch 24 - iter 2889/3218 - loss 0.20067603 - samples/sec: 59.21 - lr: 0.012500
2021-05-24 20:27:46,319 epoch 24 - iter 3210/3218 - loss 0.20140890 - samples/sec: 58.62 - lr: 0.012500
2021-05-24 20:27:48,428 ----------------------------------------------------------------------------------------------------
2021-05-24 20:27:48,428 EPOCH 24 done: loss 0.2013 - lr 0.0125000
2021-05-24 20:28:03,079 DEV : loss 0.15748246014118195 - score 0.963
Epoch    24: reducing learning rate of group 0 to 6.2500e-03.
2021-05-24 20:28:03,267 BAD EPOCHS (no improvement): 4
2021-05-24 20:28:03,267 ----------------------------------------------------------------------------------------------------
2021-05-24 20:29:31,028 epoch 25 - iter 321/3218 - loss 0.20432731 - samples/sec: 58.53 - lr: 0.006250
2021-05-24 20:30:58,934 epoch 25 - iter 642/3218 - loss 0.20102826 - samples/sec: 58.43 - lr: 0.006250
2021-05-24 20:32:26,513 epoch 25 - iter 963/3218 - loss 0.20302131 - samples/sec: 58.65 - lr: 0.006250
2021-05-24 20:33:54,222 epoch 25 - iter 1284/3218 - loss 0.19890271 - samples/sec: 58.56 - lr: 0.006250
2021-05-24 20:35:21,424 epoch 25 - iter 1605/3218 - loss 0.20139412 - samples/sec: 58.91 - lr: 0.006250
2021-05-24 20:36:47,742 epoch 25 - iter 1926/3218 - loss 0.19876026 - samples/sec: 59.51 - lr: 0.006250
2021-05-24 20:38:14,421 epoch 25 - iter 2247/3218 - loss 0.19929242 - samples/sec: 59.26 - lr: 0.006250
2021-05-24 20:39:41,500 epoch 25 - iter 2568/3218 - loss 0.19850222 - samples/sec: 58.99 - lr: 0.006250
2021-05-24 20:41:08,257 epoch 25 - iter 2889/3218 - loss 0.19978655 - samples/sec: 59.21 - lr: 0.006250
2021-05-24 20:42:35,530 epoch 25 - iter 3210/3218 - loss 0.19921811 - samples/sec: 58.86 - lr: 0.006250
2021-05-24 20:42:37,641 ----------------------------------------------------------------------------------------------------
2021-05-24 20:42:37,641 EPOCH 25 done: loss 0.1994 - lr 0.0062500
2021-05-24 20:42:49,102 DEV : loss 0.1504441350698471 - score 0.9626
2021-05-24 20:42:49,289 BAD EPOCHS (no improvement): 1
2021-05-24 20:42:49,290 ----------------------------------------------------------------------------------------------------
2021-05-24 20:44:17,095 epoch 26 - iter 321/3218 - loss 0.19512327 - samples/sec: 58.50 - lr: 0.006250
2021-05-24 20:45:44,956 epoch 26 - iter 642/3218 - loss 0.19815655 - samples/sec: 58.46 - lr: 0.006250
2021-05-24 20:47:12,199 epoch 26 - iter 963/3218 - loss 0.20141553 - samples/sec: 58.88 - lr: 0.006250
2021-05-24 20:48:38,984 epoch 26 - iter 1284/3218 - loss 0.19990826 - samples/sec: 59.19 - lr: 0.006250
2021-05-24 20:50:05,960 epoch 26 - iter 1605/3218 - loss 0.19658266 - samples/sec: 59.06 - lr: 0.006250
2021-05-24 20:51:33,008 epoch 26 - iter 1926/3218 - loss 0.20030585 - samples/sec: 59.01 - lr: 0.006250
2021-05-24 20:52:59,816 epoch 26 - iter 2247/3218 - loss 0.19868156 - samples/sec: 59.17 - lr: 0.006250
2021-05-24 20:54:26,796 epoch 26 - iter 2568/3218 - loss 0.19829367 - samples/sec: 59.06 - lr: 0.006250
2021-05-24 20:55:54,487 epoch 26 - iter 2889/3218 - loss 0.19899160 - samples/sec: 58.58 - lr: 0.006250
2021-05-24 20:57:22,289 epoch 26 - iter 3210/3218 - loss 0.20007606 - samples/sec: 58.50 - lr: 0.006250
2021-05-24 20:57:24,396 ----------------------------------------------------------------------------------------------------
2021-05-24 20:57:24,396 EPOCH 26 done: loss 0.1998 - lr 0.0062500
2021-05-24 20:57:35,842 DEV : loss 0.15173454582691193 - score 0.9635
2021-05-24 20:57:36,028 BAD EPOCHS (no improvement): 2
2021-05-24 20:57:36,028 ----------------------------------------------------------------------------------------------------
2021-05-24 20:59:02,917 epoch 27 - iter 321/3218 - loss 0.18936242 - samples/sec: 59.12 - lr: 0.006250
2021-05-24 21:00:29,798 epoch 27 - iter 642/3218 - loss 0.18676408 - samples/sec: 59.12 - lr: 0.006250
2021-05-24 21:01:56,889 epoch 27 - iter 963/3218 - loss 0.19022261 - samples/sec: 58.98 - lr: 0.006250
2021-05-24 21:03:24,024 epoch 27 - iter 1284/3218 - loss 0.19382516 - samples/sec: 58.95 - lr: 0.006250
2021-05-24 21:04:51,255 epoch 27 - iter 1605/3218 - loss 0.19632298 - samples/sec: 58.89 - lr: 0.006250
2021-05-24 21:06:18,553 epoch 27 - iter 1926/3218 - loss 0.19978268 - samples/sec: 58.84 - lr: 0.006250
2021-05-24 21:07:45,834 epoch 27 - iter 2247/3218 - loss 0.19855790 - samples/sec: 58.85 - lr: 0.006250
2021-05-24 21:09:13,410 epoch 27 - iter 2568/3218 - loss 0.19908140 - samples/sec: 58.65 - lr: 0.006250
2021-05-24 21:10:40,851 epoch 27 - iter 2889/3218 - loss 0.19871342 - samples/sec: 58.74 - lr: 0.006250
2021-05-24 21:12:07,779 epoch 27 - iter 3210/3218 - loss 0.19712828 - samples/sec: 59.09 - lr: 0.006250
2021-05-24 21:12:09,867 ----------------------------------------------------------------------------------------------------
2021-05-24 21:12:09,868 EPOCH 27 done: loss 0.1969 - lr 0.0062500
2021-05-24 21:12:21,315 DEV : loss 0.15083636343479156 - score 0.9629
2021-05-24 21:12:21,501 BAD EPOCHS (no improvement): 3
2021-05-24 21:12:21,501 ----------------------------------------------------------------------------------------------------
2021-05-24 21:13:48,763 epoch 28 - iter 321/3218 - loss 0.20052046 - samples/sec: 58.87 - lr: 0.006250
2021-05-24 21:15:16,166 epoch 28 - iter 642/3218 - loss 0.19911136 - samples/sec: 58.77 - lr: 0.006250
2021-05-24 21:16:44,033 epoch 28 - iter 963/3218 - loss 0.19322848 - samples/sec: 58.46 - lr: 0.006250
2021-05-24 21:18:11,924 epoch 28 - iter 1284/3218 - loss 0.20063138 - samples/sec: 58.44 - lr: 0.006250
2021-05-24 21:19:39,533 epoch 28 - iter 1605/3218 - loss 0.19607894 - samples/sec: 58.63 - lr: 0.006250
2021-05-24 21:21:06,783 epoch 28 - iter 1926/3218 - loss 0.19692033 - samples/sec: 58.87 - lr: 0.006250
2021-05-24 21:22:33,712 epoch 28 - iter 2247/3218 - loss 0.19753494 - samples/sec: 59.09 - lr: 0.006250
2021-05-24 21:24:01,167 epoch 28 - iter 2568/3218 - loss 0.19602676 - samples/sec: 58.73 - lr: 0.006250
2021-05-24 21:25:28,080 epoch 28 - iter 2889/3218 - loss 0.19433707 - samples/sec: 59.10 - lr: 0.006250
2021-05-24 21:26:55,009 epoch 28 - iter 3210/3218 - loss 0.19484962 - samples/sec: 59.09 - lr: 0.006250
2021-05-24 21:26:57,071 ----------------------------------------------------------------------------------------------------
2021-05-24 21:26:57,072 EPOCH 28 done: loss 0.1946 - lr 0.0062500
2021-05-24 21:27:08,527 DEV : loss 0.15130077302455902 - score 0.9637
Epoch    28: reducing learning rate of group 0 to 3.1250e-03.
2021-05-24 21:27:08,714 BAD EPOCHS (no improvement): 4
2021-05-24 21:27:08,714 ----------------------------------------------------------------------------------------------------
2021-05-24 21:28:36,307 epoch 29 - iter 321/3218 - loss 0.19174990 - samples/sec: 58.64 - lr: 0.003125
2021-05-24 21:30:04,063 epoch 29 - iter 642/3218 - loss 0.19772071 - samples/sec: 58.53 - lr: 0.003125
2021-05-24 21:31:31,222 epoch 29 - iter 963/3218 - loss 0.19583976 - samples/sec: 58.93 - lr: 0.003125
2021-05-24 21:32:59,005 epoch 29 - iter 1284/3218 - loss 0.19689422 - samples/sec: 58.52 - lr: 0.003125
2021-05-24 21:34:26,316 epoch 29 - iter 1605/3218 - loss 0.19890720 - samples/sec: 58.83 - lr: 0.003125
2021-05-24 21:35:53,490 epoch 29 - iter 1926/3218 - loss 0.19750397 - samples/sec: 58.92 - lr: 0.003125
2021-05-24 21:37:20,611 epoch 29 - iter 2247/3218 - loss 0.19652228 - samples/sec: 58.96 - lr: 0.003125
2021-05-24 21:38:47,595 epoch 29 - iter 2568/3218 - loss 0.19638881 - samples/sec: 59.05 - lr: 0.003125
2021-05-24 21:40:14,843 epoch 29 - iter 2889/3218 - loss 0.19398412 - samples/sec: 58.87 - lr: 0.003125
2021-05-24 21:41:41,808 epoch 29 - iter 3210/3218 - loss 0.19396936 - samples/sec: 59.07 - lr: 0.003125
2021-05-24 21:41:43,838 ----------------------------------------------------------------------------------------------------
2021-05-24 21:41:43,838 EPOCH 29 done: loss 0.1941 - lr 0.0031250
2021-05-24 21:41:55,290 DEV : loss 0.15048493444919586 - score 0.9656
2021-05-24 21:41:55,476 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 21:42:05,547 ----------------------------------------------------------------------------------------------------
2021-05-24 21:43:33,408 epoch 30 - iter 321/3218 - loss 0.20561279 - samples/sec: 58.47 - lr: 0.003125
2021-05-24 21:45:01,359 epoch 30 - iter 642/3218 - loss 0.19380581 - samples/sec: 58.40 - lr: 0.003125
2021-05-24 21:46:28,194 epoch 30 - iter 963/3218 - loss 0.19326814 - samples/sec: 59.15 - lr: 0.003125
2021-05-24 21:47:55,362 epoch 30 - iter 1284/3218 - loss 0.19275701 - samples/sec: 58.93 - lr: 0.003125
2021-05-24 21:49:23,045 epoch 30 - iter 1605/3218 - loss 0.19160362 - samples/sec: 58.58 - lr: 0.003125
2021-05-24 21:50:50,182 epoch 30 - iter 1926/3218 - loss 0.19075831 - samples/sec: 58.95 - lr: 0.003125
2021-05-24 21:52:17,216 epoch 30 - iter 2247/3218 - loss 0.19215724 - samples/sec: 59.02 - lr: 0.003125
2021-05-24 21:53:44,335 epoch 30 - iter 2568/3218 - loss 0.19022274 - samples/sec: 58.96 - lr: 0.003125
2021-05-24 21:55:11,331 epoch 30 - iter 2889/3218 - loss 0.19298931 - samples/sec: 59.04 - lr: 0.003125
2021-05-24 21:56:38,069 epoch 30 - iter 3210/3218 - loss 0.19301729 - samples/sec: 59.22 - lr: 0.003125
2021-05-24 21:56:40,117 ----------------------------------------------------------------------------------------------------
2021-05-24 21:56:40,117 EPOCH 30 done: loss 0.1928 - lr 0.0031250
2021-05-24 21:56:51,574 DEV : loss 0.15110357105731964 - score 0.9653
2021-05-24 21:56:51,762 BAD EPOCHS (no improvement): 1
2021-05-24 21:56:52,947 ----------------------------------------------------------------------------------------------------
2021-05-24 21:56:52,947 Testing using best model ...
2021-05-24 21:56:52,947 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/eng.pdtb.pdtb/best-model.pt
2021-05-24 21:58:10,343 0.9598	0.9497	0.9547
2021-05-24 21:58:10,344 
Results:
- F1-score (micro) 0.9547
- F1-score (macro) 0.9547

By class:
SENT       tp: 2245 - fp: 94 - fn: 119 - precision: 0.9598 - recall: 0.9497 - f1-score: 0.9547
2021-05-24 21:58:10,344 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum/
2021-05-24 21:58:10,404 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum
2021-05-24 21:58:10,405 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum/sent_train.txt
2021-05-24 21:58:10,407 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum/sent_dev.txt
2021-05-24 21:58:10,408 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum/sent_test.txt
Corpus: 3228 train + 752 dev + 767 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 21:58:31,633 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,636 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 21:58:31,636 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,636 Corpus: "Corpus: 3228 train + 752 dev + 767 test sentences"
2021-05-24 21:58:31,636 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,636 Parameters:
2021-05-24 21:58:31,636  - learning_rate: "0.1"
2021-05-24 21:58:31,636  - mini_batch_size: "16"
2021-05-24 21:58:31,636  - patience: "3"
2021-05-24 21:58:31,636  - anneal_factor: "0.5"
2021-05-24 21:58:31,637  - max_epochs: "30"
2021-05-24 21:58:31,637  - shuffle: "True"
2021-05-24 21:58:31,637  - train_with_dev: "False"
2021-05-24 21:58:31,637  - batch_growth_annealing: "False"
2021-05-24 21:58:31,637 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,637 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum"
2021-05-24 21:58:31,637 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,637 Device: cuda:0
2021-05-24 21:58:31,637 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:31,637 Embeddings storage mode: cpu
2021-05-24 21:58:31,638 ----------------------------------------------------------------------------------------------------
2021-05-24 21:58:46,955 epoch 1 - iter 20/202 - loss 8.65370228 - samples/sec: 20.89 - lr: 0.100000
2021-05-24 21:59:02,575 epoch 1 - iter 40/202 - loss 6.91403561 - samples/sec: 20.49 - lr: 0.100000
2021-05-24 21:59:17,913 epoch 1 - iter 60/202 - loss 6.12429421 - samples/sec: 20.86 - lr: 0.100000
2021-05-24 21:59:33,315 epoch 1 - iter 80/202 - loss 5.58928505 - samples/sec: 20.78 - lr: 0.100000
2021-05-24 21:59:48,676 epoch 1 - iter 100/202 - loss 5.10377438 - samples/sec: 20.83 - lr: 0.100000
2021-05-24 22:00:04,893 epoch 1 - iter 120/202 - loss 4.66610629 - samples/sec: 19.73 - lr: 0.100000
2021-05-24 22:00:21,188 epoch 1 - iter 140/202 - loss 4.37151444 - samples/sec: 19.64 - lr: 0.100000
2021-05-24 22:00:37,532 epoch 1 - iter 160/202 - loss 4.12902880 - samples/sec: 19.58 - lr: 0.100000
2021-05-24 22:00:53,895 epoch 1 - iter 180/202 - loss 3.89888824 - samples/sec: 19.56 - lr: 0.100000
2021-05-24 22:01:10,623 epoch 1 - iter 200/202 - loss 3.68474107 - samples/sec: 19.13 - lr: 0.100000
2021-05-24 22:01:12,069 ----------------------------------------------------------------------------------------------------
2021-05-24 22:01:12,069 EPOCH 1 done: loss 3.6697 - lr 0.1000000
2021-05-24 22:01:34,624 DEV : loss 1.3536977767944336 - score 0.7465
2021-05-24 22:01:34,701 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:01:35,797 ----------------------------------------------------------------------------------------------------
2021-05-24 22:01:41,467 epoch 2 - iter 20/202 - loss 1.47948656 - samples/sec: 56.45 - lr: 0.100000
2021-05-24 22:01:47,164 epoch 2 - iter 40/202 - loss 1.46922923 - samples/sec: 56.18 - lr: 0.100000
2021-05-24 22:01:52,859 epoch 2 - iter 60/202 - loss 1.45008231 - samples/sec: 56.20 - lr: 0.100000
2021-05-24 22:01:58,580 epoch 2 - iter 80/202 - loss 1.41536360 - samples/sec: 55.94 - lr: 0.100000
2021-05-24 22:02:04,324 epoch 2 - iter 100/202 - loss 1.43084078 - samples/sec: 55.72 - lr: 0.100000
2021-05-24 22:02:10,087 epoch 2 - iter 120/202 - loss 1.41391439 - samples/sec: 55.53 - lr: 0.100000
2021-05-24 22:02:15,788 epoch 2 - iter 140/202 - loss 1.39803138 - samples/sec: 56.14 - lr: 0.100000
2021-05-24 22:02:21,525 epoch 2 - iter 160/202 - loss 1.41542209 - samples/sec: 55.78 - lr: 0.100000
2021-05-24 22:02:27,263 epoch 2 - iter 180/202 - loss 1.40378760 - samples/sec: 55.77 - lr: 0.100000
2021-05-24 22:02:32,948 epoch 2 - iter 200/202 - loss 1.38507482 - samples/sec: 56.30 - lr: 0.100000
2021-05-24 22:02:33,456 ----------------------------------------------------------------------------------------------------
2021-05-24 22:02:33,456 EPOCH 2 done: loss 1.3833 - lr 0.1000000
2021-05-24 22:02:38,004 DEV : loss 0.8903414607048035 - score 0.8701
2021-05-24 22:02:38,083 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:02:47,577 ----------------------------------------------------------------------------------------------------
2021-05-24 22:02:53,270 epoch 3 - iter 20/202 - loss 1.25536044 - samples/sec: 56.23 - lr: 0.100000
2021-05-24 22:02:58,951 epoch 3 - iter 40/202 - loss 1.25524395 - samples/sec: 56.33 - lr: 0.100000
2021-05-24 22:03:04,648 epoch 3 - iter 60/202 - loss 1.19703099 - samples/sec: 56.19 - lr: 0.100000
2021-05-24 22:03:10,288 epoch 3 - iter 80/202 - loss 1.22129392 - samples/sec: 56.75 - lr: 0.100000
2021-05-24 22:03:15,947 epoch 3 - iter 100/202 - loss 1.19074632 - samples/sec: 56.55 - lr: 0.100000
2021-05-24 22:03:21,594 epoch 3 - iter 120/202 - loss 1.19247712 - samples/sec: 56.67 - lr: 0.100000
2021-05-24 22:03:27,238 epoch 3 - iter 140/202 - loss 1.19646436 - samples/sec: 56.71 - lr: 0.100000
2021-05-24 22:03:32,874 epoch 3 - iter 160/202 - loss 1.19623016 - samples/sec: 56.79 - lr: 0.100000
2021-05-24 22:03:38,511 epoch 3 - iter 180/202 - loss 1.18977163 - samples/sec: 56.77 - lr: 0.100000
2021-05-24 22:03:44,158 epoch 3 - iter 200/202 - loss 1.16687716 - samples/sec: 56.68 - lr: 0.100000
2021-05-24 22:03:44,669 ----------------------------------------------------------------------------------------------------
2021-05-24 22:03:44,669 EPOCH 3 done: loss 1.1665 - lr 0.1000000
2021-05-24 22:03:49,588 DEV : loss 0.48416391015052795 - score 0.929
2021-05-24 22:03:49,666 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:03:59,371 ----------------------------------------------------------------------------------------------------
2021-05-24 22:04:05,065 epoch 4 - iter 20/202 - loss 0.97393001 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:04:10,724 epoch 4 - iter 40/202 - loss 1.02104142 - samples/sec: 56.55 - lr: 0.100000
2021-05-24 22:04:16,393 epoch 4 - iter 60/202 - loss 1.02236177 - samples/sec: 56.46 - lr: 0.100000
2021-05-24 22:04:22,081 epoch 4 - iter 80/202 - loss 1.08399656 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:04:27,737 epoch 4 - iter 100/202 - loss 1.09951953 - samples/sec: 56.59 - lr: 0.100000
2021-05-24 22:04:33,371 epoch 4 - iter 120/202 - loss 1.11436455 - samples/sec: 56.81 - lr: 0.100000
2021-05-24 22:04:39,027 epoch 4 - iter 140/202 - loss 1.10354058 - samples/sec: 56.59 - lr: 0.100000
2021-05-24 22:04:44,694 epoch 4 - iter 160/202 - loss 1.10046208 - samples/sec: 56.47 - lr: 0.100000
2021-05-24 22:04:50,398 epoch 4 - iter 180/202 - loss 1.09669130 - samples/sec: 56.11 - lr: 0.100000
2021-05-24 22:04:56,105 epoch 4 - iter 200/202 - loss 1.11011196 - samples/sec: 56.07 - lr: 0.100000
2021-05-24 22:04:56,609 ----------------------------------------------------------------------------------------------------
2021-05-24 22:04:56,609 EPOCH 4 done: loss 1.1052 - lr 0.1000000
2021-05-24 22:05:01,142 DEV : loss 0.46802186965942383 - score 0.9344
2021-05-24 22:05:01,220 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:05:10,704 ----------------------------------------------------------------------------------------------------
2021-05-24 22:05:16,425 epoch 5 - iter 20/202 - loss 0.79202889 - samples/sec: 55.95 - lr: 0.100000
2021-05-24 22:05:22,100 epoch 5 - iter 40/202 - loss 0.82515035 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:05:27,811 epoch 5 - iter 60/202 - loss 0.92025897 - samples/sec: 56.04 - lr: 0.100000
2021-05-24 22:05:33,484 epoch 5 - iter 80/202 - loss 0.89561749 - samples/sec: 56.42 - lr: 0.100000
2021-05-24 22:05:39,169 epoch 5 - iter 100/202 - loss 0.98467330 - samples/sec: 56.30 - lr: 0.100000
2021-05-24 22:05:44,838 epoch 5 - iter 120/202 - loss 0.98448767 - samples/sec: 56.45 - lr: 0.100000
2021-05-24 22:05:50,532 epoch 5 - iter 140/202 - loss 0.97462946 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:05:56,206 epoch 5 - iter 160/202 - loss 0.96847398 - samples/sec: 56.40 - lr: 0.100000
2021-05-24 22:06:01,861 epoch 5 - iter 180/202 - loss 0.95244245 - samples/sec: 56.60 - lr: 0.100000
2021-05-24 22:06:07,538 epoch 5 - iter 200/202 - loss 0.97925086 - samples/sec: 56.37 - lr: 0.100000
2021-05-24 22:06:08,059 ----------------------------------------------------------------------------------------------------
2021-05-24 22:06:08,060 EPOCH 5 done: loss 0.9817 - lr 0.1000000
2021-05-24 22:06:12,597 DEV : loss 0.4494171142578125 - score 0.9375
2021-05-24 22:06:12,675 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:06:22,381 ----------------------------------------------------------------------------------------------------
2021-05-24 22:06:28,069 epoch 6 - iter 20/202 - loss 0.92183064 - samples/sec: 56.28 - lr: 0.100000
2021-05-24 22:06:33,736 epoch 6 - iter 40/202 - loss 0.87741590 - samples/sec: 56.47 - lr: 0.100000
2021-05-24 22:06:39,460 epoch 6 - iter 60/202 - loss 0.94535166 - samples/sec: 55.92 - lr: 0.100000
2021-05-24 22:06:45,175 epoch 6 - iter 80/202 - loss 0.93698416 - samples/sec: 56.00 - lr: 0.100000
2021-05-24 22:06:50,830 epoch 6 - iter 100/202 - loss 0.93520497 - samples/sec: 56.60 - lr: 0.100000
2021-05-24 22:06:56,478 epoch 6 - iter 120/202 - loss 0.96707562 - samples/sec: 56.66 - lr: 0.100000
2021-05-24 22:07:02,180 epoch 6 - iter 140/202 - loss 0.98739321 - samples/sec: 56.13 - lr: 0.100000
2021-05-24 22:07:07,908 epoch 6 - iter 160/202 - loss 0.96804959 - samples/sec: 55.88 - lr: 0.100000
2021-05-24 22:07:13,597 epoch 6 - iter 180/202 - loss 0.97768305 - samples/sec: 56.25 - lr: 0.100000
2021-05-24 22:07:19,307 epoch 6 - iter 200/202 - loss 0.95462572 - samples/sec: 56.06 - lr: 0.100000
2021-05-24 22:07:19,812 ----------------------------------------------------------------------------------------------------
2021-05-24 22:07:19,813 EPOCH 6 done: loss 0.9565 - lr 0.1000000
2021-05-24 22:07:24,332 DEV : loss 0.5408069491386414 - score 0.9236
2021-05-24 22:07:24,409 BAD EPOCHS (no improvement): 1
2021-05-24 22:07:24,410 ----------------------------------------------------------------------------------------------------
2021-05-24 22:07:30,122 epoch 7 - iter 20/202 - loss 1.01914788 - samples/sec: 56.03 - lr: 0.100000
2021-05-24 22:07:35,866 epoch 7 - iter 40/202 - loss 0.89677853 - samples/sec: 55.72 - lr: 0.100000
2021-05-24 22:07:41,597 epoch 7 - iter 60/202 - loss 0.82480771 - samples/sec: 55.84 - lr: 0.100000
2021-05-24 22:07:47,289 epoch 7 - iter 80/202 - loss 0.84553254 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:07:52,997 epoch 7 - iter 100/202 - loss 0.90992791 - samples/sec: 56.08 - lr: 0.100000
2021-05-24 22:07:58,634 epoch 7 - iter 120/202 - loss 0.88354176 - samples/sec: 56.77 - lr: 0.100000
2021-05-24 22:08:04,308 epoch 7 - iter 140/202 - loss 0.89930677 - samples/sec: 56.42 - lr: 0.100000
2021-05-24 22:08:09,975 epoch 7 - iter 160/202 - loss 0.90311786 - samples/sec: 56.47 - lr: 0.100000
2021-05-24 22:08:15,707 epoch 7 - iter 180/202 - loss 0.89494871 - samples/sec: 55.84 - lr: 0.100000
2021-05-24 22:08:21,386 epoch 7 - iter 200/202 - loss 0.89424283 - samples/sec: 56.35 - lr: 0.100000
2021-05-24 22:08:21,898 ----------------------------------------------------------------------------------------------------
2021-05-24 22:08:21,898 EPOCH 7 done: loss 0.9054 - lr 0.1000000
2021-05-24 22:08:26,802 DEV : loss 0.442023903131485 - score 0.9328
2021-05-24 22:08:26,880 BAD EPOCHS (no improvement): 2
2021-05-24 22:08:26,880 ----------------------------------------------------------------------------------------------------
2021-05-24 22:08:32,509 epoch 8 - iter 20/202 - loss 1.17482927 - samples/sec: 56.86 - lr: 0.100000
2021-05-24 22:08:38,200 epoch 8 - iter 40/202 - loss 1.04857576 - samples/sec: 56.24 - lr: 0.100000
2021-05-24 22:08:43,859 epoch 8 - iter 60/202 - loss 0.95972751 - samples/sec: 56.55 - lr: 0.100000
2021-05-24 22:08:49,500 epoch 8 - iter 80/202 - loss 0.92591124 - samples/sec: 56.73 - lr: 0.100000
2021-05-24 22:08:55,189 epoch 8 - iter 100/202 - loss 0.90700472 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:09:00,865 epoch 8 - iter 120/202 - loss 0.87136935 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:09:06,504 epoch 8 - iter 140/202 - loss 0.84806033 - samples/sec: 56.75 - lr: 0.100000
2021-05-24 22:09:12,126 epoch 8 - iter 160/202 - loss 0.83837065 - samples/sec: 56.93 - lr: 0.100000
2021-05-24 22:09:17,819 epoch 8 - iter 180/202 - loss 0.83338508 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:09:23,496 epoch 8 - iter 200/202 - loss 0.83968167 - samples/sec: 56.38 - lr: 0.100000
2021-05-24 22:09:24,008 ----------------------------------------------------------------------------------------------------
2021-05-24 22:09:24,008 EPOCH 8 done: loss 0.8408 - lr 0.1000000
2021-05-24 22:09:28,538 DEV : loss 0.46928733587265015 - score 0.9375
2021-05-24 22:09:28,616 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:09:38,214 ----------------------------------------------------------------------------------------------------
2021-05-24 22:09:43,922 epoch 9 - iter 20/202 - loss 0.91652364 - samples/sec: 56.08 - lr: 0.100000
2021-05-24 22:09:49,606 epoch 9 - iter 40/202 - loss 0.87562122 - samples/sec: 56.30 - lr: 0.100000
2021-05-24 22:09:55,289 epoch 9 - iter 60/202 - loss 0.84505016 - samples/sec: 56.32 - lr: 0.100000
2021-05-24 22:10:00,920 epoch 9 - iter 80/202 - loss 0.83771707 - samples/sec: 56.84 - lr: 0.100000
2021-05-24 22:10:06,580 epoch 9 - iter 100/202 - loss 0.79346526 - samples/sec: 56.54 - lr: 0.100000
2021-05-24 22:10:12,253 epoch 9 - iter 120/202 - loss 0.79834094 - samples/sec: 56.42 - lr: 0.100000
2021-05-24 22:10:17,930 epoch 9 - iter 140/202 - loss 0.79717250 - samples/sec: 56.37 - lr: 0.100000
2021-05-24 22:10:23,644 epoch 9 - iter 160/202 - loss 0.80873027 - samples/sec: 56.02 - lr: 0.100000
2021-05-24 22:10:29,359 epoch 9 - iter 180/202 - loss 0.81123870 - samples/sec: 56.00 - lr: 0.100000
2021-05-24 22:10:35,016 epoch 9 - iter 200/202 - loss 0.82209532 - samples/sec: 56.58 - lr: 0.100000
2021-05-24 22:10:35,528 ----------------------------------------------------------------------------------------------------
2021-05-24 22:10:35,528 EPOCH 9 done: loss 0.8196 - lr 0.1000000
2021-05-24 22:10:40,053 DEV : loss 0.46092140674591064 - score 0.9414
2021-05-24 22:10:40,132 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:10:50,354 ----------------------------------------------------------------------------------------------------
2021-05-24 22:10:56,035 epoch 10 - iter 20/202 - loss 0.81707194 - samples/sec: 56.34 - lr: 0.100000
2021-05-24 22:11:01,729 epoch 10 - iter 40/202 - loss 0.86544913 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:11:07,393 epoch 10 - iter 60/202 - loss 0.80622875 - samples/sec: 56.50 - lr: 0.100000
2021-05-24 22:11:13,090 epoch 10 - iter 80/202 - loss 0.80818227 - samples/sec: 56.18 - lr: 0.100000
2021-05-24 22:11:18,787 epoch 10 - iter 100/202 - loss 0.81284506 - samples/sec: 56.18 - lr: 0.100000
2021-05-24 22:11:24,462 epoch 10 - iter 120/202 - loss 0.77791718 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:11:30,191 epoch 10 - iter 140/202 - loss 0.78448345 - samples/sec: 55.87 - lr: 0.100000
2021-05-24 22:11:35,900 epoch 10 - iter 160/202 - loss 0.78448549 - samples/sec: 56.06 - lr: 0.100000
2021-05-24 22:11:41,592 epoch 10 - iter 180/202 - loss 0.79706982 - samples/sec: 56.23 - lr: 0.100000
2021-05-24 22:11:47,299 epoch 10 - iter 200/202 - loss 0.79461117 - samples/sec: 56.08 - lr: 0.100000
2021-05-24 22:11:47,801 ----------------------------------------------------------------------------------------------------
2021-05-24 22:11:47,801 EPOCH 10 done: loss 0.8001 - lr 0.1000000
2021-05-24 22:11:52,719 DEV : loss 0.47244560718536377 - score 0.9325
2021-05-24 22:11:52,798 BAD EPOCHS (no improvement): 1
2021-05-24 22:11:52,799 ----------------------------------------------------------------------------------------------------
2021-05-24 22:11:58,485 epoch 11 - iter 20/202 - loss 0.74966617 - samples/sec: 56.29 - lr: 0.100000
2021-05-24 22:12:04,184 epoch 11 - iter 40/202 - loss 0.77399028 - samples/sec: 56.16 - lr: 0.100000
2021-05-24 22:12:09,837 epoch 11 - iter 60/202 - loss 0.75378218 - samples/sec: 56.61 - lr: 0.100000
2021-05-24 22:12:15,532 epoch 11 - iter 80/202 - loss 0.72913443 - samples/sec: 56.20 - lr: 0.100000
2021-05-24 22:12:21,198 epoch 11 - iter 100/202 - loss 0.71530306 - samples/sec: 56.49 - lr: 0.100000
2021-05-24 22:12:26,889 epoch 11 - iter 120/202 - loss 0.76660375 - samples/sec: 56.24 - lr: 0.100000
2021-05-24 22:12:32,554 epoch 11 - iter 140/202 - loss 0.76010696 - samples/sec: 56.49 - lr: 0.100000
2021-05-24 22:12:38,236 epoch 11 - iter 160/202 - loss 0.78584748 - samples/sec: 56.33 - lr: 0.100000
2021-05-24 22:12:43,969 epoch 11 - iter 180/202 - loss 0.77262104 - samples/sec: 55.82 - lr: 0.100000
2021-05-24 22:12:49,688 epoch 11 - iter 200/202 - loss 0.78530165 - samples/sec: 55.96 - lr: 0.100000
2021-05-24 22:12:50,191 ----------------------------------------------------------------------------------------------------
2021-05-24 22:12:50,191 EPOCH 11 done: loss 0.7880 - lr 0.1000000
2021-05-24 22:12:54,715 DEV : loss 0.4417213499546051 - score 0.9286
2021-05-24 22:12:54,794 BAD EPOCHS (no improvement): 2
2021-05-24 22:12:54,794 ----------------------------------------------------------------------------------------------------
2021-05-24 22:13:00,431 epoch 12 - iter 20/202 - loss 0.56850460 - samples/sec: 56.78 - lr: 0.100000
2021-05-24 22:13:06,139 epoch 12 - iter 40/202 - loss 0.68464025 - samples/sec: 56.07 - lr: 0.100000
2021-05-24 22:13:11,857 epoch 12 - iter 60/202 - loss 0.74391480 - samples/sec: 55.98 - lr: 0.100000
2021-05-24 22:13:17,544 epoch 12 - iter 80/202 - loss 0.71527317 - samples/sec: 56.28 - lr: 0.100000
2021-05-24 22:13:23,243 epoch 12 - iter 100/202 - loss 0.76216417 - samples/sec: 56.16 - lr: 0.100000
2021-05-24 22:13:28,963 epoch 12 - iter 120/202 - loss 0.74904919 - samples/sec: 55.95 - lr: 0.100000
2021-05-24 22:13:34,661 epoch 12 - iter 140/202 - loss 0.80442642 - samples/sec: 56.16 - lr: 0.100000
2021-05-24 22:13:40,311 epoch 12 - iter 160/202 - loss 0.78318559 - samples/sec: 56.65 - lr: 0.100000
2021-05-24 22:13:45,970 epoch 12 - iter 180/202 - loss 0.79305097 - samples/sec: 56.55 - lr: 0.100000
2021-05-24 22:13:51,619 epoch 12 - iter 200/202 - loss 0.79043892 - samples/sec: 56.66 - lr: 0.100000
2021-05-24 22:13:52,129 ----------------------------------------------------------------------------------------------------
2021-05-24 22:13:52,129 EPOCH 12 done: loss 0.7921 - lr 0.1000000
2021-05-24 22:13:56,653 DEV : loss 0.47899186611175537 - score 0.9395
2021-05-24 22:13:56,732 BAD EPOCHS (no improvement): 3
2021-05-24 22:13:56,732 ----------------------------------------------------------------------------------------------------
2021-05-24 22:14:02,433 epoch 13 - iter 20/202 - loss 0.76738986 - samples/sec: 56.14 - lr: 0.100000
2021-05-24 22:14:08,122 epoch 13 - iter 40/202 - loss 0.76732998 - samples/sec: 56.25 - lr: 0.100000
2021-05-24 22:14:13,811 epoch 13 - iter 60/202 - loss 0.84061464 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:14:19,499 epoch 13 - iter 80/202 - loss 0.81838105 - samples/sec: 56.27 - lr: 0.100000
2021-05-24 22:14:25,252 epoch 13 - iter 100/202 - loss 0.79311864 - samples/sec: 55.63 - lr: 0.100000
2021-05-24 22:14:31,005 epoch 13 - iter 120/202 - loss 0.79282256 - samples/sec: 55.64 - lr: 0.100000
2021-05-24 22:14:36,759 epoch 13 - iter 140/202 - loss 0.79685977 - samples/sec: 55.62 - lr: 0.100000
2021-05-24 22:14:42,481 epoch 13 - iter 160/202 - loss 0.78609567 - samples/sec: 55.94 - lr: 0.100000
2021-05-24 22:14:48,200 epoch 13 - iter 180/202 - loss 0.77652537 - samples/sec: 55.96 - lr: 0.100000
2021-05-24 22:14:53,895 epoch 13 - iter 200/202 - loss 0.77176391 - samples/sec: 56.20 - lr: 0.100000
2021-05-24 22:14:54,408 ----------------------------------------------------------------------------------------------------
2021-05-24 22:14:54,408 EPOCH 13 done: loss 0.7683 - lr 0.1000000
2021-05-24 22:14:58,932 DEV : loss 0.4448533058166504 - score 0.9415
2021-05-24 22:14:59,011 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:15:08,694 ----------------------------------------------------------------------------------------------------
2021-05-24 22:15:14,428 epoch 14 - iter 20/202 - loss 0.82545874 - samples/sec: 55.82 - lr: 0.100000
2021-05-24 22:15:20,116 epoch 14 - iter 40/202 - loss 0.78990882 - samples/sec: 56.27 - lr: 0.100000
2021-05-24 22:15:25,738 epoch 14 - iter 60/202 - loss 0.82795351 - samples/sec: 56.93 - lr: 0.100000
2021-05-24 22:15:31,405 epoch 14 - iter 80/202 - loss 0.76385102 - samples/sec: 56.48 - lr: 0.100000
2021-05-24 22:15:37,117 epoch 14 - iter 100/202 - loss 0.73465479 - samples/sec: 56.03 - lr: 0.100000
2021-05-24 22:15:42,848 epoch 14 - iter 120/202 - loss 0.76300355 - samples/sec: 55.85 - lr: 0.100000
2021-05-24 22:15:48,560 epoch 14 - iter 140/202 - loss 0.75368048 - samples/sec: 56.02 - lr: 0.100000
2021-05-24 22:15:54,275 epoch 14 - iter 160/202 - loss 0.74661619 - samples/sec: 56.01 - lr: 0.100000
2021-05-24 22:15:59,998 epoch 14 - iter 180/202 - loss 0.74786099 - samples/sec: 55.93 - lr: 0.100000
2021-05-24 22:16:05,687 epoch 14 - iter 200/202 - loss 0.73542757 - samples/sec: 56.25 - lr: 0.100000
2021-05-24 22:16:06,199 ----------------------------------------------------------------------------------------------------
2021-05-24 22:16:06,200 EPOCH 14 done: loss 0.7428 - lr 0.1000000
2021-05-24 22:16:11,100 DEV : loss 0.5192499160766602 - score 0.9361
2021-05-24 22:16:11,178 BAD EPOCHS (no improvement): 1
2021-05-24 22:16:11,178 ----------------------------------------------------------------------------------------------------
2021-05-24 22:16:16,859 epoch 15 - iter 20/202 - loss 0.73874936 - samples/sec: 56.34 - lr: 0.100000
2021-05-24 22:16:22,548 epoch 15 - iter 40/202 - loss 0.78269354 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:16:28,208 epoch 15 - iter 60/202 - loss 0.81414460 - samples/sec: 56.54 - lr: 0.100000
2021-05-24 22:16:33,854 epoch 15 - iter 80/202 - loss 0.75970240 - samples/sec: 56.68 - lr: 0.100000
2021-05-24 22:16:39,529 epoch 15 - iter 100/202 - loss 0.80268204 - samples/sec: 56.40 - lr: 0.100000
2021-05-24 22:16:45,229 epoch 15 - iter 120/202 - loss 0.76043566 - samples/sec: 56.15 - lr: 0.100000
2021-05-24 22:16:50,947 epoch 15 - iter 140/202 - loss 0.74430550 - samples/sec: 55.98 - lr: 0.100000
2021-05-24 22:16:56,652 epoch 15 - iter 160/202 - loss 0.71241502 - samples/sec: 56.10 - lr: 0.100000
2021-05-24 22:17:02,341 epoch 15 - iter 180/202 - loss 0.70680706 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:17:08,036 epoch 15 - iter 200/202 - loss 0.70999252 - samples/sec: 56.19 - lr: 0.100000
2021-05-24 22:17:08,537 ----------------------------------------------------------------------------------------------------
2021-05-24 22:17:08,537 EPOCH 15 done: loss 0.7160 - lr 0.1000000
2021-05-24 22:17:13,065 DEV : loss 0.4948652684688568 - score 0.9308
2021-05-24 22:17:13,143 BAD EPOCHS (no improvement): 2
2021-05-24 22:17:13,144 ----------------------------------------------------------------------------------------------------
2021-05-24 22:17:18,846 epoch 16 - iter 20/202 - loss 0.62082739 - samples/sec: 56.12 - lr: 0.100000
2021-05-24 22:17:24,531 epoch 16 - iter 40/202 - loss 0.60692939 - samples/sec: 56.30 - lr: 0.100000
2021-05-24 22:17:30,262 epoch 16 - iter 60/202 - loss 0.61960047 - samples/sec: 55.84 - lr: 0.100000
2021-05-24 22:17:35,976 epoch 16 - iter 80/202 - loss 0.64860861 - samples/sec: 56.02 - lr: 0.100000
2021-05-24 22:17:41,652 epoch 16 - iter 100/202 - loss 0.66229208 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:17:47,344 epoch 16 - iter 120/202 - loss 0.69747012 - samples/sec: 56.23 - lr: 0.100000
2021-05-24 22:17:52,978 epoch 16 - iter 140/202 - loss 0.70324004 - samples/sec: 56.81 - lr: 0.100000
2021-05-24 22:17:58,650 epoch 16 - iter 160/202 - loss 0.69718878 - samples/sec: 56.42 - lr: 0.100000
2021-05-24 22:18:04,324 epoch 16 - iter 180/202 - loss 0.69353870 - samples/sec: 56.41 - lr: 0.100000
2021-05-24 22:18:09,973 epoch 16 - iter 200/202 - loss 0.68211951 - samples/sec: 56.65 - lr: 0.100000
2021-05-24 22:18:10,482 ----------------------------------------------------------------------------------------------------
2021-05-24 22:18:10,482 EPOCH 16 done: loss 0.6892 - lr 0.1000000
2021-05-24 22:18:15,007 DEV : loss 0.6022551655769348 - score 0.9079
2021-05-24 22:18:15,086 BAD EPOCHS (no improvement): 3
2021-05-24 22:18:15,086 ----------------------------------------------------------------------------------------------------
2021-05-24 22:18:20,750 epoch 17 - iter 20/202 - loss 0.70600215 - samples/sec: 56.51 - lr: 0.100000
2021-05-24 22:18:26,414 epoch 17 - iter 40/202 - loss 0.64921381 - samples/sec: 56.50 - lr: 0.100000
2021-05-24 22:18:32,103 epoch 17 - iter 60/202 - loss 0.65745450 - samples/sec: 56.27 - lr: 0.100000
2021-05-24 22:18:37,789 epoch 17 - iter 80/202 - loss 0.69466214 - samples/sec: 56.28 - lr: 0.100000
2021-05-24 22:18:43,512 epoch 17 - iter 100/202 - loss 0.69103456 - samples/sec: 55.93 - lr: 0.100000
2021-05-24 22:18:49,215 epoch 17 - iter 120/202 - loss 0.67186236 - samples/sec: 56.12 - lr: 0.100000
2021-05-24 22:18:54,941 epoch 17 - iter 140/202 - loss 0.67506389 - samples/sec: 55.89 - lr: 0.100000
2021-05-24 22:19:00,642 epoch 17 - iter 160/202 - loss 0.66838257 - samples/sec: 56.14 - lr: 0.100000
2021-05-24 22:19:06,318 epoch 17 - iter 180/202 - loss 0.67313180 - samples/sec: 56.38 - lr: 0.100000
2021-05-24 22:19:11,962 epoch 17 - iter 200/202 - loss 0.67950272 - samples/sec: 56.71 - lr: 0.100000
2021-05-24 22:19:12,469 ----------------------------------------------------------------------------------------------------
2021-05-24 22:19:12,469 EPOCH 17 done: loss 0.6766 - lr 0.1000000
2021-05-24 22:19:16,983 DEV : loss 0.4774840176105499 - score 0.9406
Epoch    17: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 22:19:17,061 BAD EPOCHS (no improvement): 4
2021-05-24 22:19:17,062 ----------------------------------------------------------------------------------------------------
2021-05-24 22:19:22,755 epoch 18 - iter 20/202 - loss 0.59158099 - samples/sec: 56.22 - lr: 0.050000
2021-05-24 22:19:28,419 epoch 18 - iter 40/202 - loss 0.68161207 - samples/sec: 56.50 - lr: 0.050000
2021-05-24 22:19:34,092 epoch 18 - iter 60/202 - loss 0.63597573 - samples/sec: 56.42 - lr: 0.050000
2021-05-24 22:19:39,718 epoch 18 - iter 80/202 - loss 0.60321886 - samples/sec: 56.88 - lr: 0.050000
2021-05-24 22:19:45,307 epoch 18 - iter 100/202 - loss 0.59887051 - samples/sec: 57.27 - lr: 0.050000
2021-05-24 22:19:50,907 epoch 18 - iter 120/202 - loss 0.59594842 - samples/sec: 57.15 - lr: 0.050000
2021-05-24 22:19:56,543 epoch 18 - iter 140/202 - loss 0.60412733 - samples/sec: 56.79 - lr: 0.050000
2021-05-24 22:20:02,144 epoch 18 - iter 160/202 - loss 0.59121400 - samples/sec: 57.14 - lr: 0.050000
2021-05-24 22:20:07,755 epoch 18 - iter 180/202 - loss 0.60257385 - samples/sec: 57.04 - lr: 0.050000
2021-05-24 22:20:13,403 epoch 18 - iter 200/202 - loss 0.58256080 - samples/sec: 56.67 - lr: 0.050000
2021-05-24 22:20:13,912 ----------------------------------------------------------------------------------------------------
2021-05-24 22:20:13,912 EPOCH 18 done: loss 0.5844 - lr 0.0500000
2021-05-24 22:20:18,817 DEV : loss 0.4728057086467743 - score 0.9381
2021-05-24 22:20:18,895 BAD EPOCHS (no improvement): 1
2021-05-24 22:20:18,896 ----------------------------------------------------------------------------------------------------
2021-05-24 22:20:24,548 epoch 19 - iter 20/202 - loss 0.73393290 - samples/sec: 56.62 - lr: 0.050000
2021-05-24 22:20:30,162 epoch 19 - iter 40/202 - loss 0.57458132 - samples/sec: 57.01 - lr: 0.050000
2021-05-24 22:20:35,800 epoch 19 - iter 60/202 - loss 0.55928409 - samples/sec: 56.77 - lr: 0.050000
2021-05-24 22:20:41,429 epoch 19 - iter 80/202 - loss 0.54497008 - samples/sec: 56.86 - lr: 0.050000
2021-05-24 22:20:47,072 epoch 19 - iter 100/202 - loss 0.56460250 - samples/sec: 56.72 - lr: 0.050000
2021-05-24 22:20:52,711 epoch 19 - iter 120/202 - loss 0.57903740 - samples/sec: 56.76 - lr: 0.050000
2021-05-24 22:20:58,359 epoch 19 - iter 140/202 - loss 0.57340273 - samples/sec: 56.66 - lr: 0.050000
2021-05-24 22:21:04,010 epoch 19 - iter 160/202 - loss 0.58012978 - samples/sec: 56.64 - lr: 0.050000
2021-05-24 22:21:09,701 epoch 19 - iter 180/202 - loss 0.57878325 - samples/sec: 56.24 - lr: 0.050000
2021-05-24 22:21:15,344 epoch 19 - iter 200/202 - loss 0.56136584 - samples/sec: 56.71 - lr: 0.050000
2021-05-24 22:21:15,854 ----------------------------------------------------------------------------------------------------
2021-05-24 22:21:15,855 EPOCH 19 done: loss 0.5619 - lr 0.0500000
2021-05-24 22:21:20,370 DEV : loss 0.44333240389823914 - score 0.9437
2021-05-24 22:21:20,449 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:21:30,603 ----------------------------------------------------------------------------------------------------
2021-05-24 22:21:36,257 epoch 20 - iter 20/202 - loss 0.60653770 - samples/sec: 56.62 - lr: 0.050000
2021-05-24 22:21:41,895 epoch 20 - iter 40/202 - loss 0.55857784 - samples/sec: 56.77 - lr: 0.050000
2021-05-24 22:21:47,537 epoch 20 - iter 60/202 - loss 0.55264316 - samples/sec: 56.73 - lr: 0.050000
2021-05-24 22:21:53,159 epoch 20 - iter 80/202 - loss 0.52890203 - samples/sec: 56.93 - lr: 0.050000
2021-05-24 22:21:58,791 epoch 20 - iter 100/202 - loss 0.54135126 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:22:04,500 epoch 20 - iter 120/202 - loss 0.54198677 - samples/sec: 56.06 - lr: 0.050000
2021-05-24 22:22:10,210 epoch 20 - iter 140/202 - loss 0.53391537 - samples/sec: 56.05 - lr: 0.050000
2021-05-24 22:22:15,921 epoch 20 - iter 160/202 - loss 0.54911631 - samples/sec: 56.04 - lr: 0.050000
2021-05-24 22:22:21,602 epoch 20 - iter 180/202 - loss 0.56697681 - samples/sec: 56.34 - lr: 0.050000
2021-05-24 22:22:27,251 epoch 20 - iter 200/202 - loss 0.56025789 - samples/sec: 56.65 - lr: 0.050000
2021-05-24 22:22:27,752 ----------------------------------------------------------------------------------------------------
2021-05-24 22:22:27,752 EPOCH 20 done: loss 0.5611 - lr 0.0500000
2021-05-24 22:22:32,267 DEV : loss 0.4418286383152008 - score 0.9443
2021-05-24 22:22:32,346 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:22:42,284 ----------------------------------------------------------------------------------------------------
2021-05-24 22:22:47,923 epoch 21 - iter 20/202 - loss 0.56819715 - samples/sec: 56.77 - lr: 0.050000
2021-05-24 22:22:53,527 epoch 21 - iter 40/202 - loss 0.47145522 - samples/sec: 57.11 - lr: 0.050000
2021-05-24 22:22:59,170 epoch 21 - iter 60/202 - loss 0.52261333 - samples/sec: 56.72 - lr: 0.050000
2021-05-24 22:23:04,817 epoch 21 - iter 80/202 - loss 0.50823715 - samples/sec: 56.68 - lr: 0.050000
2021-05-24 22:23:10,433 epoch 21 - iter 100/202 - loss 0.48824283 - samples/sec: 56.99 - lr: 0.050000
2021-05-24 22:23:16,029 epoch 21 - iter 120/202 - loss 0.52946609 - samples/sec: 57.19 - lr: 0.050000
2021-05-24 22:23:21,645 epoch 21 - iter 140/202 - loss 0.52721859 - samples/sec: 56.99 - lr: 0.050000
2021-05-24 22:23:27,278 epoch 21 - iter 160/202 - loss 0.52190959 - samples/sec: 56.82 - lr: 0.050000
2021-05-24 22:23:32,896 epoch 21 - iter 180/202 - loss 0.51487324 - samples/sec: 56.97 - lr: 0.050000
2021-05-24 22:23:38,553 epoch 21 - iter 200/202 - loss 0.51627020 - samples/sec: 56.58 - lr: 0.050000
2021-05-24 22:23:39,054 ----------------------------------------------------------------------------------------------------
2021-05-24 22:23:39,054 EPOCH 21 done: loss 0.5179 - lr 0.0500000
2021-05-24 22:23:43,573 DEV : loss 0.4275730550289154 - score 0.9407
2021-05-24 22:23:43,652 BAD EPOCHS (no improvement): 1
2021-05-24 22:23:43,652 ----------------------------------------------------------------------------------------------------
2021-05-24 22:23:49,256 epoch 22 - iter 20/202 - loss 0.46210531 - samples/sec: 57.11 - lr: 0.050000
2021-05-24 22:23:54,903 epoch 22 - iter 40/202 - loss 0.52198535 - samples/sec: 56.68 - lr: 0.050000
2021-05-24 22:24:00,518 epoch 22 - iter 60/202 - loss 0.48905641 - samples/sec: 57.00 - lr: 0.050000
2021-05-24 22:24:06,126 epoch 22 - iter 80/202 - loss 0.49977646 - samples/sec: 57.07 - lr: 0.050000
2021-05-24 22:24:11,748 epoch 22 - iter 100/202 - loss 0.52202338 - samples/sec: 56.93 - lr: 0.050000
2021-05-24 22:24:17,407 epoch 22 - iter 120/202 - loss 0.52106614 - samples/sec: 56.56 - lr: 0.050000
2021-05-24 22:24:23,055 epoch 22 - iter 140/202 - loss 0.52198392 - samples/sec: 56.67 - lr: 0.050000
2021-05-24 22:24:28,659 epoch 22 - iter 160/202 - loss 0.54378302 - samples/sec: 57.11 - lr: 0.050000
2021-05-24 22:24:34,296 epoch 22 - iter 180/202 - loss 0.54759052 - samples/sec: 56.78 - lr: 0.050000
2021-05-24 22:24:39,952 epoch 22 - iter 200/202 - loss 0.54267349 - samples/sec: 56.58 - lr: 0.050000
2021-05-24 22:24:40,463 ----------------------------------------------------------------------------------------------------
2021-05-24 22:24:40,463 EPOCH 22 done: loss 0.5444 - lr 0.0500000
2021-05-24 22:24:45,365 DEV : loss 0.5000857710838318 - score 0.9305
2021-05-24 22:24:45,443 BAD EPOCHS (no improvement): 2
2021-05-24 22:24:45,444 ----------------------------------------------------------------------------------------------------
2021-05-24 22:24:51,118 epoch 23 - iter 20/202 - loss 0.60887775 - samples/sec: 56.41 - lr: 0.050000
2021-05-24 22:24:56,759 epoch 23 - iter 40/202 - loss 0.52280272 - samples/sec: 56.73 - lr: 0.050000
2021-05-24 22:25:02,409 epoch 23 - iter 60/202 - loss 0.54054476 - samples/sec: 56.65 - lr: 0.050000
2021-05-24 22:25:08,018 epoch 23 - iter 80/202 - loss 0.50321129 - samples/sec: 57.06 - lr: 0.050000
2021-05-24 22:25:13,652 epoch 23 - iter 100/202 - loss 0.51803984 - samples/sec: 56.81 - lr: 0.050000
2021-05-24 22:25:19,307 epoch 23 - iter 120/202 - loss 0.51062484 - samples/sec: 56.60 - lr: 0.050000
2021-05-24 22:25:24,971 epoch 23 - iter 140/202 - loss 0.51828280 - samples/sec: 56.50 - lr: 0.050000
2021-05-24 22:25:30,575 epoch 23 - iter 160/202 - loss 0.51208182 - samples/sec: 57.11 - lr: 0.050000
2021-05-24 22:25:36,178 epoch 23 - iter 180/202 - loss 0.49742128 - samples/sec: 57.12 - lr: 0.050000
2021-05-24 22:25:41,810 epoch 23 - iter 200/202 - loss 0.50657651 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:25:42,316 ----------------------------------------------------------------------------------------------------
2021-05-24 22:25:42,316 EPOCH 23 done: loss 0.5045 - lr 0.0500000
2021-05-24 22:25:46,842 DEV : loss 0.4244122803211212 - score 0.9433
2021-05-24 22:25:46,920 BAD EPOCHS (no improvement): 3
2021-05-24 22:25:46,921 ----------------------------------------------------------------------------------------------------
2021-05-24 22:25:52,615 epoch 24 - iter 20/202 - loss 0.47564585 - samples/sec: 56.20 - lr: 0.050000
2021-05-24 22:25:58,229 epoch 24 - iter 40/202 - loss 0.50904507 - samples/sec: 57.01 - lr: 0.050000
2021-05-24 22:26:03,925 epoch 24 - iter 60/202 - loss 0.50327616 - samples/sec: 56.20 - lr: 0.050000
2021-05-24 22:26:09,559 epoch 24 - iter 80/202 - loss 0.50685481 - samples/sec: 56.81 - lr: 0.050000
2021-05-24 22:26:15,220 epoch 24 - iter 100/202 - loss 0.52694284 - samples/sec: 56.54 - lr: 0.050000
2021-05-24 22:26:20,864 epoch 24 - iter 120/202 - loss 0.52206755 - samples/sec: 56.71 - lr: 0.050000
2021-05-24 22:26:26,532 epoch 24 - iter 140/202 - loss 0.52606718 - samples/sec: 56.46 - lr: 0.050000
2021-05-24 22:26:32,179 epoch 24 - iter 160/202 - loss 0.53887663 - samples/sec: 56.68 - lr: 0.050000
2021-05-24 22:26:37,814 epoch 24 - iter 180/202 - loss 0.53297384 - samples/sec: 56.80 - lr: 0.050000
2021-05-24 22:26:43,450 epoch 24 - iter 200/202 - loss 0.52314156 - samples/sec: 56.79 - lr: 0.050000
2021-05-24 22:26:43,953 ----------------------------------------------------------------------------------------------------
2021-05-24 22:26:43,953 EPOCH 24 done: loss 0.5222 - lr 0.0500000
2021-05-24 22:26:48,479 DEV : loss 0.4435586929321289 - score 0.9444
2021-05-24 22:26:48,557 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:26:58,487 ----------------------------------------------------------------------------------------------------
2021-05-24 22:27:04,140 epoch 25 - iter 20/202 - loss 0.50592539 - samples/sec: 56.62 - lr: 0.050000
2021-05-24 22:27:09,732 epoch 25 - iter 40/202 - loss 0.48220914 - samples/sec: 57.24 - lr: 0.050000
2021-05-24 22:27:15,339 epoch 25 - iter 60/202 - loss 0.48455527 - samples/sec: 57.08 - lr: 0.050000
2021-05-24 22:27:20,946 epoch 25 - iter 80/202 - loss 0.50873620 - samples/sec: 57.08 - lr: 0.050000
2021-05-24 22:27:26,588 epoch 25 - iter 100/202 - loss 0.52384991 - samples/sec: 56.73 - lr: 0.050000
2021-05-24 22:27:32,220 epoch 25 - iter 120/202 - loss 0.50898096 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:27:37,857 epoch 25 - iter 140/202 - loss 0.51853974 - samples/sec: 56.78 - lr: 0.050000
2021-05-24 22:27:43,509 epoch 25 - iter 160/202 - loss 0.51665910 - samples/sec: 56.62 - lr: 0.050000
2021-05-24 22:27:49,135 epoch 25 - iter 180/202 - loss 0.51983632 - samples/sec: 56.89 - lr: 0.050000
2021-05-24 22:27:54,751 epoch 25 - iter 200/202 - loss 0.51905345 - samples/sec: 56.99 - lr: 0.050000
2021-05-24 22:27:55,257 ----------------------------------------------------------------------------------------------------
2021-05-24 22:27:55,257 EPOCH 25 done: loss 0.5179 - lr 0.0500000
2021-05-24 22:27:59,780 DEV : loss 0.42440158128738403 - score 0.9426
2021-05-24 22:27:59,859 BAD EPOCHS (no improvement): 1
2021-05-24 22:27:59,859 ----------------------------------------------------------------------------------------------------
2021-05-24 22:28:05,915 epoch 26 - iter 20/202 - loss 0.58554286 - samples/sec: 52.85 - lr: 0.050000
2021-05-24 22:28:11,582 epoch 26 - iter 40/202 - loss 0.57068936 - samples/sec: 56.48 - lr: 0.050000
2021-05-24 22:28:17,200 epoch 26 - iter 60/202 - loss 0.56002536 - samples/sec: 56.97 - lr: 0.050000
2021-05-24 22:28:22,841 epoch 26 - iter 80/202 - loss 0.52841009 - samples/sec: 56.74 - lr: 0.050000
2021-05-24 22:28:28,448 epoch 26 - iter 100/202 - loss 0.52750578 - samples/sec: 57.07 - lr: 0.050000
2021-05-24 22:28:34,105 epoch 26 - iter 120/202 - loss 0.54156465 - samples/sec: 56.58 - lr: 0.050000
2021-05-24 22:28:39,739 epoch 26 - iter 140/202 - loss 0.53264687 - samples/sec: 56.81 - lr: 0.050000
2021-05-24 22:28:45,426 epoch 26 - iter 160/202 - loss 0.53304222 - samples/sec: 56.28 - lr: 0.050000
2021-05-24 22:28:51,074 epoch 26 - iter 180/202 - loss 0.51263566 - samples/sec: 56.67 - lr: 0.050000
2021-05-24 22:28:56,729 epoch 26 - iter 200/202 - loss 0.51044552 - samples/sec: 56.59 - lr: 0.050000
2021-05-24 22:28:57,226 ----------------------------------------------------------------------------------------------------
2021-05-24 22:28:57,227 EPOCH 26 done: loss 0.5097 - lr 0.0500000
2021-05-24 22:29:01,745 DEV : loss 0.44930583238601685 - score 0.9421
2021-05-24 22:29:01,823 BAD EPOCHS (no improvement): 2
2021-05-24 22:29:01,823 ----------------------------------------------------------------------------------------------------
2021-05-24 22:29:07,487 epoch 27 - iter 20/202 - loss 0.52605615 - samples/sec: 56.51 - lr: 0.050000
2021-05-24 22:29:13,174 epoch 27 - iter 40/202 - loss 0.53378507 - samples/sec: 56.28 - lr: 0.050000
2021-05-24 22:29:18,907 epoch 27 - iter 60/202 - loss 0.52911238 - samples/sec: 55.82 - lr: 0.050000
2021-05-24 22:29:24,579 epoch 27 - iter 80/202 - loss 0.50921518 - samples/sec: 56.43 - lr: 0.050000
2021-05-24 22:29:30,270 epoch 27 - iter 100/202 - loss 0.53085239 - samples/sec: 56.24 - lr: 0.050000
2021-05-24 22:29:35,961 epoch 27 - iter 120/202 - loss 0.53362182 - samples/sec: 56.24 - lr: 0.050000
2021-05-24 22:29:41,629 epoch 27 - iter 140/202 - loss 0.51546756 - samples/sec: 56.47 - lr: 0.050000
2021-05-24 22:29:47,354 epoch 27 - iter 160/202 - loss 0.52481599 - samples/sec: 55.90 - lr: 0.050000
2021-05-24 22:29:52,998 epoch 27 - iter 180/202 - loss 0.51504385 - samples/sec: 56.72 - lr: 0.050000
2021-05-24 22:29:58,642 epoch 27 - iter 200/202 - loss 0.50887662 - samples/sec: 56.71 - lr: 0.050000
2021-05-24 22:29:59,150 ----------------------------------------------------------------------------------------------------
2021-05-24 22:29:59,150 EPOCH 27 done: loss 0.5059 - lr 0.0500000
2021-05-24 22:30:03,678 DEV : loss 0.4757322669029236 - score 0.9417
2021-05-24 22:30:03,756 BAD EPOCHS (no improvement): 3
2021-05-24 22:30:03,757 ----------------------------------------------------------------------------------------------------
2021-05-24 22:30:09,428 epoch 28 - iter 20/202 - loss 0.54926318 - samples/sec: 56.43 - lr: 0.050000
2021-05-24 22:30:15,114 epoch 28 - iter 40/202 - loss 0.47958332 - samples/sec: 56.29 - lr: 0.050000
2021-05-24 22:30:20,734 epoch 28 - iter 60/202 - loss 0.47661902 - samples/sec: 56.95 - lr: 0.050000
2021-05-24 22:30:26,419 epoch 28 - iter 80/202 - loss 0.50790752 - samples/sec: 56.30 - lr: 0.050000
2021-05-24 22:30:32,068 epoch 28 - iter 100/202 - loss 0.50671653 - samples/sec: 56.66 - lr: 0.050000
2021-05-24 22:30:37,746 epoch 28 - iter 120/202 - loss 0.50813857 - samples/sec: 56.37 - lr: 0.050000
2021-05-24 22:30:43,404 epoch 28 - iter 140/202 - loss 0.50093079 - samples/sec: 56.57 - lr: 0.050000
2021-05-24 22:30:49,065 epoch 28 - iter 160/202 - loss 0.50854435 - samples/sec: 56.54 - lr: 0.050000
2021-05-24 22:30:54,787 epoch 28 - iter 180/202 - loss 0.49747464 - samples/sec: 55.93 - lr: 0.050000
2021-05-24 22:31:00,478 epoch 28 - iter 200/202 - loss 0.48978541 - samples/sec: 56.24 - lr: 0.050000
2021-05-24 22:31:00,991 ----------------------------------------------------------------------------------------------------
2021-05-24 22:31:00,991 EPOCH 28 done: loss 0.4901 - lr 0.0500000
2021-05-24 22:31:05,521 DEV : loss 0.4647158682346344 - score 0.943
Epoch    28: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 22:31:05,600 BAD EPOCHS (no improvement): 4
2021-05-24 22:31:05,600 ----------------------------------------------------------------------------------------------------
2021-05-24 22:31:11,277 epoch 29 - iter 20/202 - loss 0.49113657 - samples/sec: 56.38 - lr: 0.025000
2021-05-24 22:31:16,944 epoch 29 - iter 40/202 - loss 0.50587676 - samples/sec: 56.48 - lr: 0.025000
2021-05-24 22:31:22,629 epoch 29 - iter 60/202 - loss 0.49729725 - samples/sec: 56.30 - lr: 0.025000
2021-05-24 22:31:28,304 epoch 29 - iter 80/202 - loss 0.50242002 - samples/sec: 56.39 - lr: 0.025000
2021-05-24 22:31:33,916 epoch 29 - iter 100/202 - loss 0.48466748 - samples/sec: 57.04 - lr: 0.025000
2021-05-24 22:31:39,560 epoch 29 - iter 120/202 - loss 0.49165451 - samples/sec: 56.70 - lr: 0.025000
2021-05-24 22:31:45,207 epoch 29 - iter 140/202 - loss 0.48883374 - samples/sec: 56.68 - lr: 0.025000
2021-05-24 22:31:50,818 epoch 29 - iter 160/202 - loss 0.47632621 - samples/sec: 57.04 - lr: 0.025000
2021-05-24 22:31:56,433 epoch 29 - iter 180/202 - loss 0.48349881 - samples/sec: 57.00 - lr: 0.025000
2021-05-24 22:32:02,084 epoch 29 - iter 200/202 - loss 0.47991331 - samples/sec: 56.64 - lr: 0.025000
2021-05-24 22:32:02,592 ----------------------------------------------------------------------------------------------------
2021-05-24 22:32:02,592 EPOCH 29 done: loss 0.4820 - lr 0.0250000
2021-05-24 22:32:07,499 DEV : loss 0.4731823801994324 - score 0.9394
2021-05-24 22:32:07,578 BAD EPOCHS (no improvement): 1
2021-05-24 22:32:07,579 ----------------------------------------------------------------------------------------------------
2021-05-24 22:32:13,253 epoch 30 - iter 20/202 - loss 0.39763063 - samples/sec: 56.41 - lr: 0.025000
2021-05-24 22:32:18,946 epoch 30 - iter 40/202 - loss 0.40144892 - samples/sec: 56.21 - lr: 0.025000
2021-05-24 22:32:24,640 epoch 30 - iter 60/202 - loss 0.43174667 - samples/sec: 56.21 - lr: 0.025000
2021-05-24 22:32:30,249 epoch 30 - iter 80/202 - loss 0.42596075 - samples/sec: 57.07 - lr: 0.025000
2021-05-24 22:32:35,854 epoch 30 - iter 100/202 - loss 0.44078489 - samples/sec: 57.10 - lr: 0.025000
2021-05-24 22:32:41,488 epoch 30 - iter 120/202 - loss 0.44845775 - samples/sec: 56.81 - lr: 0.025000
2021-05-24 22:32:47,161 epoch 30 - iter 140/202 - loss 0.45220308 - samples/sec: 56.42 - lr: 0.025000
2021-05-24 22:32:52,811 epoch 30 - iter 160/202 - loss 0.43857104 - samples/sec: 56.65 - lr: 0.025000
2021-05-24 22:32:58,459 epoch 30 - iter 180/202 - loss 0.43275334 - samples/sec: 56.66 - lr: 0.025000
2021-05-24 22:33:04,129 epoch 30 - iter 200/202 - loss 0.44237665 - samples/sec: 56.45 - lr: 0.025000
2021-05-24 22:33:04,641 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:04,641 EPOCH 30 done: loss 0.4485 - lr 0.0250000
2021-05-24 22:33:09,167 DEV : loss 0.44879740476608276 - score 0.9418
2021-05-24 22:33:09,245 BAD EPOCHS (no improvement): 2
2021-05-24 22:33:10,538 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:10,538 Testing using best model ...
2021-05-24 22:33:10,538 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.gum/best-model.pt
2021-05-24 22:33:33,742 0.8902	0.9202	0.9050
2021-05-24 22:33:33,742 
Results:
- F1-score (micro) 0.9050
- F1-score (macro) 0.9050

By class:
SENT       tp: 819 - fp: 101 - fn: 71 - precision: 0.8902 - recall: 0.9202 - f1-score: 0.9050
2021-05-24 22:33:33,742 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt/
2021-05-24 22:33:33,771 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt
2021-05-24 22:33:33,772 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt/sent_train.txt
2021-05-24 22:33:33,774 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt/sent_dev.txt
2021-05-24 22:33:33,775 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt/sent_test.txt
Corpus: 865 train + 186 dev + 176 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 22:33:40,767 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,770 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 22:33:40,770 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,770 Corpus: "Corpus: 865 train + 186 dev + 176 test sentences"
2021-05-24 22:33:40,770 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,770 Parameters:
2021-05-24 22:33:40,770  - learning_rate: "0.1"
2021-05-24 22:33:40,770  - mini_batch_size: "16"
2021-05-24 22:33:40,770  - patience: "3"
2021-05-24 22:33:40,770  - anneal_factor: "0.5"
2021-05-24 22:33:40,770  - max_epochs: "30"
2021-05-24 22:33:40,770  - shuffle: "True"
2021-05-24 22:33:40,771  - train_with_dev: "False"
2021-05-24 22:33:40,771  - batch_growth_annealing: "False"
2021-05-24 22:33:40,771 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,771 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt"
2021-05-24 22:33:40,771 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,771 Device: cuda:0
2021-05-24 22:33:40,771 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:40,771 Embeddings storage mode: cpu
2021-05-24 22:33:40,772 ----------------------------------------------------------------------------------------------------
2021-05-24 22:33:44,651 epoch 1 - iter 5/55 - loss 19.49018230 - samples/sec: 20.63 - lr: 0.100000
2021-05-24 22:33:48,528 epoch 1 - iter 10/55 - loss 14.39973259 - samples/sec: 20.64 - lr: 0.100000
2021-05-24 22:33:52,420 epoch 1 - iter 15/55 - loss 12.20448573 - samples/sec: 20.56 - lr: 0.100000
2021-05-24 22:33:56,307 epoch 1 - iter 20/55 - loss 10.68111634 - samples/sec: 20.58 - lr: 0.100000
2021-05-24 22:34:00,142 epoch 1 - iter 25/55 - loss 9.56357065 - samples/sec: 20.86 - lr: 0.100000
2021-05-24 22:34:03,995 epoch 1 - iter 30/55 - loss 8.98396732 - samples/sec: 20.77 - lr: 0.100000
2021-05-24 22:34:07,903 epoch 1 - iter 35/55 - loss 8.72010429 - samples/sec: 20.47 - lr: 0.100000
2021-05-24 22:34:11,790 epoch 1 - iter 40/55 - loss 8.29390992 - samples/sec: 20.59 - lr: 0.100000
2021-05-24 22:34:15,615 epoch 1 - iter 45/55 - loss 7.91022897 - samples/sec: 20.91 - lr: 0.100000
2021-05-24 22:34:19,507 epoch 1 - iter 50/55 - loss 7.52938511 - samples/sec: 20.56 - lr: 0.100000
2021-05-24 22:34:22,724 epoch 1 - iter 55/55 - loss 7.17395162 - samples/sec: 24.88 - lr: 0.100000
2021-05-24 22:34:22,724 ----------------------------------------------------------------------------------------------------
2021-05-24 22:34:22,724 EPOCH 1 done: loss 7.1740 - lr 0.1000000
2021-05-24 22:34:28,403 DEV : loss 2.763735771179199 - score 0.053
2021-05-24 22:34:28,422 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:34:30,012 ----------------------------------------------------------------------------------------------------
2021-05-24 22:34:32,549 epoch 2 - iter 5/55 - loss 3.82310567 - samples/sec: 31.55 - lr: 0.100000
2021-05-24 22:34:34,918 epoch 2 - iter 10/55 - loss 3.67047572 - samples/sec: 33.79 - lr: 0.100000
2021-05-24 22:34:36,345 epoch 2 - iter 15/55 - loss 3.44153015 - samples/sec: 56.07 - lr: 0.100000
2021-05-24 22:34:37,746 epoch 2 - iter 20/55 - loss 3.08316312 - samples/sec: 57.09 - lr: 0.100000
2021-05-24 22:34:39,145 epoch 2 - iter 25/55 - loss 2.93450091 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 22:34:40,546 epoch 2 - iter 30/55 - loss 2.82253334 - samples/sec: 57.14 - lr: 0.100000
2021-05-24 22:34:41,934 epoch 2 - iter 35/55 - loss 2.79714223 - samples/sec: 57.63 - lr: 0.100000
2021-05-24 22:34:43,361 epoch 2 - iter 40/55 - loss 2.65694863 - samples/sec: 56.11 - lr: 0.100000
2021-05-24 22:34:44,754 epoch 2 - iter 45/55 - loss 2.58293766 - samples/sec: 57.41 - lr: 0.100000
2021-05-24 22:34:46,157 epoch 2 - iter 50/55 - loss 2.51694576 - samples/sec: 57.07 - lr: 0.100000
2021-05-24 22:34:47,375 epoch 2 - iter 55/55 - loss 2.41023689 - samples/sec: 65.69 - lr: 0.100000
2021-05-24 22:34:47,375 ----------------------------------------------------------------------------------------------------
2021-05-24 22:34:47,375 EPOCH 2 done: loss 2.4102 - lr 0.1000000
2021-05-24 22:34:48,508 DEV : loss 0.7305139899253845 - score 0.9047
2021-05-24 22:34:48,527 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:34:58,251 ----------------------------------------------------------------------------------------------------
2021-05-24 22:34:59,666 epoch 3 - iter 5/55 - loss 1.52293773 - samples/sec: 56.59 - lr: 0.100000
2021-05-24 22:35:01,059 epoch 3 - iter 10/55 - loss 1.60113255 - samples/sec: 57.44 - lr: 0.100000
2021-05-24 22:35:02,479 epoch 3 - iter 15/55 - loss 1.36917229 - samples/sec: 56.35 - lr: 0.100000
2021-05-24 22:35:03,884 epoch 3 - iter 20/55 - loss 1.45776562 - samples/sec: 56.98 - lr: 0.100000
2021-05-24 22:35:05,298 epoch 3 - iter 25/55 - loss 1.41148743 - samples/sec: 56.59 - lr: 0.100000
2021-05-24 22:35:06,707 epoch 3 - iter 30/55 - loss 1.44527442 - samples/sec: 56.78 - lr: 0.100000
2021-05-24 22:35:08,135 epoch 3 - iter 35/55 - loss 1.41521104 - samples/sec: 56.07 - lr: 0.100000
2021-05-24 22:35:09,554 epoch 3 - iter 40/55 - loss 1.39377737 - samples/sec: 56.37 - lr: 0.100000
2021-05-24 22:35:10,928 epoch 3 - iter 45/55 - loss 1.34026999 - samples/sec: 58.24 - lr: 0.100000
2021-05-24 22:35:12,322 epoch 3 - iter 50/55 - loss 1.32990940 - samples/sec: 57.40 - lr: 0.100000
2021-05-24 22:35:13,494 epoch 3 - iter 55/55 - loss 1.34894592 - samples/sec: 68.32 - lr: 0.100000
2021-05-24 22:35:13,494 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:13,494 EPOCH 3 done: loss 1.3489 - lr 0.1000000
2021-05-24 22:35:14,624 DEV : loss 0.7985879778862 - score 0.8954
2021-05-24 22:35:14,643 BAD EPOCHS (no improvement): 1
2021-05-24 22:35:14,644 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:16,029 epoch 4 - iter 5/55 - loss 0.80963206 - samples/sec: 57.78 - lr: 0.100000
2021-05-24 22:35:17,387 epoch 4 - iter 10/55 - loss 0.80854368 - samples/sec: 58.93 - lr: 0.100000
2021-05-24 22:35:18,812 epoch 4 - iter 15/55 - loss 0.95255925 - samples/sec: 56.13 - lr: 0.100000
2021-05-24 22:35:20,212 epoch 4 - iter 20/55 - loss 0.99965998 - samples/sec: 57.17 - lr: 0.100000
2021-05-24 22:35:21,608 epoch 4 - iter 25/55 - loss 1.01508653 - samples/sec: 57.32 - lr: 0.100000
2021-05-24 22:35:23,007 epoch 4 - iter 30/55 - loss 1.02768579 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 22:35:24,413 epoch 4 - iter 35/55 - loss 1.01697229 - samples/sec: 56.94 - lr: 0.100000
2021-05-24 22:35:25,834 epoch 4 - iter 40/55 - loss 1.08679883 - samples/sec: 56.32 - lr: 0.100000
2021-05-24 22:35:27,237 epoch 4 - iter 45/55 - loss 1.07396589 - samples/sec: 57.04 - lr: 0.100000
2021-05-24 22:35:28,635 epoch 4 - iter 50/55 - loss 1.06487432 - samples/sec: 57.23 - lr: 0.100000
2021-05-24 22:35:29,831 epoch 4 - iter 55/55 - loss 1.12515599 - samples/sec: 66.91 - lr: 0.100000
2021-05-24 22:35:29,831 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:29,831 EPOCH 4 done: loss 1.1252 - lr 0.1000000
2021-05-24 22:35:30,963 DEV : loss 0.8918716311454773 - score 0.8961
2021-05-24 22:35:30,982 BAD EPOCHS (no improvement): 2
2021-05-24 22:35:30,982 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:32,384 epoch 5 - iter 5/55 - loss 1.40337992 - samples/sec: 57.11 - lr: 0.100000
2021-05-24 22:35:33,783 epoch 5 - iter 10/55 - loss 1.27644939 - samples/sec: 57.17 - lr: 0.100000
2021-05-24 22:35:35,158 epoch 5 - iter 15/55 - loss 1.13743459 - samples/sec: 58.21 - lr: 0.100000
2021-05-24 22:35:36,539 epoch 5 - iter 20/55 - loss 1.08065920 - samples/sec: 57.95 - lr: 0.100000
2021-05-24 22:35:37,966 epoch 5 - iter 25/55 - loss 0.98679094 - samples/sec: 56.06 - lr: 0.100000
2021-05-24 22:35:39,365 epoch 5 - iter 30/55 - loss 1.01906757 - samples/sec: 57.21 - lr: 0.100000
2021-05-24 22:35:40,774 epoch 5 - iter 35/55 - loss 1.01118081 - samples/sec: 56.82 - lr: 0.100000
2021-05-24 22:35:42,172 epoch 5 - iter 40/55 - loss 1.01867245 - samples/sec: 57.22 - lr: 0.100000
2021-05-24 22:35:43,583 epoch 5 - iter 45/55 - loss 0.98411621 - samples/sec: 56.71 - lr: 0.100000
2021-05-24 22:35:44,963 epoch 5 - iter 50/55 - loss 1.00194292 - samples/sec: 57.99 - lr: 0.100000
2021-05-24 22:35:46,155 epoch 5 - iter 55/55 - loss 0.97979766 - samples/sec: 67.13 - lr: 0.100000
2021-05-24 22:35:46,156 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:46,156 EPOCH 5 done: loss 0.9798 - lr 0.1000000
2021-05-24 22:35:47,287 DEV : loss 0.6911558508872986 - score 0.9102
2021-05-24 22:35:47,306 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:35:57,130 ----------------------------------------------------------------------------------------------------
2021-05-24 22:35:58,548 epoch 6 - iter 5/55 - loss 0.70969586 - samples/sec: 56.47 - lr: 0.100000
2021-05-24 22:35:59,951 epoch 6 - iter 10/55 - loss 0.91347706 - samples/sec: 57.04 - lr: 0.100000
2021-05-24 22:36:01,363 epoch 6 - iter 15/55 - loss 1.04712951 - samples/sec: 56.70 - lr: 0.100000
2021-05-24 22:36:02,793 epoch 6 - iter 20/55 - loss 0.95970207 - samples/sec: 55.95 - lr: 0.100000
2021-05-24 22:36:04,212 epoch 6 - iter 25/55 - loss 0.87938066 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:36:05,645 epoch 6 - iter 30/55 - loss 0.92110053 - samples/sec: 55.83 - lr: 0.100000
2021-05-24 22:36:07,043 epoch 6 - iter 35/55 - loss 0.93497674 - samples/sec: 57.28 - lr: 0.100000
2021-05-24 22:36:08,460 epoch 6 - iter 40/55 - loss 0.91591602 - samples/sec: 56.46 - lr: 0.100000
2021-05-24 22:36:09,876 epoch 6 - iter 45/55 - loss 0.90197125 - samples/sec: 56.51 - lr: 0.100000
2021-05-24 22:36:11,316 epoch 6 - iter 50/55 - loss 0.91081803 - samples/sec: 55.57 - lr: 0.100000
2021-05-24 22:36:12,515 epoch 6 - iter 55/55 - loss 0.91012465 - samples/sec: 66.74 - lr: 0.100000
2021-05-24 22:36:12,516 ----------------------------------------------------------------------------------------------------
2021-05-24 22:36:12,516 EPOCH 6 done: loss 0.9101 - lr 0.1000000
2021-05-24 22:36:13,645 DEV : loss 0.7346674799919128 - score 0.9148
2021-05-24 22:36:13,664 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:36:23,354 ----------------------------------------------------------------------------------------------------
2021-05-24 22:36:24,800 epoch 7 - iter 5/55 - loss 0.78340583 - samples/sec: 55.33 - lr: 0.100000
2021-05-24 22:36:26,328 epoch 7 - iter 10/55 - loss 1.00752198 - samples/sec: 52.37 - lr: 0.100000
2021-05-24 22:36:27,755 epoch 7 - iter 15/55 - loss 1.12055424 - samples/sec: 56.08 - lr: 0.100000
2021-05-24 22:36:29,147 epoch 7 - iter 20/55 - loss 1.03816777 - samples/sec: 57.52 - lr: 0.100000
2021-05-24 22:36:30,554 epoch 7 - iter 25/55 - loss 0.94034241 - samples/sec: 56.88 - lr: 0.100000
2021-05-24 22:36:31,960 epoch 7 - iter 30/55 - loss 0.95020778 - samples/sec: 56.91 - lr: 0.100000
2021-05-24 22:36:33,365 epoch 7 - iter 35/55 - loss 0.90456804 - samples/sec: 56.95 - lr: 0.100000
2021-05-24 22:36:34,788 epoch 7 - iter 40/55 - loss 0.88560318 - samples/sec: 56.25 - lr: 0.100000
2021-05-24 22:36:36,184 epoch 7 - iter 45/55 - loss 0.86439242 - samples/sec: 57.29 - lr: 0.100000
2021-05-24 22:36:37,603 epoch 7 - iter 50/55 - loss 0.86777799 - samples/sec: 56.41 - lr: 0.100000
2021-05-24 22:36:38,796 epoch 7 - iter 55/55 - loss 0.90083530 - samples/sec: 67.10 - lr: 0.100000
2021-05-24 22:36:38,796 ----------------------------------------------------------------------------------------------------
2021-05-24 22:36:38,796 EPOCH 7 done: loss 0.9008 - lr 0.1000000
2021-05-24 22:36:39,925 DEV : loss 0.6363691091537476 - score 0.9184
2021-05-24 22:36:39,944 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:36:49,481 ----------------------------------------------------------------------------------------------------
2021-05-24 22:36:50,885 epoch 8 - iter 5/55 - loss 1.37834995 - samples/sec: 57.01 - lr: 0.100000
2021-05-24 22:36:52,279 epoch 8 - iter 10/55 - loss 1.23265889 - samples/sec: 57.42 - lr: 0.100000
2021-05-24 22:36:53,696 epoch 8 - iter 15/55 - loss 0.95643369 - samples/sec: 56.46 - lr: 0.100000
2021-05-24 22:36:55,095 epoch 8 - iter 20/55 - loss 0.90521801 - samples/sec: 57.19 - lr: 0.100000
2021-05-24 22:36:56,487 epoch 8 - iter 25/55 - loss 0.84272542 - samples/sec: 57.51 - lr: 0.100000
2021-05-24 22:36:57,900 epoch 8 - iter 30/55 - loss 0.82187643 - samples/sec: 56.64 - lr: 0.100000
2021-05-24 22:36:59,280 epoch 8 - iter 35/55 - loss 0.78273617 - samples/sec: 57.98 - lr: 0.100000
2021-05-24 22:37:00,676 epoch 8 - iter 40/55 - loss 0.77402515 - samples/sec: 57.35 - lr: 0.100000
2021-05-24 22:37:02,099 epoch 8 - iter 45/55 - loss 0.75688490 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:37:03,499 epoch 8 - iter 50/55 - loss 0.73231109 - samples/sec: 57.16 - lr: 0.100000
2021-05-24 22:37:04,683 epoch 8 - iter 55/55 - loss 0.74300110 - samples/sec: 67.59 - lr: 0.100000
2021-05-24 22:37:04,684 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:04,684 EPOCH 8 done: loss 0.7430 - lr 0.1000000
2021-05-24 22:37:05,810 DEV : loss 1.2593460083007812 - score 0.8609
2021-05-24 22:37:05,829 BAD EPOCHS (no improvement): 1
2021-05-24 22:37:05,830 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:07,229 epoch 9 - iter 5/55 - loss 1.13486280 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 22:37:08,632 epoch 9 - iter 10/55 - loss 0.99069612 - samples/sec: 57.05 - lr: 0.100000
2021-05-24 22:37:10,056 epoch 9 - iter 15/55 - loss 1.06601151 - samples/sec: 56.18 - lr: 0.100000
2021-05-24 22:37:11,458 epoch 9 - iter 20/55 - loss 0.92973930 - samples/sec: 57.07 - lr: 0.100000
2021-05-24 22:37:12,839 epoch 9 - iter 25/55 - loss 0.88349311 - samples/sec: 57.97 - lr: 0.100000
2021-05-24 22:37:14,239 epoch 9 - iter 30/55 - loss 0.84273359 - samples/sec: 57.17 - lr: 0.100000
2021-05-24 22:37:15,627 epoch 9 - iter 35/55 - loss 0.84134097 - samples/sec: 57.66 - lr: 0.100000
2021-05-24 22:37:17,021 epoch 9 - iter 40/55 - loss 0.83637349 - samples/sec: 57.38 - lr: 0.100000
2021-05-24 22:37:18,423 epoch 9 - iter 45/55 - loss 0.84445996 - samples/sec: 57.10 - lr: 0.100000
2021-05-24 22:37:19,823 epoch 9 - iter 50/55 - loss 0.83368287 - samples/sec: 57.17 - lr: 0.100000
2021-05-24 22:37:21,009 epoch 9 - iter 55/55 - loss 0.83154775 - samples/sec: 67.48 - lr: 0.100000
2021-05-24 22:37:21,009 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:21,009 EPOCH 9 done: loss 0.8315 - lr 0.1000000
2021-05-24 22:37:22,138 DEV : loss 0.630331814289093 - score 0.9259
2021-05-24 22:37:22,157 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:37:31,924 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:33,287 epoch 10 - iter 5/55 - loss 0.73539474 - samples/sec: 58.74 - lr: 0.100000
2021-05-24 22:37:34,732 epoch 10 - iter 10/55 - loss 0.71151148 - samples/sec: 55.37 - lr: 0.100000
2021-05-24 22:37:36,152 epoch 10 - iter 15/55 - loss 0.72608322 - samples/sec: 56.37 - lr: 0.100000
2021-05-24 22:37:37,577 epoch 10 - iter 20/55 - loss 0.81714081 - samples/sec: 56.15 - lr: 0.100000
2021-05-24 22:37:39,018 epoch 10 - iter 25/55 - loss 0.79124588 - samples/sec: 55.57 - lr: 0.100000
2021-05-24 22:37:40,439 epoch 10 - iter 30/55 - loss 0.78567992 - samples/sec: 56.31 - lr: 0.100000
2021-05-24 22:37:41,869 epoch 10 - iter 35/55 - loss 0.76100683 - samples/sec: 55.95 - lr: 0.100000
2021-05-24 22:37:43,278 epoch 10 - iter 40/55 - loss 0.75091825 - samples/sec: 56.78 - lr: 0.100000
2021-05-24 22:37:44,719 epoch 10 - iter 45/55 - loss 0.73760802 - samples/sec: 55.53 - lr: 0.100000
2021-05-24 22:37:46,164 epoch 10 - iter 50/55 - loss 0.71510463 - samples/sec: 55.39 - lr: 0.100000
2021-05-24 22:37:47,338 epoch 10 - iter 55/55 - loss 0.72759570 - samples/sec: 68.20 - lr: 0.100000
2021-05-24 22:37:47,338 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:47,338 EPOCH 10 done: loss 0.7276 - lr 0.1000000
2021-05-24 22:37:48,465 DEV : loss 1.0449433326721191 - score 0.8737
2021-05-24 22:37:48,484 BAD EPOCHS (no improvement): 1
2021-05-24 22:37:48,484 ----------------------------------------------------------------------------------------------------
2021-05-24 22:37:49,883 epoch 11 - iter 5/55 - loss 0.89946830 - samples/sec: 57.20 - lr: 0.100000
2021-05-24 22:37:51,297 epoch 11 - iter 10/55 - loss 0.85217785 - samples/sec: 56.61 - lr: 0.100000
2021-05-24 22:37:52,736 epoch 11 - iter 15/55 - loss 0.79613090 - samples/sec: 55.61 - lr: 0.100000
2021-05-24 22:37:54,155 epoch 11 - iter 20/55 - loss 0.92214195 - samples/sec: 56.39 - lr: 0.100000
2021-05-24 22:37:55,572 epoch 11 - iter 25/55 - loss 0.88305616 - samples/sec: 56.48 - lr: 0.100000
2021-05-24 22:37:56,996 epoch 11 - iter 30/55 - loss 0.83760430 - samples/sec: 56.22 - lr: 0.100000
2021-05-24 22:37:58,386 epoch 11 - iter 35/55 - loss 0.80157917 - samples/sec: 57.56 - lr: 0.100000
2021-05-24 22:37:59,787 epoch 11 - iter 40/55 - loss 0.79323014 - samples/sec: 57.12 - lr: 0.100000
2021-05-24 22:38:01,152 epoch 11 - iter 45/55 - loss 0.79900191 - samples/sec: 58.63 - lr: 0.100000
2021-05-24 22:38:02,583 epoch 11 - iter 50/55 - loss 0.77194214 - samples/sec: 55.92 - lr: 0.100000
2021-05-24 22:38:03,772 epoch 11 - iter 55/55 - loss 0.81050085 - samples/sec: 67.32 - lr: 0.100000
2021-05-24 22:38:03,772 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:03,772 EPOCH 11 done: loss 0.8105 - lr 0.1000000
2021-05-24 22:38:04,896 DEV : loss 0.9819505214691162 - score 0.8881
2021-05-24 22:38:04,915 BAD EPOCHS (no improvement): 2
2021-05-24 22:38:04,915 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:06,338 epoch 12 - iter 5/55 - loss 0.45075130 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:38:07,763 epoch 12 - iter 10/55 - loss 0.64157128 - samples/sec: 56.14 - lr: 0.100000
2021-05-24 22:38:09,176 epoch 12 - iter 15/55 - loss 0.60852402 - samples/sec: 56.65 - lr: 0.100000
2021-05-24 22:38:10,617 epoch 12 - iter 20/55 - loss 0.67992996 - samples/sec: 55.52 - lr: 0.100000
2021-05-24 22:38:12,057 epoch 12 - iter 25/55 - loss 0.75094764 - samples/sec: 55.61 - lr: 0.100000
2021-05-24 22:38:13,465 epoch 12 - iter 30/55 - loss 0.81319727 - samples/sec: 56.80 - lr: 0.100000
2021-05-24 22:38:14,851 epoch 12 - iter 35/55 - loss 0.88875435 - samples/sec: 57.73 - lr: 0.100000
2021-05-24 22:38:16,276 epoch 12 - iter 40/55 - loss 0.82770150 - samples/sec: 56.18 - lr: 0.100000
2021-05-24 22:38:17,858 epoch 12 - iter 45/55 - loss 0.80372650 - samples/sec: 50.58 - lr: 0.100000
2021-05-24 22:38:19,275 epoch 12 - iter 50/55 - loss 0.76398398 - samples/sec: 56.47 - lr: 0.100000
2021-05-24 22:38:20,473 epoch 12 - iter 55/55 - loss 0.72217823 - samples/sec: 66.79 - lr: 0.100000
2021-05-24 22:38:20,474 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:20,474 EPOCH 12 done: loss 0.7222 - lr 0.1000000
2021-05-24 22:38:21,601 DEV : loss 0.8170239925384521 - score 0.9091
2021-05-24 22:38:21,620 BAD EPOCHS (no improvement): 3
2021-05-24 22:38:21,620 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:23,049 epoch 13 - iter 5/55 - loss 0.73444777 - samples/sec: 56.01 - lr: 0.100000
2021-05-24 22:38:24,458 epoch 13 - iter 10/55 - loss 0.64466286 - samples/sec: 56.80 - lr: 0.100000
2021-05-24 22:38:25,863 epoch 13 - iter 15/55 - loss 0.72187320 - samples/sec: 56.94 - lr: 0.100000
2021-05-24 22:38:27,289 epoch 13 - iter 20/55 - loss 0.74642248 - samples/sec: 56.13 - lr: 0.100000
2021-05-24 22:38:28,722 epoch 13 - iter 25/55 - loss 0.75857808 - samples/sec: 55.83 - lr: 0.100000
2021-05-24 22:38:30,152 epoch 13 - iter 30/55 - loss 0.73346808 - samples/sec: 55.97 - lr: 0.100000
2021-05-24 22:38:31,559 epoch 13 - iter 35/55 - loss 0.72742314 - samples/sec: 56.89 - lr: 0.100000
2021-05-24 22:38:33,004 epoch 13 - iter 40/55 - loss 0.73447747 - samples/sec: 55.38 - lr: 0.100000
2021-05-24 22:38:34,405 epoch 13 - iter 45/55 - loss 0.71922209 - samples/sec: 57.12 - lr: 0.100000
2021-05-24 22:38:35,815 epoch 13 - iter 50/55 - loss 0.72099450 - samples/sec: 56.77 - lr: 0.100000
2021-05-24 22:38:37,017 epoch 13 - iter 55/55 - loss 0.67803527 - samples/sec: 66.53 - lr: 0.100000
2021-05-24 22:38:37,018 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:37,018 EPOCH 13 done: loss 0.6780 - lr 0.1000000
2021-05-24 22:38:38,146 DEV : loss 1.0991251468658447 - score 0.8667
Epoch    13: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 22:38:38,165 BAD EPOCHS (no improvement): 4
2021-05-24 22:38:38,166 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:39,564 epoch 14 - iter 5/55 - loss 0.26013951 - samples/sec: 57.24 - lr: 0.050000
2021-05-24 22:38:40,970 epoch 14 - iter 10/55 - loss 0.32943747 - samples/sec: 56.92 - lr: 0.050000
2021-05-24 22:38:42,363 epoch 14 - iter 15/55 - loss 0.38544291 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:38:43,779 epoch 14 - iter 20/55 - loss 0.37308809 - samples/sec: 56.49 - lr: 0.050000
2021-05-24 22:38:45,216 epoch 14 - iter 25/55 - loss 0.41162431 - samples/sec: 55.69 - lr: 0.050000
2021-05-24 22:38:46,666 epoch 14 - iter 30/55 - loss 0.41511360 - samples/sec: 55.20 - lr: 0.050000
2021-05-24 22:38:48,069 epoch 14 - iter 35/55 - loss 0.45226876 - samples/sec: 57.05 - lr: 0.050000
2021-05-24 22:38:49,474 epoch 14 - iter 40/55 - loss 0.47391650 - samples/sec: 56.96 - lr: 0.050000
2021-05-24 22:38:50,892 epoch 14 - iter 45/55 - loss 0.50026537 - samples/sec: 56.44 - lr: 0.050000
2021-05-24 22:38:52,330 epoch 14 - iter 50/55 - loss 0.51129481 - samples/sec: 55.63 - lr: 0.050000
2021-05-24 22:38:53,538 epoch 14 - iter 55/55 - loss 0.48750200 - samples/sec: 66.25 - lr: 0.050000
2021-05-24 22:38:53,539 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:53,539 EPOCH 14 done: loss 0.4875 - lr 0.0500000
2021-05-24 22:38:54,663 DEV : loss 0.654330849647522 - score 0.9243
2021-05-24 22:38:54,683 BAD EPOCHS (no improvement): 1
2021-05-24 22:38:54,683 ----------------------------------------------------------------------------------------------------
2021-05-24 22:38:56,118 epoch 15 - iter 5/55 - loss 0.55629196 - samples/sec: 55.75 - lr: 0.050000
2021-05-24 22:38:57,542 epoch 15 - iter 10/55 - loss 0.44908826 - samples/sec: 56.21 - lr: 0.050000
2021-05-24 22:38:58,950 epoch 15 - iter 15/55 - loss 0.52845554 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:39:00,350 epoch 15 - iter 20/55 - loss 0.56815510 - samples/sec: 57.19 - lr: 0.050000
2021-05-24 22:39:01,743 epoch 15 - iter 25/55 - loss 0.59205398 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:39:03,171 epoch 15 - iter 30/55 - loss 0.56932743 - samples/sec: 56.03 - lr: 0.050000
2021-05-24 22:39:04,615 epoch 15 - iter 35/55 - loss 0.53013670 - samples/sec: 55.40 - lr: 0.050000
2021-05-24 22:39:06,001 epoch 15 - iter 40/55 - loss 0.54918958 - samples/sec: 57.77 - lr: 0.050000
2021-05-24 22:39:07,414 epoch 15 - iter 45/55 - loss 0.51516563 - samples/sec: 56.62 - lr: 0.050000
2021-05-24 22:39:08,829 epoch 15 - iter 50/55 - loss 0.53179320 - samples/sec: 56.54 - lr: 0.050000
2021-05-24 22:39:09,994 epoch 15 - iter 55/55 - loss 0.51259990 - samples/sec: 68.70 - lr: 0.050000
2021-05-24 22:39:09,995 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:09,995 EPOCH 15 done: loss 0.5126 - lr 0.0500000
2021-05-24 22:39:11,121 DEV : loss 0.7862159013748169 - score 0.9228
2021-05-24 22:39:11,140 BAD EPOCHS (no improvement): 2
2021-05-24 22:39:11,141 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:12,555 epoch 16 - iter 5/55 - loss 0.50346050 - samples/sec: 56.57 - lr: 0.050000
2021-05-24 22:39:13,961 epoch 16 - iter 10/55 - loss 0.51875839 - samples/sec: 56.92 - lr: 0.050000
2021-05-24 22:39:15,356 epoch 16 - iter 15/55 - loss 0.49831207 - samples/sec: 57.36 - lr: 0.050000
2021-05-24 22:39:16,778 epoch 16 - iter 20/55 - loss 0.46912033 - samples/sec: 56.30 - lr: 0.050000
2021-05-24 22:39:18,186 epoch 16 - iter 25/55 - loss 0.46885326 - samples/sec: 56.82 - lr: 0.050000
2021-05-24 22:39:19,607 epoch 16 - iter 30/55 - loss 0.55789966 - samples/sec: 56.32 - lr: 0.050000
2021-05-24 22:39:21,015 epoch 16 - iter 35/55 - loss 0.56967174 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:39:22,417 epoch 16 - iter 40/55 - loss 0.56915063 - samples/sec: 57.09 - lr: 0.050000
2021-05-24 22:39:23,829 epoch 16 - iter 45/55 - loss 0.54357342 - samples/sec: 56.67 - lr: 0.050000
2021-05-24 22:39:25,258 epoch 16 - iter 50/55 - loss 0.55825064 - samples/sec: 55.99 - lr: 0.050000
2021-05-24 22:39:26,473 epoch 16 - iter 55/55 - loss 0.53751496 - samples/sec: 65.90 - lr: 0.050000
2021-05-24 22:39:26,473 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:26,473 EPOCH 16 done: loss 0.5375 - lr 0.0500000
2021-05-24 22:39:27,598 DEV : loss 0.7176697254180908 - score 0.917
2021-05-24 22:39:27,618 BAD EPOCHS (no improvement): 3
2021-05-24 22:39:27,618 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:29,039 epoch 17 - iter 5/55 - loss 0.25720463 - samples/sec: 56.33 - lr: 0.050000
2021-05-24 22:39:30,447 epoch 17 - iter 10/55 - loss 0.31718323 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:39:31,876 epoch 17 - iter 15/55 - loss 0.32896062 - samples/sec: 56.00 - lr: 0.050000
2021-05-24 22:39:33,286 epoch 17 - iter 20/55 - loss 0.36820091 - samples/sec: 56.75 - lr: 0.050000
2021-05-24 22:39:34,717 epoch 17 - iter 25/55 - loss 0.41099611 - samples/sec: 55.92 - lr: 0.050000
2021-05-24 22:39:36,138 epoch 17 - iter 30/55 - loss 0.46915365 - samples/sec: 56.33 - lr: 0.050000
2021-05-24 22:39:37,532 epoch 17 - iter 35/55 - loss 0.47550580 - samples/sec: 57.40 - lr: 0.050000
2021-05-24 22:39:38,966 epoch 17 - iter 40/55 - loss 0.46887766 - samples/sec: 55.80 - lr: 0.050000
2021-05-24 22:39:40,365 epoch 17 - iter 45/55 - loss 0.46704230 - samples/sec: 57.20 - lr: 0.050000
2021-05-24 22:39:41,747 epoch 17 - iter 50/55 - loss 0.50381299 - samples/sec: 57.91 - lr: 0.050000
2021-05-24 22:39:42,951 epoch 17 - iter 55/55 - loss 0.49486280 - samples/sec: 66.47 - lr: 0.050000
2021-05-24 22:39:42,951 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:42,951 EPOCH 17 done: loss 0.4949 - lr 0.0500000
2021-05-24 22:39:44,242 DEV : loss 0.6805275678634644 - score 0.9269
2021-05-24 22:39:44,262 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:39:53,873 ----------------------------------------------------------------------------------------------------
2021-05-24 22:39:55,299 epoch 18 - iter 5/55 - loss 0.56706085 - samples/sec: 56.15 - lr: 0.050000
2021-05-24 22:39:56,731 epoch 18 - iter 10/55 - loss 0.50785709 - samples/sec: 55.91 - lr: 0.050000
2021-05-24 22:39:58,123 epoch 18 - iter 15/55 - loss 0.47469637 - samples/sec: 57.46 - lr: 0.050000
2021-05-24 22:39:59,519 epoch 18 - iter 20/55 - loss 0.46465913 - samples/sec: 57.34 - lr: 0.050000
2021-05-24 22:40:00,946 epoch 18 - iter 25/55 - loss 0.41325986 - samples/sec: 56.07 - lr: 0.050000
2021-05-24 22:40:02,356 epoch 18 - iter 30/55 - loss 0.42432571 - samples/sec: 56.76 - lr: 0.050000
2021-05-24 22:40:03,804 epoch 18 - iter 35/55 - loss 0.49924476 - samples/sec: 55.26 - lr: 0.050000
2021-05-24 22:40:05,222 epoch 18 - iter 40/55 - loss 0.49135332 - samples/sec: 56.44 - lr: 0.050000
2021-05-24 22:40:06,625 epoch 18 - iter 45/55 - loss 0.50058527 - samples/sec: 57.06 - lr: 0.050000
2021-05-24 22:40:08,016 epoch 18 - iter 50/55 - loss 0.49061757 - samples/sec: 57.50 - lr: 0.050000
2021-05-24 22:40:09,208 epoch 18 - iter 55/55 - loss 0.48005981 - samples/sec: 67.19 - lr: 0.050000
2021-05-24 22:40:09,208 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:09,243 EPOCH 18 done: loss 0.4801 - lr 0.0500000
2021-05-24 22:40:10,374 DEV : loss 0.7288462519645691 - score 0.917
2021-05-24 22:40:10,393 BAD EPOCHS (no improvement): 1
2021-05-24 22:40:10,393 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:11,812 epoch 19 - iter 5/55 - loss 0.49470286 - samples/sec: 56.43 - lr: 0.050000
2021-05-24 22:40:13,226 epoch 19 - iter 10/55 - loss 0.48136885 - samples/sec: 56.58 - lr: 0.050000
2021-05-24 22:40:14,630 epoch 19 - iter 15/55 - loss 0.48219537 - samples/sec: 56.99 - lr: 0.050000
2021-05-24 22:40:16,060 epoch 19 - iter 20/55 - loss 0.44204399 - samples/sec: 55.96 - lr: 0.050000
2021-05-24 22:40:17,483 epoch 19 - iter 25/55 - loss 0.42236292 - samples/sec: 56.23 - lr: 0.050000
2021-05-24 22:40:18,898 epoch 19 - iter 30/55 - loss 0.41462330 - samples/sec: 56.58 - lr: 0.050000
2021-05-24 22:40:20,298 epoch 19 - iter 35/55 - loss 0.43561193 - samples/sec: 57.14 - lr: 0.050000
2021-05-24 22:40:21,716 epoch 19 - iter 40/55 - loss 0.53837879 - samples/sec: 56.46 - lr: 0.050000
2021-05-24 22:40:23,130 epoch 19 - iter 45/55 - loss 0.54218906 - samples/sec: 56.57 - lr: 0.050000
2021-05-24 22:40:24,535 epoch 19 - iter 50/55 - loss 0.52752588 - samples/sec: 56.97 - lr: 0.050000
2021-05-24 22:40:25,727 epoch 19 - iter 55/55 - loss 0.55884736 - samples/sec: 67.15 - lr: 0.050000
2021-05-24 22:40:25,727 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:25,727 EPOCH 19 done: loss 0.5588 - lr 0.0500000
2021-05-24 22:40:26,852 DEV : loss 1.0482711791992188 - score 0.8656
2021-05-24 22:40:26,871 BAD EPOCHS (no improvement): 2
2021-05-24 22:40:26,871 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:28,264 epoch 20 - iter 5/55 - loss 0.61554073 - samples/sec: 57.47 - lr: 0.050000
2021-05-24 22:40:29,660 epoch 20 - iter 10/55 - loss 0.54718978 - samples/sec: 57.33 - lr: 0.050000
2021-05-24 22:40:31,061 epoch 20 - iter 15/55 - loss 0.44255684 - samples/sec: 57.08 - lr: 0.050000
2021-05-24 22:40:32,492 epoch 20 - iter 20/55 - loss 0.37210447 - samples/sec: 55.96 - lr: 0.050000
2021-05-24 22:40:33,913 epoch 20 - iter 25/55 - loss 0.40099879 - samples/sec: 56.32 - lr: 0.050000
2021-05-24 22:40:35,336 epoch 20 - iter 30/55 - loss 0.42602203 - samples/sec: 56.21 - lr: 0.050000
2021-05-24 22:40:36,743 epoch 20 - iter 35/55 - loss 0.43493194 - samples/sec: 56.88 - lr: 0.050000
2021-05-24 22:40:38,155 epoch 20 - iter 40/55 - loss 0.47476855 - samples/sec: 56.67 - lr: 0.050000
2021-05-24 22:40:39,568 epoch 20 - iter 45/55 - loss 0.43808921 - samples/sec: 56.66 - lr: 0.050000
2021-05-24 22:40:40,986 epoch 20 - iter 50/55 - loss 0.42900190 - samples/sec: 56.41 - lr: 0.050000
2021-05-24 22:40:42,143 epoch 20 - iter 55/55 - loss 0.44189850 - samples/sec: 69.21 - lr: 0.050000
2021-05-24 22:40:42,143 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:42,143 EPOCH 20 done: loss 0.4419 - lr 0.0500000
2021-05-24 22:40:43,271 DEV : loss 1.0646016597747803 - score 0.8905
2021-05-24 22:40:43,290 BAD EPOCHS (no improvement): 3
2021-05-24 22:40:43,290 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:44,688 epoch 21 - iter 5/55 - loss 0.21164174 - samples/sec: 57.24 - lr: 0.050000
2021-05-24 22:40:46,104 epoch 21 - iter 10/55 - loss 0.33750119 - samples/sec: 56.53 - lr: 0.050000
2021-05-24 22:40:47,538 epoch 21 - iter 15/55 - loss 0.34175671 - samples/sec: 55.80 - lr: 0.050000
2021-05-24 22:40:48,924 epoch 21 - iter 20/55 - loss 0.32805471 - samples/sec: 57.71 - lr: 0.050000
2021-05-24 22:40:50,318 epoch 21 - iter 25/55 - loss 0.34553655 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:40:51,726 epoch 21 - iter 30/55 - loss 0.36080218 - samples/sec: 56.82 - lr: 0.050000
2021-05-24 22:40:53,147 epoch 21 - iter 35/55 - loss 0.41963214 - samples/sec: 56.33 - lr: 0.050000
2021-05-24 22:40:54,550 epoch 21 - iter 40/55 - loss 0.43579427 - samples/sec: 57.01 - lr: 0.050000
2021-05-24 22:40:55,969 epoch 21 - iter 45/55 - loss 0.42149644 - samples/sec: 56.41 - lr: 0.050000
2021-05-24 22:40:57,376 epoch 21 - iter 50/55 - loss 0.41986340 - samples/sec: 56.87 - lr: 0.050000
2021-05-24 22:40:58,587 epoch 21 - iter 55/55 - loss 0.44125905 - samples/sec: 66.10 - lr: 0.050000
2021-05-24 22:40:58,587 ----------------------------------------------------------------------------------------------------
2021-05-24 22:40:58,587 EPOCH 21 done: loss 0.4413 - lr 0.0500000
2021-05-24 22:40:59,714 DEV : loss 2.314406394958496 - score 0.7981
Epoch    21: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 22:40:59,733 BAD EPOCHS (no improvement): 4
2021-05-24 22:40:59,733 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:01,156 epoch 22 - iter 5/55 - loss 0.89832072 - samples/sec: 56.24 - lr: 0.025000
2021-05-24 22:41:02,553 epoch 22 - iter 10/55 - loss 0.56325123 - samples/sec: 57.29 - lr: 0.025000
2021-05-24 22:41:03,983 epoch 22 - iter 15/55 - loss 0.50636855 - samples/sec: 55.97 - lr: 0.025000
2021-05-24 22:41:05,409 epoch 22 - iter 20/55 - loss 0.58419669 - samples/sec: 56.10 - lr: 0.025000
2021-05-24 22:41:06,819 epoch 22 - iter 25/55 - loss 0.53439696 - samples/sec: 56.77 - lr: 0.025000
2021-05-24 22:41:08,243 epoch 22 - iter 30/55 - loss 0.46760763 - samples/sec: 56.21 - lr: 0.025000
2021-05-24 22:41:09,671 epoch 22 - iter 35/55 - loss 0.49119962 - samples/sec: 56.05 - lr: 0.025000
2021-05-24 22:41:11,073 epoch 22 - iter 40/55 - loss 0.46875620 - samples/sec: 57.06 - lr: 0.025000
2021-05-24 22:41:12,481 epoch 22 - iter 45/55 - loss 0.46666292 - samples/sec: 56.83 - lr: 0.025000
2021-05-24 22:41:13,875 epoch 22 - iter 50/55 - loss 0.46113119 - samples/sec: 57.43 - lr: 0.025000
2021-05-24 22:41:15,074 epoch 22 - iter 55/55 - loss 0.44773146 - samples/sec: 66.70 - lr: 0.025000
2021-05-24 22:41:15,075 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:15,075 EPOCH 22 done: loss 0.4477 - lr 0.0250000
2021-05-24 22:41:16,370 DEV : loss 0.7988263964653015 - score 0.9187
2021-05-24 22:41:16,389 BAD EPOCHS (no improvement): 1
2021-05-24 22:41:16,389 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:17,785 epoch 23 - iter 5/55 - loss 0.31436852 - samples/sec: 57.32 - lr: 0.025000
2021-05-24 22:41:19,200 epoch 23 - iter 10/55 - loss 0.23480076 - samples/sec: 56.57 - lr: 0.025000
2021-05-24 22:41:20,625 epoch 23 - iter 15/55 - loss 0.41092966 - samples/sec: 56.15 - lr: 0.025000
2021-05-24 22:41:22,028 epoch 23 - iter 20/55 - loss 0.37395614 - samples/sec: 57.07 - lr: 0.025000
2021-05-24 22:41:23,411 epoch 23 - iter 25/55 - loss 0.38093265 - samples/sec: 57.83 - lr: 0.025000
2021-05-24 22:41:24,817 epoch 23 - iter 30/55 - loss 0.36490819 - samples/sec: 56.95 - lr: 0.025000
2021-05-24 22:41:26,271 epoch 23 - iter 35/55 - loss 0.40939511 - samples/sec: 55.03 - lr: 0.025000
2021-05-24 22:41:27,692 epoch 23 - iter 40/55 - loss 0.41084452 - samples/sec: 56.32 - lr: 0.025000
2021-05-24 22:41:29,116 epoch 23 - iter 45/55 - loss 0.41052841 - samples/sec: 56.21 - lr: 0.025000
2021-05-24 22:41:30,540 epoch 23 - iter 50/55 - loss 0.42412326 - samples/sec: 56.18 - lr: 0.025000
2021-05-24 22:41:31,715 epoch 23 - iter 55/55 - loss 0.39745393 - samples/sec: 68.08 - lr: 0.025000
2021-05-24 22:41:31,716 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:31,716 EPOCH 23 done: loss 0.3975 - lr 0.0250000
2021-05-24 22:41:32,842 DEV : loss 0.8707205057144165 - score 0.9125
2021-05-24 22:41:32,861 BAD EPOCHS (no improvement): 2
2021-05-24 22:41:32,861 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:34,253 epoch 24 - iter 5/55 - loss 0.39458408 - samples/sec: 57.49 - lr: 0.025000
2021-05-24 22:41:35,665 epoch 24 - iter 10/55 - loss 0.31322958 - samples/sec: 56.71 - lr: 0.025000
2021-05-24 22:41:37,085 epoch 24 - iter 15/55 - loss 0.33268369 - samples/sec: 56.34 - lr: 0.025000
2021-05-24 22:41:38,489 epoch 24 - iter 20/55 - loss 0.32585619 - samples/sec: 57.01 - lr: 0.025000
2021-05-24 22:41:39,884 epoch 24 - iter 25/55 - loss 0.32498690 - samples/sec: 57.34 - lr: 0.025000
2021-05-24 22:41:41,305 epoch 24 - iter 30/55 - loss 0.32730960 - samples/sec: 56.32 - lr: 0.025000
2021-05-24 22:41:42,679 epoch 24 - iter 35/55 - loss 0.37262865 - samples/sec: 58.26 - lr: 0.025000
2021-05-24 22:41:44,076 epoch 24 - iter 40/55 - loss 0.34377313 - samples/sec: 57.27 - lr: 0.025000
2021-05-24 22:41:45,443 epoch 24 - iter 45/55 - loss 0.35822549 - samples/sec: 58.52 - lr: 0.025000
2021-05-24 22:41:46,844 epoch 24 - iter 50/55 - loss 0.36970908 - samples/sec: 57.14 - lr: 0.025000
2021-05-24 22:41:48,060 epoch 24 - iter 55/55 - loss 0.36404740 - samples/sec: 65.82 - lr: 0.025000
2021-05-24 22:41:48,060 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:48,060 EPOCH 24 done: loss 0.3640 - lr 0.0250000
2021-05-24 22:41:49,188 DEV : loss 0.7556389570236206 - score 0.9142
2021-05-24 22:41:49,208 BAD EPOCHS (no improvement): 3
2021-05-24 22:41:49,208 ----------------------------------------------------------------------------------------------------
2021-05-24 22:41:50,604 epoch 25 - iter 5/55 - loss 0.39671187 - samples/sec: 57.31 - lr: 0.025000
2021-05-24 22:41:51,987 epoch 25 - iter 10/55 - loss 0.35013727 - samples/sec: 57.86 - lr: 0.025000
2021-05-24 22:41:53,403 epoch 25 - iter 15/55 - loss 0.30798663 - samples/sec: 56.53 - lr: 0.025000
2021-05-24 22:41:54,803 epoch 25 - iter 20/55 - loss 0.28359561 - samples/sec: 57.15 - lr: 0.025000
2021-05-24 22:41:56,198 epoch 25 - iter 25/55 - loss 0.34725112 - samples/sec: 57.40 - lr: 0.025000
2021-05-24 22:41:57,598 epoch 25 - iter 30/55 - loss 0.33901313 - samples/sec: 57.16 - lr: 0.025000
2021-05-24 22:41:59,005 epoch 25 - iter 35/55 - loss 0.36701767 - samples/sec: 56.88 - lr: 0.025000
2021-05-24 22:42:00,407 epoch 25 - iter 40/55 - loss 0.33440791 - samples/sec: 57.07 - lr: 0.025000
2021-05-24 22:42:01,825 epoch 25 - iter 45/55 - loss 0.35291707 - samples/sec: 56.44 - lr: 0.025000
2021-05-24 22:42:03,241 epoch 25 - iter 50/55 - loss 0.34943917 - samples/sec: 56.51 - lr: 0.025000
2021-05-24 22:42:04,412 epoch 25 - iter 55/55 - loss 0.33311996 - samples/sec: 68.36 - lr: 0.025000
2021-05-24 22:42:04,412 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:04,412 EPOCH 25 done: loss 0.3331 - lr 0.0250000
2021-05-24 22:42:05,541 DEV : loss 0.8203884363174438 - score 0.9153
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-24 22:42:05,560 BAD EPOCHS (no improvement): 4
2021-05-24 22:42:05,560 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:06,967 epoch 26 - iter 5/55 - loss 0.24943137 - samples/sec: 56.86 - lr: 0.012500
2021-05-24 22:42:08,387 epoch 26 - iter 10/55 - loss 0.32405994 - samples/sec: 56.39 - lr: 0.012500
2021-05-24 22:42:09,816 epoch 26 - iter 15/55 - loss 0.32270700 - samples/sec: 55.97 - lr: 0.012500
2021-05-24 22:42:11,247 epoch 26 - iter 20/55 - loss 0.42031034 - samples/sec: 55.94 - lr: 0.012500
2021-05-24 22:42:12,628 epoch 26 - iter 25/55 - loss 0.38307523 - samples/sec: 57.95 - lr: 0.012500
2021-05-24 22:42:14,038 epoch 26 - iter 30/55 - loss 0.37810108 - samples/sec: 56.76 - lr: 0.012500
2021-05-24 22:42:15,463 epoch 26 - iter 35/55 - loss 0.34180515 - samples/sec: 56.16 - lr: 0.012500
2021-05-24 22:42:16,867 epoch 26 - iter 40/55 - loss 0.33851938 - samples/sec: 56.97 - lr: 0.012500
2021-05-24 22:42:18,272 epoch 26 - iter 45/55 - loss 0.34636122 - samples/sec: 56.98 - lr: 0.012500
2021-05-24 22:42:19,653 epoch 26 - iter 50/55 - loss 0.34733717 - samples/sec: 57.93 - lr: 0.012500
2021-05-24 22:42:20,826 epoch 26 - iter 55/55 - loss 0.33031800 - samples/sec: 68.24 - lr: 0.012500
2021-05-24 22:42:20,826 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:20,827 EPOCH 26 done: loss 0.3303 - lr 0.0125000
2021-05-24 22:42:21,955 DEV : loss 0.8819588422775269 - score 0.9145
2021-05-24 22:42:21,974 BAD EPOCHS (no improvement): 1
2021-05-24 22:42:21,974 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:23,374 epoch 27 - iter 5/55 - loss 0.26614072 - samples/sec: 57.18 - lr: 0.012500
2021-05-24 22:42:24,790 epoch 27 - iter 10/55 - loss 0.28678261 - samples/sec: 56.49 - lr: 0.012500
2021-05-24 22:42:26,208 epoch 27 - iter 15/55 - loss 0.30122221 - samples/sec: 56.43 - lr: 0.012500
2021-05-24 22:42:27,633 epoch 27 - iter 20/55 - loss 0.31478445 - samples/sec: 56.16 - lr: 0.012500
2021-05-24 22:42:29,038 epoch 27 - iter 25/55 - loss 0.34527341 - samples/sec: 56.98 - lr: 0.012500
2021-05-24 22:42:30,473 epoch 27 - iter 30/55 - loss 0.31122926 - samples/sec: 55.75 - lr: 0.012500
2021-05-24 22:42:31,870 epoch 27 - iter 35/55 - loss 0.30065416 - samples/sec: 57.30 - lr: 0.012500
2021-05-24 22:42:33,292 epoch 27 - iter 40/55 - loss 0.31155213 - samples/sec: 56.27 - lr: 0.012500
2021-05-24 22:42:34,728 epoch 27 - iter 45/55 - loss 0.35829376 - samples/sec: 55.72 - lr: 0.012500
2021-05-24 22:42:36,089 epoch 27 - iter 50/55 - loss 0.35564018 - samples/sec: 58.79 - lr: 0.012500
2021-05-24 22:42:37,266 epoch 27 - iter 55/55 - loss 0.35246652 - samples/sec: 68.01 - lr: 0.012500
2021-05-24 22:42:37,266 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:37,266 EPOCH 27 done: loss 0.3525 - lr 0.0125000
2021-05-24 22:42:38,394 DEV : loss 0.8668520450592041 - score 0.9142
2021-05-24 22:42:38,413 BAD EPOCHS (no improvement): 2
2021-05-24 22:42:38,413 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:39,850 epoch 28 - iter 5/55 - loss 0.25290480 - samples/sec: 55.70 - lr: 0.012500
2021-05-24 22:42:41,238 epoch 28 - iter 10/55 - loss 0.29478033 - samples/sec: 57.69 - lr: 0.012500
2021-05-24 22:42:42,652 epoch 28 - iter 15/55 - loss 0.28778342 - samples/sec: 56.56 - lr: 0.012500
2021-05-24 22:42:44,067 epoch 28 - iter 20/55 - loss 0.26974381 - samples/sec: 56.57 - lr: 0.012500
2021-05-24 22:42:45,479 epoch 28 - iter 25/55 - loss 0.26835889 - samples/sec: 56.67 - lr: 0.012500
2021-05-24 22:42:46,893 epoch 28 - iter 30/55 - loss 0.26399091 - samples/sec: 56.60 - lr: 0.012500
2021-05-24 22:42:48,316 epoch 28 - iter 35/55 - loss 0.30237294 - samples/sec: 56.22 - lr: 0.012500
2021-05-24 22:42:49,722 epoch 28 - iter 40/55 - loss 0.35256400 - samples/sec: 56.94 - lr: 0.012500
2021-05-24 22:42:51,129 epoch 28 - iter 45/55 - loss 0.36311695 - samples/sec: 56.87 - lr: 0.012500
2021-05-24 22:42:52,697 epoch 28 - iter 50/55 - loss 0.35235338 - samples/sec: 51.03 - lr: 0.012500
2021-05-24 22:42:53,897 epoch 28 - iter 55/55 - loss 0.33786941 - samples/sec: 66.70 - lr: 0.012500
2021-05-24 22:42:53,897 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:53,897 EPOCH 28 done: loss 0.3379 - lr 0.0125000
2021-05-24 22:42:55,026 DEV : loss 0.8472315669059753 - score 0.9187
2021-05-24 22:42:55,045 BAD EPOCHS (no improvement): 3
2021-05-24 22:42:55,045 ----------------------------------------------------------------------------------------------------
2021-05-24 22:42:56,436 epoch 29 - iter 5/55 - loss 0.34925878 - samples/sec: 57.55 - lr: 0.012500
2021-05-24 22:42:57,843 epoch 29 - iter 10/55 - loss 0.38071890 - samples/sec: 56.89 - lr: 0.012500
2021-05-24 22:42:59,273 epoch 29 - iter 15/55 - loss 0.38569285 - samples/sec: 55.93 - lr: 0.012500
2021-05-24 22:43:00,681 epoch 29 - iter 20/55 - loss 0.41396589 - samples/sec: 56.85 - lr: 0.012500
2021-05-24 22:43:02,101 epoch 29 - iter 25/55 - loss 0.38833522 - samples/sec: 56.34 - lr: 0.012500
2021-05-24 22:43:03,493 epoch 29 - iter 30/55 - loss 0.36956746 - samples/sec: 57.49 - lr: 0.012500
2021-05-24 22:43:04,877 epoch 29 - iter 35/55 - loss 0.34589692 - samples/sec: 57.84 - lr: 0.012500
2021-05-24 22:43:06,286 epoch 29 - iter 40/55 - loss 0.34591728 - samples/sec: 56.77 - lr: 0.012500
2021-05-24 22:43:07,703 epoch 29 - iter 45/55 - loss 0.33122345 - samples/sec: 56.48 - lr: 0.012500
2021-05-24 22:43:09,119 epoch 29 - iter 50/55 - loss 0.32229382 - samples/sec: 56.53 - lr: 0.012500
2021-05-24 22:43:10,318 epoch 29 - iter 55/55 - loss 0.33411740 - samples/sec: 66.75 - lr: 0.012500
2021-05-24 22:43:10,318 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:10,318 EPOCH 29 done: loss 0.3341 - lr 0.0125000
2021-05-24 22:43:11,444 DEV : loss 0.9229918718338013 - score 0.902
Epoch    29: reducing learning rate of group 0 to 6.2500e-03.
2021-05-24 22:43:11,463 BAD EPOCHS (no improvement): 4
2021-05-24 22:43:11,463 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:12,873 epoch 30 - iter 5/55 - loss 0.14337044 - samples/sec: 56.74 - lr: 0.006250
2021-05-24 22:43:14,292 epoch 30 - iter 10/55 - loss 0.20496287 - samples/sec: 56.43 - lr: 0.006250
2021-05-24 22:43:15,709 epoch 30 - iter 15/55 - loss 0.31598504 - samples/sec: 56.44 - lr: 0.006250
2021-05-24 22:43:17,134 epoch 30 - iter 20/55 - loss 0.39670572 - samples/sec: 56.17 - lr: 0.006250
2021-05-24 22:43:18,533 epoch 30 - iter 25/55 - loss 0.40193484 - samples/sec: 57.22 - lr: 0.006250
2021-05-24 22:43:19,902 epoch 30 - iter 30/55 - loss 0.36696568 - samples/sec: 58.45 - lr: 0.006250
2021-05-24 22:43:21,321 epoch 30 - iter 35/55 - loss 0.36069887 - samples/sec: 56.39 - lr: 0.006250
2021-05-24 22:43:22,746 epoch 30 - iter 40/55 - loss 0.39796201 - samples/sec: 56.16 - lr: 0.006250
2021-05-24 22:43:24,155 epoch 30 - iter 45/55 - loss 0.39068130 - samples/sec: 56.78 - lr: 0.006250
2021-05-24 22:43:25,539 epoch 30 - iter 50/55 - loss 0.38624340 - samples/sec: 57.81 - lr: 0.006250
2021-05-24 22:43:26,724 epoch 30 - iter 55/55 - loss 0.37293653 - samples/sec: 67.53 - lr: 0.006250
2021-05-24 22:43:26,725 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:26,725 EPOCH 30 done: loss 0.3729 - lr 0.0062500
2021-05-24 22:43:27,856 DEV : loss 1.0030492544174194 - score 0.8913
2021-05-24 22:43:27,875 BAD EPOCHS (no improvement): 1
2021-05-24 22:43:29,188 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:29,188 Testing using best model ...
2021-05-24 22:43:29,188 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/nld.rst.nldt/best-model.pt
2021-05-24 22:43:35,821 0.9522	0.9637	0.9579
2021-05-24 22:43:35,821 
Results:
- F1-score (micro) 0.9579
- F1-score (macro) 0.9579

By class:
SENT       tp: 239 - fp: 12 - fn: 9 - precision: 0.9522 - recall: 0.9637 - f1-score: 0.9579
2021-05-24 22:43:35,821 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn/
2021-05-24 22:43:35,845 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn
2021-05-24 22:43:35,845 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn/sent_train.txt
2021-05-24 22:43:35,847 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn/sent_dev.txt
2021-05-24 22:43:35,847 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn/sent_test.txt
Corpus: 2031 train + 282 dev + 164 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 22:43:42,524 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 22:43:42,527 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Corpus: "Corpus: 2031 train + 282 dev + 164 test sentences"
2021-05-24 22:43:42,527 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Parameters:
2021-05-24 22:43:42,527  - learning_rate: "0.1"
2021-05-24 22:43:42,527  - mini_batch_size: "16"
2021-05-24 22:43:42,527  - patience: "3"
2021-05-24 22:43:42,527  - anneal_factor: "0.5"
2021-05-24 22:43:42,527  - max_epochs: "30"
2021-05-24 22:43:42,527  - shuffle: "True"
2021-05-24 22:43:42,527  - train_with_dev: "False"
2021-05-24 22:43:42,527  - batch_growth_annealing: "False"
2021-05-24 22:43:42,527 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn"
2021-05-24 22:43:42,527 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Device: cuda:0
2021-05-24 22:43:42,527 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:42,527 Embeddings storage mode: cpu
2021-05-24 22:43:42,529 ----------------------------------------------------------------------------------------------------
2021-05-24 22:43:51,841 epoch 1 - iter 12/127 - loss 4.22172832 - samples/sec: 20.62 - lr: 0.100000
2021-05-24 22:44:01,252 epoch 1 - iter 24/127 - loss 3.81298872 - samples/sec: 20.40 - lr: 0.100000
2021-05-24 22:44:10,521 epoch 1 - iter 36/127 - loss 3.58494121 - samples/sec: 20.72 - lr: 0.100000
2021-05-24 22:44:19,834 epoch 1 - iter 48/127 - loss 3.36166028 - samples/sec: 20.62 - lr: 0.100000
2021-05-24 22:44:29,147 epoch 1 - iter 60/127 - loss 3.16843101 - samples/sec: 20.62 - lr: 0.100000
2021-05-24 22:44:38,458 epoch 1 - iter 72/127 - loss 2.88022386 - samples/sec: 20.62 - lr: 0.100000
2021-05-24 22:44:47,730 epoch 1 - iter 84/127 - loss 2.74379078 - samples/sec: 20.71 - lr: 0.100000
2021-05-24 22:44:56,984 epoch 1 - iter 96/127 - loss 2.52770254 - samples/sec: 20.75 - lr: 0.100000
2021-05-24 22:45:06,258 epoch 1 - iter 108/127 - loss 2.41668335 - samples/sec: 20.71 - lr: 0.100000
2021-05-24 22:45:15,568 epoch 1 - iter 120/127 - loss 2.30885486 - samples/sec: 20.62 - lr: 0.100000
2021-05-24 22:45:20,971 ----------------------------------------------------------------------------------------------------
2021-05-24 22:45:20,971 EPOCH 1 done: loss 2.2397 - lr 0.1000000
2021-05-24 22:45:29,718 DEV : loss 0.2713991105556488 - score 0.9694
2021-05-24 22:45:29,746 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:45:30,848 ----------------------------------------------------------------------------------------------------
2021-05-24 22:45:34,162 epoch 2 - iter 12/127 - loss 0.92631687 - samples/sec: 57.96 - lr: 0.100000
2021-05-24 22:45:37,524 epoch 2 - iter 24/127 - loss 0.95740268 - samples/sec: 57.12 - lr: 0.100000
2021-05-24 22:45:40,874 epoch 2 - iter 36/127 - loss 0.97294818 - samples/sec: 57.33 - lr: 0.100000
2021-05-24 22:45:44,249 epoch 2 - iter 48/127 - loss 0.94076990 - samples/sec: 56.90 - lr: 0.100000
2021-05-24 22:45:47,635 epoch 2 - iter 60/127 - loss 0.94413802 - samples/sec: 56.70 - lr: 0.100000
2021-05-24 22:45:50,999 epoch 2 - iter 72/127 - loss 0.96746755 - samples/sec: 57.09 - lr: 0.100000
2021-05-24 22:45:54,307 epoch 2 - iter 84/127 - loss 0.94229834 - samples/sec: 58.06 - lr: 0.100000
2021-05-24 22:45:57,647 epoch 2 - iter 96/127 - loss 0.90811792 - samples/sec: 57.49 - lr: 0.100000
2021-05-24 22:46:01,007 epoch 2 - iter 108/127 - loss 0.91707287 - samples/sec: 57.16 - lr: 0.100000
2021-05-24 22:46:04,375 epoch 2 - iter 120/127 - loss 0.91430162 - samples/sec: 57.01 - lr: 0.100000
2021-05-24 22:46:06,305 ----------------------------------------------------------------------------------------------------
2021-05-24 22:46:06,305 EPOCH 2 done: loss 0.9066 - lr 0.1000000
2021-05-24 22:46:08,006 DEV : loss 0.20747597515583038 - score 0.9693
2021-05-24 22:46:08,035 BAD EPOCHS (no improvement): 1
2021-05-24 22:46:08,035 ----------------------------------------------------------------------------------------------------
2021-05-24 22:46:11,390 epoch 3 - iter 12/127 - loss 0.77359010 - samples/sec: 57.24 - lr: 0.100000
2021-05-24 22:46:14,789 epoch 3 - iter 24/127 - loss 0.84731998 - samples/sec: 56.51 - lr: 0.100000
2021-05-24 22:46:18,168 epoch 3 - iter 36/127 - loss 0.80404844 - samples/sec: 56.83 - lr: 0.100000
2021-05-24 22:46:21,534 epoch 3 - iter 48/127 - loss 0.73462044 - samples/sec: 57.06 - lr: 0.100000
2021-05-24 22:46:24,848 epoch 3 - iter 60/127 - loss 0.70117381 - samples/sec: 57.94 - lr: 0.100000
2021-05-24 22:46:28,169 epoch 3 - iter 72/127 - loss 0.72584896 - samples/sec: 57.81 - lr: 0.100000
2021-05-24 22:46:31,504 epoch 3 - iter 84/127 - loss 0.71903603 - samples/sec: 57.58 - lr: 0.100000
2021-05-24 22:46:34,852 epoch 3 - iter 96/127 - loss 0.72472478 - samples/sec: 57.37 - lr: 0.100000
2021-05-24 22:46:38,180 epoch 3 - iter 108/127 - loss 0.67351938 - samples/sec: 57.70 - lr: 0.100000
2021-05-24 22:46:41,531 epoch 3 - iter 120/127 - loss 0.65928577 - samples/sec: 57.31 - lr: 0.100000
2021-05-24 22:46:43,484 ----------------------------------------------------------------------------------------------------
2021-05-24 22:46:43,484 EPOCH 3 done: loss 0.6517 - lr 0.1000000
2021-05-24 22:46:45,180 DEV : loss 0.3777810335159302 - score 0.9333
2021-05-24 22:46:45,209 BAD EPOCHS (no improvement): 2
2021-05-24 22:46:45,210 ----------------------------------------------------------------------------------------------------
2021-05-24 22:46:48,571 epoch 4 - iter 12/127 - loss 0.58071308 - samples/sec: 57.14 - lr: 0.100000
2021-05-24 22:46:51,926 epoch 4 - iter 24/127 - loss 0.52581687 - samples/sec: 57.23 - lr: 0.100000
2021-05-24 22:46:55,292 epoch 4 - iter 36/127 - loss 0.54216011 - samples/sec: 57.05 - lr: 0.100000
2021-05-24 22:46:58,638 epoch 4 - iter 48/127 - loss 0.51986204 - samples/sec: 57.40 - lr: 0.100000
2021-05-24 22:47:01,964 epoch 4 - iter 60/127 - loss 0.51746040 - samples/sec: 57.73 - lr: 0.100000
2021-05-24 22:47:05,325 epoch 4 - iter 72/127 - loss 0.54038679 - samples/sec: 57.14 - lr: 0.100000
2021-05-24 22:47:08,702 epoch 4 - iter 84/127 - loss 0.54908684 - samples/sec: 56.87 - lr: 0.100000
2021-05-24 22:47:12,070 epoch 4 - iter 96/127 - loss 0.58246763 - samples/sec: 57.01 - lr: 0.100000
2021-05-24 22:47:15,395 epoch 4 - iter 108/127 - loss 0.57723638 - samples/sec: 57.75 - lr: 0.100000
2021-05-24 22:47:18,771 epoch 4 - iter 120/127 - loss 0.56955769 - samples/sec: 56.90 - lr: 0.100000
2021-05-24 22:47:20,688 ----------------------------------------------------------------------------------------------------
2021-05-24 22:47:20,688 EPOCH 4 done: loss 0.5718 - lr 0.1000000
2021-05-24 22:47:22,390 DEV : loss 0.17029055953025818 - score 0.9781
2021-05-24 22:47:22,419 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:47:32,105 ----------------------------------------------------------------------------------------------------
2021-05-24 22:47:35,530 epoch 5 - iter 12/127 - loss 0.47478601 - samples/sec: 56.09 - lr: 0.100000
2021-05-24 22:47:38,903 epoch 5 - iter 24/127 - loss 0.50132887 - samples/sec: 56.95 - lr: 0.100000
2021-05-24 22:47:42,290 epoch 5 - iter 36/127 - loss 0.47061215 - samples/sec: 56.69 - lr: 0.100000
2021-05-24 22:47:45,673 epoch 5 - iter 48/127 - loss 0.46962912 - samples/sec: 56.77 - lr: 0.100000
2021-05-24 22:47:49,038 epoch 5 - iter 60/127 - loss 0.45593953 - samples/sec: 57.07 - lr: 0.100000
2021-05-24 22:47:52,445 epoch 5 - iter 72/127 - loss 0.46295712 - samples/sec: 56.36 - lr: 0.100000
2021-05-24 22:47:55,847 epoch 5 - iter 84/127 - loss 0.45868267 - samples/sec: 56.45 - lr: 0.100000
2021-05-24 22:47:59,207 epoch 5 - iter 96/127 - loss 0.46469306 - samples/sec: 57.16 - lr: 0.100000
2021-05-24 22:48:02,516 epoch 5 - iter 108/127 - loss 0.48142037 - samples/sec: 58.03 - lr: 0.100000
2021-05-24 22:48:05,874 epoch 5 - iter 120/127 - loss 0.48668249 - samples/sec: 57.18 - lr: 0.100000
2021-05-24 22:48:07,815 ----------------------------------------------------------------------------------------------------
2021-05-24 22:48:07,815 EPOCH 5 done: loss 0.4836 - lr 0.1000000
2021-05-24 22:48:09,514 DEV : loss 0.2646564543247223 - score 0.955
2021-05-24 22:48:09,543 BAD EPOCHS (no improvement): 1
2021-05-24 22:48:09,543 ----------------------------------------------------------------------------------------------------
2021-05-24 22:48:13,113 epoch 6 - iter 12/127 - loss 0.53903152 - samples/sec: 53.79 - lr: 0.100000
2021-05-24 22:48:16,474 epoch 6 - iter 24/127 - loss 0.50213646 - samples/sec: 57.14 - lr: 0.100000
2021-05-24 22:48:19,828 epoch 6 - iter 36/127 - loss 0.48089146 - samples/sec: 57.26 - lr: 0.100000
2021-05-24 22:48:23,160 epoch 6 - iter 48/127 - loss 0.48775637 - samples/sec: 57.62 - lr: 0.100000
2021-05-24 22:48:26,532 epoch 6 - iter 60/127 - loss 0.46753356 - samples/sec: 56.96 - lr: 0.100000
2021-05-24 22:48:29,894 epoch 6 - iter 72/127 - loss 0.44262053 - samples/sec: 57.12 - lr: 0.100000
2021-05-24 22:48:33,200 epoch 6 - iter 84/127 - loss 0.45398680 - samples/sec: 58.08 - lr: 0.100000
2021-05-24 22:48:36,584 epoch 6 - iter 96/127 - loss 0.45411199 - samples/sec: 56.76 - lr: 0.100000
2021-05-24 22:48:39,944 epoch 6 - iter 108/127 - loss 0.44004337 - samples/sec: 57.15 - lr: 0.100000
2021-05-24 22:48:43,295 epoch 6 - iter 120/127 - loss 0.42752450 - samples/sec: 57.30 - lr: 0.100000
2021-05-24 22:48:45,228 ----------------------------------------------------------------------------------------------------
2021-05-24 22:48:45,228 EPOCH 6 done: loss 0.4369 - lr 0.1000000
2021-05-24 22:48:46,928 DEV : loss 0.4369353950023651 - score 0.9057
2021-05-24 22:48:46,956 BAD EPOCHS (no improvement): 2
2021-05-24 22:48:46,956 ----------------------------------------------------------------------------------------------------
2021-05-24 22:48:50,311 epoch 7 - iter 12/127 - loss 0.35848068 - samples/sec: 57.24 - lr: 0.100000
2021-05-24 22:48:53,691 epoch 7 - iter 24/127 - loss 0.38099397 - samples/sec: 56.83 - lr: 0.100000
2021-05-24 22:48:57,024 epoch 7 - iter 36/127 - loss 0.37876301 - samples/sec: 57.61 - lr: 0.100000
2021-05-24 22:49:00,365 epoch 7 - iter 48/127 - loss 0.33944804 - samples/sec: 57.49 - lr: 0.100000
2021-05-24 22:49:03,699 epoch 7 - iter 60/127 - loss 0.33393319 - samples/sec: 57.59 - lr: 0.100000
2021-05-24 22:49:07,054 epoch 7 - iter 72/127 - loss 0.37137733 - samples/sec: 57.25 - lr: 0.100000
2021-05-24 22:49:10,383 epoch 7 - iter 84/127 - loss 0.40910643 - samples/sec: 57.69 - lr: 0.100000
2021-05-24 22:49:13,733 epoch 7 - iter 96/127 - loss 0.42522892 - samples/sec: 57.33 - lr: 0.100000
2021-05-24 22:49:17,100 epoch 7 - iter 108/127 - loss 0.40398594 - samples/sec: 57.03 - lr: 0.100000
2021-05-24 22:49:20,462 epoch 7 - iter 120/127 - loss 0.40421305 - samples/sec: 57.11 - lr: 0.100000
2021-05-24 22:49:22,453 ----------------------------------------------------------------------------------------------------
2021-05-24 22:49:22,453 EPOCH 7 done: loss 0.4000 - lr 0.1000000
2021-05-24 22:49:24,149 DEV : loss 0.17793695628643036 - score 0.9762
2021-05-24 22:49:24,178 BAD EPOCHS (no improvement): 3
2021-05-24 22:49:24,178 ----------------------------------------------------------------------------------------------------
2021-05-24 22:49:27,565 epoch 8 - iter 12/127 - loss 0.38901098 - samples/sec: 56.69 - lr: 0.100000
2021-05-24 22:49:30,932 epoch 8 - iter 24/127 - loss 0.38703924 - samples/sec: 57.04 - lr: 0.100000
2021-05-24 22:49:34,317 epoch 8 - iter 36/127 - loss 0.38416263 - samples/sec: 56.74 - lr: 0.100000
2021-05-24 22:49:37,705 epoch 8 - iter 48/127 - loss 0.34536782 - samples/sec: 56.68 - lr: 0.100000
2021-05-24 22:49:41,090 epoch 8 - iter 60/127 - loss 0.37923193 - samples/sec: 56.73 - lr: 0.100000
2021-05-24 22:49:44,477 epoch 8 - iter 72/127 - loss 0.36724130 - samples/sec: 56.68 - lr: 0.100000
2021-05-24 22:49:47,898 epoch 8 - iter 84/127 - loss 0.38814228 - samples/sec: 56.14 - lr: 0.100000
2021-05-24 22:49:51,311 epoch 8 - iter 96/127 - loss 0.40817989 - samples/sec: 56.26 - lr: 0.100000
2021-05-24 22:49:54,700 epoch 8 - iter 108/127 - loss 0.41673035 - samples/sec: 56.66 - lr: 0.100000
2021-05-24 22:49:58,091 epoch 8 - iter 120/127 - loss 0.39555299 - samples/sec: 56.64 - lr: 0.100000
2021-05-24 22:50:00,020 ----------------------------------------------------------------------------------------------------
2021-05-24 22:50:00,020 EPOCH 8 done: loss 0.4014 - lr 0.1000000
2021-05-24 22:50:01,718 DEV : loss 0.4957271218299866 - score 0.8792
Epoch     8: reducing learning rate of group 0 to 5.0000e-02.
2021-05-24 22:50:01,747 BAD EPOCHS (no improvement): 4
2021-05-24 22:50:01,747 ----------------------------------------------------------------------------------------------------
2021-05-24 22:50:05,125 epoch 9 - iter 12/127 - loss 0.27709299 - samples/sec: 56.86 - lr: 0.050000
2021-05-24 22:50:08,477 epoch 9 - iter 24/127 - loss 0.30015989 - samples/sec: 57.29 - lr: 0.050000
2021-05-24 22:50:11,860 epoch 9 - iter 36/127 - loss 0.30737326 - samples/sec: 56.76 - lr: 0.050000
2021-05-24 22:50:15,241 epoch 9 - iter 48/127 - loss 0.33527969 - samples/sec: 56.79 - lr: 0.050000
2021-05-24 22:50:18,657 epoch 9 - iter 60/127 - loss 0.32251246 - samples/sec: 56.22 - lr: 0.050000
2021-05-24 22:50:22,034 epoch 9 - iter 72/127 - loss 0.30740189 - samples/sec: 56.88 - lr: 0.050000
2021-05-24 22:50:25,420 epoch 9 - iter 84/127 - loss 0.29435880 - samples/sec: 56.70 - lr: 0.050000
2021-05-24 22:50:28,802 epoch 9 - iter 96/127 - loss 0.29341233 - samples/sec: 56.80 - lr: 0.050000
2021-05-24 22:50:32,161 epoch 9 - iter 108/127 - loss 0.30387689 - samples/sec: 57.16 - lr: 0.050000
2021-05-24 22:50:35,531 epoch 9 - iter 120/127 - loss 0.29890471 - samples/sec: 56.98 - lr: 0.050000
2021-05-24 22:50:37,498 ----------------------------------------------------------------------------------------------------
2021-05-24 22:50:37,498 EPOCH 9 done: loss 0.2967 - lr 0.0500000
2021-05-24 22:50:39,194 DEV : loss 0.16711607575416565 - score 0.9804
2021-05-24 22:50:39,223 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:50:48,583 ----------------------------------------------------------------------------------------------------
2021-05-24 22:50:51,965 epoch 10 - iter 12/127 - loss 0.10436524 - samples/sec: 56.79 - lr: 0.050000
2021-05-24 22:50:55,325 epoch 10 - iter 24/127 - loss 0.20401289 - samples/sec: 57.15 - lr: 0.050000
2021-05-24 22:50:58,708 epoch 10 - iter 36/127 - loss 0.22158484 - samples/sec: 56.78 - lr: 0.050000
2021-05-24 22:51:02,051 epoch 10 - iter 48/127 - loss 0.24019653 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:51:05,424 epoch 10 - iter 60/127 - loss 0.24834349 - samples/sec: 56.93 - lr: 0.050000
2021-05-24 22:51:08,761 epoch 10 - iter 72/127 - loss 0.26883822 - samples/sec: 57.56 - lr: 0.050000
2021-05-24 22:51:12,138 epoch 10 - iter 84/127 - loss 0.28608116 - samples/sec: 56.86 - lr: 0.050000
2021-05-24 22:51:15,488 epoch 10 - iter 96/127 - loss 0.28934352 - samples/sec: 57.32 - lr: 0.050000
2021-05-24 22:51:18,906 epoch 10 - iter 108/127 - loss 0.28534198 - samples/sec: 56.19 - lr: 0.050000
2021-05-24 22:51:22,296 epoch 10 - iter 120/127 - loss 0.28813156 - samples/sec: 56.66 - lr: 0.050000
2021-05-24 22:51:24,266 ----------------------------------------------------------------------------------------------------
2021-05-24 22:51:24,266 EPOCH 10 done: loss 0.2977 - lr 0.0500000
2021-05-24 22:51:25,961 DEV : loss 0.14694304764270782 - score 0.9825
2021-05-24 22:51:25,990 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:51:35,983 ----------------------------------------------------------------------------------------------------
2021-05-24 22:51:39,397 epoch 11 - iter 12/127 - loss 0.16870243 - samples/sec: 56.26 - lr: 0.050000
2021-05-24 22:51:42,759 epoch 11 - iter 24/127 - loss 0.21706260 - samples/sec: 57.13 - lr: 0.050000
2021-05-24 22:51:46,113 epoch 11 - iter 36/127 - loss 0.24464615 - samples/sec: 57.26 - lr: 0.050000
2021-05-24 22:51:49,484 epoch 11 - iter 48/127 - loss 0.27416456 - samples/sec: 56.96 - lr: 0.050000
2021-05-24 22:51:52,875 epoch 11 - iter 60/127 - loss 0.26140418 - samples/sec: 56.63 - lr: 0.050000
2021-05-24 22:51:56,242 epoch 11 - iter 72/127 - loss 0.25212820 - samples/sec: 57.04 - lr: 0.050000
2021-05-24 22:51:59,612 epoch 11 - iter 84/127 - loss 0.25050420 - samples/sec: 56.98 - lr: 0.050000
2021-05-24 22:52:02,938 epoch 11 - iter 96/127 - loss 0.25267716 - samples/sec: 57.74 - lr: 0.050000
2021-05-24 22:52:06,342 epoch 11 - iter 108/127 - loss 0.25767203 - samples/sec: 56.42 - lr: 0.050000
2021-05-24 22:52:09,711 epoch 11 - iter 120/127 - loss 0.26073953 - samples/sec: 56.99 - lr: 0.050000
2021-05-24 22:52:11,648 ----------------------------------------------------------------------------------------------------
2021-05-24 22:52:11,648 EPOCH 11 done: loss 0.2612 - lr 0.0500000
2021-05-24 22:52:13,342 DEV : loss 0.18158197402954102 - score 0.9742
2021-05-24 22:52:13,371 BAD EPOCHS (no improvement): 1
2021-05-24 22:52:13,371 ----------------------------------------------------------------------------------------------------
2021-05-24 22:52:16,761 epoch 12 - iter 12/127 - loss 0.18359860 - samples/sec: 56.66 - lr: 0.050000
2021-05-24 22:52:20,106 epoch 12 - iter 24/127 - loss 0.27179741 - samples/sec: 57.41 - lr: 0.050000
2021-05-24 22:52:23,461 epoch 12 - iter 36/127 - loss 0.30583646 - samples/sec: 57.23 - lr: 0.050000
2021-05-24 22:52:26,817 epoch 12 - iter 48/127 - loss 0.30100862 - samples/sec: 57.23 - lr: 0.050000
2021-05-24 22:52:30,182 epoch 12 - iter 60/127 - loss 0.28028301 - samples/sec: 57.08 - lr: 0.050000
2021-05-24 22:52:33,524 epoch 12 - iter 72/127 - loss 0.27886236 - samples/sec: 57.46 - lr: 0.050000
2021-05-24 22:52:36,872 epoch 12 - iter 84/127 - loss 0.28593206 - samples/sec: 57.36 - lr: 0.050000
2021-05-24 22:52:40,191 epoch 12 - iter 96/127 - loss 0.27107686 - samples/sec: 57.86 - lr: 0.050000
2021-05-24 22:52:43,573 epoch 12 - iter 108/127 - loss 0.26219756 - samples/sec: 56.78 - lr: 0.050000
2021-05-24 22:52:46,965 epoch 12 - iter 120/127 - loss 0.25093093 - samples/sec: 56.63 - lr: 0.050000
2021-05-24 22:52:48,952 ----------------------------------------------------------------------------------------------------
2021-05-24 22:52:48,953 EPOCH 12 done: loss 0.2535 - lr 0.0500000
2021-05-24 22:52:50,647 DEV : loss 0.17667527496814728 - score 0.9763
2021-05-24 22:52:50,676 BAD EPOCHS (no improvement): 2
2021-05-24 22:52:50,676 ----------------------------------------------------------------------------------------------------
2021-05-24 22:52:54,030 epoch 13 - iter 12/127 - loss 0.19377754 - samples/sec: 57.27 - lr: 0.050000
2021-05-24 22:52:57,438 epoch 13 - iter 24/127 - loss 0.18460577 - samples/sec: 56.35 - lr: 0.050000
2021-05-24 22:53:00,839 epoch 13 - iter 36/127 - loss 0.22867911 - samples/sec: 56.46 - lr: 0.050000
2021-05-24 22:53:04,238 epoch 13 - iter 48/127 - loss 0.21367775 - samples/sec: 56.50 - lr: 0.050000
2021-05-24 22:53:07,623 epoch 13 - iter 60/127 - loss 0.22402283 - samples/sec: 56.73 - lr: 0.050000
2021-05-24 22:53:10,994 epoch 13 - iter 72/127 - loss 0.23193033 - samples/sec: 56.96 - lr: 0.050000
2021-05-24 22:53:14,350 epoch 13 - iter 84/127 - loss 0.22680038 - samples/sec: 57.22 - lr: 0.050000
2021-05-24 22:53:17,711 epoch 13 - iter 96/127 - loss 0.22730351 - samples/sec: 57.15 - lr: 0.050000
2021-05-24 22:53:21,110 epoch 13 - iter 108/127 - loss 0.22944557 - samples/sec: 56.49 - lr: 0.050000
2021-05-24 22:53:24,476 epoch 13 - iter 120/127 - loss 0.23365622 - samples/sec: 57.05 - lr: 0.050000
2021-05-24 22:53:26,415 ----------------------------------------------------------------------------------------------------
2021-05-24 22:53:26,415 EPOCH 13 done: loss 0.2315 - lr 0.0500000
2021-05-24 22:53:28,105 DEV : loss 0.1779359132051468 - score 0.9781
2021-05-24 22:53:28,134 BAD EPOCHS (no improvement): 3
2021-05-24 22:53:28,134 ----------------------------------------------------------------------------------------------------
2021-05-24 22:53:31,525 epoch 14 - iter 12/127 - loss 0.17428088 - samples/sec: 56.64 - lr: 0.050000
2021-05-24 22:53:34,868 epoch 14 - iter 24/127 - loss 0.24817374 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:53:38,237 epoch 14 - iter 36/127 - loss 0.22470143 - samples/sec: 57.01 - lr: 0.050000
2021-05-24 22:53:41,616 epoch 14 - iter 48/127 - loss 0.24584460 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:53:44,976 epoch 14 - iter 60/127 - loss 0.22721929 - samples/sec: 57.15 - lr: 0.050000
2021-05-24 22:53:48,388 epoch 14 - iter 72/127 - loss 0.22530434 - samples/sec: 56.27 - lr: 0.050000
2021-05-24 22:53:51,795 epoch 14 - iter 84/127 - loss 0.20949124 - samples/sec: 56.37 - lr: 0.050000
2021-05-24 22:53:55,140 epoch 14 - iter 96/127 - loss 0.22373588 - samples/sec: 57.41 - lr: 0.050000
2021-05-24 22:53:58,485 epoch 14 - iter 108/127 - loss 0.22612994 - samples/sec: 57.42 - lr: 0.050000
2021-05-24 22:54:01,833 epoch 14 - iter 120/127 - loss 0.23492543 - samples/sec: 57.35 - lr: 0.050000
2021-05-24 22:54:03,761 ----------------------------------------------------------------------------------------------------
2021-05-24 22:54:03,762 EPOCH 14 done: loss 0.2378 - lr 0.0500000
2021-05-24 22:54:05,455 DEV : loss 0.16346405446529388 - score 0.9826
2021-05-24 22:54:05,484 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:54:15,071 ----------------------------------------------------------------------------------------------------
2021-05-24 22:54:18,418 epoch 15 - iter 12/127 - loss 0.22848544 - samples/sec: 57.38 - lr: 0.050000
2021-05-24 22:54:21,778 epoch 15 - iter 24/127 - loss 0.28613588 - samples/sec: 57.16 - lr: 0.050000
2021-05-24 22:54:25,089 epoch 15 - iter 36/127 - loss 0.23114739 - samples/sec: 58.00 - lr: 0.050000
2021-05-24 22:54:28,414 epoch 15 - iter 48/127 - loss 0.20175366 - samples/sec: 57.75 - lr: 0.050000
2021-05-24 22:54:31,757 epoch 15 - iter 60/127 - loss 0.22347133 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:54:35,123 epoch 15 - iter 72/127 - loss 0.21012748 - samples/sec: 57.05 - lr: 0.050000
2021-05-24 22:54:38,433 epoch 15 - iter 84/127 - loss 0.20668599 - samples/sec: 58.02 - lr: 0.050000
2021-05-24 22:54:41,781 epoch 15 - iter 96/127 - loss 0.20767199 - samples/sec: 57.36 - lr: 0.050000
2021-05-24 22:54:45,188 epoch 15 - iter 108/127 - loss 0.23163339 - samples/sec: 56.37 - lr: 0.050000
2021-05-24 22:54:48,567 epoch 15 - iter 120/127 - loss 0.22167107 - samples/sec: 56.83 - lr: 0.050000
2021-05-24 22:54:50,533 ----------------------------------------------------------------------------------------------------
2021-05-24 22:54:50,533 EPOCH 15 done: loss 0.2156 - lr 0.0500000
2021-05-24 22:54:52,222 DEV : loss 0.1695258766412735 - score 0.9784
2021-05-24 22:54:52,250 BAD EPOCHS (no improvement): 1
2021-05-24 22:54:52,251 ----------------------------------------------------------------------------------------------------
2021-05-24 22:54:55,630 epoch 16 - iter 12/127 - loss 0.22778809 - samples/sec: 56.84 - lr: 0.050000
2021-05-24 22:54:58,999 epoch 16 - iter 24/127 - loss 0.19265450 - samples/sec: 57.00 - lr: 0.050000
2021-05-24 22:55:02,342 epoch 16 - iter 36/127 - loss 0.19022360 - samples/sec: 57.45 - lr: 0.050000
2021-05-24 22:55:05,713 epoch 16 - iter 48/127 - loss 0.22048831 - samples/sec: 56.95 - lr: 0.050000
2021-05-24 22:55:09,086 epoch 16 - iter 60/127 - loss 0.19643066 - samples/sec: 56.94 - lr: 0.050000
2021-05-24 22:55:12,648 epoch 16 - iter 72/127 - loss 0.21150564 - samples/sec: 53.92 - lr: 0.050000
2021-05-24 22:55:15,942 epoch 16 - iter 84/127 - loss 0.20096198 - samples/sec: 58.30 - lr: 0.050000
2021-05-24 22:55:19,265 epoch 16 - iter 96/127 - loss 0.19961087 - samples/sec: 57.78 - lr: 0.050000
2021-05-24 22:55:22,629 epoch 16 - iter 108/127 - loss 0.21747242 - samples/sec: 57.10 - lr: 0.050000
2021-05-24 22:55:25,943 epoch 16 - iter 120/127 - loss 0.21609502 - samples/sec: 57.95 - lr: 0.050000
2021-05-24 22:55:27,874 ----------------------------------------------------------------------------------------------------
2021-05-24 22:55:27,874 EPOCH 16 done: loss 0.2146 - lr 0.0500000
2021-05-24 22:55:29,564 DEV : loss 0.17357897758483887 - score 0.9825
2021-05-24 22:55:29,593 BAD EPOCHS (no improvement): 2
2021-05-24 22:55:29,593 ----------------------------------------------------------------------------------------------------
2021-05-24 22:55:32,959 epoch 17 - iter 12/127 - loss 0.25396324 - samples/sec: 57.06 - lr: 0.050000
2021-05-24 22:55:36,336 epoch 17 - iter 24/127 - loss 0.24966798 - samples/sec: 56.87 - lr: 0.050000
2021-05-24 22:55:39,678 epoch 17 - iter 36/127 - loss 0.25151517 - samples/sec: 57.45 - lr: 0.050000
2021-05-24 22:55:42,968 epoch 17 - iter 48/127 - loss 0.22989327 - samples/sec: 58.37 - lr: 0.050000
2021-05-24 22:55:46,272 epoch 17 - iter 60/127 - loss 0.22772209 - samples/sec: 58.12 - lr: 0.050000
2021-05-24 22:55:49,629 epoch 17 - iter 72/127 - loss 0.22605524 - samples/sec: 57.21 - lr: 0.050000
2021-05-24 22:55:52,934 epoch 17 - iter 84/127 - loss 0.21451944 - samples/sec: 58.12 - lr: 0.050000
2021-05-24 22:55:56,292 epoch 17 - iter 96/127 - loss 0.21452344 - samples/sec: 57.18 - lr: 0.050000
2021-05-24 22:55:59,603 epoch 17 - iter 108/127 - loss 0.20242318 - samples/sec: 58.00 - lr: 0.050000
2021-05-24 22:56:02,969 epoch 17 - iter 120/127 - loss 0.20580079 - samples/sec: 57.06 - lr: 0.050000
2021-05-24 22:56:04,912 ----------------------------------------------------------------------------------------------------
2021-05-24 22:56:04,912 EPOCH 17 done: loss 0.2211 - lr 0.0500000
2021-05-24 22:56:06,601 DEV : loss 0.15441808104515076 - score 0.9825
2021-05-24 22:56:06,629 BAD EPOCHS (no improvement): 3
2021-05-24 22:56:06,630 ----------------------------------------------------------------------------------------------------
2021-05-24 22:56:09,972 epoch 18 - iter 12/127 - loss 0.24836298 - samples/sec: 57.47 - lr: 0.050000
2021-05-24 22:56:13,346 epoch 18 - iter 24/127 - loss 0.25923821 - samples/sec: 56.92 - lr: 0.050000
2021-05-24 22:56:16,688 epoch 18 - iter 36/127 - loss 0.24518234 - samples/sec: 57.46 - lr: 0.050000
2021-05-24 22:56:20,053 epoch 18 - iter 48/127 - loss 0.22882804 - samples/sec: 57.07 - lr: 0.050000
2021-05-24 22:56:23,353 epoch 18 - iter 60/127 - loss 0.24610685 - samples/sec: 58.19 - lr: 0.050000
2021-05-24 22:56:26,697 epoch 18 - iter 72/127 - loss 0.24058321 - samples/sec: 57.42 - lr: 0.050000
2021-05-24 22:56:30,040 epoch 18 - iter 84/127 - loss 0.23999361 - samples/sec: 57.44 - lr: 0.050000
2021-05-24 22:56:33,384 epoch 18 - iter 96/127 - loss 0.23876374 - samples/sec: 57.43 - lr: 0.050000
2021-05-24 22:56:36,747 epoch 18 - iter 108/127 - loss 0.23409664 - samples/sec: 57.12 - lr: 0.050000
2021-05-24 22:56:40,081 epoch 18 - iter 120/127 - loss 0.23089745 - samples/sec: 57.59 - lr: 0.050000
2021-05-24 22:56:42,000 ----------------------------------------------------------------------------------------------------
2021-05-24 22:56:42,000 EPOCH 18 done: loss 0.2278 - lr 0.0500000
2021-05-24 22:56:43,687 DEV : loss 0.1572766900062561 - score 0.9825
Epoch    18: reducing learning rate of group 0 to 2.5000e-02.
2021-05-24 22:56:43,716 BAD EPOCHS (no improvement): 4
2021-05-24 22:56:43,716 ----------------------------------------------------------------------------------------------------
2021-05-24 22:56:47,039 epoch 19 - iter 12/127 - loss 0.20684819 - samples/sec: 57.81 - lr: 0.025000
2021-05-24 22:56:50,390 epoch 19 - iter 24/127 - loss 0.22339542 - samples/sec: 57.30 - lr: 0.025000
2021-05-24 22:56:53,690 epoch 19 - iter 36/127 - loss 0.20828614 - samples/sec: 58.19 - lr: 0.025000
2021-05-24 22:56:57,033 epoch 19 - iter 48/127 - loss 0.23065955 - samples/sec: 57.44 - lr: 0.025000
2021-05-24 22:57:00,370 epoch 19 - iter 60/127 - loss 0.22204636 - samples/sec: 57.56 - lr: 0.025000
2021-05-24 22:57:03,722 epoch 19 - iter 72/127 - loss 0.21738620 - samples/sec: 57.29 - lr: 0.025000
2021-05-24 22:57:07,044 epoch 19 - iter 84/127 - loss 0.20665142 - samples/sec: 57.79 - lr: 0.025000
2021-05-24 22:57:10,403 epoch 19 - iter 96/127 - loss 0.22095903 - samples/sec: 57.17 - lr: 0.025000
2021-05-24 22:57:13,706 epoch 19 - iter 108/127 - loss 0.21335135 - samples/sec: 58.15 - lr: 0.025000
2021-05-24 22:57:17,052 epoch 19 - iter 120/127 - loss 0.20348433 - samples/sec: 57.39 - lr: 0.025000
2021-05-24 22:57:18,967 ----------------------------------------------------------------------------------------------------
2021-05-24 22:57:18,967 EPOCH 19 done: loss 0.1988 - lr 0.0250000
2021-05-24 22:57:20,656 DEV : loss 0.16521503031253815 - score 0.9825
2021-05-24 22:57:20,685 BAD EPOCHS (no improvement): 1
2021-05-24 22:57:20,685 ----------------------------------------------------------------------------------------------------
2021-05-24 22:57:24,026 epoch 20 - iter 12/127 - loss 0.24333871 - samples/sec: 57.49 - lr: 0.025000
2021-05-24 22:57:27,344 epoch 20 - iter 24/127 - loss 0.19688815 - samples/sec: 57.88 - lr: 0.025000
2021-05-24 22:57:30,687 epoch 20 - iter 36/127 - loss 0.19441499 - samples/sec: 57.43 - lr: 0.025000
2021-05-24 22:57:34,022 epoch 20 - iter 48/127 - loss 0.19362584 - samples/sec: 57.60 - lr: 0.025000
2021-05-24 22:57:37,347 epoch 20 - iter 60/127 - loss 0.19398901 - samples/sec: 57.75 - lr: 0.025000
2021-05-24 22:57:40,683 epoch 20 - iter 72/127 - loss 0.19137446 - samples/sec: 57.56 - lr: 0.025000
2021-05-24 22:57:44,025 epoch 20 - iter 84/127 - loss 0.19394894 - samples/sec: 57.47 - lr: 0.025000
2021-05-24 22:57:47,369 epoch 20 - iter 96/127 - loss 0.19490124 - samples/sec: 57.43 - lr: 0.025000
2021-05-24 22:57:50,688 epoch 20 - iter 108/127 - loss 0.18598322 - samples/sec: 57.87 - lr: 0.025000
2021-05-24 22:57:54,047 epoch 20 - iter 120/127 - loss 0.19364798 - samples/sec: 57.17 - lr: 0.025000
2021-05-24 22:57:55,966 ----------------------------------------------------------------------------------------------------
2021-05-24 22:57:55,966 EPOCH 20 done: loss 0.1899 - lr 0.0250000
2021-05-24 22:57:57,656 DEV : loss 0.1682189404964447 - score 0.9803
2021-05-24 22:57:57,685 BAD EPOCHS (no improvement): 2
2021-05-24 22:57:57,685 ----------------------------------------------------------------------------------------------------
2021-05-24 22:58:01,021 epoch 21 - iter 12/127 - loss 0.21100588 - samples/sec: 57.58 - lr: 0.025000
2021-05-24 22:58:04,438 epoch 21 - iter 24/127 - loss 0.23192322 - samples/sec: 56.19 - lr: 0.025000
2021-05-24 22:58:07,804 epoch 21 - iter 36/127 - loss 0.18029672 - samples/sec: 57.06 - lr: 0.025000
2021-05-24 22:58:11,157 epoch 21 - iter 48/127 - loss 0.17917395 - samples/sec: 57.28 - lr: 0.025000
2021-05-24 22:58:14,510 epoch 21 - iter 60/127 - loss 0.19037120 - samples/sec: 57.26 - lr: 0.025000
2021-05-24 22:58:17,867 epoch 21 - iter 72/127 - loss 0.18158414 - samples/sec: 57.21 - lr: 0.025000
2021-05-24 22:58:21,168 epoch 21 - iter 84/127 - loss 0.17971267 - samples/sec: 58.17 - lr: 0.025000
2021-05-24 22:58:24,495 epoch 21 - iter 96/127 - loss 0.18070174 - samples/sec: 57.74 - lr: 0.025000
2021-05-24 22:58:27,798 epoch 21 - iter 108/127 - loss 0.18058177 - samples/sec: 58.14 - lr: 0.025000
2021-05-24 22:58:31,137 epoch 21 - iter 120/127 - loss 0.17949721 - samples/sec: 57.50 - lr: 0.025000
2021-05-24 22:58:33,072 ----------------------------------------------------------------------------------------------------
2021-05-24 22:58:33,073 EPOCH 21 done: loss 0.1790 - lr 0.0250000
2021-05-24 22:58:34,968 DEV : loss 0.16631367802619934 - score 0.9847
2021-05-24 22:58:34,997 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:58:44,591 ----------------------------------------------------------------------------------------------------
2021-05-24 22:58:47,912 epoch 22 - iter 12/127 - loss 0.24108089 - samples/sec: 57.84 - lr: 0.025000
2021-05-24 22:58:51,289 epoch 22 - iter 24/127 - loss 0.19006982 - samples/sec: 56.87 - lr: 0.025000
2021-05-24 22:58:54,619 epoch 22 - iter 36/127 - loss 0.18117469 - samples/sec: 57.68 - lr: 0.025000
2021-05-24 22:58:57,971 epoch 22 - iter 48/127 - loss 0.20825860 - samples/sec: 57.29 - lr: 0.025000
2021-05-24 22:59:01,340 epoch 22 - iter 60/127 - loss 0.22450526 - samples/sec: 57.01 - lr: 0.025000
2021-05-24 22:59:04,695 epoch 22 - iter 72/127 - loss 0.20783092 - samples/sec: 57.23 - lr: 0.025000
2021-05-24 22:59:08,059 epoch 22 - iter 84/127 - loss 0.20058822 - samples/sec: 57.10 - lr: 0.025000
2021-05-24 22:59:11,390 epoch 22 - iter 96/127 - loss 0.20073992 - samples/sec: 57.65 - lr: 0.025000
2021-05-24 22:59:14,733 epoch 22 - iter 108/127 - loss 0.19471490 - samples/sec: 57.45 - lr: 0.025000
2021-05-24 22:59:18,048 epoch 22 - iter 120/127 - loss 0.19201611 - samples/sec: 57.93 - lr: 0.025000
2021-05-24 22:59:19,992 ----------------------------------------------------------------------------------------------------
2021-05-24 22:59:19,992 EPOCH 22 done: loss 0.1873 - lr 0.0250000
2021-05-24 22:59:21,681 DEV : loss 0.16515566408634186 - score 0.9847
2021-05-24 22:59:21,710 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 22:59:31,491 ----------------------------------------------------------------------------------------------------
2021-05-24 22:59:34,841 epoch 23 - iter 12/127 - loss 0.16041935 - samples/sec: 57.35 - lr: 0.025000
2021-05-24 22:59:38,248 epoch 23 - iter 24/127 - loss 0.14433752 - samples/sec: 56.35 - lr: 0.025000
2021-05-24 22:59:41,594 epoch 23 - iter 36/127 - loss 0.15763845 - samples/sec: 57.40 - lr: 0.025000
2021-05-24 22:59:44,961 epoch 23 - iter 48/127 - loss 0.14669659 - samples/sec: 57.03 - lr: 0.025000
2021-05-24 22:59:48,282 epoch 23 - iter 60/127 - loss 0.15261897 - samples/sec: 57.83 - lr: 0.025000
2021-05-24 22:59:51,640 epoch 23 - iter 72/127 - loss 0.15719277 - samples/sec: 57.19 - lr: 0.025000
2021-05-24 22:59:54,969 epoch 23 - iter 84/127 - loss 0.16429830 - samples/sec: 57.67 - lr: 0.025000
2021-05-24 22:59:58,339 epoch 23 - iter 96/127 - loss 0.15893373 - samples/sec: 56.99 - lr: 0.025000
2021-05-24 23:00:01,686 epoch 23 - iter 108/127 - loss 0.16137388 - samples/sec: 57.38 - lr: 0.025000
2021-05-24 23:00:05,071 epoch 23 - iter 120/127 - loss 0.16480101 - samples/sec: 56.73 - lr: 0.025000
2021-05-24 23:00:07,030 ----------------------------------------------------------------------------------------------------
2021-05-24 23:00:07,031 EPOCH 23 done: loss 0.1661 - lr 0.0250000
2021-05-24 23:00:08,719 DEV : loss 0.1522686779499054 - score 0.9847
2021-05-24 23:00:08,748 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:00:18,429 ----------------------------------------------------------------------------------------------------
2021-05-24 23:00:21,792 epoch 24 - iter 12/127 - loss 0.15389255 - samples/sec: 57.13 - lr: 0.025000
2021-05-24 23:00:25,170 epoch 24 - iter 24/127 - loss 0.17303989 - samples/sec: 56.84 - lr: 0.025000
2021-05-24 23:00:28,551 epoch 24 - iter 36/127 - loss 0.16367494 - samples/sec: 56.81 - lr: 0.025000
2021-05-24 23:00:31,932 epoch 24 - iter 48/127 - loss 0.17366596 - samples/sec: 56.80 - lr: 0.025000
2021-05-24 23:00:35,299 epoch 24 - iter 60/127 - loss 0.19030198 - samples/sec: 57.04 - lr: 0.025000
2021-05-24 23:00:38,670 epoch 24 - iter 72/127 - loss 0.18045125 - samples/sec: 56.96 - lr: 0.025000
2021-05-24 23:00:42,024 epoch 24 - iter 84/127 - loss 0.18231307 - samples/sec: 57.27 - lr: 0.025000
2021-05-24 23:00:45,378 epoch 24 - iter 96/127 - loss 0.18689857 - samples/sec: 57.24 - lr: 0.025000
2021-05-24 23:00:48,714 epoch 24 - iter 108/127 - loss 0.17937464 - samples/sec: 57.58 - lr: 0.025000
2021-05-24 23:00:52,101 epoch 24 - iter 120/127 - loss 0.17298276 - samples/sec: 56.69 - lr: 0.025000
2021-05-24 23:00:54,060 ----------------------------------------------------------------------------------------------------
2021-05-24 23:00:54,060 EPOCH 24 done: loss 0.1884 - lr 0.0250000
2021-05-24 23:00:55,754 DEV : loss 0.1571013480424881 - score 0.9805
2021-05-24 23:00:55,783 BAD EPOCHS (no improvement): 1
2021-05-24 23:00:55,783 ----------------------------------------------------------------------------------------------------
2021-05-24 23:00:59,139 epoch 25 - iter 12/127 - loss 0.09342580 - samples/sec: 57.24 - lr: 0.025000
2021-05-24 23:01:02,487 epoch 25 - iter 24/127 - loss 0.18176684 - samples/sec: 57.36 - lr: 0.025000
2021-05-24 23:01:05,826 epoch 25 - iter 36/127 - loss 0.17085913 - samples/sec: 57.51 - lr: 0.025000
2021-05-24 23:01:09,140 epoch 25 - iter 48/127 - loss 0.15548470 - samples/sec: 57.95 - lr: 0.025000
2021-05-24 23:01:12,521 epoch 25 - iter 60/127 - loss 0.15708862 - samples/sec: 56.80 - lr: 0.025000
2021-05-24 23:01:15,877 epoch 25 - iter 72/127 - loss 0.16125219 - samples/sec: 57.22 - lr: 0.025000
2021-05-24 23:01:19,269 epoch 25 - iter 84/127 - loss 0.17259798 - samples/sec: 56.62 - lr: 0.025000
2021-05-24 23:01:22,673 epoch 25 - iter 96/127 - loss 0.18494090 - samples/sec: 56.42 - lr: 0.025000
2021-05-24 23:01:26,064 epoch 25 - iter 108/127 - loss 0.18863894 - samples/sec: 56.63 - lr: 0.025000
2021-05-24 23:01:29,424 epoch 25 - iter 120/127 - loss 0.18559843 - samples/sec: 57.16 - lr: 0.025000
2021-05-24 23:01:31,394 ----------------------------------------------------------------------------------------------------
2021-05-24 23:01:31,394 EPOCH 25 done: loss 0.1815 - lr 0.0250000
2021-05-24 23:01:33,090 DEV : loss 0.17072005569934845 - score 0.9825
2021-05-24 23:01:33,119 BAD EPOCHS (no improvement): 2
2021-05-24 23:01:33,120 ----------------------------------------------------------------------------------------------------
2021-05-24 23:01:36,532 epoch 26 - iter 12/127 - loss 0.11730943 - samples/sec: 56.28 - lr: 0.025000
2021-05-24 23:01:39,910 epoch 26 - iter 24/127 - loss 0.12649731 - samples/sec: 56.86 - lr: 0.025000
2021-05-24 23:01:43,303 epoch 26 - iter 36/127 - loss 0.13994348 - samples/sec: 56.59 - lr: 0.025000
2021-05-24 23:01:46,668 epoch 26 - iter 48/127 - loss 0.14811077 - samples/sec: 57.08 - lr: 0.025000
2021-05-24 23:01:50,002 epoch 26 - iter 60/127 - loss 0.16285483 - samples/sec: 57.59 - lr: 0.025000
2021-05-24 23:01:53,388 epoch 26 - iter 72/127 - loss 0.16511816 - samples/sec: 56.72 - lr: 0.025000
2021-05-24 23:01:56,771 epoch 26 - iter 84/127 - loss 0.17285640 - samples/sec: 56.77 - lr: 0.025000
2021-05-24 23:02:00,104 epoch 26 - iter 96/127 - loss 0.16947161 - samples/sec: 57.61 - lr: 0.025000
2021-05-24 23:02:03,477 epoch 26 - iter 108/127 - loss 0.16267969 - samples/sec: 56.93 - lr: 0.025000
2021-05-24 23:02:06,822 epoch 26 - iter 120/127 - loss 0.16527207 - samples/sec: 57.41 - lr: 0.025000
2021-05-24 23:02:08,781 ----------------------------------------------------------------------------------------------------
2021-05-24 23:02:08,781 EPOCH 26 done: loss 0.1657 - lr 0.0250000
2021-05-24 23:02:10,695 DEV : loss 0.1641310453414917 - score 0.9826
2021-05-24 23:02:10,723 BAD EPOCHS (no improvement): 3
2021-05-24 23:02:10,724 ----------------------------------------------------------------------------------------------------
2021-05-24 23:02:14,053 epoch 27 - iter 12/127 - loss 0.17000679 - samples/sec: 57.68 - lr: 0.025000
2021-05-24 23:02:17,399 epoch 27 - iter 24/127 - loss 0.16748282 - samples/sec: 57.39 - lr: 0.025000
2021-05-24 23:02:20,763 epoch 27 - iter 36/127 - loss 0.20852710 - samples/sec: 57.08 - lr: 0.025000
2021-05-24 23:02:24,114 epoch 27 - iter 48/127 - loss 0.17426815 - samples/sec: 57.31 - lr: 0.025000
2021-05-24 23:02:27,473 epoch 27 - iter 60/127 - loss 0.17481434 - samples/sec: 57.18 - lr: 0.025000
2021-05-24 23:02:30,881 epoch 27 - iter 72/127 - loss 0.18327889 - samples/sec: 56.35 - lr: 0.025000
2021-05-24 23:02:34,245 epoch 27 - iter 84/127 - loss 0.16820370 - samples/sec: 57.08 - lr: 0.025000
2021-05-24 23:02:37,601 epoch 27 - iter 96/127 - loss 0.16389485 - samples/sec: 57.23 - lr: 0.025000
2021-05-24 23:02:41,007 epoch 27 - iter 108/127 - loss 0.16668680 - samples/sec: 56.37 - lr: 0.025000
2021-05-24 23:02:44,373 epoch 27 - iter 120/127 - loss 0.16459602 - samples/sec: 57.06 - lr: 0.025000
2021-05-24 23:02:46,322 ----------------------------------------------------------------------------------------------------
2021-05-24 23:02:46,322 EPOCH 27 done: loss 0.1741 - lr 0.0250000
2021-05-24 23:02:48,015 DEV : loss 0.14926882088184357 - score 0.9847
2021-05-24 23:02:48,045 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:02:57,747 ----------------------------------------------------------------------------------------------------
2021-05-24 23:03:01,104 epoch 28 - iter 12/127 - loss 0.15934161 - samples/sec: 57.21 - lr: 0.025000
2021-05-24 23:03:04,495 epoch 28 - iter 24/127 - loss 0.19743740 - samples/sec: 56.63 - lr: 0.025000
2021-05-24 23:03:07,904 epoch 28 - iter 36/127 - loss 0.21604952 - samples/sec: 56.34 - lr: 0.025000
2021-05-24 23:03:11,273 epoch 28 - iter 48/127 - loss 0.20778931 - samples/sec: 57.00 - lr: 0.025000
2021-05-24 23:03:14,664 epoch 28 - iter 60/127 - loss 0.18828243 - samples/sec: 56.62 - lr: 0.025000
2021-05-24 23:03:18,013 epoch 28 - iter 72/127 - loss 0.19199635 - samples/sec: 57.34 - lr: 0.025000
2021-05-24 23:03:21,400 epoch 28 - iter 84/127 - loss 0.20125778 - samples/sec: 56.70 - lr: 0.025000
2021-05-24 23:03:24,778 epoch 28 - iter 96/127 - loss 0.19436621 - samples/sec: 56.84 - lr: 0.025000
2021-05-24 23:03:28,130 epoch 28 - iter 108/127 - loss 0.18678440 - samples/sec: 57.29 - lr: 0.025000
2021-05-24 23:03:31,496 epoch 28 - iter 120/127 - loss 0.18752345 - samples/sec: 57.05 - lr: 0.025000
2021-05-24 23:03:33,447 ----------------------------------------------------------------------------------------------------
2021-05-24 23:03:33,448 EPOCH 28 done: loss 0.1814 - lr 0.0250000
2021-05-24 23:03:35,140 DEV : loss 0.15442392230033875 - score 0.9847
2021-05-24 23:03:35,170 BAD EPOCHS (no improvement): 1
2021-05-24 23:03:35,170 ----------------------------------------------------------------------------------------------------
2021-05-24 23:03:38,546 epoch 29 - iter 12/127 - loss 0.23784417 - samples/sec: 56.88 - lr: 0.025000
2021-05-24 23:03:41,931 epoch 29 - iter 24/127 - loss 0.19497448 - samples/sec: 56.74 - lr: 0.025000
2021-05-24 23:03:45,244 epoch 29 - iter 36/127 - loss 0.18453729 - samples/sec: 57.96 - lr: 0.025000
2021-05-24 23:03:48,619 epoch 29 - iter 48/127 - loss 0.16536229 - samples/sec: 56.89 - lr: 0.025000
2021-05-24 23:03:51,975 epoch 29 - iter 60/127 - loss 0.18928877 - samples/sec: 57.23 - lr: 0.025000
2021-05-24 23:03:55,363 epoch 29 - iter 72/127 - loss 0.17269671 - samples/sec: 56.69 - lr: 0.025000
2021-05-24 23:03:58,751 epoch 29 - iter 84/127 - loss 0.17783672 - samples/sec: 56.68 - lr: 0.025000
2021-05-24 23:04:02,139 epoch 29 - iter 96/127 - loss 0.17165721 - samples/sec: 56.68 - lr: 0.025000
2021-05-24 23:04:05,497 epoch 29 - iter 108/127 - loss 0.16529336 - samples/sec: 57.19 - lr: 0.025000
2021-05-24 23:04:08,892 epoch 29 - iter 120/127 - loss 0.16280328 - samples/sec: 56.56 - lr: 0.025000
2021-05-24 23:04:10,821 ----------------------------------------------------------------------------------------------------
2021-05-24 23:04:10,822 EPOCH 29 done: loss 0.1614 - lr 0.0250000
2021-05-24 23:04:12,513 DEV : loss 0.15118691325187683 - score 0.9847
2021-05-24 23:04:12,543 BAD EPOCHS (no improvement): 2
2021-05-24 23:04:12,543 ----------------------------------------------------------------------------------------------------
2021-05-24 23:04:15,924 epoch 30 - iter 12/127 - loss 0.21388563 - samples/sec: 56.79 - lr: 0.025000
2021-05-24 23:04:19,321 epoch 30 - iter 24/127 - loss 0.20136195 - samples/sec: 56.54 - lr: 0.025000
2021-05-24 23:04:22,708 epoch 30 - iter 36/127 - loss 0.17796703 - samples/sec: 56.69 - lr: 0.025000
2021-05-24 23:04:26,035 epoch 30 - iter 48/127 - loss 0.17210703 - samples/sec: 57.72 - lr: 0.025000
2021-05-24 23:04:29,417 epoch 30 - iter 60/127 - loss 0.16166459 - samples/sec: 56.78 - lr: 0.025000
2021-05-24 23:04:32,752 epoch 30 - iter 72/127 - loss 0.16053687 - samples/sec: 57.58 - lr: 0.025000
2021-05-24 23:04:36,105 epoch 30 - iter 84/127 - loss 0.17784869 - samples/sec: 57.28 - lr: 0.025000
2021-05-24 23:04:39,481 epoch 30 - iter 96/127 - loss 0.16676370 - samples/sec: 56.89 - lr: 0.025000
2021-05-24 23:04:42,783 epoch 30 - iter 108/127 - loss 0.17329194 - samples/sec: 58.15 - lr: 0.025000
2021-05-24 23:04:46,151 epoch 30 - iter 120/127 - loss 0.17689541 - samples/sec: 57.02 - lr: 0.025000
2021-05-24 23:04:48,080 ----------------------------------------------------------------------------------------------------
2021-05-24 23:04:48,080 EPOCH 30 done: loss 0.1785 - lr 0.0250000
2021-05-24 23:04:49,774 DEV : loss 0.14827115833759308 - score 0.9847
2021-05-24 23:04:49,802 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:05:00,741 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:00,741 Testing using best model ...
2021-05-24 23:05:00,742 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/por.rst.cstn/best-model.pt
2021-05-24 23:05:07,256 0.9840	1.0000	0.9919
2021-05-24 23:05:07,256 
Results:
- F1-score (micro) 0.9919
- F1-score (macro) 0.9919

By class:
SENT       tp: 123 - fp: 2 - fn: 0 - precision: 0.9840 - recall: 1.0000 - f1-score: 0.9919
2021-05-24 23:05:07,257 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb/
2021-05-24 23:05:07,285 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb
2021-05-24 23:05:07,285 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb/sent_train.txt
2021-05-24 23:05:07,287 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb/sent_dev.txt
2021-05-24 23:05:07,290 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb/sent_test.txt
Corpus: 18701 train + 2362 dev + 2245 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-24 23:05:25,235 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,242 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-24 23:05:25,242 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,243 Corpus: "Corpus: 18701 train + 2362 dev + 2245 test sentences"
2021-05-24 23:05:25,243 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,243 Parameters:
2021-05-24 23:05:25,243  - learning_rate: "0.1"
2021-05-24 23:05:25,243  - mini_batch_size: "16"
2021-05-24 23:05:25,243  - patience: "3"
2021-05-24 23:05:25,243  - anneal_factor: "0.5"
2021-05-24 23:05:25,243  - max_epochs: "30"
2021-05-24 23:05:25,243  - shuffle: "True"
2021-05-24 23:05:25,243  - train_with_dev: "False"
2021-05-24 23:05:25,243  - batch_growth_annealing: "False"
2021-05-24 23:05:25,243 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,243 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb"
2021-05-24 23:05:25,243 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,244 Device: cuda:0
2021-05-24 23:05:25,244 ----------------------------------------------------------------------------------------------------
2021-05-24 23:05:25,244 Embeddings storage mode: cpu
2021-05-24 23:05:25,247 ----------------------------------------------------------------------------------------------------
2021-05-24 23:07:00,799 epoch 1 - iter 116/1169 - loss 5.57988343 - samples/sec: 19.43 - lr: 0.100000
2021-05-24 23:08:34,002 epoch 1 - iter 232/1169 - loss 3.94390051 - samples/sec: 19.91 - lr: 0.100000
2021-05-24 23:10:12,688 epoch 1 - iter 348/1169 - loss 3.24612196 - samples/sec: 18.81 - lr: 0.100000
2021-05-24 23:11:51,430 epoch 1 - iter 464/1169 - loss 2.84353835 - samples/sec: 18.80 - lr: 0.100000
2021-05-24 23:13:31,061 epoch 1 - iter 580/1169 - loss 2.58095937 - samples/sec: 18.63 - lr: 0.100000
2021-05-24 23:15:09,835 epoch 1 - iter 696/1169 - loss 2.38380659 - samples/sec: 18.79 - lr: 0.100000
2021-05-24 23:16:48,668 epoch 1 - iter 812/1169 - loss 2.22340563 - samples/sec: 18.78 - lr: 0.100000
2021-05-24 23:18:27,354 epoch 1 - iter 928/1169 - loss 2.10360562 - samples/sec: 18.81 - lr: 0.100000
2021-05-24 23:20:05,956 epoch 1 - iter 1044/1169 - loss 2.01761439 - samples/sec: 18.82 - lr: 0.100000
2021-05-24 23:21:44,180 epoch 1 - iter 1160/1169 - loss 1.92527913 - samples/sec: 18.90 - lr: 0.100000
2021-05-24 23:21:51,776 ----------------------------------------------------------------------------------------------------
2021-05-24 23:21:51,776 EPOCH 1 done: loss 1.9180 - lr 0.1000000
2021-05-24 23:23:11,081 DEV : loss 0.7313351035118103 - score 0.902
2021-05-24 23:23:11,325 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:23:12,694 ----------------------------------------------------------------------------------------------------
2021-05-24 23:23:47,857 epoch 2 - iter 116/1169 - loss 1.15107447 - samples/sec: 52.79 - lr: 0.100000
2021-05-24 23:24:21,318 epoch 2 - iter 232/1169 - loss 1.09216416 - samples/sec: 55.48 - lr: 0.100000
2021-05-24 23:24:54,923 epoch 2 - iter 348/1169 - loss 1.06113762 - samples/sec: 55.24 - lr: 0.100000
2021-05-24 23:25:28,396 epoch 2 - iter 464/1169 - loss 1.06945810 - samples/sec: 55.45 - lr: 0.100000
2021-05-24 23:26:01,865 epoch 2 - iter 580/1169 - loss 1.04958790 - samples/sec: 55.46 - lr: 0.100000
2021-05-24 23:26:35,267 epoch 2 - iter 696/1169 - loss 1.03605352 - samples/sec: 55.57 - lr: 0.100000
2021-05-24 23:27:08,830 epoch 2 - iter 812/1169 - loss 1.02110448 - samples/sec: 55.31 - lr: 0.100000
2021-05-24 23:27:42,325 epoch 2 - iter 928/1169 - loss 1.01415113 - samples/sec: 55.42 - lr: 0.100000
2021-05-24 23:28:15,663 epoch 2 - iter 1044/1169 - loss 0.98733707 - samples/sec: 55.68 - lr: 0.100000
2021-05-24 23:28:48,849 epoch 2 - iter 1160/1169 - loss 0.97751191 - samples/sec: 55.93 - lr: 0.100000
2021-05-24 23:28:51,388 ----------------------------------------------------------------------------------------------------
2021-05-24 23:28:51,389 EPOCH 2 done: loss 0.9780 - lr 0.1000000
2021-05-24 23:29:06,128 DEV : loss 0.5216267704963684 - score 0.9372
2021-05-24 23:29:06,374 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:29:16,087 ----------------------------------------------------------------------------------------------------
2021-05-24 23:29:49,518 epoch 3 - iter 116/1169 - loss 0.86744679 - samples/sec: 55.53 - lr: 0.100000
2021-05-24 23:30:22,889 epoch 3 - iter 232/1169 - loss 0.85566311 - samples/sec: 55.62 - lr: 0.100000
2021-05-24 23:30:56,149 epoch 3 - iter 348/1169 - loss 0.84285189 - samples/sec: 55.81 - lr: 0.100000
2021-05-24 23:31:29,630 epoch 3 - iter 464/1169 - loss 0.82210330 - samples/sec: 55.44 - lr: 0.100000
2021-05-24 23:32:03,011 epoch 3 - iter 580/1169 - loss 0.81375290 - samples/sec: 55.61 - lr: 0.100000
2021-05-24 23:32:36,503 epoch 3 - iter 696/1169 - loss 0.82403528 - samples/sec: 55.42 - lr: 0.100000
2021-05-24 23:33:09,885 epoch 3 - iter 812/1169 - loss 0.82900991 - samples/sec: 55.60 - lr: 0.100000
2021-05-24 23:33:43,281 epoch 3 - iter 928/1169 - loss 0.82313772 - samples/sec: 55.58 - lr: 0.100000
2021-05-24 23:34:16,693 epoch 3 - iter 1044/1169 - loss 0.81375662 - samples/sec: 55.56 - lr: 0.100000
2021-05-24 23:34:50,032 epoch 3 - iter 1160/1169 - loss 0.81590427 - samples/sec: 55.68 - lr: 0.100000
2021-05-24 23:34:52,608 ----------------------------------------------------------------------------------------------------
2021-05-24 23:34:52,608 EPOCH 3 done: loss 0.8150 - lr 0.1000000
2021-05-24 23:35:07,278 DEV : loss 0.488858163356781 - score 0.9348
2021-05-24 23:35:07,522 BAD EPOCHS (no improvement): 1
2021-05-24 23:35:07,522 ----------------------------------------------------------------------------------------------------
2021-05-24 23:35:41,087 epoch 4 - iter 116/1169 - loss 0.81167353 - samples/sec: 55.30 - lr: 0.100000
2021-05-24 23:36:14,489 epoch 4 - iter 232/1169 - loss 0.74028397 - samples/sec: 55.57 - lr: 0.100000
2021-05-24 23:36:47,739 epoch 4 - iter 348/1169 - loss 0.72358331 - samples/sec: 55.83 - lr: 0.100000
2021-05-24 23:37:21,275 epoch 4 - iter 464/1169 - loss 0.75380749 - samples/sec: 55.35 - lr: 0.100000
2021-05-24 23:37:54,926 epoch 4 - iter 580/1169 - loss 0.73512523 - samples/sec: 55.16 - lr: 0.100000
2021-05-24 23:38:28,471 epoch 4 - iter 696/1169 - loss 0.74313148 - samples/sec: 55.34 - lr: 0.100000
2021-05-24 23:39:01,762 epoch 4 - iter 812/1169 - loss 0.74934037 - samples/sec: 55.76 - lr: 0.100000
2021-05-24 23:39:35,410 epoch 4 - iter 928/1169 - loss 0.75066107 - samples/sec: 55.17 - lr: 0.100000
2021-05-24 23:40:08,997 epoch 4 - iter 1044/1169 - loss 0.75117924 - samples/sec: 55.27 - lr: 0.100000
2021-05-24 23:40:42,473 epoch 4 - iter 1160/1169 - loss 0.75663259 - samples/sec: 55.45 - lr: 0.100000
2021-05-24 23:40:45,026 ----------------------------------------------------------------------------------------------------
2021-05-24 23:40:45,026 EPOCH 4 done: loss 0.7545 - lr 0.1000000
2021-05-24 23:41:01,316 DEV : loss 0.4753601551055908 - score 0.9429
2021-05-24 23:41:01,563 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:41:10,964 ----------------------------------------------------------------------------------------------------
2021-05-24 23:41:44,698 epoch 5 - iter 116/1169 - loss 0.69455579 - samples/sec: 55.03 - lr: 0.100000
2021-05-24 23:42:18,431 epoch 5 - iter 232/1169 - loss 0.67752877 - samples/sec: 55.03 - lr: 0.100000
2021-05-24 23:42:52,101 epoch 5 - iter 348/1169 - loss 0.72738274 - samples/sec: 55.13 - lr: 0.100000
2021-05-24 23:43:25,718 epoch 5 - iter 464/1169 - loss 0.73994245 - samples/sec: 55.22 - lr: 0.100000
2021-05-24 23:43:59,235 epoch 5 - iter 580/1169 - loss 0.73814970 - samples/sec: 55.38 - lr: 0.100000
2021-05-24 23:44:32,670 epoch 5 - iter 696/1169 - loss 0.73124015 - samples/sec: 55.52 - lr: 0.100000
2021-05-24 23:45:06,381 epoch 5 - iter 812/1169 - loss 0.72829743 - samples/sec: 55.06 - lr: 0.100000
2021-05-24 23:45:40,209 epoch 5 - iter 928/1169 - loss 0.72218564 - samples/sec: 54.87 - lr: 0.100000
2021-05-24 23:46:14,009 epoch 5 - iter 1044/1169 - loss 0.71658456 - samples/sec: 54.92 - lr: 0.100000
2021-05-24 23:46:47,843 epoch 5 - iter 1160/1169 - loss 0.71201278 - samples/sec: 54.86 - lr: 0.100000
2021-05-24 23:46:50,429 ----------------------------------------------------------------------------------------------------
2021-05-24 23:46:50,429 EPOCH 5 done: loss 0.7110 - lr 0.1000000
2021-05-24 23:47:05,187 DEV : loss 0.47373923659324646 - score 0.943
2021-05-24 23:47:05,434 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:47:14,783 ----------------------------------------------------------------------------------------------------
2021-05-24 23:47:48,541 epoch 6 - iter 116/1169 - loss 0.65527331 - samples/sec: 54.99 - lr: 0.100000
2021-05-24 23:48:22,256 epoch 6 - iter 232/1169 - loss 0.62079875 - samples/sec: 55.06 - lr: 0.100000
2021-05-24 23:48:55,876 epoch 6 - iter 348/1169 - loss 0.64436040 - samples/sec: 55.21 - lr: 0.100000
2021-05-24 23:49:29,581 epoch 6 - iter 464/1169 - loss 0.64930282 - samples/sec: 55.07 - lr: 0.100000
2021-05-24 23:50:03,136 epoch 6 - iter 580/1169 - loss 0.65404002 - samples/sec: 55.32 - lr: 0.100000
2021-05-24 23:50:36,689 epoch 6 - iter 696/1169 - loss 0.65023515 - samples/sec: 55.32 - lr: 0.100000
2021-05-24 23:51:10,472 epoch 6 - iter 812/1169 - loss 0.65604925 - samples/sec: 54.94 - lr: 0.100000
2021-05-24 23:51:44,248 epoch 6 - iter 928/1169 - loss 0.67209813 - samples/sec: 54.96 - lr: 0.100000
2021-05-24 23:52:18,293 epoch 6 - iter 1044/1169 - loss 0.66148735 - samples/sec: 54.52 - lr: 0.100000
2021-05-24 23:52:52,220 epoch 6 - iter 1160/1169 - loss 0.65670366 - samples/sec: 54.71 - lr: 0.100000
2021-05-24 23:52:54,820 ----------------------------------------------------------------------------------------------------
2021-05-24 23:52:54,820 EPOCH 6 done: loss 0.6563 - lr 0.1000000
2021-05-24 23:53:09,723 DEV : loss 0.5003846883773804 - score 0.9465
2021-05-24 23:53:09,967 BAD EPOCHS (no improvement): 0
saving best model
2021-05-24 23:53:19,630 ----------------------------------------------------------------------------------------------------
2021-05-24 23:53:53,517 epoch 7 - iter 116/1169 - loss 0.68030554 - samples/sec: 54.78 - lr: 0.100000
2021-05-24 23:54:27,317 epoch 7 - iter 232/1169 - loss 0.67169333 - samples/sec: 54.92 - lr: 0.100000
2021-05-24 23:55:00,860 epoch 7 - iter 348/1169 - loss 0.65588097 - samples/sec: 55.34 - lr: 0.100000
2021-05-24 23:55:34,441 epoch 7 - iter 464/1169 - loss 0.65819545 - samples/sec: 55.28 - lr: 0.100000
2021-05-24 23:56:07,838 epoch 7 - iter 580/1169 - loss 0.64982627 - samples/sec: 55.58 - lr: 0.100000
2021-05-24 23:56:41,681 epoch 7 - iter 696/1169 - loss 0.66272910 - samples/sec: 54.85 - lr: 0.100000
2021-05-24 23:57:15,476 epoch 7 - iter 812/1169 - loss 0.67797656 - samples/sec: 54.93 - lr: 0.100000
2021-05-24 23:57:49,125 epoch 7 - iter 928/1169 - loss 0.68458134 - samples/sec: 55.16 - lr: 0.100000
2021-05-24 23:58:22,849 epoch 7 - iter 1044/1169 - loss 0.68010128 - samples/sec: 55.04 - lr: 0.100000
2021-05-24 23:58:56,444 epoch 7 - iter 1160/1169 - loss 0.67203847 - samples/sec: 55.25 - lr: 0.100000
2021-05-24 23:58:59,021 ----------------------------------------------------------------------------------------------------
2021-05-24 23:58:59,021 EPOCH 7 done: loss 0.6725 - lr 0.1000000
2021-05-24 23:59:13,942 DEV : loss 0.5094473958015442 - score 0.9427
2021-05-24 23:59:14,186 BAD EPOCHS (no improvement): 1
2021-05-24 23:59:14,186 ----------------------------------------------------------------------------------------------------
2021-05-24 23:59:47,718 epoch 8 - iter 116/1169 - loss 0.60399826 - samples/sec: 55.36 - lr: 0.100000
2021-05-25 00:00:21,365 epoch 8 - iter 232/1169 - loss 0.62691777 - samples/sec: 55.17 - lr: 0.100000
2021-05-25 00:00:54,998 epoch 8 - iter 348/1169 - loss 0.62733571 - samples/sec: 55.19 - lr: 0.100000
2021-05-25 00:01:28,528 epoch 8 - iter 464/1169 - loss 0.63271315 - samples/sec: 55.36 - lr: 0.100000
2021-05-25 00:02:01,924 epoch 8 - iter 580/1169 - loss 0.62250739 - samples/sec: 55.58 - lr: 0.100000
2021-05-25 00:02:35,312 epoch 8 - iter 696/1169 - loss 0.61696311 - samples/sec: 55.59 - lr: 0.100000
2021-05-25 00:03:08,883 epoch 8 - iter 812/1169 - loss 0.61310650 - samples/sec: 55.29 - lr: 0.100000
2021-05-25 00:03:42,622 epoch 8 - iter 928/1169 - loss 0.61740461 - samples/sec: 55.02 - lr: 0.100000
2021-05-25 00:04:16,307 epoch 8 - iter 1044/1169 - loss 0.60837694 - samples/sec: 55.11 - lr: 0.100000
2021-05-25 00:04:49,822 epoch 8 - iter 1160/1169 - loss 0.60203864 - samples/sec: 55.38 - lr: 0.100000
2021-05-25 00:04:52,372 ----------------------------------------------------------------------------------------------------
2021-05-25 00:04:52,372 EPOCH 8 done: loss 0.6024 - lr 0.1000000
2021-05-25 00:05:07,315 DEV : loss 0.4790433943271637 - score 0.9384
2021-05-25 00:05:07,559 BAD EPOCHS (no improvement): 2
2021-05-25 00:05:07,560 ----------------------------------------------------------------------------------------------------
2021-05-25 00:05:41,046 epoch 9 - iter 116/1169 - loss 0.53139438 - samples/sec: 55.43 - lr: 0.100000
2021-05-25 00:06:14,450 epoch 9 - iter 232/1169 - loss 0.54144514 - samples/sec: 55.57 - lr: 0.100000
2021-05-25 00:06:48,010 epoch 9 - iter 348/1169 - loss 0.56416407 - samples/sec: 55.31 - lr: 0.100000
2021-05-25 00:07:21,543 epoch 9 - iter 464/1169 - loss 0.57323151 - samples/sec: 55.36 - lr: 0.100000
2021-05-25 00:07:55,048 epoch 9 - iter 580/1169 - loss 0.57636449 - samples/sec: 55.40 - lr: 0.100000
2021-05-25 00:08:28,453 epoch 9 - iter 696/1169 - loss 0.58408086 - samples/sec: 55.57 - lr: 0.100000
2021-05-25 00:09:03,670 epoch 9 - iter 812/1169 - loss 0.58457072 - samples/sec: 52.71 - lr: 0.100000
2021-05-25 00:09:37,400 epoch 9 - iter 928/1169 - loss 0.58646090 - samples/sec: 55.03 - lr: 0.100000
2021-05-25 00:10:11,185 epoch 9 - iter 1044/1169 - loss 0.60125067 - samples/sec: 54.94 - lr: 0.100000
2021-05-25 00:10:44,931 epoch 9 - iter 1160/1169 - loss 0.60014699 - samples/sec: 55.01 - lr: 0.100000
2021-05-25 00:10:47,515 ----------------------------------------------------------------------------------------------------
2021-05-25 00:10:47,515 EPOCH 9 done: loss 0.6005 - lr 0.1000000
2021-05-25 00:11:02,421 DEV : loss 0.4598163366317749 - score 0.9446
2021-05-25 00:11:02,666 BAD EPOCHS (no improvement): 3
2021-05-25 00:11:02,666 ----------------------------------------------------------------------------------------------------
2021-05-25 00:11:36,248 epoch 10 - iter 116/1169 - loss 0.64493732 - samples/sec: 55.27 - lr: 0.100000
2021-05-25 00:12:10,038 epoch 10 - iter 232/1169 - loss 0.62548718 - samples/sec: 54.93 - lr: 0.100000
2021-05-25 00:12:43,877 epoch 10 - iter 348/1169 - loss 0.59718720 - samples/sec: 54.85 - lr: 0.100000
2021-05-25 00:13:17,714 epoch 10 - iter 464/1169 - loss 0.58581063 - samples/sec: 54.86 - lr: 0.100000
2021-05-25 00:13:51,366 epoch 10 - iter 580/1169 - loss 0.57731105 - samples/sec: 55.16 - lr: 0.100000
2021-05-25 00:14:25,014 epoch 10 - iter 696/1169 - loss 0.57801923 - samples/sec: 55.17 - lr: 0.100000
2021-05-25 00:14:58,492 epoch 10 - iter 812/1169 - loss 0.57420896 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 00:15:31,974 epoch 10 - iter 928/1169 - loss 0.57432125 - samples/sec: 55.44 - lr: 0.100000
2021-05-25 00:16:05,419 epoch 10 - iter 1044/1169 - loss 0.57594831 - samples/sec: 55.50 - lr: 0.100000
2021-05-25 00:16:38,839 epoch 10 - iter 1160/1169 - loss 0.56996470 - samples/sec: 55.54 - lr: 0.100000
2021-05-25 00:16:41,380 ----------------------------------------------------------------------------------------------------
2021-05-25 00:16:41,381 EPOCH 10 done: loss 0.5694 - lr 0.1000000
2021-05-25 00:16:56,308 DEV : loss 0.4006001353263855 - score 0.9498
2021-05-25 00:16:56,554 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 00:17:06,341 ----------------------------------------------------------------------------------------------------
2021-05-25 00:17:40,027 epoch 11 - iter 116/1169 - loss 0.54620100 - samples/sec: 55.11 - lr: 0.100000
2021-05-25 00:18:13,709 epoch 11 - iter 232/1169 - loss 0.54715467 - samples/sec: 55.11 - lr: 0.100000
2021-05-25 00:18:47,373 epoch 11 - iter 348/1169 - loss 0.56098034 - samples/sec: 55.14 - lr: 0.100000
2021-05-25 00:19:21,111 epoch 11 - iter 464/1169 - loss 0.55652174 - samples/sec: 55.02 - lr: 0.100000
2021-05-25 00:19:54,834 epoch 11 - iter 580/1169 - loss 0.54952897 - samples/sec: 55.04 - lr: 0.100000
2021-05-25 00:20:28,572 epoch 11 - iter 696/1169 - loss 0.55053881 - samples/sec: 55.02 - lr: 0.100000
2021-05-25 00:21:02,072 epoch 11 - iter 812/1169 - loss 0.54684832 - samples/sec: 55.41 - lr: 0.100000
2021-05-25 00:21:35,548 epoch 11 - iter 928/1169 - loss 0.54701139 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 00:22:08,938 epoch 11 - iter 1044/1169 - loss 0.54938122 - samples/sec: 55.59 - lr: 0.100000
2021-05-25 00:22:42,391 epoch 11 - iter 1160/1169 - loss 0.54816939 - samples/sec: 55.49 - lr: 0.100000
2021-05-25 00:22:44,945 ----------------------------------------------------------------------------------------------------
2021-05-25 00:22:44,945 EPOCH 11 done: loss 0.5481 - lr 0.1000000
2021-05-25 00:22:59,860 DEV : loss 0.4928431808948517 - score 0.9349
2021-05-25 00:23:00,106 BAD EPOCHS (no improvement): 1
2021-05-25 00:23:00,106 ----------------------------------------------------------------------------------------------------
2021-05-25 00:23:33,535 epoch 12 - iter 116/1169 - loss 0.56877808 - samples/sec: 55.53 - lr: 0.100000
2021-05-25 00:24:06,932 epoch 12 - iter 232/1169 - loss 0.56508625 - samples/sec: 55.58 - lr: 0.100000
2021-05-25 00:24:40,287 epoch 12 - iter 348/1169 - loss 0.54905652 - samples/sec: 55.65 - lr: 0.100000
2021-05-25 00:25:14,002 epoch 12 - iter 464/1169 - loss 0.53937477 - samples/sec: 55.06 - lr: 0.100000
2021-05-25 00:25:47,734 epoch 12 - iter 580/1169 - loss 0.53425069 - samples/sec: 55.03 - lr: 0.100000
2021-05-25 00:26:21,314 epoch 12 - iter 696/1169 - loss 0.52918129 - samples/sec: 55.28 - lr: 0.100000
2021-05-25 00:26:55,027 epoch 12 - iter 812/1169 - loss 0.52694157 - samples/sec: 55.06 - lr: 0.100000
2021-05-25 00:27:28,667 epoch 12 - iter 928/1169 - loss 0.52506840 - samples/sec: 55.18 - lr: 0.100000
2021-05-25 00:28:02,249 epoch 12 - iter 1044/1169 - loss 0.52151076 - samples/sec: 55.28 - lr: 0.100000
2021-05-25 00:28:35,891 epoch 12 - iter 1160/1169 - loss 0.52260192 - samples/sec: 55.18 - lr: 0.100000
2021-05-25 00:28:38,439 ----------------------------------------------------------------------------------------------------
2021-05-25 00:28:38,439 EPOCH 12 done: loss 0.5223 - lr 0.1000000
2021-05-25 00:28:53,350 DEV : loss 0.38983213901519775 - score 0.9511
2021-05-25 00:28:53,598 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 00:29:03,312 ----------------------------------------------------------------------------------------------------
2021-05-25 00:29:37,006 epoch 13 - iter 116/1169 - loss 0.60983466 - samples/sec: 55.09 - lr: 0.100000
2021-05-25 00:30:10,428 epoch 13 - iter 232/1169 - loss 0.58938012 - samples/sec: 55.54 - lr: 0.100000
2021-05-25 00:30:43,717 epoch 13 - iter 348/1169 - loss 0.56045675 - samples/sec: 55.76 - lr: 0.100000
2021-05-25 00:31:17,018 epoch 13 - iter 464/1169 - loss 0.54792121 - samples/sec: 55.74 - lr: 0.100000
2021-05-25 00:31:50,316 epoch 13 - iter 580/1169 - loss 0.54109525 - samples/sec: 55.75 - lr: 0.100000
2021-05-25 00:32:23,670 epoch 13 - iter 696/1169 - loss 0.53794753 - samples/sec: 55.65 - lr: 0.100000
2021-05-25 00:32:57,292 epoch 13 - iter 812/1169 - loss 0.53450389 - samples/sec: 55.21 - lr: 0.100000
2021-05-25 00:33:30,914 epoch 13 - iter 928/1169 - loss 0.53203390 - samples/sec: 55.21 - lr: 0.100000
2021-05-25 00:34:04,588 epoch 13 - iter 1044/1169 - loss 0.52606433 - samples/sec: 55.12 - lr: 0.100000
2021-05-25 00:34:38,151 epoch 13 - iter 1160/1169 - loss 0.52269035 - samples/sec: 55.31 - lr: 0.100000
2021-05-25 00:34:40,736 ----------------------------------------------------------------------------------------------------
2021-05-25 00:34:40,736 EPOCH 13 done: loss 0.5221 - lr 0.1000000
2021-05-25 00:34:57,168 DEV : loss 0.3815036416053772 - score 0.9504
2021-05-25 00:34:57,415 BAD EPOCHS (no improvement): 1
2021-05-25 00:34:57,416 ----------------------------------------------------------------------------------------------------
2021-05-25 00:35:31,084 epoch 14 - iter 116/1169 - loss 0.51491909 - samples/sec: 55.13 - lr: 0.100000
2021-05-25 00:36:04,754 epoch 14 - iter 232/1169 - loss 0.51030197 - samples/sec: 55.13 - lr: 0.100000
2021-05-25 00:36:38,349 epoch 14 - iter 348/1169 - loss 0.49963158 - samples/sec: 55.25 - lr: 0.100000
2021-05-25 00:37:11,912 epoch 14 - iter 464/1169 - loss 0.49280627 - samples/sec: 55.31 - lr: 0.100000
2021-05-25 00:37:45,373 epoch 14 - iter 580/1169 - loss 0.48517632 - samples/sec: 55.47 - lr: 0.100000
2021-05-25 00:38:18,916 epoch 14 - iter 696/1169 - loss 0.49254571 - samples/sec: 55.34 - lr: 0.100000
2021-05-25 00:38:52,422 epoch 14 - iter 812/1169 - loss 0.49974190 - samples/sec: 55.40 - lr: 0.100000
2021-05-25 00:39:25,912 epoch 14 - iter 928/1169 - loss 0.49524077 - samples/sec: 55.43 - lr: 0.100000
2021-05-25 00:39:59,273 epoch 14 - iter 1044/1169 - loss 0.49896741 - samples/sec: 55.64 - lr: 0.100000
2021-05-25 00:40:32,743 epoch 14 - iter 1160/1169 - loss 0.50044948 - samples/sec: 55.46 - lr: 0.100000
2021-05-25 00:40:35,307 ----------------------------------------------------------------------------------------------------
2021-05-25 00:40:35,307 EPOCH 14 done: loss 0.5003 - lr 0.1000000
2021-05-25 00:40:50,158 DEV : loss 0.3747537434101105 - score 0.9492
2021-05-25 00:40:50,403 BAD EPOCHS (no improvement): 2
2021-05-25 00:40:50,404 ----------------------------------------------------------------------------------------------------
2021-05-25 00:41:24,009 epoch 15 - iter 116/1169 - loss 0.50859038 - samples/sec: 55.24 - lr: 0.100000
2021-05-25 00:41:57,302 epoch 15 - iter 232/1169 - loss 0.51286990 - samples/sec: 55.75 - lr: 0.100000
2021-05-25 00:42:30,656 epoch 15 - iter 348/1169 - loss 0.52498437 - samples/sec: 55.65 - lr: 0.100000
2021-05-25 00:43:03,931 epoch 15 - iter 464/1169 - loss 0.52276983 - samples/sec: 55.79 - lr: 0.100000
2021-05-25 00:43:37,341 epoch 15 - iter 580/1169 - loss 0.52069964 - samples/sec: 55.56 - lr: 0.100000
2021-05-25 00:44:10,806 epoch 15 - iter 696/1169 - loss 0.53164625 - samples/sec: 55.47 - lr: 0.100000
2021-05-25 00:44:44,072 epoch 15 - iter 812/1169 - loss 0.52685805 - samples/sec: 55.80 - lr: 0.100000
2021-05-25 00:45:17,420 epoch 15 - iter 928/1169 - loss 0.51930976 - samples/sec: 55.66 - lr: 0.100000
2021-05-25 00:45:50,703 epoch 15 - iter 1044/1169 - loss 0.52283988 - samples/sec: 55.77 - lr: 0.100000
2021-05-25 00:46:23,953 epoch 15 - iter 1160/1169 - loss 0.52866487 - samples/sec: 55.83 - lr: 0.100000
2021-05-25 00:46:26,491 ----------------------------------------------------------------------------------------------------
2021-05-25 00:46:26,492 EPOCH 15 done: loss 0.5277 - lr 0.1000000
2021-05-25 00:46:41,382 DEV : loss 0.37929415702819824 - score 0.9518
2021-05-25 00:46:41,630 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 00:46:51,321 ----------------------------------------------------------------------------------------------------
2021-05-25 00:47:24,585 epoch 16 - iter 116/1169 - loss 0.53860139 - samples/sec: 55.81 - lr: 0.100000
2021-05-25 00:47:58,133 epoch 16 - iter 232/1169 - loss 0.53742461 - samples/sec: 55.33 - lr: 0.100000
2021-05-25 00:48:31,740 epoch 16 - iter 348/1169 - loss 0.53161630 - samples/sec: 55.23 - lr: 0.100000
2021-05-25 00:49:05,214 epoch 16 - iter 464/1169 - loss 0.52041128 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 00:49:38,609 epoch 16 - iter 580/1169 - loss 0.51467836 - samples/sec: 55.58 - lr: 0.100000
2021-05-25 00:50:12,054 epoch 16 - iter 696/1169 - loss 0.51406694 - samples/sec: 55.50 - lr: 0.100000
2021-05-25 00:50:45,339 epoch 16 - iter 812/1169 - loss 0.50385603 - samples/sec: 55.77 - lr: 0.100000
2021-05-25 00:51:18,580 epoch 16 - iter 928/1169 - loss 0.50439821 - samples/sec: 55.84 - lr: 0.100000
2021-05-25 00:51:51,777 epoch 16 - iter 1044/1169 - loss 0.50463045 - samples/sec: 55.92 - lr: 0.100000
2021-05-25 00:52:25,210 epoch 16 - iter 1160/1169 - loss 0.50920829 - samples/sec: 55.52 - lr: 0.100000
2021-05-25 00:52:27,746 ----------------------------------------------------------------------------------------------------
2021-05-25 00:52:27,746 EPOCH 16 done: loss 0.5084 - lr 0.1000000
2021-05-25 00:52:42,593 DEV : loss 0.3871913254261017 - score 0.9505
2021-05-25 00:52:42,840 BAD EPOCHS (no improvement): 1
2021-05-25 00:52:42,840 ----------------------------------------------------------------------------------------------------
2021-05-25 00:53:15,989 epoch 17 - iter 116/1169 - loss 0.49693570 - samples/sec: 56.00 - lr: 0.100000
2021-05-25 00:53:49,387 epoch 17 - iter 232/1169 - loss 0.50373024 - samples/sec: 55.58 - lr: 0.100000
2021-05-25 00:54:22,837 epoch 17 - iter 348/1169 - loss 0.50629601 - samples/sec: 55.49 - lr: 0.100000
2021-05-25 00:54:56,166 epoch 17 - iter 464/1169 - loss 0.49339345 - samples/sec: 55.69 - lr: 0.100000
2021-05-25 00:55:29,522 epoch 17 - iter 580/1169 - loss 0.48836034 - samples/sec: 55.65 - lr: 0.100000
2021-05-25 00:56:03,177 epoch 17 - iter 696/1169 - loss 0.49639012 - samples/sec: 55.15 - lr: 0.100000
2021-05-25 00:56:36,717 epoch 17 - iter 812/1169 - loss 0.49099890 - samples/sec: 55.34 - lr: 0.100000
2021-05-25 00:57:10,429 epoch 17 - iter 928/1169 - loss 0.48699622 - samples/sec: 55.06 - lr: 0.100000
2021-05-25 00:57:44,247 epoch 17 - iter 1044/1169 - loss 0.48906015 - samples/sec: 54.89 - lr: 0.100000
2021-05-25 00:58:17,693 epoch 17 - iter 1160/1169 - loss 0.48881395 - samples/sec: 55.50 - lr: 0.100000
2021-05-25 00:58:20,269 ----------------------------------------------------------------------------------------------------
2021-05-25 00:58:20,269 EPOCH 17 done: loss 0.4899 - lr 0.1000000
2021-05-25 00:58:35,140 DEV : loss 0.3854801058769226 - score 0.9506
2021-05-25 00:58:35,388 BAD EPOCHS (no improvement): 2
2021-05-25 00:58:35,388 ----------------------------------------------------------------------------------------------------
2021-05-25 00:59:08,969 epoch 18 - iter 116/1169 - loss 0.48851289 - samples/sec: 55.28 - lr: 0.100000
2021-05-25 00:59:42,441 epoch 18 - iter 232/1169 - loss 0.47629524 - samples/sec: 55.46 - lr: 0.100000
2021-05-25 01:00:17,032 epoch 18 - iter 348/1169 - loss 0.46923951 - samples/sec: 53.66 - lr: 0.100000
2021-05-25 01:00:50,433 epoch 18 - iter 464/1169 - loss 0.47254983 - samples/sec: 55.57 - lr: 0.100000
2021-05-25 01:01:24,026 epoch 18 - iter 580/1169 - loss 0.47204587 - samples/sec: 55.26 - lr: 0.100000
2021-05-25 01:01:57,591 epoch 18 - iter 696/1169 - loss 0.47953536 - samples/sec: 55.30 - lr: 0.100000
2021-05-25 01:02:30,794 epoch 18 - iter 812/1169 - loss 0.47284957 - samples/sec: 55.91 - lr: 0.100000
2021-05-25 01:03:03,923 epoch 18 - iter 928/1169 - loss 0.47585318 - samples/sec: 56.03 - lr: 0.100000
2021-05-25 01:03:37,129 epoch 18 - iter 1044/1169 - loss 0.47765558 - samples/sec: 55.90 - lr: 0.100000
2021-05-25 01:04:10,434 epoch 18 - iter 1160/1169 - loss 0.48048142 - samples/sec: 55.73 - lr: 0.100000
2021-05-25 01:04:12,985 ----------------------------------------------------------------------------------------------------
2021-05-25 01:04:12,986 EPOCH 18 done: loss 0.4806 - lr 0.1000000
2021-05-25 01:04:27,819 DEV : loss 0.40160295367240906 - score 0.9513
2021-05-25 01:04:28,065 BAD EPOCHS (no improvement): 3
2021-05-25 01:04:28,066 ----------------------------------------------------------------------------------------------------
2021-05-25 01:05:01,612 epoch 19 - iter 116/1169 - loss 0.47900852 - samples/sec: 55.33 - lr: 0.100000
2021-05-25 01:05:34,813 epoch 19 - iter 232/1169 - loss 0.48045790 - samples/sec: 55.91 - lr: 0.100000
2021-05-25 01:06:08,257 epoch 19 - iter 348/1169 - loss 0.49411301 - samples/sec: 55.50 - lr: 0.100000
2021-05-25 01:06:41,679 epoch 19 - iter 464/1169 - loss 0.49511613 - samples/sec: 55.54 - lr: 0.100000
2021-05-25 01:07:14,953 epoch 19 - iter 580/1169 - loss 0.49835140 - samples/sec: 55.79 - lr: 0.100000
2021-05-25 01:07:48,259 epoch 19 - iter 696/1169 - loss 0.49338461 - samples/sec: 55.73 - lr: 0.100000
2021-05-25 01:08:21,758 epoch 19 - iter 812/1169 - loss 0.48865555 - samples/sec: 55.41 - lr: 0.100000
2021-05-25 01:08:55,135 epoch 19 - iter 928/1169 - loss 0.48934674 - samples/sec: 55.62 - lr: 0.100000
2021-05-25 01:09:28,564 epoch 19 - iter 1044/1169 - loss 0.49027954 - samples/sec: 55.53 - lr: 0.100000
2021-05-25 01:10:01,760 epoch 19 - iter 1160/1169 - loss 0.48883859 - samples/sec: 55.92 - lr: 0.100000
2021-05-25 01:10:04,310 ----------------------------------------------------------------------------------------------------
2021-05-25 01:10:04,310 EPOCH 19 done: loss 0.4891 - lr 0.1000000
2021-05-25 01:10:19,148 DEV : loss 0.3904295265674591 - score 0.9484
Epoch    19: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 01:10:19,394 BAD EPOCHS (no improvement): 4
2021-05-25 01:10:19,395 ----------------------------------------------------------------------------------------------------
2021-05-25 01:10:52,737 epoch 20 - iter 116/1169 - loss 0.47483624 - samples/sec: 55.67 - lr: 0.050000
2021-05-25 01:11:26,178 epoch 20 - iter 232/1169 - loss 0.43914634 - samples/sec: 55.51 - lr: 0.050000
2021-05-25 01:11:59,606 epoch 20 - iter 348/1169 - loss 0.44714415 - samples/sec: 55.53 - lr: 0.050000
2021-05-25 01:12:33,134 epoch 20 - iter 464/1169 - loss 0.44994070 - samples/sec: 55.36 - lr: 0.050000
2021-05-25 01:13:06,463 epoch 20 - iter 580/1169 - loss 0.44497602 - samples/sec: 55.69 - lr: 0.050000
2021-05-25 01:13:40,006 epoch 20 - iter 696/1169 - loss 0.44008519 - samples/sec: 55.34 - lr: 0.050000
2021-05-25 01:14:13,480 epoch 20 - iter 812/1169 - loss 0.43943518 - samples/sec: 55.45 - lr: 0.050000
2021-05-25 01:14:46,974 epoch 20 - iter 928/1169 - loss 0.43416408 - samples/sec: 55.42 - lr: 0.050000
2021-05-25 01:15:20,295 epoch 20 - iter 1044/1169 - loss 0.43189733 - samples/sec: 55.71 - lr: 0.050000
2021-05-25 01:15:53,698 epoch 20 - iter 1160/1169 - loss 0.43297478 - samples/sec: 55.57 - lr: 0.050000
2021-05-25 01:15:56,244 ----------------------------------------------------------------------------------------------------
2021-05-25 01:15:56,244 EPOCH 20 done: loss 0.4324 - lr 0.0500000
2021-05-25 01:16:11,058 DEV : loss 0.34248271584510803 - score 0.9543
2021-05-25 01:16:11,307 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 01:16:21,001 ----------------------------------------------------------------------------------------------------
2021-05-25 01:16:54,307 epoch 21 - iter 116/1169 - loss 0.41662862 - samples/sec: 55.74 - lr: 0.050000
2021-05-25 01:17:27,656 epoch 21 - iter 232/1169 - loss 0.42623385 - samples/sec: 55.66 - lr: 0.050000
2021-05-25 01:18:00,747 epoch 21 - iter 348/1169 - loss 0.42655342 - samples/sec: 56.09 - lr: 0.050000
2021-05-25 01:18:34,154 epoch 21 - iter 464/1169 - loss 0.42614904 - samples/sec: 55.56 - lr: 0.050000
2021-05-25 01:19:07,759 epoch 21 - iter 580/1169 - loss 0.41968752 - samples/sec: 55.24 - lr: 0.050000
2021-05-25 01:19:41,311 epoch 21 - iter 696/1169 - loss 0.41596448 - samples/sec: 55.32 - lr: 0.050000
2021-05-25 01:20:14,731 epoch 21 - iter 812/1169 - loss 0.41901329 - samples/sec: 55.54 - lr: 0.050000
2021-05-25 01:20:48,305 epoch 21 - iter 928/1169 - loss 0.41677530 - samples/sec: 55.29 - lr: 0.050000
2021-05-25 01:21:21,887 epoch 21 - iter 1044/1169 - loss 0.41649034 - samples/sec: 55.27 - lr: 0.050000
2021-05-25 01:21:55,393 epoch 21 - iter 1160/1169 - loss 0.41532690 - samples/sec: 55.40 - lr: 0.050000
2021-05-25 01:21:57,948 ----------------------------------------------------------------------------------------------------
2021-05-25 01:21:57,948 EPOCH 21 done: loss 0.4151 - lr 0.0500000
2021-05-25 01:22:12,713 DEV : loss 0.3435756266117096 - score 0.9565
2021-05-25 01:22:12,961 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 01:22:22,541 ----------------------------------------------------------------------------------------------------
2021-05-25 01:22:56,066 epoch 22 - iter 116/1169 - loss 0.45903406 - samples/sec: 55.37 - lr: 0.050000
2021-05-25 01:23:29,508 epoch 22 - iter 232/1169 - loss 0.39040234 - samples/sec: 55.51 - lr: 0.050000
2021-05-25 01:24:02,890 epoch 22 - iter 348/1169 - loss 0.40752536 - samples/sec: 55.61 - lr: 0.050000
2021-05-25 01:24:36,428 epoch 22 - iter 464/1169 - loss 0.40327732 - samples/sec: 55.35 - lr: 0.050000
2021-05-25 01:25:09,951 epoch 22 - iter 580/1169 - loss 0.41841840 - samples/sec: 55.37 - lr: 0.050000
2021-05-25 01:25:43,564 epoch 22 - iter 696/1169 - loss 0.41169770 - samples/sec: 55.22 - lr: 0.050000
2021-05-25 01:26:16,913 epoch 22 - iter 812/1169 - loss 0.41168045 - samples/sec: 55.66 - lr: 0.050000
2021-05-25 01:26:50,437 epoch 22 - iter 928/1169 - loss 0.40831668 - samples/sec: 55.37 - lr: 0.050000
2021-05-25 01:27:23,968 epoch 22 - iter 1044/1169 - loss 0.40754345 - samples/sec: 55.36 - lr: 0.050000
2021-05-25 01:27:57,493 epoch 22 - iter 1160/1169 - loss 0.40819136 - samples/sec: 55.37 - lr: 0.050000
2021-05-25 01:28:00,040 ----------------------------------------------------------------------------------------------------
2021-05-25 01:28:00,040 EPOCH 22 done: loss 0.4080 - lr 0.0500000
2021-05-25 01:28:16,312 DEV : loss 0.3329278528690338 - score 0.9558
2021-05-25 01:28:16,558 BAD EPOCHS (no improvement): 1
2021-05-25 01:28:16,558 ----------------------------------------------------------------------------------------------------
2021-05-25 01:28:49,988 epoch 23 - iter 116/1169 - loss 0.39984687 - samples/sec: 55.53 - lr: 0.050000
2021-05-25 01:29:23,299 epoch 23 - iter 232/1169 - loss 0.40025038 - samples/sec: 55.72 - lr: 0.050000
2021-05-25 01:29:56,699 epoch 23 - iter 348/1169 - loss 0.41083089 - samples/sec: 55.58 - lr: 0.050000
2021-05-25 01:30:30,148 epoch 23 - iter 464/1169 - loss 0.40219725 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 01:31:03,713 epoch 23 - iter 580/1169 - loss 0.40682185 - samples/sec: 55.30 - lr: 0.050000
2021-05-25 01:31:37,213 epoch 23 - iter 696/1169 - loss 0.40382959 - samples/sec: 55.41 - lr: 0.050000
2021-05-25 01:32:10,639 epoch 23 - iter 812/1169 - loss 0.40145920 - samples/sec: 55.53 - lr: 0.050000
2021-05-25 01:32:44,101 epoch 23 - iter 928/1169 - loss 0.39738753 - samples/sec: 55.47 - lr: 0.050000
2021-05-25 01:33:17,268 epoch 23 - iter 1044/1169 - loss 0.40019844 - samples/sec: 55.97 - lr: 0.050000
2021-05-25 01:33:50,464 epoch 23 - iter 1160/1169 - loss 0.40333532 - samples/sec: 55.92 - lr: 0.050000
2021-05-25 01:33:52,987 ----------------------------------------------------------------------------------------------------
2021-05-25 01:33:52,987 EPOCH 23 done: loss 0.4042 - lr 0.0500000
2021-05-25 01:34:07,772 DEV : loss 0.35941311717033386 - score 0.9511
2021-05-25 01:34:08,020 BAD EPOCHS (no improvement): 2
2021-05-25 01:34:08,020 ----------------------------------------------------------------------------------------------------
2021-05-25 01:34:41,174 epoch 24 - iter 116/1169 - loss 0.41235111 - samples/sec: 55.99 - lr: 0.050000
2021-05-25 01:35:14,376 epoch 24 - iter 232/1169 - loss 0.38567881 - samples/sec: 55.91 - lr: 0.050000
2021-05-25 01:35:47,588 epoch 24 - iter 348/1169 - loss 0.37676292 - samples/sec: 55.89 - lr: 0.050000
2021-05-25 01:36:21,013 epoch 24 - iter 464/1169 - loss 0.38261728 - samples/sec: 55.53 - lr: 0.050000
2021-05-25 01:36:54,251 epoch 24 - iter 580/1169 - loss 0.38470815 - samples/sec: 55.85 - lr: 0.050000
2021-05-25 01:37:27,442 epoch 24 - iter 696/1169 - loss 0.39309187 - samples/sec: 55.93 - lr: 0.050000
2021-05-25 01:38:00,564 epoch 24 - iter 812/1169 - loss 0.39947861 - samples/sec: 56.04 - lr: 0.050000
2021-05-25 01:38:33,902 epoch 24 - iter 928/1169 - loss 0.40128515 - samples/sec: 55.68 - lr: 0.050000
2021-05-25 01:39:07,054 epoch 24 - iter 1044/1169 - loss 0.40275636 - samples/sec: 55.99 - lr: 0.050000
2021-05-25 01:39:40,161 epoch 24 - iter 1160/1169 - loss 0.40241546 - samples/sec: 56.07 - lr: 0.050000
2021-05-25 01:39:42,680 ----------------------------------------------------------------------------------------------------
2021-05-25 01:39:42,680 EPOCH 24 done: loss 0.4020 - lr 0.0500000
2021-05-25 01:39:57,485 DEV : loss 0.33893439173698425 - score 0.9563
2021-05-25 01:39:57,733 BAD EPOCHS (no improvement): 3
2021-05-25 01:39:57,734 ----------------------------------------------------------------------------------------------------
2021-05-25 01:40:30,851 epoch 25 - iter 116/1169 - loss 0.37949189 - samples/sec: 56.05 - lr: 0.050000
2021-05-25 01:41:04,037 epoch 25 - iter 232/1169 - loss 0.39438580 - samples/sec: 55.93 - lr: 0.050000
2021-05-25 01:41:37,079 epoch 25 - iter 348/1169 - loss 0.40948475 - samples/sec: 56.18 - lr: 0.050000
2021-05-25 01:42:10,147 epoch 25 - iter 464/1169 - loss 0.40999444 - samples/sec: 56.13 - lr: 0.050000
2021-05-25 01:42:43,685 epoch 25 - iter 580/1169 - loss 0.40147290 - samples/sec: 55.35 - lr: 0.050000
2021-05-25 01:43:17,111 epoch 25 - iter 696/1169 - loss 0.40000628 - samples/sec: 55.53 - lr: 0.050000
2021-05-25 01:43:50,235 epoch 25 - iter 812/1169 - loss 0.39667742 - samples/sec: 56.04 - lr: 0.050000
2021-05-25 01:44:23,679 epoch 25 - iter 928/1169 - loss 0.39663969 - samples/sec: 55.50 - lr: 0.050000
2021-05-25 01:44:57,185 epoch 25 - iter 1044/1169 - loss 0.39986369 - samples/sec: 55.40 - lr: 0.050000
2021-05-25 01:45:30,793 epoch 25 - iter 1160/1169 - loss 0.39879033 - samples/sec: 55.23 - lr: 0.050000
2021-05-25 01:45:33,373 ----------------------------------------------------------------------------------------------------
2021-05-25 01:45:33,373 EPOCH 25 done: loss 0.3987 - lr 0.0500000
2021-05-25 01:45:48,123 DEV : loss 0.34214961528778076 - score 0.9571
2021-05-25 01:45:48,368 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 01:45:58,128 ----------------------------------------------------------------------------------------------------
2021-05-25 01:46:31,676 epoch 26 - iter 116/1169 - loss 0.40873929 - samples/sec: 55.33 - lr: 0.050000
2021-05-25 01:47:05,215 epoch 26 - iter 232/1169 - loss 0.40253485 - samples/sec: 55.35 - lr: 0.050000
2021-05-25 01:47:38,848 epoch 26 - iter 348/1169 - loss 0.40013443 - samples/sec: 55.19 - lr: 0.050000
2021-05-25 01:48:12,199 epoch 26 - iter 464/1169 - loss 0.39349354 - samples/sec: 55.66 - lr: 0.050000
2021-05-25 01:48:45,484 epoch 26 - iter 580/1169 - loss 0.39405266 - samples/sec: 55.77 - lr: 0.050000
2021-05-25 01:49:18,719 epoch 26 - iter 696/1169 - loss 0.39474596 - samples/sec: 55.85 - lr: 0.050000
2021-05-25 01:49:52,153 epoch 26 - iter 812/1169 - loss 0.39396980 - samples/sec: 55.52 - lr: 0.050000
2021-05-25 01:50:25,625 epoch 26 - iter 928/1169 - loss 0.39018465 - samples/sec: 55.46 - lr: 0.050000
2021-05-25 01:50:59,121 epoch 26 - iter 1044/1169 - loss 0.38906704 - samples/sec: 55.42 - lr: 0.050000
2021-05-25 01:51:32,424 epoch 26 - iter 1160/1169 - loss 0.38905811 - samples/sec: 55.74 - lr: 0.050000
2021-05-25 01:51:34,935 ----------------------------------------------------------------------------------------------------
2021-05-25 01:51:34,935 EPOCH 26 done: loss 0.3893 - lr 0.0500000
2021-05-25 01:51:51,276 DEV : loss 0.32171595096588135 - score 0.9587
2021-05-25 01:51:51,523 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 01:52:01,210 ----------------------------------------------------------------------------------------------------
2021-05-25 01:52:34,474 epoch 27 - iter 116/1169 - loss 0.36910027 - samples/sec: 55.80 - lr: 0.050000
2021-05-25 01:53:07,805 epoch 27 - iter 232/1169 - loss 0.39553345 - samples/sec: 55.69 - lr: 0.050000
2021-05-25 01:53:41,296 epoch 27 - iter 348/1169 - loss 0.40021581 - samples/sec: 55.43 - lr: 0.050000
2021-05-25 01:54:14,745 epoch 27 - iter 464/1169 - loss 0.39867776 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 01:54:48,082 epoch 27 - iter 580/1169 - loss 0.39947695 - samples/sec: 55.68 - lr: 0.050000
2021-05-25 01:55:21,555 epoch 27 - iter 696/1169 - loss 0.39526064 - samples/sec: 55.45 - lr: 0.050000
2021-05-25 01:55:54,990 epoch 27 - iter 812/1169 - loss 0.39520641 - samples/sec: 55.52 - lr: 0.050000
2021-05-25 01:56:28,509 epoch 27 - iter 928/1169 - loss 0.38894262 - samples/sec: 55.38 - lr: 0.050000
2021-05-25 01:57:01,859 epoch 27 - iter 1044/1169 - loss 0.39086088 - samples/sec: 55.66 - lr: 0.050000
2021-05-25 01:57:35,125 epoch 27 - iter 1160/1169 - loss 0.39184069 - samples/sec: 55.80 - lr: 0.050000
2021-05-25 01:57:37,649 ----------------------------------------------------------------------------------------------------
2021-05-25 01:57:37,649 EPOCH 27 done: loss 0.3914 - lr 0.0500000
2021-05-25 01:57:52,417 DEV : loss 0.34095484018325806 - score 0.9553
2021-05-25 01:57:52,665 BAD EPOCHS (no improvement): 1
2021-05-25 01:57:52,665 ----------------------------------------------------------------------------------------------------
2021-05-25 01:58:26,043 epoch 28 - iter 116/1169 - loss 0.41638737 - samples/sec: 55.61 - lr: 0.050000
2021-05-25 01:58:59,493 epoch 28 - iter 232/1169 - loss 0.41133643 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 01:59:32,996 epoch 28 - iter 348/1169 - loss 0.40192130 - samples/sec: 55.40 - lr: 0.050000
2021-05-25 02:00:06,657 epoch 28 - iter 464/1169 - loss 0.39276473 - samples/sec: 55.14 - lr: 0.050000
2021-05-25 02:00:40,106 epoch 28 - iter 580/1169 - loss 0.38820262 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 02:01:13,442 epoch 28 - iter 696/1169 - loss 0.38413334 - samples/sec: 55.68 - lr: 0.050000
2021-05-25 02:01:46,647 epoch 28 - iter 812/1169 - loss 0.38868881 - samples/sec: 55.90 - lr: 0.050000
2021-05-25 02:02:19,920 epoch 28 - iter 928/1169 - loss 0.38705929 - samples/sec: 55.79 - lr: 0.050000
2021-05-25 02:02:53,104 epoch 28 - iter 1044/1169 - loss 0.38433268 - samples/sec: 55.94 - lr: 0.050000
2021-05-25 02:03:26,332 epoch 28 - iter 1160/1169 - loss 0.38376123 - samples/sec: 55.86 - lr: 0.050000
2021-05-25 02:03:28,869 ----------------------------------------------------------------------------------------------------
2021-05-25 02:03:28,870 EPOCH 28 done: loss 0.3825 - lr 0.0500000
2021-05-25 02:03:43,632 DEV : loss 0.3325689136981964 - score 0.9561
2021-05-25 02:03:43,880 BAD EPOCHS (no improvement): 2
2021-05-25 02:03:43,880 ----------------------------------------------------------------------------------------------------
2021-05-25 02:04:17,166 epoch 29 - iter 116/1169 - loss 0.37550006 - samples/sec: 55.77 - lr: 0.050000
2021-05-25 02:04:50,378 epoch 29 - iter 232/1169 - loss 0.37649825 - samples/sec: 55.89 - lr: 0.050000
2021-05-25 02:05:23,575 epoch 29 - iter 348/1169 - loss 0.37316523 - samples/sec: 55.92 - lr: 0.050000
2021-05-25 02:05:56,939 epoch 29 - iter 464/1169 - loss 0.37406583 - samples/sec: 55.64 - lr: 0.050000
2021-05-25 02:06:30,191 epoch 29 - iter 580/1169 - loss 0.37195549 - samples/sec: 55.82 - lr: 0.050000
2021-05-25 02:07:03,506 epoch 29 - iter 696/1169 - loss 0.37141735 - samples/sec: 55.72 - lr: 0.050000
2021-05-25 02:07:36,699 epoch 29 - iter 812/1169 - loss 0.37342944 - samples/sec: 55.92 - lr: 0.050000
2021-05-25 02:08:09,857 epoch 29 - iter 928/1169 - loss 0.37770089 - samples/sec: 55.98 - lr: 0.050000
2021-05-25 02:08:43,056 epoch 29 - iter 1044/1169 - loss 0.38044033 - samples/sec: 55.91 - lr: 0.050000
2021-05-25 02:09:16,239 epoch 29 - iter 1160/1169 - loss 0.38193650 - samples/sec: 55.94 - lr: 0.050000
2021-05-25 02:09:18,763 ----------------------------------------------------------------------------------------------------
2021-05-25 02:09:18,763 EPOCH 29 done: loss 0.3809 - lr 0.0500000
2021-05-25 02:09:33,573 DEV : loss 0.32799381017684937 - score 0.9563
2021-05-25 02:09:33,822 BAD EPOCHS (no improvement): 3
2021-05-25 02:09:33,823 ----------------------------------------------------------------------------------------------------
2021-05-25 02:10:07,204 epoch 30 - iter 116/1169 - loss 0.33385601 - samples/sec: 55.61 - lr: 0.050000
2021-05-25 02:10:40,722 epoch 30 - iter 232/1169 - loss 0.36804388 - samples/sec: 55.38 - lr: 0.050000
2021-05-25 02:11:14,191 epoch 30 - iter 348/1169 - loss 0.37074920 - samples/sec: 55.46 - lr: 0.050000
2021-05-25 02:11:47,659 epoch 30 - iter 464/1169 - loss 0.38376775 - samples/sec: 55.46 - lr: 0.050000
2021-05-25 02:12:21,083 epoch 30 - iter 580/1169 - loss 0.38621197 - samples/sec: 55.54 - lr: 0.050000
2021-05-25 02:12:54,434 epoch 30 - iter 696/1169 - loss 0.38716020 - samples/sec: 55.66 - lr: 0.050000
2021-05-25 02:13:27,821 epoch 30 - iter 812/1169 - loss 0.38454118 - samples/sec: 55.60 - lr: 0.050000
2021-05-25 02:14:00,939 epoch 30 - iter 928/1169 - loss 0.38571703 - samples/sec: 56.05 - lr: 0.050000
2021-05-25 02:14:34,173 epoch 30 - iter 1044/1169 - loss 0.38395168 - samples/sec: 55.85 - lr: 0.050000
2021-05-25 02:15:07,257 epoch 30 - iter 1160/1169 - loss 0.38147868 - samples/sec: 56.11 - lr: 0.050000
2021-05-25 02:15:09,813 ----------------------------------------------------------------------------------------------------
2021-05-25 02:15:09,813 EPOCH 30 done: loss 0.3806 - lr 0.0500000
2021-05-25 02:15:24,596 DEV : loss 0.3253198266029358 - score 0.9582
Epoch    30: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 02:15:24,842 BAD EPOCHS (no improvement): 4
2021-05-25 02:15:26,154 ----------------------------------------------------------------------------------------------------
2021-05-25 02:15:26,155 Testing using best model ...
2021-05-25 02:15:26,155 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/tur.pdtb.tdb/best-model.pt
2021-05-25 02:16:36,364 0.9639	0.9448	0.9543
2021-05-25 02:16:36,364 
Results:
- F1-score (micro) 0.9543
- F1-score (macro) 0.9543

By class:
SENT       tp: 2775 - fp: 104 - fn: 162 - precision: 0.9639 - recall: 0.9448 - f1-score: 0.9543
2021-05-25 02:16:36,364 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac/
2021-05-25 02:16:36,399 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac
2021-05-25 02:16:36,399 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac/sent_train.txt
2021-05-25 02:16:36,402 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac/sent_dev.txt
2021-05-25 02:16:36,404 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac/sent_test.txt
Corpus: 1751 train + 228 dev + 315 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-25 02:16:42,651 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,655 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-25 02:16:42,655 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,655 Corpus: "Corpus: 1751 train + 228 dev + 315 test sentences"
2021-05-25 02:16:42,655 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,655 Parameters:
2021-05-25 02:16:42,655  - learning_rate: "0.1"
2021-05-25 02:16:42,655  - mini_batch_size: "16"
2021-05-25 02:16:42,655  - patience: "3"
2021-05-25 02:16:42,655  - anneal_factor: "0.5"
2021-05-25 02:16:42,655  - max_epochs: "30"
2021-05-25 02:16:42,655  - shuffle: "True"
2021-05-25 02:16:42,655  - train_with_dev: "False"
2021-05-25 02:16:42,655  - batch_growth_annealing: "False"
2021-05-25 02:16:42,655 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,655 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac"
2021-05-25 02:16:42,655 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,655 Device: cuda:0
2021-05-25 02:16:42,655 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:42,656 Embeddings storage mode: cpu
2021-05-25 02:16:42,657 ----------------------------------------------------------------------------------------------------
2021-05-25 02:16:51,342 epoch 1 - iter 11/110 - loss 19.13514354 - samples/sec: 20.27 - lr: 0.100000
2021-05-25 02:16:59,603 epoch 1 - iter 22/110 - loss 16.94932851 - samples/sec: 21.31 - lr: 0.100000
2021-05-25 02:17:07,869 epoch 1 - iter 33/110 - loss 15.51645707 - samples/sec: 21.29 - lr: 0.100000
2021-05-25 02:17:16,144 epoch 1 - iter 44/110 - loss 14.67491312 - samples/sec: 21.27 - lr: 0.100000
2021-05-25 02:17:24,444 epoch 1 - iter 55/110 - loss 13.96220720 - samples/sec: 21.21 - lr: 0.100000
2021-05-25 02:17:32,731 epoch 1 - iter 66/110 - loss 13.41132418 - samples/sec: 21.24 - lr: 0.100000
2021-05-25 02:17:40,955 epoch 1 - iter 77/110 - loss 12.88696698 - samples/sec: 21.40 - lr: 0.100000
2021-05-25 02:17:49,228 epoch 1 - iter 88/110 - loss 12.42264734 - samples/sec: 21.28 - lr: 0.100000
2021-05-25 02:17:57,508 epoch 1 - iter 99/110 - loss 12.02170287 - samples/sec: 21.26 - lr: 0.100000
2021-05-25 02:18:05,394 epoch 1 - iter 110/110 - loss 11.72477797 - samples/sec: 22.32 - lr: 0.100000
2021-05-25 02:18:05,394 ----------------------------------------------------------------------------------------------------
2021-05-25 02:18:05,394 EPOCH 1 done: loss 11.7248 - lr 0.1000000
2021-05-25 02:18:12,223 DEV : loss 5.983729362487793 - score 0.7295
2021-05-25 02:18:12,247 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:18:13,384 ----------------------------------------------------------------------------------------------------
2021-05-25 02:18:16,395 epoch 2 - iter 11/110 - loss 8.27638943 - samples/sec: 58.47 - lr: 0.100000
2021-05-25 02:18:19,415 epoch 2 - iter 22/110 - loss 8.39897013 - samples/sec: 58.28 - lr: 0.100000
2021-05-25 02:18:22,432 epoch 2 - iter 33/110 - loss 8.29095878 - samples/sec: 58.36 - lr: 0.100000
2021-05-25 02:18:25,464 epoch 2 - iter 44/110 - loss 8.07422780 - samples/sec: 58.06 - lr: 0.100000
2021-05-25 02:18:28,500 epoch 2 - iter 55/110 - loss 7.86845573 - samples/sec: 57.97 - lr: 0.100000
2021-05-25 02:18:31,534 epoch 2 - iter 66/110 - loss 7.73155303 - samples/sec: 58.02 - lr: 0.100000
2021-05-25 02:18:34,571 epoch 2 - iter 77/110 - loss 7.58238732 - samples/sec: 57.97 - lr: 0.100000
2021-05-25 02:18:37,593 epoch 2 - iter 88/110 - loss 7.54184173 - samples/sec: 58.25 - lr: 0.100000
2021-05-25 02:18:40,617 epoch 2 - iter 99/110 - loss 7.47952006 - samples/sec: 58.21 - lr: 0.100000
2021-05-25 02:18:43,532 epoch 2 - iter 110/110 - loss 7.48326076 - samples/sec: 60.39 - lr: 0.100000
2021-05-25 02:18:43,533 ----------------------------------------------------------------------------------------------------
2021-05-25 02:18:43,533 EPOCH 2 done: loss 7.4833 - lr 0.1000000
2021-05-25 02:18:44,918 DEV : loss 5.21965217590332 - score 0.7707
2021-05-25 02:18:44,942 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:18:54,696 ----------------------------------------------------------------------------------------------------
2021-05-25 02:18:57,695 epoch 3 - iter 11/110 - loss 6.71946946 - samples/sec: 58.72 - lr: 0.100000
2021-05-25 02:19:00,692 epoch 3 - iter 22/110 - loss 6.75549845 - samples/sec: 58.72 - lr: 0.100000
2021-05-25 02:19:03,715 epoch 3 - iter 33/110 - loss 6.71228110 - samples/sec: 58.24 - lr: 0.100000
2021-05-25 02:19:06,773 epoch 3 - iter 44/110 - loss 6.65114560 - samples/sec: 57.56 - lr: 0.100000
2021-05-25 02:19:09,787 epoch 3 - iter 55/110 - loss 6.63412467 - samples/sec: 58.41 - lr: 0.100000
2021-05-25 02:19:12,829 epoch 3 - iter 66/110 - loss 6.53308426 - samples/sec: 57.86 - lr: 0.100000
2021-05-25 02:19:15,854 epoch 3 - iter 77/110 - loss 6.50888692 - samples/sec: 58.20 - lr: 0.100000
2021-05-25 02:19:18,917 epoch 3 - iter 88/110 - loss 6.51744656 - samples/sec: 57.47 - lr: 0.100000
2021-05-25 02:19:21,948 epoch 3 - iter 99/110 - loss 6.47210896 - samples/sec: 58.08 - lr: 0.100000
2021-05-25 02:19:24,827 epoch 3 - iter 110/110 - loss 6.45173331 - samples/sec: 61.14 - lr: 0.100000
2021-05-25 02:19:24,827 ----------------------------------------------------------------------------------------------------
2021-05-25 02:19:24,827 EPOCH 3 done: loss 6.4517 - lr 0.1000000
2021-05-25 02:19:26,205 DEV : loss 4.5174455642700195 - score 0.8002
2021-05-25 02:19:26,229 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:19:35,931 ----------------------------------------------------------------------------------------------------
2021-05-25 02:19:38,941 epoch 4 - iter 11/110 - loss 6.44440521 - samples/sec: 58.50 - lr: 0.100000
2021-05-25 02:19:41,939 epoch 4 - iter 22/110 - loss 6.11518223 - samples/sec: 58.72 - lr: 0.100000
2021-05-25 02:19:44,975 epoch 4 - iter 33/110 - loss 6.18837601 - samples/sec: 57.98 - lr: 0.100000
2021-05-25 02:19:47,969 epoch 4 - iter 44/110 - loss 6.06673986 - samples/sec: 58.81 - lr: 0.100000
2021-05-25 02:19:50,965 epoch 4 - iter 55/110 - loss 6.08358987 - samples/sec: 58.75 - lr: 0.100000
2021-05-25 02:19:53,991 epoch 4 - iter 66/110 - loss 6.09641016 - samples/sec: 58.17 - lr: 0.100000
2021-05-25 02:19:57,013 epoch 4 - iter 77/110 - loss 6.05239661 - samples/sec: 58.25 - lr: 0.100000
2021-05-25 02:20:00,047 epoch 4 - iter 88/110 - loss 6.00149351 - samples/sec: 58.03 - lr: 0.100000
2021-05-25 02:20:03,070 epoch 4 - iter 99/110 - loss 5.98933004 - samples/sec: 58.23 - lr: 0.100000
2021-05-25 02:20:05,959 epoch 4 - iter 110/110 - loss 5.97092198 - samples/sec: 60.95 - lr: 0.100000
2021-05-25 02:20:05,959 ----------------------------------------------------------------------------------------------------
2021-05-25 02:20:05,959 EPOCH 4 done: loss 5.9709 - lr 0.1000000
2021-05-25 02:20:07,337 DEV : loss 3.9999663829803467 - score 0.8013
2021-05-25 02:20:07,361 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:20:17,076 ----------------------------------------------------------------------------------------------------
2021-05-25 02:20:20,076 epoch 5 - iter 11/110 - loss 5.59060782 - samples/sec: 58.69 - lr: 0.100000
2021-05-25 02:20:23,076 epoch 5 - iter 22/110 - loss 6.03820294 - samples/sec: 58.69 - lr: 0.100000
2021-05-25 02:20:26,061 epoch 5 - iter 33/110 - loss 6.01197075 - samples/sec: 58.98 - lr: 0.100000
2021-05-25 02:20:29,109 epoch 5 - iter 44/110 - loss 5.95233210 - samples/sec: 57.74 - lr: 0.100000
2021-05-25 02:20:32,126 epoch 5 - iter 55/110 - loss 6.00123443 - samples/sec: 58.35 - lr: 0.100000
2021-05-25 02:20:35,160 epoch 5 - iter 66/110 - loss 5.87559491 - samples/sec: 58.03 - lr: 0.100000
2021-05-25 02:20:38,178 epoch 5 - iter 77/110 - loss 5.76923447 - samples/sec: 58.33 - lr: 0.100000
2021-05-25 02:20:41,230 epoch 5 - iter 88/110 - loss 5.72503499 - samples/sec: 57.69 - lr: 0.100000
2021-05-25 02:20:44,253 epoch 5 - iter 99/110 - loss 5.71068625 - samples/sec: 58.24 - lr: 0.100000
2021-05-25 02:20:47,190 epoch 5 - iter 110/110 - loss 5.71277846 - samples/sec: 59.93 - lr: 0.100000
2021-05-25 02:20:47,190 ----------------------------------------------------------------------------------------------------
2021-05-25 02:20:47,190 EPOCH 5 done: loss 5.7128 - lr 0.1000000
2021-05-25 02:20:48,568 DEV : loss 4.524598598480225 - score 0.7967
2021-05-25 02:20:48,592 BAD EPOCHS (no improvement): 1
2021-05-25 02:20:48,593 ----------------------------------------------------------------------------------------------------
2021-05-25 02:20:51,646 epoch 6 - iter 11/110 - loss 6.04501481 - samples/sec: 57.66 - lr: 0.100000
2021-05-25 02:20:54,667 epoch 6 - iter 22/110 - loss 5.92291661 - samples/sec: 58.27 - lr: 0.100000
2021-05-25 02:20:57,688 epoch 6 - iter 33/110 - loss 5.60924608 - samples/sec: 58.27 - lr: 0.100000
2021-05-25 02:21:00,729 epoch 6 - iter 44/110 - loss 5.60729247 - samples/sec: 57.89 - lr: 0.100000
2021-05-25 02:21:03,750 epoch 6 - iter 55/110 - loss 5.51476121 - samples/sec: 58.26 - lr: 0.100000
2021-05-25 02:21:06,787 epoch 6 - iter 66/110 - loss 5.55688459 - samples/sec: 57.97 - lr: 0.100000
2021-05-25 02:21:09,800 epoch 6 - iter 77/110 - loss 5.56340704 - samples/sec: 58.43 - lr: 0.100000
2021-05-25 02:21:12,819 epoch 6 - iter 88/110 - loss 5.49696877 - samples/sec: 58.31 - lr: 0.100000
2021-05-25 02:21:15,834 epoch 6 - iter 99/110 - loss 5.51583857 - samples/sec: 58.39 - lr: 0.100000
2021-05-25 02:21:18,713 epoch 6 - iter 110/110 - loss 5.55115603 - samples/sec: 61.15 - lr: 0.100000
2021-05-25 02:21:18,713 ----------------------------------------------------------------------------------------------------
2021-05-25 02:21:18,713 EPOCH 6 done: loss 5.5512 - lr 0.1000000
2021-05-25 02:21:20,088 DEV : loss 3.8653225898742676 - score 0.825
2021-05-25 02:21:20,112 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:21:29,975 ----------------------------------------------------------------------------------------------------
2021-05-25 02:21:32,988 epoch 7 - iter 11/110 - loss 5.33906577 - samples/sec: 58.44 - lr: 0.100000
2021-05-25 02:21:36,004 epoch 7 - iter 22/110 - loss 5.37886637 - samples/sec: 58.38 - lr: 0.100000
2021-05-25 02:21:39,007 epoch 7 - iter 33/110 - loss 5.25473150 - samples/sec: 58.61 - lr: 0.100000
2021-05-25 02:21:42,034 epoch 7 - iter 44/110 - loss 5.34349559 - samples/sec: 58.16 - lr: 0.100000
2021-05-25 02:21:45,035 epoch 7 - iter 55/110 - loss 5.35371316 - samples/sec: 58.67 - lr: 0.100000
2021-05-25 02:21:48,087 epoch 7 - iter 66/110 - loss 5.29858981 - samples/sec: 57.67 - lr: 0.100000
2021-05-25 02:21:51,139 epoch 7 - iter 77/110 - loss 5.29690481 - samples/sec: 57.69 - lr: 0.100000
2021-05-25 02:21:54,143 epoch 7 - iter 88/110 - loss 5.34346403 - samples/sec: 58.60 - lr: 0.100000
2021-05-25 02:21:57,184 epoch 7 - iter 99/110 - loss 5.29139964 - samples/sec: 57.88 - lr: 0.100000
2021-05-25 02:22:00,105 epoch 7 - iter 110/110 - loss 5.33743577 - samples/sec: 60.27 - lr: 0.100000
2021-05-25 02:22:00,105 ----------------------------------------------------------------------------------------------------
2021-05-25 02:22:00,106 EPOCH 7 done: loss 5.3374 - lr 0.1000000
2021-05-25 02:22:01,482 DEV : loss 3.956806182861328 - score 0.8061
2021-05-25 02:22:01,506 BAD EPOCHS (no improvement): 1
2021-05-25 02:22:01,507 ----------------------------------------------------------------------------------------------------
2021-05-25 02:22:04,563 epoch 8 - iter 11/110 - loss 5.59663352 - samples/sec: 57.60 - lr: 0.100000
2021-05-25 02:22:07,632 epoch 8 - iter 22/110 - loss 5.28223335 - samples/sec: 57.36 - lr: 0.100000
2021-05-25 02:22:10,650 epoch 8 - iter 33/110 - loss 5.31252785 - samples/sec: 58.33 - lr: 0.100000
2021-05-25 02:22:13,689 epoch 8 - iter 44/110 - loss 5.42635326 - samples/sec: 57.93 - lr: 0.100000
2021-05-25 02:22:16,712 epoch 8 - iter 55/110 - loss 5.34773754 - samples/sec: 58.24 - lr: 0.100000
2021-05-25 02:22:19,750 epoch 8 - iter 66/110 - loss 5.29589371 - samples/sec: 57.95 - lr: 0.100000
2021-05-25 02:22:22,768 epoch 8 - iter 77/110 - loss 5.23752170 - samples/sec: 58.32 - lr: 0.100000
2021-05-25 02:22:25,800 epoch 8 - iter 88/110 - loss 5.21902462 - samples/sec: 58.06 - lr: 0.100000
2021-05-25 02:22:28,827 epoch 8 - iter 99/110 - loss 5.21275514 - samples/sec: 58.16 - lr: 0.100000
2021-05-25 02:22:31,746 epoch 8 - iter 110/110 - loss 5.20286205 - samples/sec: 60.31 - lr: 0.100000
2021-05-25 02:22:31,746 ----------------------------------------------------------------------------------------------------
2021-05-25 02:22:31,746 EPOCH 8 done: loss 5.2029 - lr 0.1000000
2021-05-25 02:22:33,126 DEV : loss 4.0224199295043945 - score 0.8157
2021-05-25 02:22:33,150 BAD EPOCHS (no improvement): 2
2021-05-25 02:22:33,150 ----------------------------------------------------------------------------------------------------
2021-05-25 02:22:36,207 epoch 9 - iter 11/110 - loss 5.01446184 - samples/sec: 57.59 - lr: 0.100000
2021-05-25 02:22:39,246 epoch 9 - iter 22/110 - loss 4.91557174 - samples/sec: 57.93 - lr: 0.100000
2021-05-25 02:22:42,285 epoch 9 - iter 33/110 - loss 5.06736681 - samples/sec: 57.92 - lr: 0.100000
2021-05-25 02:22:45,295 epoch 9 - iter 44/110 - loss 5.00741994 - samples/sec: 58.48 - lr: 0.100000
2021-05-25 02:22:48,308 epoch 9 - iter 55/110 - loss 5.06036958 - samples/sec: 58.42 - lr: 0.100000
2021-05-25 02:22:51,312 epoch 9 - iter 66/110 - loss 5.11467796 - samples/sec: 58.61 - lr: 0.100000
2021-05-25 02:22:54,364 epoch 9 - iter 77/110 - loss 5.11643532 - samples/sec: 57.68 - lr: 0.100000
2021-05-25 02:22:57,391 epoch 9 - iter 88/110 - loss 5.16354476 - samples/sec: 58.16 - lr: 0.100000
2021-05-25 02:23:00,435 epoch 9 - iter 99/110 - loss 5.16274229 - samples/sec: 57.83 - lr: 0.100000
2021-05-25 02:23:03,403 epoch 9 - iter 110/110 - loss 5.14973157 - samples/sec: 59.30 - lr: 0.100000
2021-05-25 02:23:03,404 ----------------------------------------------------------------------------------------------------
2021-05-25 02:23:03,404 EPOCH 9 done: loss 5.1497 - lr 0.1000000
2021-05-25 02:23:04,781 DEV : loss 3.7457451820373535 - score 0.8291
2021-05-25 02:23:04,804 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:23:14,439 ----------------------------------------------------------------------------------------------------
2021-05-25 02:23:17,461 epoch 10 - iter 11/110 - loss 4.78412095 - samples/sec: 58.26 - lr: 0.100000
2021-05-25 02:23:20,504 epoch 10 - iter 22/110 - loss 4.93769226 - samples/sec: 57.85 - lr: 0.100000
2021-05-25 02:23:23,551 epoch 10 - iter 33/110 - loss 5.10881822 - samples/sec: 57.78 - lr: 0.100000
2021-05-25 02:23:26,611 epoch 10 - iter 44/110 - loss 5.23128329 - samples/sec: 57.52 - lr: 0.100000
2021-05-25 02:23:29,646 epoch 10 - iter 55/110 - loss 5.20612858 - samples/sec: 58.00 - lr: 0.100000
2021-05-25 02:23:32,711 epoch 10 - iter 66/110 - loss 5.14660012 - samples/sec: 57.44 - lr: 0.100000
2021-05-25 02:23:35,732 epoch 10 - iter 77/110 - loss 5.13732548 - samples/sec: 58.26 - lr: 0.100000
2021-05-25 02:23:38,755 epoch 10 - iter 88/110 - loss 5.10222663 - samples/sec: 58.24 - lr: 0.100000
2021-05-25 02:23:41,774 epoch 10 - iter 99/110 - loss 5.07724736 - samples/sec: 58.31 - lr: 0.100000
2021-05-25 02:23:44,673 epoch 10 - iter 110/110 - loss 5.10136718 - samples/sec: 60.73 - lr: 0.100000
2021-05-25 02:23:44,673 ----------------------------------------------------------------------------------------------------
2021-05-25 02:23:44,673 EPOCH 10 done: loss 5.1014 - lr 0.1000000
2021-05-25 02:23:46,050 DEV : loss 4.3038740158081055 - score 0.8004
2021-05-25 02:23:46,073 BAD EPOCHS (no improvement): 1
2021-05-25 02:23:46,074 ----------------------------------------------------------------------------------------------------
2021-05-25 02:23:49,094 epoch 11 - iter 11/110 - loss 5.03670621 - samples/sec: 58.30 - lr: 0.100000
2021-05-25 02:23:52,142 epoch 11 - iter 22/110 - loss 4.95716012 - samples/sec: 57.74 - lr: 0.100000
2021-05-25 02:23:55,198 epoch 11 - iter 33/110 - loss 4.95422124 - samples/sec: 57.61 - lr: 0.100000
2021-05-25 02:23:58,225 epoch 11 - iter 44/110 - loss 4.90626273 - samples/sec: 58.16 - lr: 0.100000
2021-05-25 02:24:01,281 epoch 11 - iter 55/110 - loss 4.84827416 - samples/sec: 57.60 - lr: 0.100000
2021-05-25 02:24:04,297 epoch 11 - iter 66/110 - loss 4.84149344 - samples/sec: 58.36 - lr: 0.100000
2021-05-25 02:24:07,350 epoch 11 - iter 77/110 - loss 4.94163091 - samples/sec: 57.66 - lr: 0.100000
2021-05-25 02:24:10,370 epoch 11 - iter 88/110 - loss 4.96389564 - samples/sec: 58.29 - lr: 0.100000
2021-05-25 02:24:13,421 epoch 11 - iter 99/110 - loss 4.99440532 - samples/sec: 57.70 - lr: 0.100000
2021-05-25 02:24:16,341 epoch 11 - iter 110/110 - loss 4.97662862 - samples/sec: 60.28 - lr: 0.100000
2021-05-25 02:24:16,342 ----------------------------------------------------------------------------------------------------
2021-05-25 02:24:16,342 EPOCH 11 done: loss 4.9766 - lr 0.1000000
2021-05-25 02:24:17,718 DEV : loss 3.7543160915374756 - score 0.8242
2021-05-25 02:24:17,742 BAD EPOCHS (no improvement): 2
2021-05-25 02:24:17,742 ----------------------------------------------------------------------------------------------------
2021-05-25 02:24:20,789 epoch 12 - iter 11/110 - loss 5.38496377 - samples/sec: 57.79 - lr: 0.100000
2021-05-25 02:24:23,793 epoch 12 - iter 22/110 - loss 5.26246159 - samples/sec: 58.59 - lr: 0.100000
2021-05-25 02:24:26,801 epoch 12 - iter 33/110 - loss 5.23430925 - samples/sec: 58.54 - lr: 0.100000
2021-05-25 02:24:29,799 epoch 12 - iter 44/110 - loss 5.21676002 - samples/sec: 58.70 - lr: 0.100000
2021-05-25 02:24:32,830 epoch 12 - iter 55/110 - loss 5.22061949 - samples/sec: 58.09 - lr: 0.100000
2021-05-25 02:24:35,851 epoch 12 - iter 66/110 - loss 5.23079586 - samples/sec: 58.27 - lr: 0.100000
2021-05-25 02:24:38,859 epoch 12 - iter 77/110 - loss 5.14826136 - samples/sec: 58.53 - lr: 0.100000
2021-05-25 02:24:41,857 epoch 12 - iter 88/110 - loss 5.16262685 - samples/sec: 58.72 - lr: 0.100000
2021-05-25 02:24:44,861 epoch 12 - iter 99/110 - loss 5.12991991 - samples/sec: 58.60 - lr: 0.100000
2021-05-25 02:24:47,743 epoch 12 - iter 110/110 - loss 5.07352896 - samples/sec: 61.07 - lr: 0.100000
2021-05-25 02:24:47,744 ----------------------------------------------------------------------------------------------------
2021-05-25 02:24:47,744 EPOCH 12 done: loss 5.0735 - lr 0.1000000
2021-05-25 02:24:49,119 DEV : loss 3.764756917953491 - score 0.8108
2021-05-25 02:24:49,143 BAD EPOCHS (no improvement): 3
2021-05-25 02:24:49,143 ----------------------------------------------------------------------------------------------------
2021-05-25 02:24:52,134 epoch 13 - iter 11/110 - loss 4.80754074 - samples/sec: 58.86 - lr: 0.100000
2021-05-25 02:24:55,135 epoch 13 - iter 22/110 - loss 4.76480082 - samples/sec: 58.65 - lr: 0.100000
2021-05-25 02:24:58,126 epoch 13 - iter 33/110 - loss 4.95688402 - samples/sec: 58.85 - lr: 0.100000
2021-05-25 02:25:01,161 epoch 13 - iter 44/110 - loss 4.92263307 - samples/sec: 57.99 - lr: 0.100000
2021-05-25 02:25:04,166 epoch 13 - iter 55/110 - loss 4.95808413 - samples/sec: 58.59 - lr: 0.100000
2021-05-25 02:25:07,154 epoch 13 - iter 66/110 - loss 5.01762459 - samples/sec: 58.91 - lr: 0.100000
2021-05-25 02:25:10,173 epoch 13 - iter 77/110 - loss 5.02099155 - samples/sec: 58.31 - lr: 0.100000
2021-05-25 02:25:13,173 epoch 13 - iter 88/110 - loss 5.02112108 - samples/sec: 58.68 - lr: 0.100000
2021-05-25 02:25:16,162 epoch 13 - iter 99/110 - loss 4.98824902 - samples/sec: 58.90 - lr: 0.100000
2021-05-25 02:25:19,028 epoch 13 - iter 110/110 - loss 5.00320454 - samples/sec: 61.43 - lr: 0.100000
2021-05-25 02:25:19,028 ----------------------------------------------------------------------------------------------------
2021-05-25 02:25:19,028 EPOCH 13 done: loss 5.0032 - lr 0.1000000
2021-05-25 02:25:20,404 DEV : loss 3.8405065536499023 - score 0.8308
2021-05-25 02:25:20,428 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:25:30,701 ----------------------------------------------------------------------------------------------------
2021-05-25 02:25:33,742 epoch 14 - iter 11/110 - loss 4.71881333 - samples/sec: 57.91 - lr: 0.100000
2021-05-25 02:25:36,789 epoch 14 - iter 22/110 - loss 4.62829388 - samples/sec: 57.77 - lr: 0.100000
2021-05-25 02:25:39,798 epoch 14 - iter 33/110 - loss 4.67777051 - samples/sec: 58.51 - lr: 0.100000
2021-05-25 02:25:42,823 epoch 14 - iter 44/110 - loss 4.71066532 - samples/sec: 58.19 - lr: 0.100000
2021-05-25 02:25:45,849 epoch 14 - iter 55/110 - loss 4.72922004 - samples/sec: 58.17 - lr: 0.100000
2021-05-25 02:25:48,887 epoch 14 - iter 66/110 - loss 4.74144107 - samples/sec: 57.96 - lr: 0.100000
2021-05-25 02:25:51,913 epoch 14 - iter 77/110 - loss 4.72414045 - samples/sec: 58.16 - lr: 0.100000
2021-05-25 02:25:54,954 epoch 14 - iter 88/110 - loss 4.77814434 - samples/sec: 57.90 - lr: 0.100000
2021-05-25 02:25:57,981 epoch 14 - iter 99/110 - loss 4.82161825 - samples/sec: 58.15 - lr: 0.100000
2021-05-25 02:26:00,874 epoch 14 - iter 110/110 - loss 4.84637566 - samples/sec: 60.86 - lr: 0.100000
2021-05-25 02:26:00,874 ----------------------------------------------------------------------------------------------------
2021-05-25 02:26:00,874 EPOCH 14 done: loss 4.8464 - lr 0.1000000
2021-05-25 02:26:02,259 DEV : loss 3.8794097900390625 - score 0.8292
2021-05-25 02:26:02,283 BAD EPOCHS (no improvement): 1
2021-05-25 02:26:02,283 ----------------------------------------------------------------------------------------------------
2021-05-25 02:26:05,342 epoch 15 - iter 11/110 - loss 4.88163471 - samples/sec: 57.56 - lr: 0.100000
2021-05-25 02:26:08,407 epoch 15 - iter 22/110 - loss 4.62575782 - samples/sec: 57.42 - lr: 0.100000
2021-05-25 02:26:11,418 epoch 15 - iter 33/110 - loss 4.62326957 - samples/sec: 58.47 - lr: 0.100000
2021-05-25 02:26:14,372 epoch 15 - iter 44/110 - loss 4.69774241 - samples/sec: 59.59 - lr: 0.100000
2021-05-25 02:26:17,376 epoch 15 - iter 55/110 - loss 4.83488483 - samples/sec: 58.60 - lr: 0.100000
2021-05-25 02:26:20,385 epoch 15 - iter 66/110 - loss 4.82884652 - samples/sec: 58.51 - lr: 0.100000
2021-05-25 02:26:23,370 epoch 15 - iter 77/110 - loss 4.85922854 - samples/sec: 58.98 - lr: 0.100000
2021-05-25 02:26:26,401 epoch 15 - iter 88/110 - loss 4.91717031 - samples/sec: 58.07 - lr: 0.100000
2021-05-25 02:26:29,433 epoch 15 - iter 99/110 - loss 4.89982641 - samples/sec: 58.06 - lr: 0.100000
2021-05-25 02:26:32,356 epoch 15 - iter 110/110 - loss 4.87354340 - samples/sec: 60.23 - lr: 0.100000
2021-05-25 02:26:32,356 ----------------------------------------------------------------------------------------------------
2021-05-25 02:26:32,356 EPOCH 15 done: loss 4.8735 - lr 0.1000000
2021-05-25 02:26:33,736 DEV : loss 3.982541799545288 - score 0.8152
2021-05-25 02:26:33,760 BAD EPOCHS (no improvement): 2
2021-05-25 02:26:33,760 ----------------------------------------------------------------------------------------------------
2021-05-25 02:26:36,784 epoch 16 - iter 11/110 - loss 4.77244592 - samples/sec: 58.22 - lr: 0.100000
2021-05-25 02:26:39,797 epoch 16 - iter 22/110 - loss 4.82504657 - samples/sec: 58.42 - lr: 0.100000
2021-05-25 02:26:42,807 epoch 16 - iter 33/110 - loss 4.84299673 - samples/sec: 58.50 - lr: 0.100000
2021-05-25 02:26:45,839 epoch 16 - iter 44/110 - loss 4.87994045 - samples/sec: 58.07 - lr: 0.100000
2021-05-25 02:26:48,855 epoch 16 - iter 55/110 - loss 4.87605143 - samples/sec: 58.36 - lr: 0.100000
2021-05-25 02:26:51,887 epoch 16 - iter 66/110 - loss 4.87346408 - samples/sec: 58.05 - lr: 0.100000
2021-05-25 02:26:54,918 epoch 16 - iter 77/110 - loss 4.90758539 - samples/sec: 58.09 - lr: 0.100000
2021-05-25 02:26:57,961 epoch 16 - iter 88/110 - loss 4.86257419 - samples/sec: 57.84 - lr: 0.100000
2021-05-25 02:27:01,006 epoch 16 - iter 99/110 - loss 4.88768244 - samples/sec: 57.82 - lr: 0.100000
2021-05-25 02:27:03,909 epoch 16 - iter 110/110 - loss 4.88932409 - samples/sec: 60.63 - lr: 0.100000
2021-05-25 02:27:03,910 ----------------------------------------------------------------------------------------------------
2021-05-25 02:27:03,910 EPOCH 16 done: loss 4.8893 - lr 0.1000000
2021-05-25 02:27:05,283 DEV : loss 3.789557695388794 - score 0.8065
2021-05-25 02:27:05,307 BAD EPOCHS (no improvement): 3
2021-05-25 02:27:05,307 ----------------------------------------------------------------------------------------------------
2021-05-25 02:27:08,326 epoch 17 - iter 11/110 - loss 5.10309974 - samples/sec: 58.32 - lr: 0.100000
2021-05-25 02:27:11,334 epoch 17 - iter 22/110 - loss 4.99981607 - samples/sec: 58.52 - lr: 0.100000
2021-05-25 02:27:14,367 epoch 17 - iter 33/110 - loss 4.97843235 - samples/sec: 58.03 - lr: 0.100000
2021-05-25 02:27:17,392 epoch 17 - iter 44/110 - loss 4.85525155 - samples/sec: 58.20 - lr: 0.100000
2021-05-25 02:27:20,409 epoch 17 - iter 55/110 - loss 4.78844642 - samples/sec: 58.36 - lr: 0.100000
2021-05-25 02:27:23,443 epoch 17 - iter 66/110 - loss 4.72744539 - samples/sec: 58.02 - lr: 0.100000
2021-05-25 02:27:26,462 epoch 17 - iter 77/110 - loss 4.78171293 - samples/sec: 58.30 - lr: 0.100000
2021-05-25 02:27:29,510 epoch 17 - iter 88/110 - loss 4.78017447 - samples/sec: 57.77 - lr: 0.100000
2021-05-25 02:27:32,539 epoch 17 - iter 99/110 - loss 4.78144308 - samples/sec: 58.10 - lr: 0.100000
2021-05-25 02:27:35,420 epoch 17 - iter 110/110 - loss 4.78305275 - samples/sec: 61.11 - lr: 0.100000
2021-05-25 02:27:35,420 ----------------------------------------------------------------------------------------------------
2021-05-25 02:27:35,420 EPOCH 17 done: loss 4.7831 - lr 0.1000000
2021-05-25 02:27:36,797 DEV : loss 3.4621877670288086 - score 0.8383
2021-05-25 02:27:36,821 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:27:46,540 ----------------------------------------------------------------------------------------------------
2021-05-25 02:27:49,565 epoch 18 - iter 11/110 - loss 4.64085683 - samples/sec: 58.22 - lr: 0.100000
2021-05-25 02:27:52,562 epoch 18 - iter 22/110 - loss 4.72117994 - samples/sec: 58.72 - lr: 0.100000
2021-05-25 02:27:55,584 epoch 18 - iter 33/110 - loss 4.80632246 - samples/sec: 58.25 - lr: 0.100000
2021-05-25 02:27:58,616 epoch 18 - iter 44/110 - loss 4.91985265 - samples/sec: 58.07 - lr: 0.100000
2021-05-25 02:28:01,647 epoch 18 - iter 55/110 - loss 4.96373062 - samples/sec: 58.08 - lr: 0.100000
2021-05-25 02:28:04,699 epoch 18 - iter 66/110 - loss 4.88686977 - samples/sec: 57.68 - lr: 0.100000
2021-05-25 02:28:07,747 epoch 18 - iter 77/110 - loss 4.79641917 - samples/sec: 57.75 - lr: 0.100000
2021-05-25 02:28:10,799 epoch 18 - iter 88/110 - loss 4.81202081 - samples/sec: 57.67 - lr: 0.100000
2021-05-25 02:28:13,855 epoch 18 - iter 99/110 - loss 4.80327326 - samples/sec: 57.60 - lr: 0.100000
2021-05-25 02:28:16,804 epoch 18 - iter 110/110 - loss 4.84627217 - samples/sec: 59.69 - lr: 0.100000
2021-05-25 02:28:16,804 ----------------------------------------------------------------------------------------------------
2021-05-25 02:28:16,805 EPOCH 18 done: loss 4.8463 - lr 0.1000000
2021-05-25 02:28:18,181 DEV : loss 3.5398712158203125 - score 0.8301
2021-05-25 02:28:18,205 BAD EPOCHS (no improvement): 1
2021-05-25 02:28:18,205 ----------------------------------------------------------------------------------------------------
2021-05-25 02:28:21,259 epoch 19 - iter 11/110 - loss 4.71497011 - samples/sec: 57.65 - lr: 0.100000
2021-05-25 02:28:24,303 epoch 19 - iter 22/110 - loss 4.84877593 - samples/sec: 57.82 - lr: 0.100000
2021-05-25 02:28:27,357 epoch 19 - iter 33/110 - loss 4.77825863 - samples/sec: 57.64 - lr: 0.100000
2021-05-25 02:28:30,415 epoch 19 - iter 44/110 - loss 4.87203788 - samples/sec: 57.57 - lr: 0.100000
2021-05-25 02:28:33,475 epoch 19 - iter 55/110 - loss 4.85415604 - samples/sec: 57.53 - lr: 0.100000
2021-05-25 02:28:36,538 epoch 19 - iter 66/110 - loss 4.84141488 - samples/sec: 57.46 - lr: 0.100000
2021-05-25 02:28:39,586 epoch 19 - iter 77/110 - loss 4.81730550 - samples/sec: 57.76 - lr: 0.100000
2021-05-25 02:28:42,648 epoch 19 - iter 88/110 - loss 4.75854208 - samples/sec: 57.49 - lr: 0.100000
2021-05-25 02:28:45,715 epoch 19 - iter 99/110 - loss 4.79467268 - samples/sec: 57.39 - lr: 0.100000
2021-05-25 02:28:48,628 epoch 19 - iter 110/110 - loss 4.75640118 - samples/sec: 60.43 - lr: 0.100000
2021-05-25 02:28:48,629 ----------------------------------------------------------------------------------------------------
2021-05-25 02:28:48,629 EPOCH 19 done: loss 4.7564 - lr 0.1000000
2021-05-25 02:28:50,009 DEV : loss 3.7121872901916504 - score 0.8356
2021-05-25 02:28:50,033 BAD EPOCHS (no improvement): 2
2021-05-25 02:28:50,033 ----------------------------------------------------------------------------------------------------
2021-05-25 02:28:53,078 epoch 20 - iter 11/110 - loss 4.43334061 - samples/sec: 57.81 - lr: 0.100000
2021-05-25 02:28:56,131 epoch 20 - iter 22/110 - loss 4.47974145 - samples/sec: 57.65 - lr: 0.100000
2021-05-25 02:28:59,171 epoch 20 - iter 33/110 - loss 4.64188783 - samples/sec: 57.92 - lr: 0.100000
2021-05-25 02:29:02,201 epoch 20 - iter 44/110 - loss 4.73130383 - samples/sec: 58.09 - lr: 0.100000
2021-05-25 02:29:05,246 epoch 20 - iter 55/110 - loss 4.75734002 - samples/sec: 57.81 - lr: 0.100000
2021-05-25 02:29:08,308 epoch 20 - iter 66/110 - loss 4.71525926 - samples/sec: 57.50 - lr: 0.100000
2021-05-25 02:29:11,378 epoch 20 - iter 77/110 - loss 4.78972278 - samples/sec: 57.34 - lr: 0.100000
2021-05-25 02:29:14,443 epoch 20 - iter 88/110 - loss 4.78385041 - samples/sec: 57.43 - lr: 0.100000
2021-05-25 02:29:17,488 epoch 20 - iter 99/110 - loss 4.76391590 - samples/sec: 57.82 - lr: 0.100000
2021-05-25 02:29:20,410 epoch 20 - iter 110/110 - loss 4.80567338 - samples/sec: 60.25 - lr: 0.100000
2021-05-25 02:29:20,410 ----------------------------------------------------------------------------------------------------
2021-05-25 02:29:20,410 EPOCH 20 done: loss 4.8057 - lr 0.1000000
2021-05-25 02:29:21,786 DEV : loss 3.605431318283081 - score 0.8256
2021-05-25 02:29:21,810 BAD EPOCHS (no improvement): 3
2021-05-25 02:29:21,810 ----------------------------------------------------------------------------------------------------
2021-05-25 02:29:24,865 epoch 21 - iter 11/110 - loss 4.41288833 - samples/sec: 57.62 - lr: 0.100000
2021-05-25 02:29:27,921 epoch 21 - iter 22/110 - loss 4.63743040 - samples/sec: 57.62 - lr: 0.100000
2021-05-25 02:29:30,979 epoch 21 - iter 33/110 - loss 4.65879645 - samples/sec: 57.56 - lr: 0.100000
2021-05-25 02:29:34,043 epoch 21 - iter 44/110 - loss 4.73020590 - samples/sec: 57.46 - lr: 0.100000
2021-05-25 02:29:37,105 epoch 21 - iter 55/110 - loss 4.70501595 - samples/sec: 57.49 - lr: 0.100000
2021-05-25 02:29:40,158 epoch 21 - iter 66/110 - loss 4.72022355 - samples/sec: 57.65 - lr: 0.100000
2021-05-25 02:29:43,224 epoch 21 - iter 77/110 - loss 4.69344942 - samples/sec: 57.43 - lr: 0.100000
2021-05-25 02:29:46,258 epoch 21 - iter 88/110 - loss 4.69600936 - samples/sec: 58.02 - lr: 0.100000
2021-05-25 02:29:49,315 epoch 21 - iter 99/110 - loss 4.72143185 - samples/sec: 57.58 - lr: 0.100000
2021-05-25 02:29:52,238 epoch 21 - iter 110/110 - loss 4.70834021 - samples/sec: 60.23 - lr: 0.100000
2021-05-25 02:29:52,238 ----------------------------------------------------------------------------------------------------
2021-05-25 02:29:52,238 EPOCH 21 done: loss 4.7083 - lr 0.1000000
2021-05-25 02:29:53,617 DEV : loss 4.235897064208984 - score 0.8198
Epoch    21: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 02:29:53,641 BAD EPOCHS (no improvement): 4
2021-05-25 02:29:53,641 ----------------------------------------------------------------------------------------------------
2021-05-25 02:29:56,670 epoch 22 - iter 11/110 - loss 4.58153283 - samples/sec: 58.12 - lr: 0.050000
2021-05-25 02:29:59,722 epoch 22 - iter 22/110 - loss 4.37836044 - samples/sec: 57.68 - lr: 0.050000
2021-05-25 02:30:02,785 epoch 22 - iter 33/110 - loss 4.40513591 - samples/sec: 57.48 - lr: 0.050000
2021-05-25 02:30:05,831 epoch 22 - iter 44/110 - loss 4.47725344 - samples/sec: 57.79 - lr: 0.050000
2021-05-25 02:30:08,891 epoch 22 - iter 55/110 - loss 4.44791285 - samples/sec: 57.52 - lr: 0.050000
2021-05-25 02:30:11,937 epoch 22 - iter 66/110 - loss 4.46006006 - samples/sec: 57.79 - lr: 0.050000
2021-05-25 02:30:14,980 epoch 22 - iter 77/110 - loss 4.49348934 - samples/sec: 57.85 - lr: 0.050000
2021-05-25 02:30:18,032 epoch 22 - iter 88/110 - loss 4.47212677 - samples/sec: 57.68 - lr: 0.050000
2021-05-25 02:30:21,061 epoch 22 - iter 99/110 - loss 4.41923210 - samples/sec: 58.12 - lr: 0.050000
2021-05-25 02:30:23,965 epoch 22 - iter 110/110 - loss 4.40519195 - samples/sec: 60.61 - lr: 0.050000
2021-05-25 02:30:23,965 ----------------------------------------------------------------------------------------------------
2021-05-25 02:30:23,965 EPOCH 22 done: loss 4.4052 - lr 0.0500000
2021-05-25 02:30:25,341 DEV : loss 3.472860336303711 - score 0.838
2021-05-25 02:30:25,365 BAD EPOCHS (no improvement): 1
2021-05-25 02:30:25,365 ----------------------------------------------------------------------------------------------------
2021-05-25 02:30:28,409 epoch 23 - iter 11/110 - loss 4.45464724 - samples/sec: 57.84 - lr: 0.050000
2021-05-25 02:30:31,445 epoch 23 - iter 22/110 - loss 4.46880618 - samples/sec: 57.99 - lr: 0.050000
2021-05-25 02:30:34,489 epoch 23 - iter 33/110 - loss 4.44529538 - samples/sec: 57.81 - lr: 0.050000
2021-05-25 02:30:37,495 epoch 23 - iter 44/110 - loss 4.44789587 - samples/sec: 58.56 - lr: 0.050000
2021-05-25 02:30:40,517 epoch 23 - iter 55/110 - loss 4.47314015 - samples/sec: 58.25 - lr: 0.050000
2021-05-25 02:30:43,536 epoch 23 - iter 66/110 - loss 4.41435014 - samples/sec: 58.31 - lr: 0.050000
2021-05-25 02:30:46,532 epoch 23 - iter 77/110 - loss 4.41273564 - samples/sec: 58.77 - lr: 0.050000
2021-05-25 02:30:49,544 epoch 23 - iter 88/110 - loss 4.37185806 - samples/sec: 58.44 - lr: 0.050000
2021-05-25 02:30:52,537 epoch 23 - iter 99/110 - loss 4.35878856 - samples/sec: 58.81 - lr: 0.050000
2021-05-25 02:30:55,405 epoch 23 - iter 110/110 - loss 4.38501516 - samples/sec: 61.40 - lr: 0.050000
2021-05-25 02:30:55,405 ----------------------------------------------------------------------------------------------------
2021-05-25 02:30:55,405 EPOCH 23 done: loss 4.3850 - lr 0.0500000
2021-05-25 02:30:56,784 DEV : loss 3.5813279151916504 - score 0.8391
2021-05-25 02:30:56,808 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:31:06,685 ----------------------------------------------------------------------------------------------------
2021-05-25 02:31:09,713 epoch 24 - iter 11/110 - loss 4.14753810 - samples/sec: 58.14 - lr: 0.050000
2021-05-25 02:31:12,715 epoch 24 - iter 22/110 - loss 4.26581902 - samples/sec: 58.64 - lr: 0.050000
2021-05-25 02:31:15,746 epoch 24 - iter 33/110 - loss 4.27187652 - samples/sec: 58.09 - lr: 0.050000
2021-05-25 02:31:18,785 epoch 24 - iter 44/110 - loss 4.25266267 - samples/sec: 57.91 - lr: 0.050000
2021-05-25 02:31:21,818 epoch 24 - iter 55/110 - loss 4.25938562 - samples/sec: 58.04 - lr: 0.050000
2021-05-25 02:31:24,875 epoch 24 - iter 66/110 - loss 4.25556288 - samples/sec: 57.58 - lr: 0.050000
2021-05-25 02:31:27,930 epoch 24 - iter 77/110 - loss 4.26943921 - samples/sec: 57.62 - lr: 0.050000
2021-05-25 02:31:30,961 epoch 24 - iter 88/110 - loss 4.28054171 - samples/sec: 58.07 - lr: 0.050000
2021-05-25 02:31:33,997 epoch 24 - iter 99/110 - loss 4.27878481 - samples/sec: 57.99 - lr: 0.050000
2021-05-25 02:31:36,919 epoch 24 - iter 110/110 - loss 4.28409301 - samples/sec: 60.25 - lr: 0.050000
2021-05-25 02:31:36,919 ----------------------------------------------------------------------------------------------------
2021-05-25 02:31:36,919 EPOCH 24 done: loss 4.2841 - lr 0.0500000
2021-05-25 02:31:38,296 DEV : loss 3.418116331100464 - score 0.8396
2021-05-25 02:31:38,320 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:31:47,993 ----------------------------------------------------------------------------------------------------
2021-05-25 02:31:51,041 epoch 25 - iter 11/110 - loss 4.30542666 - samples/sec: 57.77 - lr: 0.050000
2021-05-25 02:31:54,093 epoch 25 - iter 22/110 - loss 4.26910794 - samples/sec: 57.67 - lr: 0.050000
2021-05-25 02:31:57,132 epoch 25 - iter 33/110 - loss 4.22229382 - samples/sec: 57.94 - lr: 0.050000
2021-05-25 02:32:00,160 epoch 25 - iter 44/110 - loss 4.31952715 - samples/sec: 58.12 - lr: 0.050000
2021-05-25 02:32:03,154 epoch 25 - iter 55/110 - loss 4.29775313 - samples/sec: 58.81 - lr: 0.050000
2021-05-25 02:32:06,197 epoch 25 - iter 66/110 - loss 4.29793960 - samples/sec: 57.84 - lr: 0.050000
2021-05-25 02:32:09,208 epoch 25 - iter 77/110 - loss 4.37387176 - samples/sec: 58.48 - lr: 0.050000
2021-05-25 02:32:12,239 epoch 25 - iter 88/110 - loss 4.40959525 - samples/sec: 58.08 - lr: 0.050000
2021-05-25 02:32:15,292 epoch 25 - iter 99/110 - loss 4.38327667 - samples/sec: 57.65 - lr: 0.050000
2021-05-25 02:32:18,197 epoch 25 - iter 110/110 - loss 4.35048135 - samples/sec: 60.61 - lr: 0.050000
2021-05-25 02:32:18,197 ----------------------------------------------------------------------------------------------------
2021-05-25 02:32:18,197 EPOCH 25 done: loss 4.3505 - lr 0.0500000
2021-05-25 02:32:19,574 DEV : loss 3.409126043319702 - score 0.8435
2021-05-25 02:32:19,598 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:32:29,548 ----------------------------------------------------------------------------------------------------
2021-05-25 02:32:32,580 epoch 26 - iter 11/110 - loss 4.35790079 - samples/sec: 58.07 - lr: 0.050000
2021-05-25 02:32:35,626 epoch 26 - iter 22/110 - loss 4.27990907 - samples/sec: 57.79 - lr: 0.050000
2021-05-25 02:32:38,656 epoch 26 - iter 33/110 - loss 4.23495164 - samples/sec: 58.10 - lr: 0.050000
2021-05-25 02:32:41,725 epoch 26 - iter 44/110 - loss 4.23196864 - samples/sec: 57.37 - lr: 0.050000
2021-05-25 02:32:44,744 epoch 26 - iter 55/110 - loss 4.27262834 - samples/sec: 58.30 - lr: 0.050000
2021-05-25 02:32:47,766 epoch 26 - iter 66/110 - loss 4.25166251 - samples/sec: 58.26 - lr: 0.050000
2021-05-25 02:32:50,840 epoch 26 - iter 77/110 - loss 4.23011187 - samples/sec: 57.26 - lr: 0.050000
2021-05-25 02:32:53,875 epoch 26 - iter 88/110 - loss 4.22821977 - samples/sec: 58.01 - lr: 0.050000
2021-05-25 02:32:56,895 epoch 26 - iter 99/110 - loss 4.23852600 - samples/sec: 58.29 - lr: 0.050000
2021-05-25 02:32:59,793 epoch 26 - iter 110/110 - loss 4.24838963 - samples/sec: 60.74 - lr: 0.050000
2021-05-25 02:32:59,794 ----------------------------------------------------------------------------------------------------
2021-05-25 02:32:59,794 EPOCH 26 done: loss 4.2484 - lr 0.0500000
2021-05-25 02:33:01,170 DEV : loss 3.4365968704223633 - score 0.8418
2021-05-25 02:33:01,195 BAD EPOCHS (no improvement): 1
2021-05-25 02:33:01,195 ----------------------------------------------------------------------------------------------------
2021-05-25 02:33:04,227 epoch 27 - iter 11/110 - loss 4.12341933 - samples/sec: 58.06 - lr: 0.050000
2021-05-25 02:33:07,281 epoch 27 - iter 22/110 - loss 4.08651582 - samples/sec: 57.64 - lr: 0.050000
2021-05-25 02:33:10,317 epoch 27 - iter 33/110 - loss 4.14035598 - samples/sec: 57.98 - lr: 0.050000
2021-05-25 02:33:13,355 epoch 27 - iter 44/110 - loss 4.13226847 - samples/sec: 57.96 - lr: 0.050000
2021-05-25 02:33:16,367 epoch 27 - iter 55/110 - loss 4.17857629 - samples/sec: 58.43 - lr: 0.050000
2021-05-25 02:33:19,341 epoch 27 - iter 66/110 - loss 4.13390734 - samples/sec: 59.20 - lr: 0.050000
2021-05-25 02:33:22,353 epoch 27 - iter 77/110 - loss 4.12182843 - samples/sec: 58.44 - lr: 0.050000
2021-05-25 02:33:25,362 epoch 27 - iter 88/110 - loss 4.19391663 - samples/sec: 58.50 - lr: 0.050000
2021-05-25 02:33:28,404 epoch 27 - iter 99/110 - loss 4.22610732 - samples/sec: 57.88 - lr: 0.050000
2021-05-25 02:33:31,289 epoch 27 - iter 110/110 - loss 4.26171192 - samples/sec: 61.02 - lr: 0.050000
2021-05-25 02:33:31,289 ----------------------------------------------------------------------------------------------------
2021-05-25 02:33:31,289 EPOCH 27 done: loss 4.2617 - lr 0.0500000
2021-05-25 02:33:32,667 DEV : loss 3.6196651458740234 - score 0.8377
2021-05-25 02:33:32,691 BAD EPOCHS (no improvement): 2
2021-05-25 02:33:32,692 ----------------------------------------------------------------------------------------------------
2021-05-25 02:33:41,969 epoch 28 - iter 11/110 - loss 4.20292304 - samples/sec: 18.97 - lr: 0.050000
2021-05-25 02:33:44,995 epoch 28 - iter 22/110 - loss 4.26179619 - samples/sec: 58.16 - lr: 0.050000
2021-05-25 02:33:48,047 epoch 28 - iter 33/110 - loss 4.23654488 - samples/sec: 57.68 - lr: 0.050000
2021-05-25 02:33:51,086 epoch 28 - iter 44/110 - loss 4.17214469 - samples/sec: 57.94 - lr: 0.050000
2021-05-25 02:33:54,131 epoch 28 - iter 55/110 - loss 4.13634560 - samples/sec: 57.80 - lr: 0.050000
2021-05-25 02:33:57,142 epoch 28 - iter 66/110 - loss 4.10337095 - samples/sec: 58.47 - lr: 0.050000
2021-05-25 02:34:00,142 epoch 28 - iter 77/110 - loss 4.11589569 - samples/sec: 58.69 - lr: 0.050000
2021-05-25 02:34:03,163 epoch 28 - iter 88/110 - loss 4.11380227 - samples/sec: 58.27 - lr: 0.050000
2021-05-25 02:34:06,178 epoch 28 - iter 99/110 - loss 4.11824596 - samples/sec: 58.38 - lr: 0.050000
2021-05-25 02:34:09,068 epoch 28 - iter 110/110 - loss 4.16716458 - samples/sec: 60.92 - lr: 0.050000
2021-05-25 02:34:09,068 ----------------------------------------------------------------------------------------------------
2021-05-25 02:34:09,068 EPOCH 28 done: loss 4.1672 - lr 0.0500000
2021-05-25 02:34:10,449 DEV : loss 3.779407024383545 - score 0.8393
2021-05-25 02:34:10,473 BAD EPOCHS (no improvement): 3
2021-05-25 02:34:10,473 ----------------------------------------------------------------------------------------------------
2021-05-25 02:34:13,479 epoch 29 - iter 11/110 - loss 4.26405499 - samples/sec: 58.57 - lr: 0.050000
2021-05-25 02:34:16,478 epoch 29 - iter 22/110 - loss 4.27627504 - samples/sec: 58.69 - lr: 0.050000
2021-05-25 02:34:19,486 epoch 29 - iter 33/110 - loss 4.28300419 - samples/sec: 58.53 - lr: 0.050000
2021-05-25 02:34:22,492 epoch 29 - iter 44/110 - loss 4.22554257 - samples/sec: 58.55 - lr: 0.050000
2021-05-25 02:34:25,496 epoch 29 - iter 55/110 - loss 4.26265089 - samples/sec: 58.61 - lr: 0.050000
2021-05-25 02:34:28,524 epoch 29 - iter 66/110 - loss 4.25829464 - samples/sec: 58.13 - lr: 0.050000
2021-05-25 02:34:31,544 epoch 29 - iter 77/110 - loss 4.26589583 - samples/sec: 58.30 - lr: 0.050000
2021-05-25 02:34:34,576 epoch 29 - iter 88/110 - loss 4.27980394 - samples/sec: 58.07 - lr: 0.050000
2021-05-25 02:34:37,577 epoch 29 - iter 99/110 - loss 4.23190078 - samples/sec: 58.65 - lr: 0.050000
2021-05-25 02:34:40,454 epoch 29 - iter 110/110 - loss 4.22957048 - samples/sec: 61.20 - lr: 0.050000
2021-05-25 02:34:40,454 ----------------------------------------------------------------------------------------------------
2021-05-25 02:34:40,454 EPOCH 29 done: loss 4.2296 - lr 0.0500000
2021-05-25 02:34:41,834 DEV : loss 3.414090394973755 - score 0.8421
Epoch    29: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 02:34:41,857 BAD EPOCHS (no improvement): 4
2021-05-25 02:34:41,858 ----------------------------------------------------------------------------------------------------
2021-05-25 02:34:44,883 epoch 30 - iter 11/110 - loss 4.11871377 - samples/sec: 58.18 - lr: 0.025000
2021-05-25 02:34:47,896 epoch 30 - iter 22/110 - loss 4.14063150 - samples/sec: 58.43 - lr: 0.025000
2021-05-25 02:34:50,890 epoch 30 - iter 33/110 - loss 4.18008034 - samples/sec: 58.79 - lr: 0.025000
2021-05-25 02:34:53,918 epoch 30 - iter 44/110 - loss 4.14654455 - samples/sec: 58.15 - lr: 0.025000
2021-05-25 02:34:56,905 epoch 30 - iter 55/110 - loss 4.13713681 - samples/sec: 58.93 - lr: 0.025000
2021-05-25 02:34:59,912 epoch 30 - iter 66/110 - loss 4.12538364 - samples/sec: 58.56 - lr: 0.025000
2021-05-25 02:35:02,930 epoch 30 - iter 77/110 - loss 4.07231015 - samples/sec: 58.32 - lr: 0.025000
2021-05-25 02:35:05,922 epoch 30 - iter 88/110 - loss 4.04554423 - samples/sec: 58.83 - lr: 0.025000
2021-05-25 02:35:08,945 epoch 30 - iter 99/110 - loss 4.04835113 - samples/sec: 58.24 - lr: 0.025000
2021-05-25 02:35:11,805 epoch 30 - iter 110/110 - loss 4.08015723 - samples/sec: 61.55 - lr: 0.025000
2021-05-25 02:35:11,805 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:11,805 EPOCH 30 done: loss 4.0802 - lr 0.0250000
2021-05-25 02:35:13,185 DEV : loss 3.3255200386047363 - score 0.8386
2021-05-25 02:35:13,209 BAD EPOCHS (no improvement): 1
2021-05-25 02:35:14,335 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:14,336 Testing using best model ...
2021-05-25 02:35:14,336 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/eng.sdrt.stac/best-model.pt
2021-05-25 02:35:24,592 0.8270	0.8111	0.8190
2021-05-25 02:35:24,592 
Results:
- F1-score (micro) 0.8190
- F1-score (macro) 0.8190

By class:
SENT       tp: 1095 - fp: 229 - fn: 255 - precision: 0.8270 - recall: 0.8111 - f1-score: 0.8190
2021-05-25 02:35:24,592 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb/
2021-05-25 02:35:24,620 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb
2021-05-25 02:35:24,620 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb/sent_train.txt
2021-05-25 02:35:24,623 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb/sent_dev.txt
2021-05-25 02:35:24,623 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb/sent_test.txt
Corpus: 501 train + 119 dev + 186 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-25 02:35:30,759 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,762 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-25 02:35:30,762 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,762 Corpus: "Corpus: 501 train + 119 dev + 186 test sentences"
2021-05-25 02:35:30,762 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,762 Parameters:
2021-05-25 02:35:30,762  - learning_rate: "0.1"
2021-05-25 02:35:30,762  - mini_batch_size: "16"
2021-05-25 02:35:30,762  - patience: "3"
2021-05-25 02:35:30,762  - anneal_factor: "0.5"
2021-05-25 02:35:30,762  - max_epochs: "30"
2021-05-25 02:35:30,762  - shuffle: "True"
2021-05-25 02:35:30,762  - train_with_dev: "False"
2021-05-25 02:35:30,762  - batch_growth_annealing: "False"
2021-05-25 02:35:30,762 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,762 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb"
2021-05-25 02:35:30,762 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,763 Device: cuda:0
2021-05-25 02:35:30,763 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:30,763 Embeddings storage mode: cpu
2021-05-25 02:35:30,764 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:33,109 epoch 1 - iter 3/32 - loss 8.70104027 - samples/sec: 20.47 - lr: 0.100000
2021-05-25 02:35:35,424 epoch 1 - iter 6/32 - loss 6.56512133 - samples/sec: 20.74 - lr: 0.100000
2021-05-25 02:35:37,754 epoch 1 - iter 9/32 - loss 5.40762711 - samples/sec: 20.60 - lr: 0.100000
2021-05-25 02:35:40,088 epoch 1 - iter 12/32 - loss 5.21569796 - samples/sec: 20.57 - lr: 0.100000
2021-05-25 02:35:42,443 epoch 1 - iter 15/32 - loss 4.95188936 - samples/sec: 20.39 - lr: 0.100000
2021-05-25 02:35:44,769 epoch 1 - iter 18/32 - loss 4.70614530 - samples/sec: 20.63 - lr: 0.100000
2021-05-25 02:35:47,092 epoch 1 - iter 21/32 - loss 4.53673662 - samples/sec: 20.67 - lr: 0.100000
2021-05-25 02:35:49,436 epoch 1 - iter 24/32 - loss 4.31323144 - samples/sec: 20.49 - lr: 0.100000
2021-05-25 02:35:51,768 epoch 1 - iter 27/32 - loss 4.16235584 - samples/sec: 20.58 - lr: 0.100000
2021-05-25 02:35:54,073 epoch 1 - iter 30/32 - loss 3.98973119 - samples/sec: 20.83 - lr: 0.100000
2021-05-25 02:35:55,134 ----------------------------------------------------------------------------------------------------
2021-05-25 02:35:55,134 EPOCH 1 done: loss 3.8884 - lr 0.1000000
2021-05-25 02:35:58,909 DEV : loss 3.264822006225586 - score 0.0
2021-05-25 02:35:58,921 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:36:00,110 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:00,959 epoch 2 - iter 3/32 - loss 2.44541693 - samples/sec: 56.56 - lr: 0.100000
2021-05-25 02:36:01,802 epoch 2 - iter 6/32 - loss 2.62775771 - samples/sec: 56.99 - lr: 0.100000
2021-05-25 02:36:02,644 epoch 2 - iter 9/32 - loss 2.65321231 - samples/sec: 57.02 - lr: 0.100000
2021-05-25 02:36:03,489 epoch 2 - iter 12/32 - loss 2.65451829 - samples/sec: 56.87 - lr: 0.100000
2021-05-25 02:36:04,346 epoch 2 - iter 15/32 - loss 2.50691058 - samples/sec: 56.02 - lr: 0.100000
2021-05-25 02:36:05,195 epoch 2 - iter 18/32 - loss 2.43696581 - samples/sec: 56.55 - lr: 0.100000
2021-05-25 02:36:06,056 epoch 2 - iter 21/32 - loss 2.28319908 - samples/sec: 55.75 - lr: 0.100000
2021-05-25 02:36:06,894 epoch 2 - iter 24/32 - loss 2.14200966 - samples/sec: 57.30 - lr: 0.100000
2021-05-25 02:36:07,760 epoch 2 - iter 27/32 - loss 2.11768206 - samples/sec: 55.49 - lr: 0.100000
2021-05-25 02:36:08,623 epoch 2 - iter 30/32 - loss 2.09103962 - samples/sec: 55.62 - lr: 0.100000
2021-05-25 02:36:09,042 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:09,042 EPOCH 2 done: loss 2.0396 - lr 0.1000000
2021-05-25 02:36:09,775 DEV : loss 1.227936863899231 - score 0.15
2021-05-25 02:36:09,787 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:36:19,671 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:20,516 epoch 3 - iter 3/32 - loss 1.28424128 - samples/sec: 56.86 - lr: 0.100000
2021-05-25 02:36:21,373 epoch 3 - iter 6/32 - loss 1.25999900 - samples/sec: 56.02 - lr: 0.100000
2021-05-25 02:36:22,243 epoch 3 - iter 9/32 - loss 1.63465865 - samples/sec: 55.21 - lr: 0.100000
2021-05-25 02:36:23,116 epoch 3 - iter 12/32 - loss 1.67851722 - samples/sec: 55.05 - lr: 0.100000
2021-05-25 02:36:23,982 epoch 3 - iter 15/32 - loss 1.73127516 - samples/sec: 55.39 - lr: 0.100000
2021-05-25 02:36:24,815 epoch 3 - iter 18/32 - loss 1.72156335 - samples/sec: 57.68 - lr: 0.100000
2021-05-25 02:36:25,668 epoch 3 - iter 21/32 - loss 1.61669892 - samples/sec: 56.29 - lr: 0.100000
2021-05-25 02:36:26,530 epoch 3 - iter 24/32 - loss 1.62153925 - samples/sec: 55.72 - lr: 0.100000
2021-05-25 02:36:27,397 epoch 3 - iter 27/32 - loss 1.59531197 - samples/sec: 55.39 - lr: 0.100000
2021-05-25 02:36:28,253 epoch 3 - iter 30/32 - loss 1.53980637 - samples/sec: 56.11 - lr: 0.100000
2021-05-25 02:36:28,656 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:28,657 EPOCH 3 done: loss 1.4715 - lr 0.1000000
2021-05-25 02:36:29,400 DEV : loss 0.4470658302307129 - score 0.8296
2021-05-25 02:36:29,412 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:36:39,232 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:40,091 epoch 4 - iter 3/32 - loss 1.27414354 - samples/sec: 55.93 - lr: 0.100000
2021-05-25 02:36:40,946 epoch 4 - iter 6/32 - loss 1.09217091 - samples/sec: 56.14 - lr: 0.100000
2021-05-25 02:36:41,784 epoch 4 - iter 9/32 - loss 1.11660041 - samples/sec: 57.31 - lr: 0.100000
2021-05-25 02:36:42,620 epoch 4 - iter 12/32 - loss 0.98971118 - samples/sec: 57.45 - lr: 0.100000
2021-05-25 02:36:43,456 epoch 4 - iter 15/32 - loss 0.99528911 - samples/sec: 57.47 - lr: 0.100000
2021-05-25 02:36:44,321 epoch 4 - iter 18/32 - loss 1.10456637 - samples/sec: 55.47 - lr: 0.100000
2021-05-25 02:36:45,169 epoch 4 - iter 21/32 - loss 1.13442651 - samples/sec: 56.69 - lr: 0.100000
2021-05-25 02:36:46,009 epoch 4 - iter 24/32 - loss 1.13114157 - samples/sec: 57.11 - lr: 0.100000
2021-05-25 02:36:46,858 epoch 4 - iter 27/32 - loss 1.16650977 - samples/sec: 56.58 - lr: 0.100000
2021-05-25 02:36:47,714 epoch 4 - iter 30/32 - loss 1.15957631 - samples/sec: 56.15 - lr: 0.100000
2021-05-25 02:36:48,138 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:48,138 EPOCH 4 done: loss 1.1086 - lr 0.1000000
2021-05-25 02:36:48,866 DEV : loss 0.5350256562232971 - score 0.8092
2021-05-25 02:36:48,878 BAD EPOCHS (no improvement): 1
2021-05-25 02:36:48,879 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:49,722 epoch 5 - iter 3/32 - loss 0.99564187 - samples/sec: 56.94 - lr: 0.100000
2021-05-25 02:36:50,570 epoch 5 - iter 6/32 - loss 0.98944644 - samples/sec: 56.64 - lr: 0.100000
2021-05-25 02:36:51,434 epoch 5 - iter 9/32 - loss 1.15147220 - samples/sec: 55.53 - lr: 0.100000
2021-05-25 02:36:52,278 epoch 5 - iter 12/32 - loss 1.13637578 - samples/sec: 56.90 - lr: 0.100000
2021-05-25 02:36:53,137 epoch 5 - iter 15/32 - loss 1.09658712 - samples/sec: 55.91 - lr: 0.100000
2021-05-25 02:36:53,999 epoch 5 - iter 18/32 - loss 1.10788690 - samples/sec: 55.72 - lr: 0.100000
2021-05-25 02:36:54,834 epoch 5 - iter 21/32 - loss 1.15754234 - samples/sec: 57.52 - lr: 0.100000
2021-05-25 02:36:55,682 epoch 5 - iter 24/32 - loss 1.09521372 - samples/sec: 56.61 - lr: 0.100000
2021-05-25 02:36:56,521 epoch 5 - iter 27/32 - loss 1.04392898 - samples/sec: 57.27 - lr: 0.100000
2021-05-25 02:36:57,378 epoch 5 - iter 30/32 - loss 1.05564814 - samples/sec: 55.99 - lr: 0.100000
2021-05-25 02:36:57,772 ----------------------------------------------------------------------------------------------------
2021-05-25 02:36:57,772 EPOCH 5 done: loss 1.0814 - lr 0.1000000
2021-05-25 02:36:58,499 DEV : loss 0.39691153168678284 - score 0.8633
2021-05-25 02:36:58,511 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:37:08,325 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:09,189 epoch 6 - iter 3/32 - loss 1.65155602 - samples/sec: 55.63 - lr: 0.100000
2021-05-25 02:37:10,046 epoch 6 - iter 6/32 - loss 1.28407582 - samples/sec: 56.02 - lr: 0.100000
2021-05-25 02:37:10,910 epoch 6 - iter 9/32 - loss 1.08899657 - samples/sec: 55.59 - lr: 0.100000
2021-05-25 02:37:11,762 epoch 6 - iter 12/32 - loss 1.06049140 - samples/sec: 56.35 - lr: 0.100000
2021-05-25 02:37:12,614 epoch 6 - iter 15/32 - loss 0.96321247 - samples/sec: 56.35 - lr: 0.100000
2021-05-25 02:37:13,470 epoch 6 - iter 18/32 - loss 1.08794446 - samples/sec: 56.11 - lr: 0.100000
2021-05-25 02:37:14,307 epoch 6 - iter 21/32 - loss 1.00747043 - samples/sec: 57.36 - lr: 0.100000
2021-05-25 02:37:15,165 epoch 6 - iter 24/32 - loss 1.01338066 - samples/sec: 55.98 - lr: 0.100000
2021-05-25 02:37:16,017 epoch 6 - iter 27/32 - loss 1.06080399 - samples/sec: 56.37 - lr: 0.100000
2021-05-25 02:37:16,884 epoch 6 - iter 30/32 - loss 1.04689081 - samples/sec: 55.40 - lr: 0.100000
2021-05-25 02:37:17,290 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:17,290 EPOCH 6 done: loss 1.0659 - lr 0.1000000
2021-05-25 02:37:18,025 DEV : loss 0.390644907951355 - score 0.8774
2021-05-25 02:37:18,038 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:37:28,391 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:29,217 epoch 7 - iter 3/32 - loss 0.39078385 - samples/sec: 58.15 - lr: 0.100000
2021-05-25 02:37:30,077 epoch 7 - iter 6/32 - loss 0.66939254 - samples/sec: 55.82 - lr: 0.100000
2021-05-25 02:37:30,912 epoch 7 - iter 9/32 - loss 0.74800483 - samples/sec: 57.54 - lr: 0.100000
2021-05-25 02:37:31,763 epoch 7 - iter 12/32 - loss 0.73825775 - samples/sec: 56.39 - lr: 0.100000
2021-05-25 02:37:32,623 epoch 7 - iter 15/32 - loss 0.81560756 - samples/sec: 55.85 - lr: 0.100000
2021-05-25 02:37:33,483 epoch 7 - iter 18/32 - loss 0.91427696 - samples/sec: 55.85 - lr: 0.100000
2021-05-25 02:37:34,338 epoch 7 - iter 21/32 - loss 0.92177558 - samples/sec: 56.16 - lr: 0.100000
2021-05-25 02:37:35,179 epoch 7 - iter 24/32 - loss 0.94885603 - samples/sec: 57.09 - lr: 0.100000
2021-05-25 02:37:36,032 epoch 7 - iter 27/32 - loss 0.95749341 - samples/sec: 56.29 - lr: 0.100000
2021-05-25 02:37:36,864 epoch 7 - iter 30/32 - loss 0.94821500 - samples/sec: 57.72 - lr: 0.100000
2021-05-25 02:37:37,267 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:37,268 EPOCH 7 done: loss 0.9202 - lr 0.1000000
2021-05-25 02:37:37,990 DEV : loss 0.5279465913772583 - score 0.8217
2021-05-25 02:37:38,002 BAD EPOCHS (no improvement): 1
2021-05-25 02:37:38,002 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:38,846 epoch 8 - iter 3/32 - loss 0.91264854 - samples/sec: 56.95 - lr: 0.100000
2021-05-25 02:37:39,841 epoch 8 - iter 6/32 - loss 0.86871816 - samples/sec: 48.23 - lr: 0.100000
2021-05-25 02:37:40,676 epoch 8 - iter 9/32 - loss 0.80291346 - samples/sec: 57.52 - lr: 0.100000
2021-05-25 02:37:41,531 epoch 8 - iter 12/32 - loss 0.86536924 - samples/sec: 56.19 - lr: 0.100000
2021-05-25 02:37:42,363 epoch 8 - iter 15/32 - loss 0.87886009 - samples/sec: 57.69 - lr: 0.100000
2021-05-25 02:37:43,198 epoch 8 - iter 18/32 - loss 0.86762509 - samples/sec: 57.49 - lr: 0.100000
2021-05-25 02:37:44,035 epoch 8 - iter 21/32 - loss 0.86828738 - samples/sec: 57.42 - lr: 0.100000
2021-05-25 02:37:44,885 epoch 8 - iter 24/32 - loss 0.88366391 - samples/sec: 56.46 - lr: 0.100000
2021-05-25 02:37:45,736 epoch 8 - iter 27/32 - loss 0.86875710 - samples/sec: 56.42 - lr: 0.100000
2021-05-25 02:37:46,578 epoch 8 - iter 30/32 - loss 0.84592908 - samples/sec: 57.08 - lr: 0.100000
2021-05-25 02:37:46,988 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:46,988 EPOCH 8 done: loss 0.8762 - lr 0.1000000
2021-05-25 02:37:47,710 DEV : loss 0.8832606077194214 - score 0.6296
2021-05-25 02:37:47,722 BAD EPOCHS (no improvement): 2
2021-05-25 02:37:47,722 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:48,572 epoch 9 - iter 3/32 - loss 0.96779617 - samples/sec: 56.55 - lr: 0.100000
2021-05-25 02:37:49,421 epoch 9 - iter 6/32 - loss 1.02775500 - samples/sec: 56.50 - lr: 0.100000
2021-05-25 02:37:50,255 epoch 9 - iter 9/32 - loss 0.81389551 - samples/sec: 57.60 - lr: 0.100000
2021-05-25 02:37:51,097 epoch 9 - iter 12/32 - loss 0.77401419 - samples/sec: 57.02 - lr: 0.100000
2021-05-25 02:37:51,957 epoch 9 - iter 15/32 - loss 0.74487565 - samples/sec: 55.85 - lr: 0.100000
2021-05-25 02:37:52,806 epoch 9 - iter 18/32 - loss 0.76843655 - samples/sec: 56.58 - lr: 0.100000
2021-05-25 02:37:53,645 epoch 9 - iter 21/32 - loss 0.77252947 - samples/sec: 57.24 - lr: 0.100000
2021-05-25 02:37:54,509 epoch 9 - iter 24/32 - loss 0.75152526 - samples/sec: 55.59 - lr: 0.100000
2021-05-25 02:37:55,368 epoch 9 - iter 27/32 - loss 0.73393878 - samples/sec: 55.86 - lr: 0.100000
2021-05-25 02:37:56,225 epoch 9 - iter 30/32 - loss 0.68501606 - samples/sec: 56.04 - lr: 0.100000
2021-05-25 02:37:56,635 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:56,636 EPOCH 9 done: loss 0.7193 - lr 0.1000000
2021-05-25 02:37:57,371 DEV : loss 1.1265443563461304 - score 0.7461
2021-05-25 02:37:57,383 BAD EPOCHS (no improvement): 3
2021-05-25 02:37:57,383 ----------------------------------------------------------------------------------------------------
2021-05-25 02:37:58,230 epoch 10 - iter 3/32 - loss 1.44674087 - samples/sec: 56.73 - lr: 0.100000
2021-05-25 02:37:59,105 epoch 10 - iter 6/32 - loss 1.31642258 - samples/sec: 54.87 - lr: 0.100000
2021-05-25 02:37:59,971 epoch 10 - iter 9/32 - loss 1.12704041 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 02:38:00,837 epoch 10 - iter 12/32 - loss 1.00784051 - samples/sec: 55.46 - lr: 0.100000
2021-05-25 02:38:01,703 epoch 10 - iter 15/32 - loss 0.96838708 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 02:38:02,540 epoch 10 - iter 18/32 - loss 0.95066208 - samples/sec: 57.33 - lr: 0.100000
2021-05-25 02:38:03,398 epoch 10 - iter 21/32 - loss 0.89961604 - samples/sec: 56.00 - lr: 0.100000
2021-05-25 02:38:04,246 epoch 10 - iter 24/32 - loss 0.90729976 - samples/sec: 56.63 - lr: 0.100000
2021-05-25 02:38:05,111 epoch 10 - iter 27/32 - loss 0.88905119 - samples/sec: 55.54 - lr: 0.100000
2021-05-25 02:38:05,955 epoch 10 - iter 30/32 - loss 0.85083308 - samples/sec: 56.88 - lr: 0.100000
2021-05-25 02:38:06,351 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:06,352 EPOCH 10 done: loss 0.8485 - lr 0.1000000
2021-05-25 02:38:07,084 DEV : loss 0.45577362179756165 - score 0.8676
Epoch    10: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 02:38:07,096 BAD EPOCHS (no improvement): 4
2021-05-25 02:38:07,097 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:07,942 epoch 11 - iter 3/32 - loss 0.90661136 - samples/sec: 56.80 - lr: 0.050000
2021-05-25 02:38:08,780 epoch 11 - iter 6/32 - loss 0.69017061 - samples/sec: 57.31 - lr: 0.050000
2021-05-25 02:38:09,623 epoch 11 - iter 9/32 - loss 0.68114356 - samples/sec: 56.99 - lr: 0.050000
2021-05-25 02:38:10,464 epoch 11 - iter 12/32 - loss 0.66103068 - samples/sec: 57.07 - lr: 0.050000
2021-05-25 02:38:11,306 epoch 11 - iter 15/32 - loss 0.61771065 - samples/sec: 57.06 - lr: 0.050000
2021-05-25 02:38:12,165 epoch 11 - iter 18/32 - loss 0.63299452 - samples/sec: 55.90 - lr: 0.050000
2021-05-25 02:38:13,008 epoch 11 - iter 21/32 - loss 0.58203334 - samples/sec: 56.92 - lr: 0.050000
2021-05-25 02:38:13,861 epoch 11 - iter 24/32 - loss 0.56617617 - samples/sec: 56.33 - lr: 0.050000
2021-05-25 02:38:14,701 epoch 11 - iter 27/32 - loss 0.57683978 - samples/sec: 57.15 - lr: 0.050000
2021-05-25 02:38:15,559 epoch 11 - iter 30/32 - loss 0.53957413 - samples/sec: 55.95 - lr: 0.050000
2021-05-25 02:38:15,978 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:15,978 EPOCH 11 done: loss 0.5303 - lr 0.0500000
2021-05-25 02:38:16,712 DEV : loss 0.3238344192504883 - score 0.9091
2021-05-25 02:38:16,724 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:38:26,541 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:27,386 epoch 12 - iter 3/32 - loss 0.50132517 - samples/sec: 56.87 - lr: 0.050000
2021-05-25 02:38:28,236 epoch 12 - iter 6/32 - loss 0.53384544 - samples/sec: 56.49 - lr: 0.050000
2021-05-25 02:38:29,083 epoch 12 - iter 9/32 - loss 0.41379925 - samples/sec: 56.68 - lr: 0.050000
2021-05-25 02:38:29,926 epoch 12 - iter 12/32 - loss 0.47756673 - samples/sec: 56.93 - lr: 0.050000
2021-05-25 02:38:30,786 epoch 12 - iter 15/32 - loss 0.45571481 - samples/sec: 55.88 - lr: 0.050000
2021-05-25 02:38:31,636 epoch 12 - iter 18/32 - loss 0.46547834 - samples/sec: 56.45 - lr: 0.050000
2021-05-25 02:38:32,498 epoch 12 - iter 21/32 - loss 0.45339280 - samples/sec: 55.77 - lr: 0.050000
2021-05-25 02:38:33,342 epoch 12 - iter 24/32 - loss 0.46957298 - samples/sec: 56.83 - lr: 0.050000
2021-05-25 02:38:34,209 epoch 12 - iter 27/32 - loss 0.47275540 - samples/sec: 55.40 - lr: 0.050000
2021-05-25 02:38:35,055 epoch 12 - iter 30/32 - loss 0.44754333 - samples/sec: 56.77 - lr: 0.050000
2021-05-25 02:38:35,471 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:35,471 EPOCH 12 done: loss 0.4881 - lr 0.0500000
2021-05-25 02:38:36,204 DEV : loss 0.3471251130104065 - score 0.9114
2021-05-25 02:38:36,216 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:38:46,326 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:47,181 epoch 13 - iter 3/32 - loss 0.65497080 - samples/sec: 56.20 - lr: 0.050000
2021-05-25 02:38:48,046 epoch 13 - iter 6/32 - loss 0.57213616 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 02:38:48,907 epoch 13 - iter 9/32 - loss 0.56323041 - samples/sec: 55.79 - lr: 0.050000
2021-05-25 02:38:49,752 epoch 13 - iter 12/32 - loss 0.49042978 - samples/sec: 56.87 - lr: 0.050000
2021-05-25 02:38:50,620 epoch 13 - iter 15/32 - loss 0.47066646 - samples/sec: 55.31 - lr: 0.050000
2021-05-25 02:38:51,452 epoch 13 - iter 18/32 - loss 0.45671987 - samples/sec: 57.70 - lr: 0.050000
2021-05-25 02:38:52,299 epoch 13 - iter 21/32 - loss 0.45228112 - samples/sec: 56.69 - lr: 0.050000
2021-05-25 02:38:53,137 epoch 13 - iter 24/32 - loss 0.44926398 - samples/sec: 57.36 - lr: 0.050000
2021-05-25 02:38:53,986 epoch 13 - iter 27/32 - loss 0.43195773 - samples/sec: 56.54 - lr: 0.050000
2021-05-25 02:38:54,853 epoch 13 - iter 30/32 - loss 0.44939538 - samples/sec: 55.41 - lr: 0.050000
2021-05-25 02:38:55,258 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:55,258 EPOCH 13 done: loss 0.4730 - lr 0.0500000
2021-05-25 02:38:55,989 DEV : loss 0.33236607909202576 - score 0.9057
2021-05-25 02:38:56,002 BAD EPOCHS (no improvement): 1
2021-05-25 02:38:56,002 ----------------------------------------------------------------------------------------------------
2021-05-25 02:38:56,971 epoch 14 - iter 3/32 - loss 0.36105267 - samples/sec: 49.54 - lr: 0.050000
2021-05-25 02:38:57,836 epoch 14 - iter 6/32 - loss 0.40803250 - samples/sec: 55.51 - lr: 0.050000
2021-05-25 02:38:58,697 epoch 14 - iter 9/32 - loss 0.41586039 - samples/sec: 55.82 - lr: 0.050000
2021-05-25 02:38:59,546 epoch 14 - iter 12/32 - loss 0.44689293 - samples/sec: 56.56 - lr: 0.050000
2021-05-25 02:39:00,414 epoch 14 - iter 15/32 - loss 0.48101351 - samples/sec: 55.30 - lr: 0.050000
2021-05-25 02:39:01,276 epoch 14 - iter 18/32 - loss 0.48461381 - samples/sec: 55.74 - lr: 0.050000
2021-05-25 02:39:02,125 epoch 14 - iter 21/32 - loss 0.44373111 - samples/sec: 56.51 - lr: 0.050000
2021-05-25 02:39:02,983 epoch 14 - iter 24/32 - loss 0.45423564 - samples/sec: 55.97 - lr: 0.050000
2021-05-25 02:39:03,832 epoch 14 - iter 27/32 - loss 0.45622360 - samples/sec: 56.56 - lr: 0.050000
2021-05-25 02:39:04,652 epoch 14 - iter 30/32 - loss 0.45543597 - samples/sec: 58.60 - lr: 0.050000
2021-05-25 02:39:05,066 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:05,066 EPOCH 14 done: loss 0.4497 - lr 0.0500000
2021-05-25 02:39:05,798 DEV : loss 0.2900645136833191 - score 0.898
2021-05-25 02:39:05,811 BAD EPOCHS (no improvement): 2
2021-05-25 02:39:05,811 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:06,656 epoch 15 - iter 3/32 - loss 0.43285179 - samples/sec: 56.85 - lr: 0.050000
2021-05-25 02:39:07,510 epoch 15 - iter 6/32 - loss 0.39311035 - samples/sec: 56.23 - lr: 0.050000
2021-05-25 02:39:08,370 epoch 15 - iter 9/32 - loss 0.39333879 - samples/sec: 55.80 - lr: 0.050000
2021-05-25 02:39:09,202 epoch 15 - iter 12/32 - loss 0.39332160 - samples/sec: 57.74 - lr: 0.050000
2021-05-25 02:39:10,049 epoch 15 - iter 15/32 - loss 0.34930592 - samples/sec: 56.70 - lr: 0.050000
2021-05-25 02:39:10,912 epoch 15 - iter 18/32 - loss 0.32126422 - samples/sec: 55.63 - lr: 0.050000
2021-05-25 02:39:11,749 epoch 15 - iter 21/32 - loss 0.33418013 - samples/sec: 57.35 - lr: 0.050000
2021-05-25 02:39:12,615 epoch 15 - iter 24/32 - loss 0.34956525 - samples/sec: 55.48 - lr: 0.050000
2021-05-25 02:39:13,462 epoch 15 - iter 27/32 - loss 0.34646982 - samples/sec: 56.69 - lr: 0.050000
2021-05-25 02:39:14,313 epoch 15 - iter 30/32 - loss 0.35095298 - samples/sec: 56.45 - lr: 0.050000
2021-05-25 02:39:14,726 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:14,726 EPOCH 15 done: loss 0.3543 - lr 0.0500000
2021-05-25 02:39:15,459 DEV : loss 0.2884262800216675 - score 0.8966
2021-05-25 02:39:15,471 BAD EPOCHS (no improvement): 3
2021-05-25 02:39:15,471 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:16,317 epoch 16 - iter 3/32 - loss 0.42749401 - samples/sec: 56.80 - lr: 0.050000
2021-05-25 02:39:17,183 epoch 16 - iter 6/32 - loss 0.43846041 - samples/sec: 55.46 - lr: 0.050000
2021-05-25 02:39:18,047 epoch 16 - iter 9/32 - loss 0.46890867 - samples/sec: 55.56 - lr: 0.050000
2021-05-25 02:39:18,898 epoch 16 - iter 12/32 - loss 0.46686850 - samples/sec: 56.42 - lr: 0.050000
2021-05-25 02:39:19,732 epoch 16 - iter 15/32 - loss 0.46006354 - samples/sec: 57.54 - lr: 0.050000
2021-05-25 02:39:20,598 epoch 16 - iter 18/32 - loss 0.45176593 - samples/sec: 55.50 - lr: 0.050000
2021-05-25 02:39:21,460 epoch 16 - iter 21/32 - loss 0.44375588 - samples/sec: 55.69 - lr: 0.050000
2021-05-25 02:39:22,318 epoch 16 - iter 24/32 - loss 0.42168727 - samples/sec: 55.99 - lr: 0.050000
2021-05-25 02:39:23,178 epoch 16 - iter 27/32 - loss 0.40482400 - samples/sec: 55.82 - lr: 0.050000
2021-05-25 02:39:24,041 epoch 16 - iter 30/32 - loss 0.38745321 - samples/sec: 55.61 - lr: 0.050000
2021-05-25 02:39:24,448 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:24,448 EPOCH 16 done: loss 0.3675 - lr 0.0500000
2021-05-25 02:39:25,183 DEV : loss 0.3053044378757477 - score 0.9211
2021-05-25 02:39:25,195 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:39:35,119 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:35,986 epoch 17 - iter 3/32 - loss 0.43510596 - samples/sec: 55.43 - lr: 0.050000
2021-05-25 02:39:36,819 epoch 17 - iter 6/32 - loss 0.37104623 - samples/sec: 57.65 - lr: 0.050000
2021-05-25 02:39:37,687 epoch 17 - iter 9/32 - loss 0.44747363 - samples/sec: 55.34 - lr: 0.050000
2021-05-25 02:39:38,553 epoch 17 - iter 12/32 - loss 0.41466077 - samples/sec: 55.45 - lr: 0.050000
2021-05-25 02:39:39,425 epoch 17 - iter 15/32 - loss 0.40651220 - samples/sec: 55.08 - lr: 0.050000
2021-05-25 02:39:40,270 epoch 17 - iter 18/32 - loss 0.37973575 - samples/sec: 56.84 - lr: 0.050000
2021-05-25 02:39:41,140 epoch 17 - iter 21/32 - loss 0.37943319 - samples/sec: 55.22 - lr: 0.050000
2021-05-25 02:39:41,980 epoch 17 - iter 24/32 - loss 0.38294323 - samples/sec: 57.14 - lr: 0.050000
2021-05-25 02:39:42,841 epoch 17 - iter 27/32 - loss 0.39433537 - samples/sec: 55.79 - lr: 0.050000
2021-05-25 02:39:43,691 epoch 17 - iter 30/32 - loss 0.40038499 - samples/sec: 56.48 - lr: 0.050000
2021-05-25 02:39:44,108 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:44,108 EPOCH 17 done: loss 0.4519 - lr 0.0500000
2021-05-25 02:39:44,839 DEV : loss 0.3599381744861603 - score 0.9057
2021-05-25 02:39:44,852 BAD EPOCHS (no improvement): 1
2021-05-25 02:39:44,852 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:45,704 epoch 18 - iter 3/32 - loss 0.34121950 - samples/sec: 56.35 - lr: 0.050000
2021-05-25 02:39:46,559 epoch 18 - iter 6/32 - loss 0.50217342 - samples/sec: 56.20 - lr: 0.050000
2021-05-25 02:39:47,397 epoch 18 - iter 9/32 - loss 0.38137464 - samples/sec: 57.28 - lr: 0.050000
2021-05-25 02:39:48,259 epoch 18 - iter 12/32 - loss 0.33970603 - samples/sec: 55.69 - lr: 0.050000
2021-05-25 02:39:49,133 epoch 18 - iter 15/32 - loss 0.33784975 - samples/sec: 54.95 - lr: 0.050000
2021-05-25 02:39:49,992 epoch 18 - iter 18/32 - loss 0.30717760 - samples/sec: 55.90 - lr: 0.050000
2021-05-25 02:39:50,843 epoch 18 - iter 21/32 - loss 0.33634360 - samples/sec: 56.48 - lr: 0.050000
2021-05-25 02:39:51,717 epoch 18 - iter 24/32 - loss 0.34596870 - samples/sec: 54.91 - lr: 0.050000
2021-05-25 02:39:52,574 epoch 18 - iter 27/32 - loss 0.32917168 - samples/sec: 56.06 - lr: 0.050000
2021-05-25 02:39:53,443 epoch 18 - iter 30/32 - loss 0.34448705 - samples/sec: 55.25 - lr: 0.050000
2021-05-25 02:39:53,857 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:53,858 EPOCH 18 done: loss 0.3562 - lr 0.0500000
2021-05-25 02:39:54,590 DEV : loss 0.4271547496318817 - score 0.8889
2021-05-25 02:39:54,602 BAD EPOCHS (no improvement): 2
2021-05-25 02:39:54,602 ----------------------------------------------------------------------------------------------------
2021-05-25 02:39:55,464 epoch 19 - iter 3/32 - loss 0.33103069 - samples/sec: 55.69 - lr: 0.050000
2021-05-25 02:39:56,305 epoch 19 - iter 6/32 - loss 0.27856394 - samples/sec: 57.17 - lr: 0.050000
2021-05-25 02:39:57,157 epoch 19 - iter 9/32 - loss 0.28084172 - samples/sec: 56.31 - lr: 0.050000
2021-05-25 02:39:58,019 epoch 19 - iter 12/32 - loss 0.32561461 - samples/sec: 55.75 - lr: 0.050000
2021-05-25 02:39:58,878 epoch 19 - iter 15/32 - loss 0.35385625 - samples/sec: 55.88 - lr: 0.050000
2021-05-25 02:39:59,735 epoch 19 - iter 18/32 - loss 0.34044242 - samples/sec: 56.06 - lr: 0.050000
2021-05-25 02:40:00,596 epoch 19 - iter 21/32 - loss 0.33374130 - samples/sec: 55.77 - lr: 0.050000
2021-05-25 02:40:01,440 epoch 19 - iter 24/32 - loss 0.33434732 - samples/sec: 56.91 - lr: 0.050000
2021-05-25 02:40:02,290 epoch 19 - iter 27/32 - loss 0.33395822 - samples/sec: 56.48 - lr: 0.050000
2021-05-25 02:40:03,125 epoch 19 - iter 30/32 - loss 0.33523724 - samples/sec: 57.48 - lr: 0.050000
2021-05-25 02:40:03,531 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:03,531 EPOCH 19 done: loss 0.3330 - lr 0.0500000
2021-05-25 02:40:04,263 DEV : loss 0.369206041097641 - score 0.9114
2021-05-25 02:40:04,275 BAD EPOCHS (no improvement): 3
2021-05-25 02:40:04,275 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:05,122 epoch 20 - iter 3/32 - loss 0.38712637 - samples/sec: 56.74 - lr: 0.050000
2021-05-25 02:40:05,963 epoch 20 - iter 6/32 - loss 0.33009827 - samples/sec: 57.08 - lr: 0.050000
2021-05-25 02:40:06,806 epoch 20 - iter 9/32 - loss 0.29018048 - samples/sec: 56.97 - lr: 0.050000
2021-05-25 02:40:07,669 epoch 20 - iter 12/32 - loss 0.29818627 - samples/sec: 55.62 - lr: 0.050000
2021-05-25 02:40:08,527 epoch 20 - iter 15/32 - loss 0.30088619 - samples/sec: 56.02 - lr: 0.050000
2021-05-25 02:40:09,506 epoch 20 - iter 18/32 - loss 0.29455671 - samples/sec: 49.04 - lr: 0.050000
2021-05-25 02:40:10,348 epoch 20 - iter 21/32 - loss 0.30804711 - samples/sec: 56.99 - lr: 0.050000
2021-05-25 02:40:11,188 epoch 20 - iter 24/32 - loss 0.31224524 - samples/sec: 57.21 - lr: 0.050000
2021-05-25 02:40:12,039 epoch 20 - iter 27/32 - loss 0.30623357 - samples/sec: 56.44 - lr: 0.050000
2021-05-25 02:40:12,883 epoch 20 - iter 30/32 - loss 0.32337561 - samples/sec: 56.87 - lr: 0.050000
2021-05-25 02:40:13,299 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:13,299 EPOCH 20 done: loss 0.3573 - lr 0.0500000
2021-05-25 02:40:14,033 DEV : loss 0.30231428146362305 - score 0.9161
Epoch    20: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 02:40:14,045 BAD EPOCHS (no improvement): 4
2021-05-25 02:40:14,045 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:14,867 epoch 21 - iter 3/32 - loss 0.16287637 - samples/sec: 58.47 - lr: 0.025000
2021-05-25 02:40:15,718 epoch 21 - iter 6/32 - loss 0.13879697 - samples/sec: 56.44 - lr: 0.025000
2021-05-25 02:40:16,580 epoch 21 - iter 9/32 - loss 0.17751874 - samples/sec: 55.67 - lr: 0.025000
2021-05-25 02:40:17,429 epoch 21 - iter 12/32 - loss 0.20158901 - samples/sec: 56.56 - lr: 0.025000
2021-05-25 02:40:18,271 epoch 21 - iter 15/32 - loss 0.25125988 - samples/sec: 57.07 - lr: 0.025000
2021-05-25 02:40:19,119 epoch 21 - iter 18/32 - loss 0.24822188 - samples/sec: 56.60 - lr: 0.025000
2021-05-25 02:40:19,972 epoch 21 - iter 21/32 - loss 0.28287938 - samples/sec: 56.28 - lr: 0.025000
2021-05-25 02:40:20,829 epoch 21 - iter 24/32 - loss 0.31468352 - samples/sec: 56.03 - lr: 0.025000
2021-05-25 02:40:21,686 epoch 21 - iter 27/32 - loss 0.33748700 - samples/sec: 56.05 - lr: 0.025000
2021-05-25 02:40:22,523 epoch 21 - iter 30/32 - loss 0.33137869 - samples/sec: 57.36 - lr: 0.025000
2021-05-25 02:40:22,947 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:22,947 EPOCH 21 done: loss 0.3314 - lr 0.0250000
2021-05-25 02:40:23,680 DEV : loss 0.34914302825927734 - score 0.9045
2021-05-25 02:40:23,692 BAD EPOCHS (no improvement): 1
2021-05-25 02:40:23,692 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:24,547 epoch 22 - iter 3/32 - loss 0.19649935 - samples/sec: 56.18 - lr: 0.025000
2021-05-25 02:40:25,397 epoch 22 - iter 6/32 - loss 0.17643523 - samples/sec: 56.53 - lr: 0.025000
2021-05-25 02:40:26,259 epoch 22 - iter 9/32 - loss 0.28269402 - samples/sec: 55.68 - lr: 0.025000
2021-05-25 02:40:27,123 epoch 22 - iter 12/32 - loss 0.26830033 - samples/sec: 55.59 - lr: 0.025000
2021-05-25 02:40:27,962 epoch 22 - iter 15/32 - loss 0.26143675 - samples/sec: 57.25 - lr: 0.025000
2021-05-25 02:40:28,822 epoch 22 - iter 18/32 - loss 0.22417596 - samples/sec: 55.84 - lr: 0.025000
2021-05-25 02:40:29,661 epoch 22 - iter 21/32 - loss 0.25156531 - samples/sec: 57.21 - lr: 0.025000
2021-05-25 02:40:30,512 epoch 22 - iter 24/32 - loss 0.24626099 - samples/sec: 56.46 - lr: 0.025000
2021-05-25 02:40:31,358 epoch 22 - iter 27/32 - loss 0.23074031 - samples/sec: 56.78 - lr: 0.025000
2021-05-25 02:40:32,188 epoch 22 - iter 30/32 - loss 0.24158714 - samples/sec: 57.80 - lr: 0.025000
2021-05-25 02:40:32,601 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:32,601 EPOCH 22 done: loss 0.2354 - lr 0.0250000
2021-05-25 02:40:33,334 DEV : loss 0.353982150554657 - score 0.9114
2021-05-25 02:40:33,346 BAD EPOCHS (no improvement): 2
2021-05-25 02:40:33,347 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:34,192 epoch 23 - iter 3/32 - loss 0.23387146 - samples/sec: 56.80 - lr: 0.025000
2021-05-25 02:40:35,049 epoch 23 - iter 6/32 - loss 0.25565358 - samples/sec: 56.04 - lr: 0.025000
2021-05-25 02:40:35,880 epoch 23 - iter 9/32 - loss 0.21763044 - samples/sec: 57.75 - lr: 0.025000
2021-05-25 02:40:36,728 epoch 23 - iter 12/32 - loss 0.21108508 - samples/sec: 56.66 - lr: 0.025000
2021-05-25 02:40:37,565 epoch 23 - iter 15/32 - loss 0.21008378 - samples/sec: 57.34 - lr: 0.025000
2021-05-25 02:40:38,405 epoch 23 - iter 18/32 - loss 0.20222900 - samples/sec: 57.22 - lr: 0.025000
2021-05-25 02:40:39,255 epoch 23 - iter 21/32 - loss 0.20200267 - samples/sec: 56.47 - lr: 0.025000
2021-05-25 02:40:40,118 epoch 23 - iter 24/32 - loss 0.20684752 - samples/sec: 55.66 - lr: 0.025000
2021-05-25 02:40:40,987 epoch 23 - iter 27/32 - loss 0.23487208 - samples/sec: 55.25 - lr: 0.025000
2021-05-25 02:40:41,854 epoch 23 - iter 30/32 - loss 0.23513351 - samples/sec: 55.37 - lr: 0.025000
2021-05-25 02:40:42,271 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:42,271 EPOCH 23 done: loss 0.2619 - lr 0.0250000
2021-05-25 02:40:43,003 DEV : loss 0.3566569983959198 - score 0.9172
2021-05-25 02:40:43,016 BAD EPOCHS (no improvement): 3
2021-05-25 02:40:43,016 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:43,851 epoch 24 - iter 3/32 - loss 0.21681877 - samples/sec: 57.52 - lr: 0.025000
2021-05-25 02:40:44,716 epoch 24 - iter 6/32 - loss 0.26165626 - samples/sec: 55.49 - lr: 0.025000
2021-05-25 02:40:45,569 epoch 24 - iter 9/32 - loss 0.38053638 - samples/sec: 56.33 - lr: 0.025000
2021-05-25 02:40:46,400 epoch 24 - iter 12/32 - loss 0.30738514 - samples/sec: 57.73 - lr: 0.025000
2021-05-25 02:40:47,253 epoch 24 - iter 15/32 - loss 0.30438449 - samples/sec: 56.29 - lr: 0.025000
2021-05-25 02:40:48,118 epoch 24 - iter 18/32 - loss 0.29388508 - samples/sec: 55.51 - lr: 0.025000
2021-05-25 02:40:48,972 epoch 24 - iter 21/32 - loss 0.28490042 - samples/sec: 56.24 - lr: 0.025000
2021-05-25 02:40:49,826 epoch 24 - iter 24/32 - loss 0.28770916 - samples/sec: 56.24 - lr: 0.025000
2021-05-25 02:40:50,673 epoch 24 - iter 27/32 - loss 0.27492912 - samples/sec: 56.73 - lr: 0.025000
2021-05-25 02:40:51,534 epoch 24 - iter 30/32 - loss 0.26118889 - samples/sec: 55.77 - lr: 0.025000
2021-05-25 02:40:51,946 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:51,946 EPOCH 24 done: loss 0.2572 - lr 0.0250000
2021-05-25 02:40:52,678 DEV : loss 0.4358271062374115 - score 0.8889
Epoch    24: reducing learning rate of group 0 to 1.2500e-02.
2021-05-25 02:40:52,691 BAD EPOCHS (no improvement): 4
2021-05-25 02:40:52,691 ----------------------------------------------------------------------------------------------------
2021-05-25 02:40:53,542 epoch 25 - iter 3/32 - loss 0.42991114 - samples/sec: 56.42 - lr: 0.012500
2021-05-25 02:40:54,422 epoch 25 - iter 6/32 - loss 0.36333617 - samples/sec: 54.61 - lr: 0.012500
2021-05-25 02:40:55,286 epoch 25 - iter 9/32 - loss 0.26856152 - samples/sec: 55.55 - lr: 0.012500
2021-05-25 02:40:56,133 epoch 25 - iter 12/32 - loss 0.25551874 - samples/sec: 56.72 - lr: 0.012500
2021-05-25 02:40:56,995 epoch 25 - iter 15/32 - loss 0.24747879 - samples/sec: 55.68 - lr: 0.012500
2021-05-25 02:40:57,863 epoch 25 - iter 18/32 - loss 0.24079057 - samples/sec: 55.31 - lr: 0.012500
2021-05-25 02:40:58,706 epoch 25 - iter 21/32 - loss 0.25176049 - samples/sec: 57.01 - lr: 0.012500
2021-05-25 02:40:59,555 epoch 25 - iter 24/32 - loss 0.24928793 - samples/sec: 56.55 - lr: 0.012500
2021-05-25 02:41:00,395 epoch 25 - iter 27/32 - loss 0.26302705 - samples/sec: 57.14 - lr: 0.012500
2021-05-25 02:41:01,252 epoch 25 - iter 30/32 - loss 0.27638975 - samples/sec: 56.01 - lr: 0.012500
2021-05-25 02:41:01,658 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:01,659 EPOCH 25 done: loss 0.2655 - lr 0.0125000
2021-05-25 02:41:02,390 DEV : loss 0.4680667221546173 - score 0.8727
2021-05-25 02:41:02,403 BAD EPOCHS (no improvement): 1
2021-05-25 02:41:02,403 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:03,253 epoch 26 - iter 3/32 - loss 0.27903430 - samples/sec: 56.50 - lr: 0.012500
2021-05-25 02:41:04,094 epoch 26 - iter 6/32 - loss 0.22479735 - samples/sec: 57.12 - lr: 0.012500
2021-05-25 02:41:04,937 epoch 26 - iter 9/32 - loss 0.22893213 - samples/sec: 56.96 - lr: 0.012500
2021-05-25 02:41:05,778 epoch 26 - iter 12/32 - loss 0.25172761 - samples/sec: 57.11 - lr: 0.012500
2021-05-25 02:41:06,634 epoch 26 - iter 15/32 - loss 0.25624563 - samples/sec: 56.11 - lr: 0.012500
2021-05-25 02:41:07,465 epoch 26 - iter 18/32 - loss 0.23424291 - samples/sec: 57.73 - lr: 0.012500
2021-05-25 02:41:08,324 epoch 26 - iter 21/32 - loss 0.23271655 - samples/sec: 55.90 - lr: 0.012500
2021-05-25 02:41:09,177 epoch 26 - iter 24/32 - loss 0.24388728 - samples/sec: 56.34 - lr: 0.012500
2021-05-25 02:41:10,051 epoch 26 - iter 27/32 - loss 0.23284390 - samples/sec: 54.96 - lr: 0.012500
2021-05-25 02:41:10,894 epoch 26 - iter 30/32 - loss 0.22941497 - samples/sec: 56.93 - lr: 0.012500
2021-05-25 02:41:11,307 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:11,308 EPOCH 26 done: loss 0.2231 - lr 0.0125000
2021-05-25 02:41:12,042 DEV : loss 0.27759280800819397 - score 0.9272
2021-05-25 02:41:12,054 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:41:21,955 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:22,812 epoch 27 - iter 3/32 - loss 0.21726004 - samples/sec: 56.13 - lr: 0.012500
2021-05-25 02:41:23,657 epoch 27 - iter 6/32 - loss 0.27489626 - samples/sec: 56.78 - lr: 0.012500
2021-05-25 02:41:24,497 epoch 27 - iter 9/32 - loss 0.25503167 - samples/sec: 57.20 - lr: 0.012500
2021-05-25 02:41:25,349 epoch 27 - iter 12/32 - loss 0.27117124 - samples/sec: 56.36 - lr: 0.012500
2021-05-25 02:41:26,209 epoch 27 - iter 15/32 - loss 0.26893227 - samples/sec: 55.83 - lr: 0.012500
2021-05-25 02:41:27,065 epoch 27 - iter 18/32 - loss 0.25212668 - samples/sec: 56.12 - lr: 0.012500
2021-05-25 02:41:27,910 epoch 27 - iter 21/32 - loss 0.24109197 - samples/sec: 56.84 - lr: 0.012500
2021-05-25 02:41:28,759 epoch 27 - iter 24/32 - loss 0.25390515 - samples/sec: 56.51 - lr: 0.012500
2021-05-25 02:41:29,609 epoch 27 - iter 27/32 - loss 0.23240704 - samples/sec: 56.54 - lr: 0.012500
2021-05-25 02:41:30,438 epoch 27 - iter 30/32 - loss 0.24022585 - samples/sec: 57.87 - lr: 0.012500
2021-05-25 02:41:30,852 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:30,852 EPOCH 27 done: loss 0.2306 - lr 0.0125000
2021-05-25 02:41:31,585 DEV : loss 0.3918485939502716 - score 0.9
2021-05-25 02:41:31,598 BAD EPOCHS (no improvement): 1
2021-05-25 02:41:31,598 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:32,450 epoch 28 - iter 3/32 - loss 0.18422222 - samples/sec: 56.37 - lr: 0.012500
2021-05-25 02:41:33,300 epoch 28 - iter 6/32 - loss 0.17557053 - samples/sec: 56.46 - lr: 0.012500
2021-05-25 02:41:34,134 epoch 28 - iter 9/32 - loss 0.17654710 - samples/sec: 57.64 - lr: 0.012500
2021-05-25 02:41:34,988 epoch 28 - iter 12/32 - loss 0.23856258 - samples/sec: 56.19 - lr: 0.012500
2021-05-25 02:41:35,837 epoch 28 - iter 15/32 - loss 0.20661199 - samples/sec: 56.56 - lr: 0.012500
2021-05-25 02:41:36,694 epoch 28 - iter 18/32 - loss 0.22952201 - samples/sec: 56.06 - lr: 0.012500
2021-05-25 02:41:37,546 epoch 28 - iter 21/32 - loss 0.22674297 - samples/sec: 56.37 - lr: 0.012500
2021-05-25 02:41:38,379 epoch 28 - iter 24/32 - loss 0.24033459 - samples/sec: 57.63 - lr: 0.012500
2021-05-25 02:41:39,244 epoch 28 - iter 27/32 - loss 0.23905319 - samples/sec: 55.51 - lr: 0.012500
2021-05-25 02:41:40,094 epoch 28 - iter 30/32 - loss 0.22593942 - samples/sec: 56.48 - lr: 0.012500
2021-05-25 02:41:40,508 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:40,508 EPOCH 28 done: loss 0.2172 - lr 0.0125000
2021-05-25 02:41:41,242 DEV : loss 0.45777183771133423 - score 0.8944
2021-05-25 02:41:41,254 BAD EPOCHS (no improvement): 2
2021-05-25 02:41:41,254 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:42,097 epoch 29 - iter 3/32 - loss 0.14821927 - samples/sec: 57.00 - lr: 0.012500
2021-05-25 02:41:42,961 epoch 29 - iter 6/32 - loss 0.12363680 - samples/sec: 55.58 - lr: 0.012500
2021-05-25 02:41:43,806 epoch 29 - iter 9/32 - loss 0.16499283 - samples/sec: 56.80 - lr: 0.012500
2021-05-25 02:41:44,658 epoch 29 - iter 12/32 - loss 0.19914346 - samples/sec: 56.35 - lr: 0.012500
2021-05-25 02:41:45,527 epoch 29 - iter 15/32 - loss 0.22534057 - samples/sec: 55.27 - lr: 0.012500
2021-05-25 02:41:46,371 epoch 29 - iter 18/32 - loss 0.20499766 - samples/sec: 56.90 - lr: 0.012500
2021-05-25 02:41:47,204 epoch 29 - iter 21/32 - loss 0.20427715 - samples/sec: 57.62 - lr: 0.012500
2021-05-25 02:41:48,049 epoch 29 - iter 24/32 - loss 0.21550882 - samples/sec: 56.82 - lr: 0.012500
2021-05-25 02:41:48,890 epoch 29 - iter 27/32 - loss 0.21571334 - samples/sec: 57.13 - lr: 0.012500
2021-05-25 02:41:49,737 epoch 29 - iter 30/32 - loss 0.20562508 - samples/sec: 56.68 - lr: 0.012500
2021-05-25 02:41:50,152 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:50,152 EPOCH 29 done: loss 0.1938 - lr 0.0125000
2021-05-25 02:41:50,885 DEV : loss 0.28002193570137024 - score 0.9211
2021-05-25 02:41:50,897 BAD EPOCHS (no improvement): 3
2021-05-25 02:41:50,898 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:51,740 epoch 30 - iter 3/32 - loss 0.28498928 - samples/sec: 57.03 - lr: 0.012500
2021-05-25 02:41:52,564 epoch 30 - iter 6/32 - loss 0.25339017 - samples/sec: 58.24 - lr: 0.012500
2021-05-25 02:41:53,396 epoch 30 - iter 9/32 - loss 0.23330908 - samples/sec: 57.71 - lr: 0.012500
2021-05-25 02:41:54,249 epoch 30 - iter 12/32 - loss 0.21842283 - samples/sec: 56.35 - lr: 0.012500
2021-05-25 02:41:55,106 epoch 30 - iter 15/32 - loss 0.21980828 - samples/sec: 55.98 - lr: 0.012500
2021-05-25 02:41:55,965 epoch 30 - iter 18/32 - loss 0.22407782 - samples/sec: 55.95 - lr: 0.012500
2021-05-25 02:41:56,820 epoch 30 - iter 21/32 - loss 0.23836768 - samples/sec: 56.14 - lr: 0.012500
2021-05-25 02:41:57,691 epoch 30 - iter 24/32 - loss 0.24147623 - samples/sec: 55.16 - lr: 0.012500
2021-05-25 02:41:58,554 epoch 30 - iter 27/32 - loss 0.23028629 - samples/sec: 55.65 - lr: 0.012500
2021-05-25 02:41:59,393 epoch 30 - iter 30/32 - loss 0.22011324 - samples/sec: 57.23 - lr: 0.012500
2021-05-25 02:41:59,797 ----------------------------------------------------------------------------------------------------
2021-05-25 02:41:59,797 EPOCH 30 done: loss 0.2101 - lr 0.0125000
2021-05-25 02:42:00,529 DEV : loss 0.2999037504196167 - score 0.9032
Epoch    30: reducing learning rate of group 0 to 6.2500e-03.
2021-05-25 02:42:00,542 BAD EPOCHS (no improvement): 4
2021-05-25 02:42:01,895 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:01,895 Testing using best model ...
2021-05-25 02:42:01,895 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/spa.rst.sctb/best-model.pt
2021-05-25 02:42:09,328 0.7538	0.9800	0.8522
2021-05-25 02:42:09,328 
Results:
- F1-score (micro) 0.8522
- F1-score (macro) 0.8522

By class:
SENT       tp: 98 - fp: 32 - fn: 2 - precision: 0.7538 - recall: 0.9800 - f1-score: 0.8522
2021-05-25 02:42:09,328 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt/
2021-05-25 02:42:09,353 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt
2021-05-25 02:42:09,356 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt/sent_train.txt
2021-05-25 02:42:09,356 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt/sent_dev.txt
2021-05-25 02:42:09,357 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt/sent_test.txt
Corpus: 8099 train + 840 dev + 1047 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-25 02:42:18,963 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-25 02:42:18,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Corpus: "Corpus: 8099 train + 840 dev + 1047 test sentences"
2021-05-25 02:42:18,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Parameters:
2021-05-25 02:42:18,966  - learning_rate: "0.1"
2021-05-25 02:42:18,966  - mini_batch_size: "16"
2021-05-25 02:42:18,966  - patience: "3"
2021-05-25 02:42:18,966  - anneal_factor: "0.5"
2021-05-25 02:42:18,966  - max_epochs: "30"
2021-05-25 02:42:18,966  - shuffle: "True"
2021-05-25 02:42:18,966  - train_with_dev: "False"
2021-05-25 02:42:18,966  - batch_growth_annealing: "False"
2021-05-25 02:42:18,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt"
2021-05-25 02:42:18,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Device: cuda:0
2021-05-25 02:42:18,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:18,966 Embeddings storage mode: cpu
2021-05-25 02:42:18,968 ----------------------------------------------------------------------------------------------------
2021-05-25 02:42:57,810 epoch 1 - iter 50/507 - loss 4.13248062 - samples/sec: 20.60 - lr: 0.100000
2021-05-25 02:43:36,190 epoch 1 - iter 100/507 - loss 3.32178758 - samples/sec: 20.85 - lr: 0.100000
2021-05-25 02:44:14,618 epoch 1 - iter 150/507 - loss 2.84612904 - samples/sec: 20.82 - lr: 0.100000
2021-05-25 02:44:53,074 epoch 1 - iter 200/507 - loss 2.46791940 - samples/sec: 20.80 - lr: 0.100000
2021-05-25 02:45:31,482 epoch 1 - iter 250/507 - loss 2.18106009 - samples/sec: 20.83 - lr: 0.100000
2021-05-25 02:46:10,615 epoch 1 - iter 300/507 - loss 1.99649400 - samples/sec: 20.44 - lr: 0.100000
2021-05-25 02:46:49,030 epoch 1 - iter 350/507 - loss 1.82256481 - samples/sec: 20.83 - lr: 0.100000
2021-05-25 02:47:27,560 epoch 1 - iter 400/507 - loss 1.70651792 - samples/sec: 20.76 - lr: 0.100000
2021-05-25 02:48:06,114 epoch 1 - iter 450/507 - loss 1.61311063 - samples/sec: 20.75 - lr: 0.100000
2021-05-25 02:48:44,625 epoch 1 - iter 500/507 - loss 1.53968454 - samples/sec: 20.77 - lr: 0.100000
2021-05-25 02:48:49,445 ----------------------------------------------------------------------------------------------------
2021-05-25 02:48:49,445 EPOCH 1 done: loss 1.5265 - lr 0.1000000
2021-05-25 02:49:14,717 DEV : loss 0.5003674030303955 - score 0.8832
2021-05-25 02:49:14,802 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:49:15,981 ----------------------------------------------------------------------------------------------------
2021-05-25 02:49:30,214 epoch 2 - iter 50/507 - loss 0.78095496 - samples/sec: 56.22 - lr: 0.100000
2021-05-25 02:49:44,327 epoch 2 - iter 100/507 - loss 0.68279406 - samples/sec: 56.69 - lr: 0.100000
2021-05-25 02:49:58,356 epoch 2 - iter 150/507 - loss 0.66408871 - samples/sec: 57.03 - lr: 0.100000
2021-05-25 02:50:12,449 epoch 2 - iter 200/507 - loss 0.67897370 - samples/sec: 56.77 - lr: 0.100000
2021-05-25 02:50:26,633 epoch 2 - iter 250/507 - loss 0.68310910 - samples/sec: 56.41 - lr: 0.100000
2021-05-25 02:50:41,384 epoch 2 - iter 300/507 - loss 0.66630858 - samples/sec: 54.24 - lr: 0.100000
2021-05-25 02:50:55,446 epoch 2 - iter 350/507 - loss 0.66635844 - samples/sec: 56.90 - lr: 0.100000
2021-05-25 02:51:09,627 epoch 2 - iter 400/507 - loss 0.67272280 - samples/sec: 56.42 - lr: 0.100000
2021-05-25 02:51:23,933 epoch 2 - iter 450/507 - loss 0.67139870 - samples/sec: 55.93 - lr: 0.100000
2021-05-25 02:51:38,196 epoch 2 - iter 500/507 - loss 0.66862528 - samples/sec: 56.10 - lr: 0.100000
2021-05-25 02:51:39,966 ----------------------------------------------------------------------------------------------------
2021-05-25 02:51:39,966 EPOCH 2 done: loss 0.6655 - lr 0.1000000
2021-05-25 02:51:45,072 DEV : loss 0.27676922082901 - score 0.92
2021-05-25 02:51:45,158 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:51:54,848 ----------------------------------------------------------------------------------------------------
2021-05-25 02:52:09,074 epoch 3 - iter 50/507 - loss 0.55571784 - samples/sec: 56.25 - lr: 0.100000
2021-05-25 02:52:23,167 epoch 3 - iter 100/507 - loss 0.54159862 - samples/sec: 56.77 - lr: 0.100000
2021-05-25 02:52:37,327 epoch 3 - iter 150/507 - loss 0.52587072 - samples/sec: 56.50 - lr: 0.100000
2021-05-25 02:52:51,558 epoch 3 - iter 200/507 - loss 0.55245925 - samples/sec: 56.22 - lr: 0.100000
2021-05-25 02:53:05,790 epoch 3 - iter 250/507 - loss 0.56907357 - samples/sec: 56.22 - lr: 0.100000
2021-05-25 02:53:19,986 epoch 3 - iter 300/507 - loss 0.55283362 - samples/sec: 56.36 - lr: 0.100000
2021-05-25 02:53:34,088 epoch 3 - iter 350/507 - loss 0.55983466 - samples/sec: 56.74 - lr: 0.100000
2021-05-25 02:53:48,240 epoch 3 - iter 400/507 - loss 0.55824278 - samples/sec: 56.54 - lr: 0.100000
2021-05-25 02:54:02,386 epoch 3 - iter 450/507 - loss 0.56750654 - samples/sec: 56.56 - lr: 0.100000
2021-05-25 02:54:16,633 epoch 3 - iter 500/507 - loss 0.57184510 - samples/sec: 56.16 - lr: 0.100000
2021-05-25 02:54:18,409 ----------------------------------------------------------------------------------------------------
2021-05-25 02:54:18,409 EPOCH 3 done: loss 0.5710 - lr 0.1000000
2021-05-25 02:54:23,498 DEV : loss 0.3037480413913727 - score 0.9102
2021-05-25 02:54:23,583 BAD EPOCHS (no improvement): 1
2021-05-25 02:54:23,583 ----------------------------------------------------------------------------------------------------
2021-05-25 02:54:37,595 epoch 4 - iter 50/507 - loss 0.58626233 - samples/sec: 57.10 - lr: 0.100000
2021-05-25 02:54:51,710 epoch 4 - iter 100/507 - loss 0.59158524 - samples/sec: 56.68 - lr: 0.100000
2021-05-25 02:55:05,755 epoch 4 - iter 150/507 - loss 0.57851002 - samples/sec: 56.97 - lr: 0.100000
2021-05-25 02:55:19,907 epoch 4 - iter 200/507 - loss 0.58127112 - samples/sec: 56.54 - lr: 0.100000
2021-05-25 02:55:33,942 epoch 4 - iter 250/507 - loss 0.57067378 - samples/sec: 57.01 - lr: 0.100000
2021-05-25 02:55:47,966 epoch 4 - iter 300/507 - loss 0.55289989 - samples/sec: 57.05 - lr: 0.100000
2021-05-25 02:56:02,124 epoch 4 - iter 350/507 - loss 0.55322683 - samples/sec: 56.51 - lr: 0.100000
2021-05-25 02:56:16,221 epoch 4 - iter 400/507 - loss 0.55646935 - samples/sec: 56.76 - lr: 0.100000
2021-05-25 02:56:30,310 epoch 4 - iter 450/507 - loss 0.55390313 - samples/sec: 56.79 - lr: 0.100000
2021-05-25 02:56:44,347 epoch 4 - iter 500/507 - loss 0.55628115 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 02:56:46,140 ----------------------------------------------------------------------------------------------------
2021-05-25 02:56:46,141 EPOCH 4 done: loss 0.5557 - lr 0.1000000
2021-05-25 02:56:51,230 DEV : loss 0.23632287979125977 - score 0.9364
2021-05-25 02:56:51,316 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 02:57:01,064 ----------------------------------------------------------------------------------------------------
2021-05-25 02:57:15,151 epoch 5 - iter 50/507 - loss 0.47832746 - samples/sec: 56.80 - lr: 0.100000
2021-05-25 02:57:29,332 epoch 5 - iter 100/507 - loss 0.45543265 - samples/sec: 56.42 - lr: 0.100000
2021-05-25 02:57:43,540 epoch 5 - iter 150/507 - loss 0.48851647 - samples/sec: 56.32 - lr: 0.100000
2021-05-25 02:57:57,564 epoch 5 - iter 200/507 - loss 0.49220004 - samples/sec: 57.05 - lr: 0.100000
2021-05-25 02:58:11,646 epoch 5 - iter 250/507 - loss 0.50526342 - samples/sec: 56.82 - lr: 0.100000
2021-05-25 02:58:25,727 epoch 5 - iter 300/507 - loss 0.51036579 - samples/sec: 56.82 - lr: 0.100000
2021-05-25 02:58:39,812 epoch 5 - iter 350/507 - loss 0.52354430 - samples/sec: 56.81 - lr: 0.100000
2021-05-25 02:58:53,897 epoch 5 - iter 400/507 - loss 0.52240976 - samples/sec: 56.80 - lr: 0.100000
2021-05-25 02:59:08,085 epoch 5 - iter 450/507 - loss 0.51005585 - samples/sec: 56.39 - lr: 0.100000
2021-05-25 02:59:22,213 epoch 5 - iter 500/507 - loss 0.51127397 - samples/sec: 56.63 - lr: 0.100000
2021-05-25 02:59:24,001 ----------------------------------------------------------------------------------------------------
2021-05-25 02:59:24,001 EPOCH 5 done: loss 0.5120 - lr 0.1000000
2021-05-25 02:59:29,086 DEV : loss 0.2671554386615753 - score 0.932
2021-05-25 02:59:29,172 BAD EPOCHS (no improvement): 1
2021-05-25 02:59:29,172 ----------------------------------------------------------------------------------------------------
2021-05-25 02:59:43,292 epoch 6 - iter 50/507 - loss 0.45732648 - samples/sec: 56.67 - lr: 0.100000
2021-05-25 02:59:57,375 epoch 6 - iter 100/507 - loss 0.46073302 - samples/sec: 56.81 - lr: 0.100000
2021-05-25 03:00:11,438 epoch 6 - iter 150/507 - loss 0.46710556 - samples/sec: 56.89 - lr: 0.100000
2021-05-25 03:00:25,497 epoch 6 - iter 200/507 - loss 0.47357985 - samples/sec: 56.91 - lr: 0.100000
2021-05-25 03:00:39,581 epoch 6 - iter 250/507 - loss 0.47600364 - samples/sec: 56.81 - lr: 0.100000
2021-05-25 03:00:53,720 epoch 6 - iter 300/507 - loss 0.46635223 - samples/sec: 56.59 - lr: 0.100000
2021-05-25 03:01:07,763 epoch 6 - iter 350/507 - loss 0.46598720 - samples/sec: 56.98 - lr: 0.100000
2021-05-25 03:01:21,842 epoch 6 - iter 400/507 - loss 0.47453304 - samples/sec: 56.83 - lr: 0.100000
2021-05-25 03:01:35,892 epoch 6 - iter 450/507 - loss 0.47112735 - samples/sec: 56.95 - lr: 0.100000
2021-05-25 03:01:49,937 epoch 6 - iter 500/507 - loss 0.46984651 - samples/sec: 56.97 - lr: 0.100000
2021-05-25 03:01:51,722 ----------------------------------------------------------------------------------------------------
2021-05-25 03:01:51,722 EPOCH 6 done: loss 0.4668 - lr 0.1000000
2021-05-25 03:01:56,811 DEV : loss 0.22540748119354248 - score 0.9424
2021-05-25 03:01:56,897 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:02:06,768 ----------------------------------------------------------------------------------------------------
2021-05-25 03:02:20,824 epoch 7 - iter 50/507 - loss 0.45181540 - samples/sec: 56.93 - lr: 0.100000
2021-05-25 03:02:35,010 epoch 7 - iter 100/507 - loss 0.45327079 - samples/sec: 56.40 - lr: 0.100000
2021-05-25 03:02:49,165 epoch 7 - iter 150/507 - loss 0.45241629 - samples/sec: 56.53 - lr: 0.100000
2021-05-25 03:03:03,231 epoch 7 - iter 200/507 - loss 0.44857096 - samples/sec: 56.88 - lr: 0.100000
2021-05-25 03:03:17,272 epoch 7 - iter 250/507 - loss 0.44097761 - samples/sec: 56.98 - lr: 0.100000
2021-05-25 03:03:31,382 epoch 7 - iter 300/507 - loss 0.43549399 - samples/sec: 56.70 - lr: 0.100000
2021-05-25 03:03:45,528 epoch 7 - iter 350/507 - loss 0.44086491 - samples/sec: 56.56 - lr: 0.100000
2021-05-25 03:03:59,547 epoch 7 - iter 400/507 - loss 0.44025388 - samples/sec: 57.07 - lr: 0.100000
2021-05-25 03:04:13,665 epoch 7 - iter 450/507 - loss 0.44292278 - samples/sec: 56.67 - lr: 0.100000
2021-05-25 03:04:28,423 epoch 7 - iter 500/507 - loss 0.43791431 - samples/sec: 54.22 - lr: 0.100000
2021-05-25 03:04:30,198 ----------------------------------------------------------------------------------------------------
2021-05-25 03:04:30,199 EPOCH 7 done: loss 0.4353 - lr 0.1000000
2021-05-25 03:04:35,298 DEV : loss 0.27956095337867737 - score 0.9337
2021-05-25 03:04:35,385 BAD EPOCHS (no improvement): 1
2021-05-25 03:04:35,385 ----------------------------------------------------------------------------------------------------
2021-05-25 03:04:49,471 epoch 8 - iter 50/507 - loss 0.35890309 - samples/sec: 56.80 - lr: 0.100000
2021-05-25 03:05:03,527 epoch 8 - iter 100/507 - loss 0.43137685 - samples/sec: 56.92 - lr: 0.100000
2021-05-25 03:05:17,554 epoch 8 - iter 150/507 - loss 0.42995697 - samples/sec: 57.04 - lr: 0.100000
2021-05-25 03:05:31,682 epoch 8 - iter 200/507 - loss 0.41724294 - samples/sec: 56.63 - lr: 0.100000
2021-05-25 03:05:45,761 epoch 8 - iter 250/507 - loss 0.42584530 - samples/sec: 56.83 - lr: 0.100000
2021-05-25 03:05:59,807 epoch 8 - iter 300/507 - loss 0.42802881 - samples/sec: 56.96 - lr: 0.100000
2021-05-25 03:06:13,884 epoch 8 - iter 350/507 - loss 0.42291921 - samples/sec: 56.84 - lr: 0.100000
2021-05-25 03:06:27,952 epoch 8 - iter 400/507 - loss 0.42411850 - samples/sec: 56.87 - lr: 0.100000
2021-05-25 03:06:42,019 epoch 8 - iter 450/507 - loss 0.41181118 - samples/sec: 56.88 - lr: 0.100000
2021-05-25 03:06:56,137 epoch 8 - iter 500/507 - loss 0.40575223 - samples/sec: 56.68 - lr: 0.100000
2021-05-25 03:06:57,923 ----------------------------------------------------------------------------------------------------
2021-05-25 03:06:57,923 EPOCH 8 done: loss 0.4080 - lr 0.1000000
2021-05-25 03:07:03,019 DEV : loss 0.3035140931606293 - score 0.9211
2021-05-25 03:07:03,104 BAD EPOCHS (no improvement): 2
2021-05-25 03:07:03,105 ----------------------------------------------------------------------------------------------------
2021-05-25 03:07:17,290 epoch 9 - iter 50/507 - loss 0.34922374 - samples/sec: 56.41 - lr: 0.100000
2021-05-25 03:07:31,407 epoch 9 - iter 100/507 - loss 0.39608590 - samples/sec: 56.68 - lr: 0.100000
2021-05-25 03:07:45,362 epoch 9 - iter 150/507 - loss 0.41872290 - samples/sec: 57.34 - lr: 0.100000
2021-05-25 03:07:59,367 epoch 9 - iter 200/507 - loss 0.40444692 - samples/sec: 57.13 - lr: 0.100000
2021-05-25 03:08:13,522 epoch 9 - iter 250/507 - loss 0.40514643 - samples/sec: 56.53 - lr: 0.100000
2021-05-25 03:08:27,649 epoch 9 - iter 300/507 - loss 0.39788546 - samples/sec: 56.64 - lr: 0.100000
2021-05-25 03:08:41,784 epoch 9 - iter 350/507 - loss 0.40168769 - samples/sec: 56.60 - lr: 0.100000
2021-05-25 03:08:55,877 epoch 9 - iter 400/507 - loss 0.40431025 - samples/sec: 56.77 - lr: 0.100000
2021-05-25 03:09:09,914 epoch 9 - iter 450/507 - loss 0.41424020 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 03:09:24,036 epoch 9 - iter 500/507 - loss 0.41397499 - samples/sec: 56.65 - lr: 0.100000
2021-05-25 03:09:25,813 ----------------------------------------------------------------------------------------------------
2021-05-25 03:09:25,813 EPOCH 9 done: loss 0.4146 - lr 0.1000000
2021-05-25 03:09:30,907 DEV : loss 0.20462119579315186 - score 0.9487
2021-05-25 03:09:30,993 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:09:40,772 ----------------------------------------------------------------------------------------------------
2021-05-25 03:09:54,788 epoch 10 - iter 50/507 - loss 0.44367377 - samples/sec: 57.09 - lr: 0.100000
2021-05-25 03:10:08,868 epoch 10 - iter 100/507 - loss 0.41709392 - samples/sec: 56.83 - lr: 0.100000
2021-05-25 03:10:22,899 epoch 10 - iter 150/507 - loss 0.41168308 - samples/sec: 57.02 - lr: 0.100000
2021-05-25 03:10:36,991 epoch 10 - iter 200/507 - loss 0.42022981 - samples/sec: 56.78 - lr: 0.100000
2021-05-25 03:10:51,088 epoch 10 - iter 250/507 - loss 0.41367752 - samples/sec: 56.76 - lr: 0.100000
2021-05-25 03:11:05,153 epoch 10 - iter 300/507 - loss 0.40639122 - samples/sec: 56.88 - lr: 0.100000
2021-05-25 03:11:19,267 epoch 10 - iter 350/507 - loss 0.39734488 - samples/sec: 56.69 - lr: 0.100000
2021-05-25 03:11:33,413 epoch 10 - iter 400/507 - loss 0.39109452 - samples/sec: 56.56 - lr: 0.100000
2021-05-25 03:11:47,389 epoch 10 - iter 450/507 - loss 0.39174882 - samples/sec: 57.25 - lr: 0.100000
2021-05-25 03:12:01,485 epoch 10 - iter 500/507 - loss 0.38782265 - samples/sec: 56.76 - lr: 0.100000
2021-05-25 03:12:03,274 ----------------------------------------------------------------------------------------------------
2021-05-25 03:12:03,275 EPOCH 10 done: loss 0.3913 - lr 0.1000000
2021-05-25 03:12:08,378 DEV : loss 0.2086755335330963 - score 0.9354
2021-05-25 03:12:08,464 BAD EPOCHS (no improvement): 1
2021-05-25 03:12:08,464 ----------------------------------------------------------------------------------------------------
2021-05-25 03:12:22,495 epoch 11 - iter 50/507 - loss 0.33023883 - samples/sec: 57.02 - lr: 0.100000
2021-05-25 03:12:36,613 epoch 11 - iter 100/507 - loss 0.35047389 - samples/sec: 56.67 - lr: 0.100000
2021-05-25 03:12:50,775 epoch 11 - iter 150/507 - loss 0.34611083 - samples/sec: 56.50 - lr: 0.100000
2021-05-25 03:13:04,918 epoch 11 - iter 200/507 - loss 0.35419507 - samples/sec: 56.57 - lr: 0.100000
2021-05-25 03:13:19,031 epoch 11 - iter 250/507 - loss 0.35689826 - samples/sec: 56.69 - lr: 0.100000
2021-05-25 03:13:33,072 epoch 11 - iter 300/507 - loss 0.35475474 - samples/sec: 56.99 - lr: 0.100000
2021-05-25 03:13:47,203 epoch 11 - iter 350/507 - loss 0.34997177 - samples/sec: 56.62 - lr: 0.100000
2021-05-25 03:14:01,511 epoch 11 - iter 400/507 - loss 0.35108377 - samples/sec: 55.92 - lr: 0.100000
2021-05-25 03:14:15,650 epoch 11 - iter 450/507 - loss 0.35435525 - samples/sec: 56.59 - lr: 0.100000
2021-05-25 03:14:29,816 epoch 11 - iter 500/507 - loss 0.34933738 - samples/sec: 56.48 - lr: 0.100000
2021-05-25 03:14:31,619 ----------------------------------------------------------------------------------------------------
2021-05-25 03:14:31,619 EPOCH 11 done: loss 0.3501 - lr 0.1000000
2021-05-25 03:14:36,723 DEV : loss 0.1856652945280075 - score 0.9455
2021-05-25 03:14:36,808 BAD EPOCHS (no improvement): 2
2021-05-25 03:14:36,809 ----------------------------------------------------------------------------------------------------
2021-05-25 03:14:51,070 epoch 12 - iter 50/507 - loss 0.33837167 - samples/sec: 56.11 - lr: 0.100000
2021-05-25 03:15:05,107 epoch 12 - iter 100/507 - loss 0.33010500 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 03:15:19,301 epoch 12 - iter 150/507 - loss 0.34753671 - samples/sec: 56.37 - lr: 0.100000
2021-05-25 03:15:33,420 epoch 12 - iter 200/507 - loss 0.36169662 - samples/sec: 56.67 - lr: 0.100000
2021-05-25 03:15:47,631 epoch 12 - iter 250/507 - loss 0.34839179 - samples/sec: 56.30 - lr: 0.100000
2021-05-25 03:16:01,607 epoch 12 - iter 300/507 - loss 0.34492614 - samples/sec: 57.25 - lr: 0.100000
2021-05-25 03:16:15,610 epoch 12 - iter 350/507 - loss 0.34712247 - samples/sec: 57.14 - lr: 0.100000
2021-05-25 03:16:29,643 epoch 12 - iter 400/507 - loss 0.35755543 - samples/sec: 57.02 - lr: 0.100000
2021-05-25 03:16:43,670 epoch 12 - iter 450/507 - loss 0.35606704 - samples/sec: 57.04 - lr: 0.100000
2021-05-25 03:16:57,792 epoch 12 - iter 500/507 - loss 0.35337762 - samples/sec: 56.66 - lr: 0.100000
2021-05-25 03:16:59,577 ----------------------------------------------------------------------------------------------------
2021-05-25 03:16:59,577 EPOCH 12 done: loss 0.3535 - lr 0.1000000
2021-05-25 03:17:05,354 DEV : loss 0.25528234243392944 - score 0.9373
2021-05-25 03:17:05,440 BAD EPOCHS (no improvement): 3
2021-05-25 03:17:05,441 ----------------------------------------------------------------------------------------------------
2021-05-25 03:17:19,512 epoch 13 - iter 50/507 - loss 0.40224394 - samples/sec: 56.86 - lr: 0.100000
2021-05-25 03:17:33,628 epoch 13 - iter 100/507 - loss 0.35940819 - samples/sec: 56.68 - lr: 0.100000
2021-05-25 03:17:47,705 epoch 13 - iter 150/507 - loss 0.34679260 - samples/sec: 56.84 - lr: 0.100000
2021-05-25 03:18:01,758 epoch 13 - iter 200/507 - loss 0.34112034 - samples/sec: 56.93 - lr: 0.100000
2021-05-25 03:18:15,843 epoch 13 - iter 250/507 - loss 0.34908205 - samples/sec: 56.81 - lr: 0.100000
2021-05-25 03:18:29,912 epoch 13 - iter 300/507 - loss 0.34562551 - samples/sec: 56.87 - lr: 0.100000
2021-05-25 03:18:44,050 epoch 13 - iter 350/507 - loss 0.34538894 - samples/sec: 56.59 - lr: 0.100000
2021-05-25 03:18:58,133 epoch 13 - iter 400/507 - loss 0.34798908 - samples/sec: 56.82 - lr: 0.100000
2021-05-25 03:19:12,346 epoch 13 - iter 450/507 - loss 0.33859316 - samples/sec: 56.29 - lr: 0.100000
2021-05-25 03:19:26,340 epoch 13 - iter 500/507 - loss 0.33910295 - samples/sec: 57.18 - lr: 0.100000
2021-05-25 03:19:28,122 ----------------------------------------------------------------------------------------------------
2021-05-25 03:19:28,122 EPOCH 13 done: loss 0.3398 - lr 0.1000000
2021-05-25 03:19:33,219 DEV : loss 0.21770602464675903 - score 0.9389
Epoch    13: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 03:19:33,306 BAD EPOCHS (no improvement): 4
2021-05-25 03:19:33,306 ----------------------------------------------------------------------------------------------------
2021-05-25 03:19:47,458 epoch 14 - iter 50/507 - loss 0.36606040 - samples/sec: 56.54 - lr: 0.050000
2021-05-25 03:20:01,562 epoch 14 - iter 100/507 - loss 0.32697623 - samples/sec: 56.73 - lr: 0.050000
2021-05-25 03:20:15,768 epoch 14 - iter 150/507 - loss 0.30512979 - samples/sec: 56.32 - lr: 0.050000
2021-05-25 03:20:29,905 epoch 14 - iter 200/507 - loss 0.30135583 - samples/sec: 56.60 - lr: 0.050000
2021-05-25 03:20:43,923 epoch 14 - iter 250/507 - loss 0.30641086 - samples/sec: 57.08 - lr: 0.050000
2021-05-25 03:20:57,945 epoch 14 - iter 300/507 - loss 0.30838972 - samples/sec: 57.06 - lr: 0.050000
2021-05-25 03:21:11,987 epoch 14 - iter 350/507 - loss 0.31415089 - samples/sec: 56.98 - lr: 0.050000
2021-05-25 03:21:26,067 epoch 14 - iter 400/507 - loss 0.31350739 - samples/sec: 56.83 - lr: 0.050000
2021-05-25 03:21:40,155 epoch 14 - iter 450/507 - loss 0.30409354 - samples/sec: 56.79 - lr: 0.050000
2021-05-25 03:21:54,223 epoch 14 - iter 500/507 - loss 0.30239299 - samples/sec: 56.87 - lr: 0.050000
2021-05-25 03:21:56,013 ----------------------------------------------------------------------------------------------------
2021-05-25 03:21:56,013 EPOCH 14 done: loss 0.3036 - lr 0.0500000
2021-05-25 03:22:01,115 DEV : loss 0.17562811076641083 - score 0.944
2021-05-25 03:22:01,202 BAD EPOCHS (no improvement): 1
2021-05-25 03:22:01,202 ----------------------------------------------------------------------------------------------------
2021-05-25 03:22:15,390 epoch 15 - iter 50/507 - loss 0.24733778 - samples/sec: 56.40 - lr: 0.050000
2021-05-25 03:22:29,550 epoch 15 - iter 100/507 - loss 0.27407529 - samples/sec: 56.50 - lr: 0.050000
2021-05-25 03:22:43,746 epoch 15 - iter 150/507 - loss 0.26736858 - samples/sec: 56.36 - lr: 0.050000
2021-05-25 03:22:57,901 epoch 15 - iter 200/507 - loss 0.27850776 - samples/sec: 56.52 - lr: 0.050000
2021-05-25 03:23:12,020 epoch 15 - iter 250/507 - loss 0.27091674 - samples/sec: 56.67 - lr: 0.050000
2021-05-25 03:23:26,250 epoch 15 - iter 300/507 - loss 0.26807381 - samples/sec: 56.23 - lr: 0.050000
2021-05-25 03:23:40,410 epoch 15 - iter 350/507 - loss 0.27566795 - samples/sec: 56.51 - lr: 0.050000
2021-05-25 03:23:54,446 epoch 15 - iter 400/507 - loss 0.27410809 - samples/sec: 57.00 - lr: 0.050000
2021-05-25 03:24:08,472 epoch 15 - iter 450/507 - loss 0.27639938 - samples/sec: 57.05 - lr: 0.050000
2021-05-25 03:24:22,608 epoch 15 - iter 500/507 - loss 0.27707771 - samples/sec: 56.60 - lr: 0.050000
2021-05-25 03:24:24,404 ----------------------------------------------------------------------------------------------------
2021-05-25 03:24:24,404 EPOCH 15 done: loss 0.2763 - lr 0.0500000
2021-05-25 03:24:29,515 DEV : loss 0.19621407985687256 - score 0.948
2021-05-25 03:24:29,602 BAD EPOCHS (no improvement): 2
2021-05-25 03:24:29,602 ----------------------------------------------------------------------------------------------------
2021-05-25 03:24:43,655 epoch 16 - iter 50/507 - loss 0.31972873 - samples/sec: 56.94 - lr: 0.050000
2021-05-25 03:24:57,815 epoch 16 - iter 100/507 - loss 0.30472502 - samples/sec: 56.50 - lr: 0.050000
2021-05-25 03:25:11,854 epoch 16 - iter 150/507 - loss 0.27681340 - samples/sec: 56.99 - lr: 0.050000
2021-05-25 03:25:25,968 epoch 16 - iter 200/507 - loss 0.27424659 - samples/sec: 56.69 - lr: 0.050000
2021-05-25 03:25:40,114 epoch 16 - iter 250/507 - loss 0.26394100 - samples/sec: 56.56 - lr: 0.050000
2021-05-25 03:25:54,383 epoch 16 - iter 300/507 - loss 0.25808505 - samples/sec: 56.07 - lr: 0.050000
2021-05-25 03:26:08,519 epoch 16 - iter 350/507 - loss 0.25909818 - samples/sec: 56.60 - lr: 0.050000
2021-05-25 03:26:22,777 epoch 16 - iter 400/507 - loss 0.26498861 - samples/sec: 56.12 - lr: 0.050000
2021-05-25 03:26:37,026 epoch 16 - iter 450/507 - loss 0.26280675 - samples/sec: 56.15 - lr: 0.050000
2021-05-25 03:26:51,133 epoch 16 - iter 500/507 - loss 0.25809693 - samples/sec: 56.72 - lr: 0.050000
2021-05-25 03:26:52,937 ----------------------------------------------------------------------------------------------------
2021-05-25 03:26:52,937 EPOCH 16 done: loss 0.2591 - lr 0.0500000
2021-05-25 03:26:58,031 DEV : loss 0.22768129408359528 - score 0.9438
2021-05-25 03:26:58,118 BAD EPOCHS (no improvement): 3
2021-05-25 03:26:58,119 ----------------------------------------------------------------------------------------------------
2021-05-25 03:27:12,221 epoch 17 - iter 50/507 - loss 0.20770288 - samples/sec: 56.74 - lr: 0.050000
2021-05-25 03:27:26,280 epoch 17 - iter 100/507 - loss 0.23375541 - samples/sec: 56.91 - lr: 0.050000
2021-05-25 03:27:40,277 epoch 17 - iter 150/507 - loss 0.23835271 - samples/sec: 57.17 - lr: 0.050000
2021-05-25 03:27:54,328 epoch 17 - iter 200/507 - loss 0.24609464 - samples/sec: 56.94 - lr: 0.050000
2021-05-25 03:28:08,342 epoch 17 - iter 250/507 - loss 0.25091117 - samples/sec: 57.09 - lr: 0.050000
2021-05-25 03:28:22,398 epoch 17 - iter 300/507 - loss 0.25148812 - samples/sec: 56.92 - lr: 0.050000
2021-05-25 03:28:36,404 epoch 17 - iter 350/507 - loss 0.26347710 - samples/sec: 57.13 - lr: 0.050000
2021-05-25 03:28:50,453 epoch 17 - iter 400/507 - loss 0.26679312 - samples/sec: 56.95 - lr: 0.050000
2021-05-25 03:29:04,482 epoch 17 - iter 450/507 - loss 0.26275731 - samples/sec: 57.03 - lr: 0.050000
2021-05-25 03:29:18,559 epoch 17 - iter 500/507 - loss 0.25957554 - samples/sec: 56.84 - lr: 0.050000
2021-05-25 03:29:20,333 ----------------------------------------------------------------------------------------------------
2021-05-25 03:29:20,333 EPOCH 17 done: loss 0.2596 - lr 0.0500000
2021-05-25 03:29:26,115 DEV : loss 0.18399815261363983 - score 0.9417
Epoch    17: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 03:29:26,201 BAD EPOCHS (no improvement): 4
2021-05-25 03:29:26,201 ----------------------------------------------------------------------------------------------------
2021-05-25 03:29:40,211 epoch 18 - iter 50/507 - loss 0.30116296 - samples/sec: 57.11 - lr: 0.025000
2021-05-25 03:29:54,259 epoch 18 - iter 100/507 - loss 0.27223020 - samples/sec: 56.95 - lr: 0.025000
2021-05-25 03:30:08,369 epoch 18 - iter 150/507 - loss 0.28522383 - samples/sec: 56.71 - lr: 0.025000
2021-05-25 03:30:22,352 epoch 18 - iter 200/507 - loss 0.26622385 - samples/sec: 57.22 - lr: 0.025000
2021-05-25 03:30:36,316 epoch 18 - iter 250/507 - loss 0.26066015 - samples/sec: 57.30 - lr: 0.025000
2021-05-25 03:30:50,361 epoch 18 - iter 300/507 - loss 0.26288616 - samples/sec: 56.97 - lr: 0.025000
2021-05-25 03:31:04,372 epoch 18 - iter 350/507 - loss 0.25285429 - samples/sec: 57.11 - lr: 0.025000
2021-05-25 03:31:18,430 epoch 18 - iter 400/507 - loss 0.24829975 - samples/sec: 56.91 - lr: 0.025000
2021-05-25 03:31:32,436 epoch 18 - iter 450/507 - loss 0.24236381 - samples/sec: 57.13 - lr: 0.025000
2021-05-25 03:31:46,458 epoch 18 - iter 500/507 - loss 0.24197158 - samples/sec: 57.06 - lr: 0.025000
2021-05-25 03:31:48,234 ----------------------------------------------------------------------------------------------------
2021-05-25 03:31:48,235 EPOCH 18 done: loss 0.2441 - lr 0.0250000
2021-05-25 03:31:53,334 DEV : loss 0.17347142100334167 - score 0.9463
2021-05-25 03:31:53,421 BAD EPOCHS (no improvement): 1
2021-05-25 03:31:53,421 ----------------------------------------------------------------------------------------------------
2021-05-25 03:32:07,472 epoch 19 - iter 50/507 - loss 0.21430488 - samples/sec: 56.94 - lr: 0.025000
2021-05-25 03:32:21,511 epoch 19 - iter 100/507 - loss 0.22412517 - samples/sec: 56.99 - lr: 0.025000
2021-05-25 03:32:35,492 epoch 19 - iter 150/507 - loss 0.22242491 - samples/sec: 57.23 - lr: 0.025000
2021-05-25 03:32:49,503 epoch 19 - iter 200/507 - loss 0.21948359 - samples/sec: 57.11 - lr: 0.025000
2021-05-25 03:33:03,557 epoch 19 - iter 250/507 - loss 0.23367061 - samples/sec: 56.93 - lr: 0.025000
2021-05-25 03:33:17,705 epoch 19 - iter 300/507 - loss 0.23814337 - samples/sec: 56.55 - lr: 0.025000
2021-05-25 03:33:31,683 epoch 19 - iter 350/507 - loss 0.23952747 - samples/sec: 57.24 - lr: 0.025000
2021-05-25 03:33:45,724 epoch 19 - iter 400/507 - loss 0.23801601 - samples/sec: 56.98 - lr: 0.025000
2021-05-25 03:33:59,789 epoch 19 - iter 450/507 - loss 0.23808076 - samples/sec: 56.89 - lr: 0.025000
2021-05-25 03:34:13,931 epoch 19 - iter 500/507 - loss 0.23577204 - samples/sec: 56.58 - lr: 0.025000
2021-05-25 03:34:15,726 ----------------------------------------------------------------------------------------------------
2021-05-25 03:34:15,727 EPOCH 19 done: loss 0.2351 - lr 0.0250000
2021-05-25 03:34:20,821 DEV : loss 0.16115394234657288 - score 0.9521
2021-05-25 03:34:20,907 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:34:31,389 ----------------------------------------------------------------------------------------------------
2021-05-25 03:34:45,459 epoch 20 - iter 50/507 - loss 0.20206821 - samples/sec: 56.87 - lr: 0.025000
2021-05-25 03:34:59,655 epoch 20 - iter 100/507 - loss 0.22928335 - samples/sec: 56.36 - lr: 0.025000
2021-05-25 03:35:13,828 epoch 20 - iter 150/507 - loss 0.21523280 - samples/sec: 56.45 - lr: 0.025000
2021-05-25 03:35:27,998 epoch 20 - iter 200/507 - loss 0.21174816 - samples/sec: 56.47 - lr: 0.025000
2021-05-25 03:35:42,177 epoch 20 - iter 250/507 - loss 0.21903554 - samples/sec: 56.43 - lr: 0.025000
2021-05-25 03:35:56,306 epoch 20 - iter 300/507 - loss 0.21424757 - samples/sec: 56.63 - lr: 0.025000
2021-05-25 03:36:10,456 epoch 20 - iter 350/507 - loss 0.21710530 - samples/sec: 56.54 - lr: 0.025000
2021-05-25 03:36:24,698 epoch 20 - iter 400/507 - loss 0.22030266 - samples/sec: 56.18 - lr: 0.025000
2021-05-25 03:36:38,929 epoch 20 - iter 450/507 - loss 0.21993211 - samples/sec: 56.22 - lr: 0.025000
2021-05-25 03:36:53,106 epoch 20 - iter 500/507 - loss 0.22103628 - samples/sec: 56.44 - lr: 0.025000
2021-05-25 03:36:54,892 ----------------------------------------------------------------------------------------------------
2021-05-25 03:36:54,892 EPOCH 20 done: loss 0.2207 - lr 0.0250000
2021-05-25 03:36:59,991 DEV : loss 0.17313086986541748 - score 0.9464
2021-05-25 03:37:00,077 BAD EPOCHS (no improvement): 1
2021-05-25 03:37:00,077 ----------------------------------------------------------------------------------------------------
2021-05-25 03:37:14,289 epoch 21 - iter 50/507 - loss 0.22281406 - samples/sec: 56.30 - lr: 0.025000
2021-05-25 03:37:28,377 epoch 21 - iter 100/507 - loss 0.21416036 - samples/sec: 56.79 - lr: 0.025000
2021-05-25 03:37:42,560 epoch 21 - iter 150/507 - loss 0.21146186 - samples/sec: 56.41 - lr: 0.025000
2021-05-25 03:37:56,703 epoch 21 - iter 200/507 - loss 0.20945069 - samples/sec: 56.57 - lr: 0.025000
2021-05-25 03:38:10,940 epoch 21 - iter 250/507 - loss 0.20951849 - samples/sec: 56.20 - lr: 0.025000
2021-05-25 03:38:25,020 epoch 21 - iter 300/507 - loss 0.20658658 - samples/sec: 56.83 - lr: 0.025000
2021-05-25 03:38:39,081 epoch 21 - iter 350/507 - loss 0.20820661 - samples/sec: 56.91 - lr: 0.025000
2021-05-25 03:38:53,175 epoch 21 - iter 400/507 - loss 0.20878979 - samples/sec: 56.77 - lr: 0.025000
2021-05-25 03:39:07,157 epoch 21 - iter 450/507 - loss 0.21359448 - samples/sec: 57.22 - lr: 0.025000
2021-05-25 03:39:21,184 epoch 21 - iter 500/507 - loss 0.21293164 - samples/sec: 57.04 - lr: 0.025000
2021-05-25 03:39:22,989 ----------------------------------------------------------------------------------------------------
2021-05-25 03:39:22,990 EPOCH 21 done: loss 0.2139 - lr 0.0250000
2021-05-25 03:39:28,086 DEV : loss 0.16727429628372192 - score 0.9503
2021-05-25 03:39:28,172 BAD EPOCHS (no improvement): 2
2021-05-25 03:39:28,172 ----------------------------------------------------------------------------------------------------
2021-05-25 03:39:42,344 epoch 22 - iter 50/507 - loss 0.19965314 - samples/sec: 56.46 - lr: 0.025000
2021-05-25 03:39:56,419 epoch 22 - iter 100/507 - loss 0.20238751 - samples/sec: 56.85 - lr: 0.025000
2021-05-25 03:40:10,569 epoch 22 - iter 150/507 - loss 0.21888628 - samples/sec: 56.55 - lr: 0.025000
2021-05-25 03:40:24,621 epoch 22 - iter 200/507 - loss 0.21908740 - samples/sec: 56.94 - lr: 0.025000
2021-05-25 03:40:38,645 epoch 22 - iter 250/507 - loss 0.22066568 - samples/sec: 57.05 - lr: 0.025000
2021-05-25 03:40:52,745 epoch 22 - iter 300/507 - loss 0.21815802 - samples/sec: 56.75 - lr: 0.025000
2021-05-25 03:41:06,845 epoch 22 - iter 350/507 - loss 0.22166659 - samples/sec: 56.75 - lr: 0.025000
2021-05-25 03:41:20,873 epoch 22 - iter 400/507 - loss 0.22266067 - samples/sec: 57.04 - lr: 0.025000
2021-05-25 03:41:34,940 epoch 22 - iter 450/507 - loss 0.22171426 - samples/sec: 56.88 - lr: 0.025000
2021-05-25 03:41:48,961 epoch 22 - iter 500/507 - loss 0.21634434 - samples/sec: 57.07 - lr: 0.025000
2021-05-25 03:41:50,733 ----------------------------------------------------------------------------------------------------
2021-05-25 03:41:50,733 EPOCH 22 done: loss 0.2165 - lr 0.0250000
2021-05-25 03:41:56,494 DEV : loss 0.1770581156015396 - score 0.9417
2021-05-25 03:41:56,581 BAD EPOCHS (no improvement): 3
2021-05-25 03:41:56,581 ----------------------------------------------------------------------------------------------------
2021-05-25 03:42:10,601 epoch 23 - iter 50/507 - loss 0.24103331 - samples/sec: 57.07 - lr: 0.025000
2021-05-25 03:42:24,645 epoch 23 - iter 100/507 - loss 0.22667573 - samples/sec: 56.97 - lr: 0.025000
2021-05-25 03:42:38,673 epoch 23 - iter 150/507 - loss 0.22160399 - samples/sec: 57.04 - lr: 0.025000
2021-05-25 03:42:52,660 epoch 23 - iter 200/507 - loss 0.22759810 - samples/sec: 57.20 - lr: 0.025000
2021-05-25 03:43:06,664 epoch 23 - iter 250/507 - loss 0.22839518 - samples/sec: 57.13 - lr: 0.025000
2021-05-25 03:43:20,669 epoch 23 - iter 300/507 - loss 0.22387599 - samples/sec: 57.13 - lr: 0.025000
2021-05-25 03:43:34,668 epoch 23 - iter 350/507 - loss 0.22281040 - samples/sec: 57.15 - lr: 0.025000
2021-05-25 03:43:48,649 epoch 23 - iter 400/507 - loss 0.22013251 - samples/sec: 57.23 - lr: 0.025000
2021-05-25 03:44:02,700 epoch 23 - iter 450/507 - loss 0.21848540 - samples/sec: 56.94 - lr: 0.025000
2021-05-25 03:44:16,901 epoch 23 - iter 500/507 - loss 0.21672035 - samples/sec: 56.35 - lr: 0.025000
2021-05-25 03:44:18,685 ----------------------------------------------------------------------------------------------------
2021-05-25 03:44:18,685 EPOCH 23 done: loss 0.2151 - lr 0.0250000
2021-05-25 03:44:23,773 DEV : loss 0.17629337310791016 - score 0.9461
Epoch    23: reducing learning rate of group 0 to 1.2500e-02.
2021-05-25 03:44:23,859 BAD EPOCHS (no improvement): 4
2021-05-25 03:44:23,859 ----------------------------------------------------------------------------------------------------
2021-05-25 03:44:38,064 epoch 24 - iter 50/507 - loss 0.19967363 - samples/sec: 56.33 - lr: 0.012500
2021-05-25 03:44:52,276 epoch 24 - iter 100/507 - loss 0.21484330 - samples/sec: 56.30 - lr: 0.012500
2021-05-25 03:45:06,485 epoch 24 - iter 150/507 - loss 0.20637962 - samples/sec: 56.31 - lr: 0.012500
2021-05-25 03:45:20,595 epoch 24 - iter 200/507 - loss 0.21055803 - samples/sec: 56.71 - lr: 0.012500
2021-05-25 03:45:34,728 epoch 24 - iter 250/507 - loss 0.21453097 - samples/sec: 56.61 - lr: 0.012500
2021-05-25 03:45:48,911 epoch 24 - iter 300/507 - loss 0.21665079 - samples/sec: 56.42 - lr: 0.012500
2021-05-25 03:46:03,019 epoch 24 - iter 350/507 - loss 0.21373837 - samples/sec: 56.71 - lr: 0.012500
2021-05-25 03:46:17,136 epoch 24 - iter 400/507 - loss 0.21643069 - samples/sec: 56.68 - lr: 0.012500
2021-05-25 03:46:31,274 epoch 24 - iter 450/507 - loss 0.21365974 - samples/sec: 56.59 - lr: 0.012500
2021-05-25 03:46:45,496 epoch 24 - iter 500/507 - loss 0.21162149 - samples/sec: 56.26 - lr: 0.012500
2021-05-25 03:46:47,315 ----------------------------------------------------------------------------------------------------
2021-05-25 03:46:47,315 EPOCH 24 done: loss 0.2101 - lr 0.0125000
2021-05-25 03:46:52,407 DEV : loss 0.1705649197101593 - score 0.9503
2021-05-25 03:46:52,494 BAD EPOCHS (no improvement): 1
2021-05-25 03:46:52,494 ----------------------------------------------------------------------------------------------------
2021-05-25 03:47:06,605 epoch 25 - iter 50/507 - loss 0.20970501 - samples/sec: 56.70 - lr: 0.012500
2021-05-25 03:47:20,705 epoch 25 - iter 100/507 - loss 0.21577067 - samples/sec: 56.75 - lr: 0.012500
2021-05-25 03:47:34,682 epoch 25 - iter 150/507 - loss 0.20753049 - samples/sec: 57.24 - lr: 0.012500
2021-05-25 03:47:48,728 epoch 25 - iter 200/507 - loss 0.20875060 - samples/sec: 56.96 - lr: 0.012500
2021-05-25 03:48:02,760 epoch 25 - iter 250/507 - loss 0.20718646 - samples/sec: 57.02 - lr: 0.012500
2021-05-25 03:48:16,773 epoch 25 - iter 300/507 - loss 0.19901039 - samples/sec: 57.10 - lr: 0.012500
2021-05-25 03:48:30,807 epoch 25 - iter 350/507 - loss 0.19667025 - samples/sec: 57.01 - lr: 0.012500
2021-05-25 03:48:44,737 epoch 25 - iter 400/507 - loss 0.19793190 - samples/sec: 57.44 - lr: 0.012500
2021-05-25 03:48:58,717 epoch 25 - iter 450/507 - loss 0.19832070 - samples/sec: 57.23 - lr: 0.012500
2021-05-25 03:49:12,762 epoch 25 - iter 500/507 - loss 0.20007382 - samples/sec: 56.97 - lr: 0.012500
2021-05-25 03:49:14,537 ----------------------------------------------------------------------------------------------------
2021-05-25 03:49:14,538 EPOCH 25 done: loss 0.2016 - lr 0.0125000
2021-05-25 03:49:19,630 DEV : loss 0.16849707067012787 - score 0.9481
2021-05-25 03:49:19,716 BAD EPOCHS (no improvement): 2
2021-05-25 03:49:19,716 ----------------------------------------------------------------------------------------------------
2021-05-25 03:49:33,752 epoch 26 - iter 50/507 - loss 0.18454754 - samples/sec: 57.01 - lr: 0.012500
2021-05-25 03:49:47,872 epoch 26 - iter 100/507 - loss 0.17451492 - samples/sec: 56.66 - lr: 0.012500
2021-05-25 03:50:01,978 epoch 26 - iter 150/507 - loss 0.20825188 - samples/sec: 56.72 - lr: 0.012500
2021-05-25 03:50:16,103 epoch 26 - iter 200/507 - loss 0.20860647 - samples/sec: 56.65 - lr: 0.012500
2021-05-25 03:50:30,168 epoch 26 - iter 250/507 - loss 0.21717119 - samples/sec: 56.89 - lr: 0.012500
2021-05-25 03:50:44,299 epoch 26 - iter 300/507 - loss 0.21515144 - samples/sec: 56.62 - lr: 0.012500
2021-05-25 03:50:58,423 epoch 26 - iter 350/507 - loss 0.21551803 - samples/sec: 56.65 - lr: 0.012500
2021-05-25 03:51:12,463 epoch 26 - iter 400/507 - loss 0.21015929 - samples/sec: 56.99 - lr: 0.012500
2021-05-25 03:51:26,499 epoch 26 - iter 450/507 - loss 0.20932252 - samples/sec: 57.00 - lr: 0.012500
2021-05-25 03:51:40,481 epoch 26 - iter 500/507 - loss 0.20487313 - samples/sec: 57.23 - lr: 0.012500
2021-05-25 03:51:42,276 ----------------------------------------------------------------------------------------------------
2021-05-25 03:51:42,277 EPOCH 26 done: loss 0.2072 - lr 0.0125000
2021-05-25 03:51:47,366 DEV : loss 0.16389885544776917 - score 0.9528
2021-05-25 03:51:47,453 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:51:57,660 ----------------------------------------------------------------------------------------------------
2021-05-25 03:52:11,774 epoch 27 - iter 50/507 - loss 0.16860286 - samples/sec: 56.69 - lr: 0.012500
2021-05-25 03:52:25,835 epoch 27 - iter 100/507 - loss 0.17484766 - samples/sec: 56.90 - lr: 0.012500
2021-05-25 03:52:39,874 epoch 27 - iter 150/507 - loss 0.19281225 - samples/sec: 56.99 - lr: 0.012500
2021-05-25 03:52:53,856 epoch 27 - iter 200/507 - loss 0.19624810 - samples/sec: 57.22 - lr: 0.012500
2021-05-25 03:53:07,934 epoch 27 - iter 250/507 - loss 0.18910082 - samples/sec: 56.83 - lr: 0.012500
2021-05-25 03:53:22,070 epoch 27 - iter 300/507 - loss 0.19214476 - samples/sec: 56.60 - lr: 0.012500
2021-05-25 03:53:36,255 epoch 27 - iter 350/507 - loss 0.19034822 - samples/sec: 56.41 - lr: 0.012500
2021-05-25 03:53:50,387 epoch 27 - iter 400/507 - loss 0.19449117 - samples/sec: 56.62 - lr: 0.012500
2021-05-25 03:54:04,482 epoch 27 - iter 450/507 - loss 0.19514928 - samples/sec: 56.76 - lr: 0.012500
2021-05-25 03:54:18,565 epoch 27 - iter 500/507 - loss 0.19408887 - samples/sec: 56.81 - lr: 0.012500
2021-05-25 03:54:20,368 ----------------------------------------------------------------------------------------------------
2021-05-25 03:54:20,368 EPOCH 27 done: loss 0.1940 - lr 0.0125000
2021-05-25 03:54:25,461 DEV : loss 0.1562894880771637 - score 0.9548
2021-05-25 03:54:25,547 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:54:35,500 ----------------------------------------------------------------------------------------------------
2021-05-25 03:54:49,614 epoch 28 - iter 50/507 - loss 0.18017565 - samples/sec: 56.69 - lr: 0.012500
2021-05-25 03:55:04,307 epoch 28 - iter 100/507 - loss 0.18286909 - samples/sec: 54.45 - lr: 0.012500
2021-05-25 03:55:18,298 epoch 28 - iter 150/507 - loss 0.18728008 - samples/sec: 57.19 - lr: 0.012500
2021-05-25 03:55:32,481 epoch 28 - iter 200/507 - loss 0.18610834 - samples/sec: 56.41 - lr: 0.012500
2021-05-25 03:55:46,593 epoch 28 - iter 250/507 - loss 0.18877369 - samples/sec: 56.70 - lr: 0.012500
2021-05-25 03:56:00,848 epoch 28 - iter 300/507 - loss 0.18560839 - samples/sec: 56.13 - lr: 0.012500
2021-05-25 03:56:14,985 epoch 28 - iter 350/507 - loss 0.19063131 - samples/sec: 56.60 - lr: 0.012500
2021-05-25 03:56:29,020 epoch 28 - iter 400/507 - loss 0.19229738 - samples/sec: 57.01 - lr: 0.012500
2021-05-25 03:56:43,214 epoch 28 - iter 450/507 - loss 0.19179928 - samples/sec: 56.37 - lr: 0.012500
2021-05-25 03:56:57,362 epoch 28 - iter 500/507 - loss 0.19197037 - samples/sec: 56.55 - lr: 0.012500
2021-05-25 03:56:59,160 ----------------------------------------------------------------------------------------------------
2021-05-25 03:56:59,160 EPOCH 28 done: loss 0.1918 - lr 0.0125000
2021-05-25 03:57:04,267 DEV : loss 0.15402913093566895 - score 0.9537
2021-05-25 03:57:04,353 BAD EPOCHS (no improvement): 1
2021-05-25 03:57:04,354 ----------------------------------------------------------------------------------------------------
2021-05-25 03:57:18,620 epoch 29 - iter 50/507 - loss 0.22293470 - samples/sec: 56.08 - lr: 0.012500
2021-05-25 03:57:32,746 epoch 29 - iter 100/507 - loss 0.19877119 - samples/sec: 56.64 - lr: 0.012500
2021-05-25 03:57:46,872 epoch 29 - iter 150/507 - loss 0.18294927 - samples/sec: 56.64 - lr: 0.012500
2021-05-25 03:58:01,010 epoch 29 - iter 200/507 - loss 0.18282955 - samples/sec: 56.59 - lr: 0.012500
2021-05-25 03:58:15,250 epoch 29 - iter 250/507 - loss 0.18309365 - samples/sec: 56.19 - lr: 0.012500
2021-05-25 03:58:29,392 epoch 29 - iter 300/507 - loss 0.18290775 - samples/sec: 56.58 - lr: 0.012500
2021-05-25 03:58:43,544 epoch 29 - iter 350/507 - loss 0.18721765 - samples/sec: 56.54 - lr: 0.012500
2021-05-25 03:58:57,664 epoch 29 - iter 400/507 - loss 0.18756798 - samples/sec: 56.67 - lr: 0.012500
2021-05-25 03:59:11,729 epoch 29 - iter 450/507 - loss 0.19085791 - samples/sec: 56.88 - lr: 0.012500
2021-05-25 03:59:25,778 epoch 29 - iter 500/507 - loss 0.19157685 - samples/sec: 56.95 - lr: 0.012500
2021-05-25 03:59:27,576 ----------------------------------------------------------------------------------------------------
2021-05-25 03:59:27,576 EPOCH 29 done: loss 0.1908 - lr 0.0125000
2021-05-25 03:59:32,671 DEV : loss 0.16057679057121277 - score 0.9554
2021-05-25 03:59:32,757 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 03:59:42,603 ----------------------------------------------------------------------------------------------------
2021-05-25 03:59:56,708 epoch 30 - iter 50/507 - loss 0.19193993 - samples/sec: 56.73 - lr: 0.012500
2021-05-25 04:00:10,870 epoch 30 - iter 100/507 - loss 0.19110524 - samples/sec: 56.50 - lr: 0.012500
2021-05-25 04:00:24,993 epoch 30 - iter 150/507 - loss 0.19083517 - samples/sec: 56.65 - lr: 0.012500
2021-05-25 04:00:39,150 epoch 30 - iter 200/507 - loss 0.18391363 - samples/sec: 56.52 - lr: 0.012500
2021-05-25 04:00:53,352 epoch 30 - iter 250/507 - loss 0.17811375 - samples/sec: 56.34 - lr: 0.012500
2021-05-25 04:01:07,466 epoch 30 - iter 300/507 - loss 0.18133200 - samples/sec: 56.69 - lr: 0.012500
2021-05-25 04:01:21,629 epoch 30 - iter 350/507 - loss 0.18472257 - samples/sec: 56.49 - lr: 0.012500
2021-05-25 04:01:35,806 epoch 30 - iter 400/507 - loss 0.18355424 - samples/sec: 56.44 - lr: 0.012500
2021-05-25 04:01:49,858 epoch 30 - iter 450/507 - loss 0.18605121 - samples/sec: 56.94 - lr: 0.012500
2021-05-25 04:02:03,975 epoch 30 - iter 500/507 - loss 0.18470086 - samples/sec: 56.67 - lr: 0.012500
2021-05-25 04:02:05,753 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:05,754 EPOCH 30 done: loss 0.1838 - lr 0.0125000
2021-05-25 04:02:10,855 DEV : loss 0.1655532717704773 - score 0.9512
2021-05-25 04:02:10,941 BAD EPOCHS (no improvement): 1
2021-05-25 04:02:12,313 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:12,313 Testing using best model ...
2021-05-25 04:02:12,314 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/eng.rst.rstdt/best-model.pt
2021-05-25 04:02:43,707 0.9208	0.9634	0.9416
2021-05-25 04:02:43,707 
Results:
- F1-score (micro) 0.9416
- F1-score (macro) 0.9416

By class:
SENT       tp: 895 - fp: 77 - fn: 34 - precision: 0.9208 - recall: 0.9634 - f1-score: 0.9416
2021-05-25 04:02:43,707 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc/
2021-05-25 04:02:43,733 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc
2021-05-25 04:02:43,733 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc/sent_train.txt
2021-05-25 04:02:43,735 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc/sent_dev.txt
2021-05-25 04:02:43,736 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc/sent_test.txt
Corpus: 1343 train + 158 dev + 161 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-25 04:02:52,575 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-25 04:02:52,578 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Corpus: "Corpus: 1343 train + 158 dev + 161 test sentences"
2021-05-25 04:02:52,578 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Parameters:
2021-05-25 04:02:52,578  - learning_rate: "0.1"
2021-05-25 04:02:52,578  - mini_batch_size: "16"
2021-05-25 04:02:52,578  - patience: "3"
2021-05-25 04:02:52,578  - anneal_factor: "0.5"
2021-05-25 04:02:52,578  - max_epochs: "30"
2021-05-25 04:02:52,578  - shuffle: "True"
2021-05-25 04:02:52,578  - train_with_dev: "False"
2021-05-25 04:02:52,578  - batch_growth_annealing: "False"
2021-05-25 04:02:52,578 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc"
2021-05-25 04:02:52,578 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Device: cuda:0
2021-05-25 04:02:52,578 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:52,578 Embeddings storage mode: cpu
2021-05-25 04:02:52,580 ----------------------------------------------------------------------------------------------------
2021-05-25 04:02:58,791 epoch 1 - iter 8/84 - loss 9.62214541 - samples/sec: 20.61 - lr: 0.100000
2021-05-25 04:03:05,002 epoch 1 - iter 16/84 - loss 7.72143319 - samples/sec: 20.61 - lr: 0.100000
2021-05-25 04:03:11,255 epoch 1 - iter 24/84 - loss 7.16616286 - samples/sec: 20.47 - lr: 0.100000
2021-05-25 04:03:17,433 epoch 1 - iter 32/84 - loss 6.59919553 - samples/sec: 20.72 - lr: 0.100000
2021-05-25 04:03:23,648 epoch 1 - iter 40/84 - loss 6.20280517 - samples/sec: 20.60 - lr: 0.100000
2021-05-25 04:03:29,819 epoch 1 - iter 48/84 - loss 5.86306259 - samples/sec: 20.74 - lr: 0.100000
2021-05-25 04:03:36,058 epoch 1 - iter 56/84 - loss 5.56632429 - samples/sec: 20.52 - lr: 0.100000
2021-05-25 04:03:42,308 epoch 1 - iter 64/84 - loss 5.25510324 - samples/sec: 20.48 - lr: 0.100000
2021-05-25 04:03:48,700 epoch 1 - iter 72/84 - loss 4.94566767 - samples/sec: 20.02 - lr: 0.100000
2021-05-25 04:03:54,939 epoch 1 - iter 80/84 - loss 4.70914296 - samples/sec: 20.52 - lr: 0.100000
2021-05-25 04:03:58,030 ----------------------------------------------------------------------------------------------------
2021-05-25 04:03:58,030 EPOCH 1 done: loss 4.5901 - lr 0.1000000
2021-05-25 04:04:02,884 DEV : loss 1.0961610078811646 - score 0.8742
2021-05-25 04:04:02,900 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:04:04,002 ----------------------------------------------------------------------------------------------------
2021-05-25 04:04:06,291 epoch 2 - iter 8/84 - loss 2.47867134 - samples/sec: 55.95 - lr: 0.100000
2021-05-25 04:04:08,503 epoch 2 - iter 16/84 - loss 2.00352889 - samples/sec: 57.88 - lr: 0.100000
2021-05-25 04:04:10,749 epoch 2 - iter 24/84 - loss 1.85002247 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 04:04:13,058 epoch 2 - iter 32/84 - loss 1.90178905 - samples/sec: 55.45 - lr: 0.100000
2021-05-25 04:04:15,314 epoch 2 - iter 40/84 - loss 1.90740180 - samples/sec: 56.75 - lr: 0.100000
2021-05-25 04:04:17,594 epoch 2 - iter 48/84 - loss 1.85118251 - samples/sec: 56.15 - lr: 0.100000
2021-05-25 04:04:19,851 epoch 2 - iter 56/84 - loss 1.82288760 - samples/sec: 56.72 - lr: 0.100000
2021-05-25 04:04:22,094 epoch 2 - iter 64/84 - loss 1.78079132 - samples/sec: 57.08 - lr: 0.100000
2021-05-25 04:04:24,358 epoch 2 - iter 72/84 - loss 1.75943642 - samples/sec: 56.55 - lr: 0.100000
2021-05-25 04:04:26,569 epoch 2 - iter 80/84 - loss 1.71917996 - samples/sec: 57.91 - lr: 0.100000
2021-05-25 04:04:27,695 ----------------------------------------------------------------------------------------------------
2021-05-25 04:04:27,695 EPOCH 2 done: loss 1.6816 - lr 0.1000000
2021-05-25 04:04:28,660 DEV : loss 0.5772086381912231 - score 0.9242
2021-05-25 04:04:28,676 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:04:38,403 ----------------------------------------------------------------------------------------------------
2021-05-25 04:04:40,666 epoch 3 - iter 8/84 - loss 0.83924250 - samples/sec: 56.59 - lr: 0.100000
2021-05-25 04:04:42,923 epoch 3 - iter 16/84 - loss 0.97314076 - samples/sec: 56.72 - lr: 0.100000
2021-05-25 04:04:45,157 epoch 3 - iter 24/84 - loss 1.03008058 - samples/sec: 57.30 - lr: 0.100000
2021-05-25 04:04:47,416 epoch 3 - iter 32/84 - loss 1.09223126 - samples/sec: 56.69 - lr: 0.100000
2021-05-25 04:04:49,670 epoch 3 - iter 40/84 - loss 1.15525724 - samples/sec: 56.79 - lr: 0.100000
2021-05-25 04:04:51,950 epoch 3 - iter 48/84 - loss 1.13260723 - samples/sec: 56.16 - lr: 0.100000
2021-05-25 04:04:54,197 epoch 3 - iter 56/84 - loss 1.14231786 - samples/sec: 56.97 - lr: 0.100000
2021-05-25 04:04:56,474 epoch 3 - iter 64/84 - loss 1.13289140 - samples/sec: 56.24 - lr: 0.100000
2021-05-25 04:04:58,777 epoch 3 - iter 72/84 - loss 1.15555467 - samples/sec: 55.58 - lr: 0.100000
2021-05-25 04:05:01,043 epoch 3 - iter 80/84 - loss 1.16389895 - samples/sec: 56.51 - lr: 0.100000
2021-05-25 04:05:02,163 ----------------------------------------------------------------------------------------------------
2021-05-25 04:05:02,163 EPOCH 3 done: loss 1.1604 - lr 0.1000000
2021-05-25 04:05:03,128 DEV : loss 0.4959650933742523 - score 0.9263
2021-05-25 04:05:03,143 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:05:12,863 ----------------------------------------------------------------------------------------------------
2021-05-25 04:05:15,121 epoch 4 - iter 8/84 - loss 1.40346077 - samples/sec: 56.74 - lr: 0.100000
2021-05-25 04:05:17,361 epoch 4 - iter 16/84 - loss 1.12432243 - samples/sec: 57.15 - lr: 0.100000
2021-05-25 04:05:19,665 epoch 4 - iter 24/84 - loss 1.04391183 - samples/sec: 55.57 - lr: 0.100000
2021-05-25 04:05:21,910 epoch 4 - iter 32/84 - loss 1.13596932 - samples/sec: 57.01 - lr: 0.100000
2021-05-25 04:05:24,161 epoch 4 - iter 40/84 - loss 1.21702111 - samples/sec: 56.89 - lr: 0.100000
2021-05-25 04:05:26,436 epoch 4 - iter 48/84 - loss 1.19691016 - samples/sec: 56.27 - lr: 0.100000
2021-05-25 04:05:28,668 epoch 4 - iter 56/84 - loss 1.18921705 - samples/sec: 57.36 - lr: 0.100000
2021-05-25 04:05:30,925 epoch 4 - iter 64/84 - loss 1.14132529 - samples/sec: 56.74 - lr: 0.100000
2021-05-25 04:05:33,193 epoch 4 - iter 72/84 - loss 1.13720432 - samples/sec: 56.44 - lr: 0.100000
2021-05-25 04:05:35,443 epoch 4 - iter 80/84 - loss 1.13589019 - samples/sec: 56.90 - lr: 0.100000
2021-05-25 04:05:36,562 ----------------------------------------------------------------------------------------------------
2021-05-25 04:05:36,562 EPOCH 4 done: loss 1.1473 - lr 0.1000000
2021-05-25 04:05:37,535 DEV : loss 1.3435077667236328 - score 0.8419
2021-05-25 04:05:37,550 BAD EPOCHS (no improvement): 1
2021-05-25 04:05:37,551 ----------------------------------------------------------------------------------------------------
2021-05-25 04:05:39,989 epoch 5 - iter 8/84 - loss 1.46739483 - samples/sec: 52.50 - lr: 0.100000
2021-05-25 04:05:42,273 epoch 5 - iter 16/84 - loss 1.21288469 - samples/sec: 56.07 - lr: 0.100000
2021-05-25 04:05:44,559 epoch 5 - iter 24/84 - loss 1.17451753 - samples/sec: 56.00 - lr: 0.100000
2021-05-25 04:05:46,832 epoch 5 - iter 32/84 - loss 1.19414302 - samples/sec: 56.33 - lr: 0.100000
2021-05-25 04:05:49,128 epoch 5 - iter 40/84 - loss 1.14637077 - samples/sec: 55.76 - lr: 0.100000
2021-05-25 04:05:51,380 epoch 5 - iter 48/84 - loss 1.11898104 - samples/sec: 56.86 - lr: 0.100000
2021-05-25 04:05:53,596 epoch 5 - iter 56/84 - loss 1.10884730 - samples/sec: 57.77 - lr: 0.100000
2021-05-25 04:05:55,884 epoch 5 - iter 64/84 - loss 1.10573950 - samples/sec: 55.95 - lr: 0.100000
2021-05-25 04:05:58,182 epoch 5 - iter 72/84 - loss 1.12493923 - samples/sec: 55.71 - lr: 0.100000
2021-05-25 04:06:00,434 epoch 5 - iter 80/84 - loss 1.08551965 - samples/sec: 56.87 - lr: 0.100000
2021-05-25 04:06:01,553 ----------------------------------------------------------------------------------------------------
2021-05-25 04:06:01,553 EPOCH 5 done: loss 1.0733 - lr 0.1000000
2021-05-25 04:06:02,522 DEV : loss 0.5944925546646118 - score 0.9367
2021-05-25 04:06:02,537 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:06:12,257 ----------------------------------------------------------------------------------------------------
2021-05-25 04:06:14,503 epoch 6 - iter 8/84 - loss 0.91130477 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 04:06:16,756 epoch 6 - iter 16/84 - loss 0.86877353 - samples/sec: 56.84 - lr: 0.100000
2021-05-25 04:06:19,037 epoch 6 - iter 24/84 - loss 0.88394546 - samples/sec: 56.13 - lr: 0.100000
2021-05-25 04:06:21,337 epoch 6 - iter 32/84 - loss 0.83394234 - samples/sec: 55.67 - lr: 0.100000
2021-05-25 04:06:23,547 epoch 6 - iter 40/84 - loss 0.82119642 - samples/sec: 57.92 - lr: 0.100000
2021-05-25 04:06:25,767 epoch 6 - iter 48/84 - loss 0.80986714 - samples/sec: 57.68 - lr: 0.100000
2021-05-25 04:06:28,087 epoch 6 - iter 56/84 - loss 0.80640840 - samples/sec: 55.18 - lr: 0.100000
2021-05-25 04:06:30,396 epoch 6 - iter 64/84 - loss 0.78512587 - samples/sec: 55.46 - lr: 0.100000
2021-05-25 04:06:32,703 epoch 6 - iter 72/84 - loss 0.78703479 - samples/sec: 55.49 - lr: 0.100000
2021-05-25 04:06:34,979 epoch 6 - iter 80/84 - loss 0.81868340 - samples/sec: 56.26 - lr: 0.100000
2021-05-25 04:06:36,103 ----------------------------------------------------------------------------------------------------
2021-05-25 04:06:36,103 EPOCH 6 done: loss 0.8161 - lr 0.1000000
2021-05-25 04:06:37,065 DEV : loss 0.3760015070438385 - score 0.9463
2021-05-25 04:06:37,081 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:06:46,847 ----------------------------------------------------------------------------------------------------
2021-05-25 04:06:49,152 epoch 7 - iter 8/84 - loss 0.66327797 - samples/sec: 55.56 - lr: 0.100000
2021-05-25 04:06:51,437 epoch 7 - iter 16/84 - loss 0.75821042 - samples/sec: 56.04 - lr: 0.100000
2021-05-25 04:06:53,727 epoch 7 - iter 24/84 - loss 0.85319711 - samples/sec: 55.90 - lr: 0.100000
2021-05-25 04:06:56,043 epoch 7 - iter 32/84 - loss 0.86092320 - samples/sec: 55.27 - lr: 0.100000
2021-05-25 04:06:58,322 epoch 7 - iter 40/84 - loss 0.88808988 - samples/sec: 56.18 - lr: 0.100000
2021-05-25 04:07:00,582 epoch 7 - iter 48/84 - loss 0.89885250 - samples/sec: 56.67 - lr: 0.100000
2021-05-25 04:07:02,870 epoch 7 - iter 56/84 - loss 0.86902157 - samples/sec: 55.96 - lr: 0.100000
2021-05-25 04:07:05,126 epoch 7 - iter 64/84 - loss 0.89571087 - samples/sec: 56.75 - lr: 0.100000
2021-05-25 04:07:07,406 epoch 7 - iter 72/84 - loss 0.87985218 - samples/sec: 56.14 - lr: 0.100000
2021-05-25 04:07:09,670 epoch 7 - iter 80/84 - loss 0.88811114 - samples/sec: 56.55 - lr: 0.100000
2021-05-25 04:07:10,777 ----------------------------------------------------------------------------------------------------
2021-05-25 04:07:10,778 EPOCH 7 done: loss 0.8699 - lr 0.1000000
2021-05-25 04:07:11,738 DEV : loss 0.6215811371803284 - score 0.9234
2021-05-25 04:07:11,754 BAD EPOCHS (no improvement): 1
2021-05-25 04:07:11,754 ----------------------------------------------------------------------------------------------------
2021-05-25 04:07:14,049 epoch 8 - iter 8/84 - loss 0.76583367 - samples/sec: 55.79 - lr: 0.100000
2021-05-25 04:07:16,343 epoch 8 - iter 16/84 - loss 0.84965136 - samples/sec: 55.82 - lr: 0.100000
2021-05-25 04:07:18,634 epoch 8 - iter 24/84 - loss 0.80474626 - samples/sec: 55.90 - lr: 0.100000
2021-05-25 04:07:20,886 epoch 8 - iter 32/84 - loss 0.80363689 - samples/sec: 56.85 - lr: 0.100000
2021-05-25 04:07:23,177 epoch 8 - iter 40/84 - loss 0.80079664 - samples/sec: 55.88 - lr: 0.100000
2021-05-25 04:07:25,460 epoch 8 - iter 48/84 - loss 0.76569447 - samples/sec: 56.07 - lr: 0.100000
2021-05-25 04:07:27,678 epoch 8 - iter 56/84 - loss 0.80221332 - samples/sec: 57.74 - lr: 0.100000
2021-05-25 04:07:29,940 epoch 8 - iter 64/84 - loss 0.82086331 - samples/sec: 56.59 - lr: 0.100000
2021-05-25 04:07:32,230 epoch 8 - iter 72/84 - loss 0.79855896 - samples/sec: 55.92 - lr: 0.100000
2021-05-25 04:07:34,455 epoch 8 - iter 80/84 - loss 0.83627056 - samples/sec: 57.53 - lr: 0.100000
2021-05-25 04:07:35,585 ----------------------------------------------------------------------------------------------------
2021-05-25 04:07:35,585 EPOCH 8 done: loss 0.8413 - lr 0.1000000
2021-05-25 04:07:36,549 DEV : loss 0.4634288251399994 - score 0.9461
2021-05-25 04:07:36,565 BAD EPOCHS (no improvement): 2
2021-05-25 04:07:36,566 ----------------------------------------------------------------------------------------------------
2021-05-25 04:07:38,824 epoch 9 - iter 8/84 - loss 0.66446520 - samples/sec: 56.70 - lr: 0.100000
2021-05-25 04:07:41,155 epoch 9 - iter 16/84 - loss 0.61139912 - samples/sec: 54.93 - lr: 0.100000
2021-05-25 04:07:43,397 epoch 9 - iter 24/84 - loss 0.67190903 - samples/sec: 57.11 - lr: 0.100000
2021-05-25 04:07:45,669 epoch 9 - iter 32/84 - loss 0.70699332 - samples/sec: 56.34 - lr: 0.100000
2021-05-25 04:07:47,936 epoch 9 - iter 40/84 - loss 0.75826335 - samples/sec: 56.46 - lr: 0.100000
2021-05-25 04:07:50,245 epoch 9 - iter 48/84 - loss 0.84633205 - samples/sec: 55.46 - lr: 0.100000
2021-05-25 04:07:52,546 epoch 9 - iter 56/84 - loss 0.81545446 - samples/sec: 55.62 - lr: 0.100000
2021-05-25 04:07:54,852 epoch 9 - iter 64/84 - loss 0.83080578 - samples/sec: 55.53 - lr: 0.100000
2021-05-25 04:07:57,081 epoch 9 - iter 72/84 - loss 0.81837069 - samples/sec: 57.43 - lr: 0.100000
2021-05-25 04:07:59,310 epoch 9 - iter 80/84 - loss 0.81312766 - samples/sec: 57.46 - lr: 0.100000
2021-05-25 04:08:00,417 ----------------------------------------------------------------------------------------------------
2021-05-25 04:08:00,418 EPOCH 9 done: loss 0.8139 - lr 0.1000000
2021-05-25 04:08:01,381 DEV : loss 0.5849140286445618 - score 0.9309
2021-05-25 04:08:01,397 BAD EPOCHS (no improvement): 3
2021-05-25 04:08:01,397 ----------------------------------------------------------------------------------------------------
2021-05-25 04:08:03,680 epoch 10 - iter 8/84 - loss 1.07015485 - samples/sec: 56.10 - lr: 0.100000
2021-05-25 04:08:05,940 epoch 10 - iter 16/84 - loss 0.85151378 - samples/sec: 56.64 - lr: 0.100000
2021-05-25 04:08:08,215 epoch 10 - iter 24/84 - loss 0.83979755 - samples/sec: 56.28 - lr: 0.100000
2021-05-25 04:08:10,544 epoch 10 - iter 32/84 - loss 0.81276684 - samples/sec: 54.98 - lr: 0.100000
2021-05-25 04:08:12,805 epoch 10 - iter 40/84 - loss 0.81945859 - samples/sec: 56.61 - lr: 0.100000
2021-05-25 04:08:15,027 epoch 10 - iter 48/84 - loss 0.79394345 - samples/sec: 57.62 - lr: 0.100000
2021-05-25 04:08:17,288 epoch 10 - iter 56/84 - loss 0.77961946 - samples/sec: 56.63 - lr: 0.100000
2021-05-25 04:08:19,534 epoch 10 - iter 64/84 - loss 0.78197583 - samples/sec: 57.00 - lr: 0.100000
2021-05-25 04:08:21,798 epoch 10 - iter 72/84 - loss 0.81806057 - samples/sec: 56.57 - lr: 0.100000
2021-05-25 04:08:24,092 epoch 10 - iter 80/84 - loss 0.81212731 - samples/sec: 55.80 - lr: 0.100000
2021-05-25 04:08:25,207 ----------------------------------------------------------------------------------------------------
2021-05-25 04:08:25,207 EPOCH 10 done: loss 0.7968 - lr 0.1000000
2021-05-25 04:08:26,176 DEV : loss 0.41612863540649414 - score 0.9504
2021-05-25 04:08:26,191 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:08:36,647 ----------------------------------------------------------------------------------------------------
2021-05-25 04:08:38,930 epoch 11 - iter 8/84 - loss 0.76967981 - samples/sec: 56.11 - lr: 0.100000
2021-05-25 04:08:41,189 epoch 11 - iter 16/84 - loss 0.88792536 - samples/sec: 56.66 - lr: 0.100000
2021-05-25 04:08:43,451 epoch 11 - iter 24/84 - loss 0.96963495 - samples/sec: 56.60 - lr: 0.100000
2021-05-25 04:08:45,694 epoch 11 - iter 32/84 - loss 0.88304568 - samples/sec: 57.08 - lr: 0.100000
2021-05-25 04:08:47,977 epoch 11 - iter 40/84 - loss 0.86234585 - samples/sec: 56.07 - lr: 0.100000
2021-05-25 04:08:50,256 epoch 11 - iter 48/84 - loss 0.79608887 - samples/sec: 56.20 - lr: 0.100000
2021-05-25 04:08:52,473 epoch 11 - iter 56/84 - loss 0.77923279 - samples/sec: 57.75 - lr: 0.100000
2021-05-25 04:08:54,691 epoch 11 - iter 64/84 - loss 0.73526979 - samples/sec: 57.71 - lr: 0.100000
2021-05-25 04:08:57,144 epoch 11 - iter 72/84 - loss 0.71365704 - samples/sec: 52.20 - lr: 0.100000
2021-05-25 04:08:59,451 epoch 11 - iter 80/84 - loss 0.72758915 - samples/sec: 55.49 - lr: 0.100000
2021-05-25 04:09:00,579 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:00,579 EPOCH 11 done: loss 0.7470 - lr 0.1000000
2021-05-25 04:09:01,542 DEV : loss 0.43855053186416626 - score 0.9403
2021-05-25 04:09:01,558 BAD EPOCHS (no improvement): 1
2021-05-25 04:09:01,558 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:03,860 epoch 12 - iter 8/84 - loss 0.71206595 - samples/sec: 55.61 - lr: 0.100000
2021-05-25 04:09:06,163 epoch 12 - iter 16/84 - loss 0.75135996 - samples/sec: 55.60 - lr: 0.100000
2021-05-25 04:09:08,448 epoch 12 - iter 24/84 - loss 0.74618834 - samples/sec: 56.04 - lr: 0.100000
2021-05-25 04:09:10,702 epoch 12 - iter 32/84 - loss 0.81955391 - samples/sec: 56.78 - lr: 0.100000
2021-05-25 04:09:12,962 epoch 12 - iter 40/84 - loss 0.81098780 - samples/sec: 56.65 - lr: 0.100000
2021-05-25 04:09:15,243 epoch 12 - iter 48/84 - loss 0.87349511 - samples/sec: 56.13 - lr: 0.100000
2021-05-25 04:09:17,462 epoch 12 - iter 56/84 - loss 0.86889711 - samples/sec: 57.70 - lr: 0.100000
2021-05-25 04:09:19,775 epoch 12 - iter 64/84 - loss 0.85722985 - samples/sec: 55.35 - lr: 0.100000
2021-05-25 04:09:21,957 epoch 12 - iter 72/84 - loss 0.83219392 - samples/sec: 58.69 - lr: 0.100000
2021-05-25 04:09:24,282 epoch 12 - iter 80/84 - loss 0.81876998 - samples/sec: 55.07 - lr: 0.100000
2021-05-25 04:09:25,427 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:25,427 EPOCH 12 done: loss 0.8048 - lr 0.1000000
2021-05-25 04:09:26,392 DEV : loss 0.9344511032104492 - score 0.8874
2021-05-25 04:09:26,408 BAD EPOCHS (no improvement): 2
2021-05-25 04:09:26,408 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:28,701 epoch 13 - iter 8/84 - loss 0.50940211 - samples/sec: 55.83 - lr: 0.100000
2021-05-25 04:09:30,964 epoch 13 - iter 16/84 - loss 0.65439233 - samples/sec: 56.57 - lr: 0.100000
2021-05-25 04:09:33,196 epoch 13 - iter 24/84 - loss 0.65676604 - samples/sec: 57.36 - lr: 0.100000
2021-05-25 04:09:35,499 epoch 13 - iter 32/84 - loss 0.70290867 - samples/sec: 55.61 - lr: 0.100000
2021-05-25 04:09:37,772 epoch 13 - iter 40/84 - loss 0.70108105 - samples/sec: 56.33 - lr: 0.100000
2021-05-25 04:09:40,047 epoch 13 - iter 48/84 - loss 0.79631667 - samples/sec: 56.27 - lr: 0.100000
2021-05-25 04:09:42,300 epoch 13 - iter 56/84 - loss 0.81433495 - samples/sec: 56.83 - lr: 0.100000
2021-05-25 04:09:44,608 epoch 13 - iter 64/84 - loss 0.81820851 - samples/sec: 55.47 - lr: 0.100000
2021-05-25 04:09:46,862 epoch 13 - iter 72/84 - loss 0.80837522 - samples/sec: 56.81 - lr: 0.100000
2021-05-25 04:09:49,144 epoch 13 - iter 80/84 - loss 0.81576647 - samples/sec: 56.11 - lr: 0.100000
2021-05-25 04:09:50,298 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:50,298 EPOCH 13 done: loss 0.8093 - lr 0.1000000
2021-05-25 04:09:51,263 DEV : loss 0.8351575136184692 - score 0.8898
2021-05-25 04:09:51,279 BAD EPOCHS (no improvement): 3
2021-05-25 04:09:51,279 ----------------------------------------------------------------------------------------------------
2021-05-25 04:09:53,577 epoch 14 - iter 8/84 - loss 0.75777152 - samples/sec: 55.73 - lr: 0.100000
2021-05-25 04:09:55,857 epoch 14 - iter 16/84 - loss 0.57508453 - samples/sec: 56.15 - lr: 0.100000
2021-05-25 04:09:58,154 epoch 14 - iter 24/84 - loss 0.55012138 - samples/sec: 55.73 - lr: 0.100000
2021-05-25 04:10:00,427 epoch 14 - iter 32/84 - loss 0.63086862 - samples/sec: 56.34 - lr: 0.100000
2021-05-25 04:10:02,692 epoch 14 - iter 40/84 - loss 0.64551663 - samples/sec: 56.53 - lr: 0.100000
2021-05-25 04:10:04,936 epoch 14 - iter 48/84 - loss 0.64549182 - samples/sec: 57.05 - lr: 0.100000
2021-05-25 04:10:07,225 epoch 14 - iter 56/84 - loss 0.63169762 - samples/sec: 55.92 - lr: 0.100000
2021-05-25 04:10:09,475 epoch 14 - iter 64/84 - loss 0.64300286 - samples/sec: 56.91 - lr: 0.100000
2021-05-25 04:10:11,769 epoch 14 - iter 72/84 - loss 0.68985711 - samples/sec: 55.82 - lr: 0.100000
2021-05-25 04:10:14,055 epoch 14 - iter 80/84 - loss 0.68570982 - samples/sec: 56.01 - lr: 0.100000
2021-05-25 04:10:15,183 ----------------------------------------------------------------------------------------------------
2021-05-25 04:10:15,183 EPOCH 14 done: loss 0.6868 - lr 0.1000000
2021-05-25 04:10:16,148 DEV : loss 0.43935781717300415 - score 0.9424
Epoch    14: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 04:10:16,164 BAD EPOCHS (no improvement): 4
2021-05-25 04:10:16,164 ----------------------------------------------------------------------------------------------------
2021-05-25 04:10:18,425 epoch 15 - iter 8/84 - loss 0.55132055 - samples/sec: 56.62 - lr: 0.050000
2021-05-25 04:10:20,737 epoch 15 - iter 16/84 - loss 0.61390645 - samples/sec: 55.38 - lr: 0.050000
2021-05-25 04:10:23,029 epoch 15 - iter 24/84 - loss 0.57616121 - samples/sec: 55.86 - lr: 0.050000
2021-05-25 04:10:25,298 epoch 15 - iter 32/84 - loss 0.64502506 - samples/sec: 56.43 - lr: 0.050000
2021-05-25 04:10:27,556 epoch 15 - iter 40/84 - loss 0.58437411 - samples/sec: 56.70 - lr: 0.050000
2021-05-25 04:10:29,796 epoch 15 - iter 48/84 - loss 0.57684418 - samples/sec: 57.14 - lr: 0.050000
2021-05-25 04:10:32,089 epoch 15 - iter 56/84 - loss 0.58606429 - samples/sec: 55.84 - lr: 0.050000
2021-05-25 04:10:34,335 epoch 15 - iter 64/84 - loss 0.58462428 - samples/sec: 57.01 - lr: 0.050000
2021-05-25 04:10:36,613 epoch 15 - iter 72/84 - loss 0.58808565 - samples/sec: 56.21 - lr: 0.050000
2021-05-25 04:10:38,920 epoch 15 - iter 80/84 - loss 0.58561976 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 04:10:40,034 ----------------------------------------------------------------------------------------------------
2021-05-25 04:10:40,034 EPOCH 15 done: loss 0.5886 - lr 0.0500000
2021-05-25 04:10:40,997 DEV : loss 0.3437236547470093 - score 0.9551
2021-05-25 04:10:41,013 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:10:50,740 ----------------------------------------------------------------------------------------------------
2021-05-25 04:10:52,984 epoch 16 - iter 8/84 - loss 0.44930163 - samples/sec: 57.07 - lr: 0.050000
2021-05-25 04:10:55,304 epoch 16 - iter 16/84 - loss 0.47255845 - samples/sec: 55.18 - lr: 0.050000
2021-05-25 04:10:57,613 epoch 16 - iter 24/84 - loss 0.53196142 - samples/sec: 55.47 - lr: 0.050000
2021-05-25 04:10:59,886 epoch 16 - iter 32/84 - loss 0.58346397 - samples/sec: 56.33 - lr: 0.050000
2021-05-25 04:11:02,183 epoch 16 - iter 40/84 - loss 0.60847464 - samples/sec: 55.73 - lr: 0.050000
2021-05-25 04:11:04,487 epoch 16 - iter 48/84 - loss 0.62299778 - samples/sec: 55.56 - lr: 0.050000
2021-05-25 04:11:06,740 epoch 16 - iter 56/84 - loss 0.62517032 - samples/sec: 56.84 - lr: 0.050000
2021-05-25 04:11:08,992 epoch 16 - iter 64/84 - loss 0.61389517 - samples/sec: 56.84 - lr: 0.050000
2021-05-25 04:11:11,248 epoch 16 - iter 72/84 - loss 0.58081616 - samples/sec: 56.76 - lr: 0.050000
2021-05-25 04:11:13,485 epoch 16 - iter 80/84 - loss 0.56311795 - samples/sec: 57.24 - lr: 0.050000
2021-05-25 04:11:14,598 ----------------------------------------------------------------------------------------------------
2021-05-25 04:11:14,598 EPOCH 16 done: loss 0.5697 - lr 0.0500000
2021-05-25 04:11:15,560 DEV : loss 0.3690972328186035 - score 0.9466
2021-05-25 04:11:15,575 BAD EPOCHS (no improvement): 1
2021-05-25 04:11:15,576 ----------------------------------------------------------------------------------------------------
2021-05-25 04:11:17,852 epoch 17 - iter 8/84 - loss 0.51849498 - samples/sec: 56.26 - lr: 0.050000
2021-05-25 04:11:20,127 epoch 17 - iter 16/84 - loss 0.51856893 - samples/sec: 56.26 - lr: 0.050000
2021-05-25 04:11:22,434 epoch 17 - iter 24/84 - loss 0.54924630 - samples/sec: 55.49 - lr: 0.050000
2021-05-25 04:11:24,677 epoch 17 - iter 32/84 - loss 0.51516682 - samples/sec: 57.08 - lr: 0.050000
2021-05-25 04:11:26,954 epoch 17 - iter 40/84 - loss 0.51894322 - samples/sec: 56.24 - lr: 0.050000
2021-05-25 04:11:29,263 epoch 17 - iter 48/84 - loss 0.53998146 - samples/sec: 55.43 - lr: 0.050000
2021-05-25 04:11:31,502 epoch 17 - iter 56/84 - loss 0.51531161 - samples/sec: 57.19 - lr: 0.050000
2021-05-25 04:11:33,757 epoch 17 - iter 64/84 - loss 0.51284301 - samples/sec: 56.79 - lr: 0.050000
2021-05-25 04:11:36,030 epoch 17 - iter 72/84 - loss 0.51840392 - samples/sec: 56.32 - lr: 0.050000
2021-05-25 04:11:38,293 epoch 17 - iter 80/84 - loss 0.51186041 - samples/sec: 56.57 - lr: 0.050000
2021-05-25 04:11:39,402 ----------------------------------------------------------------------------------------------------
2021-05-25 04:11:39,403 EPOCH 17 done: loss 0.5118 - lr 0.0500000
2021-05-25 04:11:40,529 DEV : loss 0.33964452147483826 - score 0.9608
2021-05-25 04:11:40,545 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:11:50,180 ----------------------------------------------------------------------------------------------------
2021-05-25 04:11:52,402 epoch 18 - iter 8/84 - loss 0.44609233 - samples/sec: 57.62 - lr: 0.050000
2021-05-25 04:11:54,651 epoch 18 - iter 16/84 - loss 0.50704620 - samples/sec: 56.94 - lr: 0.050000
2021-05-25 04:11:56,924 epoch 18 - iter 24/84 - loss 0.48406382 - samples/sec: 56.32 - lr: 0.050000
2021-05-25 04:11:59,162 epoch 18 - iter 32/84 - loss 0.46950241 - samples/sec: 57.22 - lr: 0.050000
2021-05-25 04:12:01,433 epoch 18 - iter 40/84 - loss 0.46702274 - samples/sec: 56.36 - lr: 0.050000
2021-05-25 04:12:03,684 epoch 18 - iter 48/84 - loss 0.49869201 - samples/sec: 56.90 - lr: 0.050000
2021-05-25 04:12:05,899 epoch 18 - iter 56/84 - loss 0.49780031 - samples/sec: 57.78 - lr: 0.050000
2021-05-25 04:12:08,132 epoch 18 - iter 64/84 - loss 0.52366499 - samples/sec: 57.34 - lr: 0.050000
2021-05-25 04:12:10,393 epoch 18 - iter 72/84 - loss 0.52724726 - samples/sec: 56.63 - lr: 0.050000
2021-05-25 04:12:12,681 epoch 18 - iter 80/84 - loss 0.53327009 - samples/sec: 55.95 - lr: 0.050000
2021-05-25 04:12:13,787 ----------------------------------------------------------------------------------------------------
2021-05-25 04:12:13,787 EPOCH 18 done: loss 0.5328 - lr 0.0500000
2021-05-25 04:12:14,752 DEV : loss 0.3471725285053253 - score 0.9484
2021-05-25 04:12:14,768 BAD EPOCHS (no improvement): 1
2021-05-25 04:12:14,768 ----------------------------------------------------------------------------------------------------
2021-05-25 04:12:17,026 epoch 19 - iter 8/84 - loss 0.67361534 - samples/sec: 56.70 - lr: 0.050000
2021-05-25 04:12:19,295 epoch 19 - iter 16/84 - loss 0.56164068 - samples/sec: 56.42 - lr: 0.050000
2021-05-25 04:12:21,594 epoch 19 - iter 24/84 - loss 0.49908609 - samples/sec: 55.71 - lr: 0.050000
2021-05-25 04:12:23,796 epoch 19 - iter 32/84 - loss 0.60288280 - samples/sec: 58.12 - lr: 0.050000
2021-05-25 04:12:26,043 epoch 19 - iter 40/84 - loss 0.55186156 - samples/sec: 56.99 - lr: 0.050000
2021-05-25 04:12:28,305 epoch 19 - iter 48/84 - loss 0.52165956 - samples/sec: 56.60 - lr: 0.050000
2021-05-25 04:12:30,600 epoch 19 - iter 56/84 - loss 0.54166356 - samples/sec: 55.79 - lr: 0.050000
2021-05-25 04:12:32,859 epoch 19 - iter 64/84 - loss 0.53773613 - samples/sec: 56.66 - lr: 0.050000
2021-05-25 04:12:35,142 epoch 19 - iter 72/84 - loss 0.54090253 - samples/sec: 56.09 - lr: 0.050000
2021-05-25 04:12:37,410 epoch 19 - iter 80/84 - loss 0.53582013 - samples/sec: 56.45 - lr: 0.050000
2021-05-25 04:12:38,480 ----------------------------------------------------------------------------------------------------
2021-05-25 04:12:38,481 EPOCH 19 done: loss 0.5397 - lr 0.0500000
2021-05-25 04:12:39,442 DEV : loss 0.4752356708049774 - score 0.9444
2021-05-25 04:12:39,458 BAD EPOCHS (no improvement): 2
2021-05-25 04:12:39,458 ----------------------------------------------------------------------------------------------------
2021-05-25 04:12:41,702 epoch 20 - iter 8/84 - loss 0.33363392 - samples/sec: 57.06 - lr: 0.050000
2021-05-25 04:12:43,994 epoch 20 - iter 16/84 - loss 0.48960010 - samples/sec: 55.85 - lr: 0.050000
2021-05-25 04:12:46,310 epoch 20 - iter 24/84 - loss 0.46273347 - samples/sec: 55.29 - lr: 0.050000
2021-05-25 04:12:48,535 epoch 20 - iter 32/84 - loss 0.46682587 - samples/sec: 57.53 - lr: 0.050000
2021-05-25 04:12:50,836 epoch 20 - iter 40/84 - loss 0.47604610 - samples/sec: 55.65 - lr: 0.050000
2021-05-25 04:12:53,087 epoch 20 - iter 48/84 - loss 0.48609965 - samples/sec: 56.88 - lr: 0.050000
2021-05-25 04:12:55,359 epoch 20 - iter 56/84 - loss 0.46713940 - samples/sec: 56.35 - lr: 0.050000
2021-05-25 04:12:57,640 epoch 20 - iter 64/84 - loss 0.46178912 - samples/sec: 56.12 - lr: 0.050000
2021-05-25 04:12:59,910 epoch 20 - iter 72/84 - loss 0.46889661 - samples/sec: 56.41 - lr: 0.050000
2021-05-25 04:13:02,167 epoch 20 - iter 80/84 - loss 0.46360655 - samples/sec: 56.72 - lr: 0.050000
2021-05-25 04:13:03,265 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:03,265 EPOCH 20 done: loss 0.4590 - lr 0.0500000
2021-05-25 04:13:04,228 DEV : loss 0.35622653365135193 - score 0.9519
2021-05-25 04:13:04,244 BAD EPOCHS (no improvement): 3
2021-05-25 04:13:04,244 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:06,491 epoch 21 - iter 8/84 - loss 0.34922693 - samples/sec: 56.98 - lr: 0.050000
2021-05-25 04:13:08,771 epoch 21 - iter 16/84 - loss 0.34697739 - samples/sec: 56.15 - lr: 0.050000
2021-05-25 04:13:11,001 epoch 21 - iter 24/84 - loss 0.39758336 - samples/sec: 57.42 - lr: 0.050000
2021-05-25 04:13:13,282 epoch 21 - iter 32/84 - loss 0.41047549 - samples/sec: 56.13 - lr: 0.050000
2021-05-25 04:13:15,552 epoch 21 - iter 40/84 - loss 0.41106925 - samples/sec: 56.40 - lr: 0.050000
2021-05-25 04:13:17,833 epoch 21 - iter 48/84 - loss 0.41871673 - samples/sec: 56.12 - lr: 0.050000
2021-05-25 04:13:20,072 epoch 21 - iter 56/84 - loss 0.41850461 - samples/sec: 57.19 - lr: 0.050000
2021-05-25 04:13:22,346 epoch 21 - iter 64/84 - loss 0.44012352 - samples/sec: 56.30 - lr: 0.050000
2021-05-25 04:13:24,627 epoch 21 - iter 72/84 - loss 0.44846682 - samples/sec: 56.14 - lr: 0.050000
2021-05-25 04:13:26,930 epoch 21 - iter 80/84 - loss 0.43984946 - samples/sec: 55.59 - lr: 0.050000
2021-05-25 04:13:28,062 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:28,062 EPOCH 21 done: loss 0.4340 - lr 0.0500000
2021-05-25 04:13:29,024 DEV : loss 0.5039315819740295 - score 0.9425
Epoch    21: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 04:13:29,040 BAD EPOCHS (no improvement): 4
2021-05-25 04:13:29,040 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:31,356 epoch 22 - iter 8/84 - loss 0.56363881 - samples/sec: 55.28 - lr: 0.025000
2021-05-25 04:13:33,604 epoch 22 - iter 16/84 - loss 0.64280515 - samples/sec: 56.96 - lr: 0.025000
2021-05-25 04:13:35,824 epoch 22 - iter 24/84 - loss 0.55337448 - samples/sec: 57.66 - lr: 0.025000
2021-05-25 04:13:38,105 epoch 22 - iter 32/84 - loss 0.51449150 - samples/sec: 56.15 - lr: 0.025000
2021-05-25 04:13:40,315 epoch 22 - iter 40/84 - loss 0.54071715 - samples/sec: 57.92 - lr: 0.025000
2021-05-25 04:13:42,613 epoch 22 - iter 48/84 - loss 0.53839005 - samples/sec: 55.71 - lr: 0.025000
2021-05-25 04:13:44,865 epoch 22 - iter 56/84 - loss 0.51593208 - samples/sec: 56.87 - lr: 0.025000
2021-05-25 04:13:47,122 epoch 22 - iter 64/84 - loss 0.49444927 - samples/sec: 56.70 - lr: 0.025000
2021-05-25 04:13:49,423 epoch 22 - iter 72/84 - loss 0.50240878 - samples/sec: 55.65 - lr: 0.025000
2021-05-25 04:13:51,709 epoch 22 - iter 80/84 - loss 0.48566774 - samples/sec: 56.01 - lr: 0.025000
2021-05-25 04:13:52,834 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:52,834 EPOCH 22 done: loss 0.4938 - lr 0.0250000
2021-05-25 04:13:53,799 DEV : loss 0.3907356858253479 - score 0.9488
2021-05-25 04:13:53,816 BAD EPOCHS (no improvement): 1
2021-05-25 04:13:53,816 ----------------------------------------------------------------------------------------------------
2021-05-25 04:13:56,085 epoch 23 - iter 8/84 - loss 0.27661297 - samples/sec: 56.43 - lr: 0.025000
2021-05-25 04:13:58,363 epoch 23 - iter 16/84 - loss 0.28530323 - samples/sec: 56.20 - lr: 0.025000
2021-05-25 04:14:00,650 epoch 23 - iter 24/84 - loss 0.37675435 - samples/sec: 55.99 - lr: 0.025000
2021-05-25 04:14:02,929 epoch 23 - iter 32/84 - loss 0.36278738 - samples/sec: 56.16 - lr: 0.025000
2021-05-25 04:14:05,208 epoch 23 - iter 40/84 - loss 0.40242852 - samples/sec: 56.18 - lr: 0.025000
2021-05-25 04:14:07,463 epoch 23 - iter 48/84 - loss 0.41067661 - samples/sec: 56.79 - lr: 0.025000
2021-05-25 04:14:09,696 epoch 23 - iter 56/84 - loss 0.41683361 - samples/sec: 57.33 - lr: 0.025000
2021-05-25 04:14:11,996 epoch 23 - iter 64/84 - loss 0.43211998 - samples/sec: 55.65 - lr: 0.025000
2021-05-25 04:14:14,283 epoch 23 - iter 72/84 - loss 0.42866134 - samples/sec: 55.98 - lr: 0.025000
2021-05-25 04:14:16,488 epoch 23 - iter 80/84 - loss 0.42528925 - samples/sec: 58.07 - lr: 0.025000
2021-05-25 04:14:17,615 ----------------------------------------------------------------------------------------------------
2021-05-25 04:14:17,615 EPOCH 23 done: loss 0.4267 - lr 0.0250000
2021-05-25 04:14:18,579 DEV : loss 0.35483184456825256 - score 0.9535
2021-05-25 04:14:18,595 BAD EPOCHS (no improvement): 2
2021-05-25 04:14:18,595 ----------------------------------------------------------------------------------------------------
2021-05-25 04:14:20,849 epoch 24 - iter 8/84 - loss 0.49052428 - samples/sec: 56.80 - lr: 0.025000
2021-05-25 04:14:23,292 epoch 24 - iter 16/84 - loss 0.43447122 - samples/sec: 52.41 - lr: 0.025000
2021-05-25 04:14:25,591 epoch 24 - iter 24/84 - loss 0.44019596 - samples/sec: 55.70 - lr: 0.025000
2021-05-25 04:14:27,870 epoch 24 - iter 32/84 - loss 0.47011996 - samples/sec: 56.16 - lr: 0.025000
2021-05-25 04:14:30,121 epoch 24 - iter 40/84 - loss 0.47558851 - samples/sec: 56.88 - lr: 0.025000
2021-05-25 04:14:32,389 epoch 24 - iter 48/84 - loss 0.45844668 - samples/sec: 56.44 - lr: 0.025000
2021-05-25 04:14:34,649 epoch 24 - iter 56/84 - loss 0.45513728 - samples/sec: 56.67 - lr: 0.025000
2021-05-25 04:14:36,913 epoch 24 - iter 64/84 - loss 0.42634847 - samples/sec: 56.54 - lr: 0.025000
2021-05-25 04:14:39,130 epoch 24 - iter 72/84 - loss 0.41222975 - samples/sec: 57.75 - lr: 0.025000
2021-05-25 04:14:41,357 epoch 24 - iter 80/84 - loss 0.39761296 - samples/sec: 57.48 - lr: 0.025000
2021-05-25 04:14:42,456 ----------------------------------------------------------------------------------------------------
2021-05-25 04:14:42,457 EPOCH 24 done: loss 0.4013 - lr 0.0250000
2021-05-25 04:14:43,421 DEV : loss 0.45754289627075195 - score 0.9425
2021-05-25 04:14:43,437 BAD EPOCHS (no improvement): 3
2021-05-25 04:14:43,437 ----------------------------------------------------------------------------------------------------
2021-05-25 04:14:45,661 epoch 25 - iter 8/84 - loss 0.29917640 - samples/sec: 57.57 - lr: 0.025000
2021-05-25 04:14:47,892 epoch 25 - iter 16/84 - loss 0.39736396 - samples/sec: 57.39 - lr: 0.025000
2021-05-25 04:14:50,148 epoch 25 - iter 24/84 - loss 0.43511274 - samples/sec: 56.73 - lr: 0.025000
2021-05-25 04:14:52,427 epoch 25 - iter 32/84 - loss 0.43540289 - samples/sec: 56.18 - lr: 0.025000
2021-05-25 04:14:54,649 epoch 25 - iter 40/84 - loss 0.43814039 - samples/sec: 57.63 - lr: 0.025000
2021-05-25 04:14:56,914 epoch 25 - iter 48/84 - loss 0.44585742 - samples/sec: 56.54 - lr: 0.025000
2021-05-25 04:14:59,149 epoch 25 - iter 56/84 - loss 0.43150729 - samples/sec: 57.28 - lr: 0.025000
2021-05-25 04:15:01,412 epoch 25 - iter 64/84 - loss 0.43670936 - samples/sec: 56.57 - lr: 0.025000
2021-05-25 04:15:03,684 epoch 25 - iter 72/84 - loss 0.42121954 - samples/sec: 56.34 - lr: 0.025000
2021-05-25 04:15:05,943 epoch 25 - iter 80/84 - loss 0.42297984 - samples/sec: 56.69 - lr: 0.025000
2021-05-25 04:15:07,065 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:07,065 EPOCH 25 done: loss 0.4229 - lr 0.0250000
2021-05-25 04:15:08,029 DEV : loss 0.3338913917541504 - score 0.951
Epoch    25: reducing learning rate of group 0 to 1.2500e-02.
2021-05-25 04:15:08,045 BAD EPOCHS (no improvement): 4
2021-05-25 04:15:08,045 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:10,297 epoch 26 - iter 8/84 - loss 0.41745931 - samples/sec: 56.85 - lr: 0.012500
2021-05-25 04:15:12,558 epoch 26 - iter 16/84 - loss 0.35284271 - samples/sec: 56.63 - lr: 0.012500
2021-05-25 04:15:14,834 epoch 26 - iter 24/84 - loss 0.39659077 - samples/sec: 56.26 - lr: 0.012500
2021-05-25 04:15:17,119 epoch 26 - iter 32/84 - loss 0.39077233 - samples/sec: 56.03 - lr: 0.012500
2021-05-25 04:15:19,336 epoch 26 - iter 40/84 - loss 0.40787138 - samples/sec: 57.75 - lr: 0.012500
2021-05-25 04:15:21,579 epoch 26 - iter 48/84 - loss 0.38201816 - samples/sec: 57.07 - lr: 0.012500
2021-05-25 04:15:23,820 epoch 26 - iter 56/84 - loss 0.35621844 - samples/sec: 57.14 - lr: 0.012500
2021-05-25 04:15:26,045 epoch 26 - iter 64/84 - loss 0.34552381 - samples/sec: 57.54 - lr: 0.012500
2021-05-25 04:15:28,331 epoch 26 - iter 72/84 - loss 0.34534375 - samples/sec: 56.01 - lr: 0.012500
2021-05-25 04:15:30,583 epoch 26 - iter 80/84 - loss 0.34706605 - samples/sec: 56.85 - lr: 0.012500
2021-05-25 04:15:31,687 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:31,687 EPOCH 26 done: loss 0.3566 - lr 0.0125000
2021-05-25 04:15:32,651 DEV : loss 0.35621508955955505 - score 0.9535
2021-05-25 04:15:32,667 BAD EPOCHS (no improvement): 1
2021-05-25 04:15:32,667 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:34,926 epoch 27 - iter 8/84 - loss 0.18748027 - samples/sec: 56.69 - lr: 0.012500
2021-05-25 04:15:37,212 epoch 27 - iter 16/84 - loss 0.30371925 - samples/sec: 56.00 - lr: 0.012500
2021-05-25 04:15:39,469 epoch 27 - iter 24/84 - loss 0.29443662 - samples/sec: 56.72 - lr: 0.012500
2021-05-25 04:15:41,735 epoch 27 - iter 32/84 - loss 0.31091785 - samples/sec: 56.50 - lr: 0.012500
2021-05-25 04:15:43,999 epoch 27 - iter 40/84 - loss 0.32908811 - samples/sec: 56.56 - lr: 0.012500
2021-05-25 04:15:46,247 epoch 27 - iter 48/84 - loss 0.34279551 - samples/sec: 56.94 - lr: 0.012500
2021-05-25 04:15:48,484 epoch 27 - iter 56/84 - loss 0.36815067 - samples/sec: 57.24 - lr: 0.012500
2021-05-25 04:15:50,681 epoch 27 - iter 64/84 - loss 0.38676103 - samples/sec: 58.28 - lr: 0.012500
2021-05-25 04:15:52,900 epoch 27 - iter 72/84 - loss 0.39629317 - samples/sec: 57.71 - lr: 0.012500
2021-05-25 04:15:55,069 epoch 27 - iter 80/84 - loss 0.38939550 - samples/sec: 59.00 - lr: 0.012500
2021-05-25 04:15:56,179 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:56,179 EPOCH 27 done: loss 0.3868 - lr 0.0125000
2021-05-25 04:15:57,146 DEV : loss 0.3631078898906708 - score 0.9535
2021-05-25 04:15:57,162 BAD EPOCHS (no improvement): 2
2021-05-25 04:15:57,162 ----------------------------------------------------------------------------------------------------
2021-05-25 04:15:59,438 epoch 28 - iter 8/84 - loss 0.60543099 - samples/sec: 56.26 - lr: 0.012500
2021-05-25 04:16:01,679 epoch 28 - iter 16/84 - loss 0.47078516 - samples/sec: 57.12 - lr: 0.012500
2021-05-25 04:16:04,011 epoch 28 - iter 24/84 - loss 0.42772069 - samples/sec: 54.91 - lr: 0.012500
2021-05-25 04:16:06,271 epoch 28 - iter 32/84 - loss 0.43028268 - samples/sec: 56.64 - lr: 0.012500
2021-05-25 04:16:08,552 epoch 28 - iter 40/84 - loss 0.41755951 - samples/sec: 56.14 - lr: 0.012500
2021-05-25 04:16:10,834 epoch 28 - iter 48/84 - loss 0.40709589 - samples/sec: 56.11 - lr: 0.012500
2021-05-25 04:16:13,149 epoch 28 - iter 56/84 - loss 0.40974722 - samples/sec: 55.30 - lr: 0.012500
2021-05-25 04:16:15,417 epoch 28 - iter 64/84 - loss 0.39087622 - samples/sec: 56.45 - lr: 0.012500
2021-05-25 04:16:17,695 epoch 28 - iter 72/84 - loss 0.39098110 - samples/sec: 56.19 - lr: 0.012500
2021-05-25 04:16:19,988 epoch 28 - iter 80/84 - loss 0.39212067 - samples/sec: 55.85 - lr: 0.012500
2021-05-25 04:16:21,104 ----------------------------------------------------------------------------------------------------
2021-05-25 04:16:21,104 EPOCH 28 done: loss 0.3889 - lr 0.0125000
2021-05-25 04:16:22,071 DEV : loss 0.3890087306499481 - score 0.9513
2021-05-25 04:16:22,087 BAD EPOCHS (no improvement): 3
2021-05-25 04:16:22,087 ----------------------------------------------------------------------------------------------------
2021-05-25 04:16:24,324 epoch 29 - iter 8/84 - loss 0.32876220 - samples/sec: 57.25 - lr: 0.012500
2021-05-25 04:16:26,616 epoch 29 - iter 16/84 - loss 0.38879900 - samples/sec: 55.86 - lr: 0.012500
2021-05-25 04:16:28,912 epoch 29 - iter 24/84 - loss 0.41248835 - samples/sec: 55.74 - lr: 0.012500
2021-05-25 04:16:31,176 epoch 29 - iter 32/84 - loss 0.44370671 - samples/sec: 56.57 - lr: 0.012500
2021-05-25 04:16:33,404 epoch 29 - iter 40/84 - loss 0.42777468 - samples/sec: 57.44 - lr: 0.012500
2021-05-25 04:16:35,651 epoch 29 - iter 48/84 - loss 0.39616406 - samples/sec: 56.97 - lr: 0.012500
2021-05-25 04:16:37,938 epoch 29 - iter 56/84 - loss 0.39259454 - samples/sec: 56.00 - lr: 0.012500
2021-05-25 04:16:40,227 epoch 29 - iter 64/84 - loss 0.37548696 - samples/sec: 55.93 - lr: 0.012500
2021-05-25 04:16:42,512 epoch 29 - iter 72/84 - loss 0.37840731 - samples/sec: 56.03 - lr: 0.012500
2021-05-25 04:16:44,801 epoch 29 - iter 80/84 - loss 0.37482459 - samples/sec: 55.93 - lr: 0.012500
2021-05-25 04:16:45,946 ----------------------------------------------------------------------------------------------------
2021-05-25 04:16:45,946 EPOCH 29 done: loss 0.3662 - lr 0.0125000
2021-05-25 04:16:46,912 DEV : loss 0.29959285259246826 - score 0.9575
Epoch    29: reducing learning rate of group 0 to 6.2500e-03.
2021-05-25 04:16:46,928 BAD EPOCHS (no improvement): 4
2021-05-25 04:16:46,928 ----------------------------------------------------------------------------------------------------
2021-05-25 04:16:49,214 epoch 30 - iter 8/84 - loss 0.33560504 - samples/sec: 56.01 - lr: 0.006250
2021-05-25 04:16:51,446 epoch 30 - iter 16/84 - loss 0.31434098 - samples/sec: 57.35 - lr: 0.006250
2021-05-25 04:16:53,701 epoch 30 - iter 24/84 - loss 0.34539463 - samples/sec: 56.79 - lr: 0.006250
2021-05-25 04:16:55,947 epoch 30 - iter 32/84 - loss 0.33288575 - samples/sec: 56.99 - lr: 0.006250
2021-05-25 04:16:58,188 epoch 30 - iter 40/84 - loss 0.36659810 - samples/sec: 57.15 - lr: 0.006250
2021-05-25 04:17:00,426 epoch 30 - iter 48/84 - loss 0.37291673 - samples/sec: 57.19 - lr: 0.006250
2021-05-25 04:17:02,681 epoch 30 - iter 56/84 - loss 0.39670543 - samples/sec: 56.79 - lr: 0.006250
2021-05-25 04:17:04,899 epoch 30 - iter 64/84 - loss 0.38635453 - samples/sec: 57.73 - lr: 0.006250
2021-05-25 04:17:07,121 epoch 30 - iter 72/84 - loss 0.38619054 - samples/sec: 57.62 - lr: 0.006250
2021-05-25 04:17:09,390 epoch 30 - iter 80/84 - loss 0.38128949 - samples/sec: 56.41 - lr: 0.006250
2021-05-25 04:17:10,500 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:10,500 EPOCH 30 done: loss 0.3766 - lr 0.0062500
2021-05-25 04:17:11,466 DEV : loss 0.3626466691493988 - score 0.9557
2021-05-25 04:17:11,482 BAD EPOCHS (no improvement): 1
2021-05-25 04:17:12,797 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:12,797 Testing using best model ...
2021-05-25 04:17:12,797 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/deu.rst.pcc/best-model.pt
2021-05-25 04:17:19,442 0.9899	0.9249	0.9563
2021-05-25 04:17:19,442 
Results:
- F1-score (micro) 0.9563
- F1-score (macro) 0.9563

By class:
SENT       tp: 197 - fp: 2 - fn: 16 - precision: 0.9899 - recall: 0.9249 - f1-score: 0.9563
2021-05-25 04:17:19,442 ----------------------------------------------------------------------------------------------------
/home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt/
2021-05-25 04:17:19,471 Reading data from /home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt
2021-05-25 04:17:19,471 Train: /home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt/sent_train.txt
2021-05-25 04:17:19,474 Dev: /home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt/sent_dev.txt
2021-05-25 04:17:19,476 Test: /home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt/sent_test.txt
Corpus: 10286 train + 1410 dev + 1379 test sentences
Dictionary with 5 tags: <unk>, O, B-SENT, <START>, <STOP>
2021-05-25 04:17:32,888 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,891 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): CharacterEmbeddings(
      (char_embedding): Embedding(275, 25)
      (char_rnn): LSTM(25, 25, bidirectional=True)
    )
    (list_embedding_1): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(119547, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
    (list_embedding_2): TransformerWordEmbeddings(
      (model): BertModel(
        (embeddings): BertEmbeddings(
          (word_embeddings): Embedding(28996, 768, padding_idx=0)
          (position_embeddings): Embedding(512, 768)
          (token_type_embeddings): Embedding(2, 768)
          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (encoder): BertEncoder(
          (layer): ModuleList(
            (0): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (1): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (2): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (3): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (4): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (5): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (6): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (7): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (8): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (9): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (10): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
            (11): BertLayer(
              (attention): BertAttention(
                (self): BertSelfAttention(
                  (query): Linear(in_features=768, out_features=768, bias=True)
                  (key): Linear(in_features=768, out_features=768, bias=True)
                  (value): Linear(in_features=768, out_features=768, bias=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
                (output): BertSelfOutput(
                  (dense): Linear(in_features=768, out_features=768, bias=True)
                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                  (dropout): Dropout(p=0.1, inplace=False)
                )
              )
              (intermediate): BertIntermediate(
                (dense): Linear(in_features=768, out_features=3072, bias=True)
              )
              (output): BertOutput(
                (dense): Linear(in_features=3072, out_features=768, bias=True)
                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
                (dropout): Dropout(p=0.1, inplace=False)
              )
            )
          )
        )
        (pooler): BertPooler(
          (dense): Linear(in_features=768, out_features=768, bias=True)
          (activation): Tanh()
        )
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=6194, out_features=6194, bias=True)
  (rnn): LSTM(6194, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=5, bias=True)
  (beta): 1.0
  (weights): None
  (weight_tensor) None
)"
2021-05-25 04:17:32,891 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,891 Corpus: "Corpus: 10286 train + 1410 dev + 1379 test sentences"
2021-05-25 04:17:32,891 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,891 Parameters:
2021-05-25 04:17:32,891  - learning_rate: "0.1"
2021-05-25 04:17:32,891  - mini_batch_size: "16"
2021-05-25 04:17:32,891  - patience: "3"
2021-05-25 04:17:32,891  - anneal_factor: "0.5"
2021-05-25 04:17:32,891  - max_epochs: "30"
2021-05-25 04:17:32,891  - shuffle: "True"
2021-05-25 04:17:32,891  - train_with_dev: "False"
2021-05-25 04:17:32,891  - batch_growth_annealing: "False"
2021-05-25 04:17:32,891 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,891 Model training base path: "/home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt"
2021-05-25 04:17:32,891 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,892 Device: cuda:0
2021-05-25 04:17:32,892 ----------------------------------------------------------------------------------------------------
2021-05-25 04:17:32,892 Embeddings storage mode: cpu
2021-05-25 04:17:32,893 ----------------------------------------------------------------------------------------------------
2021-05-25 04:18:25,548 epoch 1 - iter 64/643 - loss 3.89950289 - samples/sec: 19.45 - lr: 0.100000
2021-05-25 04:19:17,611 epoch 1 - iter 128/643 - loss 2.98630455 - samples/sec: 19.67 - lr: 0.100000
2021-05-25 04:20:09,779 epoch 1 - iter 192/643 - loss 2.60622346 - samples/sec: 19.63 - lr: 0.100000
2021-05-25 04:21:01,964 epoch 1 - iter 256/643 - loss 2.26602852 - samples/sec: 19.62 - lr: 0.100000
2021-05-25 04:21:53,979 epoch 1 - iter 320/643 - loss 2.07538324 - samples/sec: 19.69 - lr: 0.100000
2021-05-25 04:22:46,134 epoch 1 - iter 384/643 - loss 1.95187896 - samples/sec: 19.63 - lr: 0.100000
2021-05-25 04:23:38,991 epoch 1 - iter 448/643 - loss 1.83834640 - samples/sec: 19.37 - lr: 0.100000
2021-05-25 04:24:31,084 epoch 1 - iter 512/643 - loss 1.73124945 - samples/sec: 19.66 - lr: 0.100000
2021-05-25 04:25:23,303 epoch 1 - iter 576/643 - loss 1.64112467 - samples/sec: 19.61 - lr: 0.100000
2021-05-25 04:26:15,371 epoch 1 - iter 640/643 - loss 1.57700274 - samples/sec: 19.67 - lr: 0.100000
2021-05-25 04:26:17,700 ----------------------------------------------------------------------------------------------------
2021-05-25 04:26:17,700 EPOCH 1 done: loss 1.5773 - lr 0.1000000
2021-05-25 04:27:03,262 DEV : loss 0.5853402614593506 - score 0.8751
2021-05-25 04:27:03,408 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:27:05,695 ----------------------------------------------------------------------------------------------------
2021-05-25 04:27:24,354 epoch 2 - iter 64/643 - loss 0.78562561 - samples/sec: 54.89 - lr: 0.100000
2021-05-25 04:27:43,057 epoch 2 - iter 128/643 - loss 0.86120551 - samples/sec: 54.76 - lr: 0.100000
2021-05-25 04:28:01,949 epoch 2 - iter 192/643 - loss 0.87727704 - samples/sec: 54.21 - lr: 0.100000
2021-05-25 04:28:20,707 epoch 2 - iter 256/643 - loss 0.86168660 - samples/sec: 54.60 - lr: 0.100000
2021-05-25 04:28:39,618 epoch 2 - iter 320/643 - loss 0.87922522 - samples/sec: 54.16 - lr: 0.100000
2021-05-25 04:28:58,247 epoch 2 - iter 384/643 - loss 0.88528552 - samples/sec: 54.97 - lr: 0.100000
2021-05-25 04:29:16,842 epoch 2 - iter 448/643 - loss 0.87685264 - samples/sec: 55.08 - lr: 0.100000
2021-05-25 04:29:35,380 epoch 2 - iter 512/643 - loss 0.88062717 - samples/sec: 55.24 - lr: 0.100000
2021-05-25 04:29:54,076 epoch 2 - iter 576/643 - loss 0.86976019 - samples/sec: 54.78 - lr: 0.100000
2021-05-25 04:30:12,729 epoch 2 - iter 640/643 - loss 0.86075167 - samples/sec: 54.91 - lr: 0.100000
2021-05-25 04:30:13,568 ----------------------------------------------------------------------------------------------------
2021-05-25 04:30:13,568 EPOCH 2 done: loss 0.8634 - lr 0.1000000
2021-05-25 04:30:22,444 DEV : loss 0.4658150374889374 - score 0.9119
2021-05-25 04:30:22,591 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:30:32,653 ----------------------------------------------------------------------------------------------------
2021-05-25 04:30:51,325 epoch 3 - iter 64/643 - loss 0.96242100 - samples/sec: 54.85 - lr: 0.100000
2021-05-25 04:31:10,096 epoch 3 - iter 128/643 - loss 0.86569644 - samples/sec: 54.56 - lr: 0.100000
2021-05-25 04:31:28,951 epoch 3 - iter 192/643 - loss 0.84681422 - samples/sec: 54.32 - lr: 0.100000
2021-05-25 04:31:47,693 epoch 3 - iter 256/643 - loss 0.83761044 - samples/sec: 54.64 - lr: 0.100000
2021-05-25 04:32:06,398 epoch 3 - iter 320/643 - loss 0.83031383 - samples/sec: 54.75 - lr: 0.100000
2021-05-25 04:32:25,118 epoch 3 - iter 384/643 - loss 0.80595649 - samples/sec: 54.71 - lr: 0.100000
2021-05-25 04:32:43,814 epoch 3 - iter 448/643 - loss 0.79866122 - samples/sec: 54.78 - lr: 0.100000
2021-05-25 04:33:02,570 epoch 3 - iter 512/643 - loss 0.79187102 - samples/sec: 54.60 - lr: 0.100000
2021-05-25 04:33:21,134 epoch 3 - iter 576/643 - loss 0.78097015 - samples/sec: 55.17 - lr: 0.100000
2021-05-25 04:33:39,728 epoch 3 - iter 640/643 - loss 0.78009555 - samples/sec: 55.08 - lr: 0.100000
2021-05-25 04:33:40,562 ----------------------------------------------------------------------------------------------------
2021-05-25 04:33:40,562 EPOCH 3 done: loss 0.7779 - lr 0.1000000
2021-05-25 04:33:49,386 DEV : loss 0.4147326946258545 - score 0.9171
2021-05-25 04:33:49,532 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:33:59,173 ----------------------------------------------------------------------------------------------------
2021-05-25 04:34:17,757 epoch 4 - iter 64/643 - loss 0.70221993 - samples/sec: 55.11 - lr: 0.100000
2021-05-25 04:34:36,284 epoch 4 - iter 128/643 - loss 0.68326409 - samples/sec: 55.28 - lr: 0.100000
2021-05-25 04:34:55,025 epoch 4 - iter 192/643 - loss 0.68461246 - samples/sec: 54.65 - lr: 0.100000
2021-05-25 04:35:13,632 epoch 4 - iter 256/643 - loss 0.67345378 - samples/sec: 55.04 - lr: 0.100000
2021-05-25 04:35:32,194 epoch 4 - iter 320/643 - loss 0.66305389 - samples/sec: 55.17 - lr: 0.100000
2021-05-25 04:35:50,838 epoch 4 - iter 384/643 - loss 0.64266812 - samples/sec: 54.93 - lr: 0.100000
2021-05-25 04:36:09,496 epoch 4 - iter 448/643 - loss 0.64831766 - samples/sec: 54.89 - lr: 0.100000
2021-05-25 04:36:28,079 epoch 4 - iter 512/643 - loss 0.65159286 - samples/sec: 55.11 - lr: 0.100000
2021-05-25 04:36:46,757 epoch 4 - iter 576/643 - loss 0.64966898 - samples/sec: 54.83 - lr: 0.100000
2021-05-25 04:37:05,298 epoch 4 - iter 640/643 - loss 0.63980548 - samples/sec: 55.24 - lr: 0.100000
2021-05-25 04:37:06,133 ----------------------------------------------------------------------------------------------------
2021-05-25 04:37:06,133 EPOCH 4 done: loss 0.6392 - lr 0.1000000
2021-05-25 04:37:14,981 DEV : loss 0.45677632093429565 - score 0.9228
2021-05-25 04:37:15,129 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:37:24,240 ----------------------------------------------------------------------------------------------------
2021-05-25 04:37:42,896 epoch 5 - iter 64/643 - loss 0.50388975 - samples/sec: 54.90 - lr: 0.100000
2021-05-25 04:38:01,526 epoch 5 - iter 128/643 - loss 0.51755601 - samples/sec: 54.97 - lr: 0.100000
2021-05-25 04:38:20,115 epoch 5 - iter 192/643 - loss 0.55085464 - samples/sec: 55.09 - lr: 0.100000
2021-05-25 04:38:38,712 epoch 5 - iter 256/643 - loss 0.54139397 - samples/sec: 55.07 - lr: 0.100000
2021-05-25 04:38:57,339 epoch 5 - iter 320/643 - loss 0.55565470 - samples/sec: 54.98 - lr: 0.100000
2021-05-25 04:39:15,947 epoch 5 - iter 384/643 - loss 0.55482214 - samples/sec: 55.04 - lr: 0.100000
2021-05-25 04:39:34,451 epoch 5 - iter 448/643 - loss 0.56134386 - samples/sec: 55.35 - lr: 0.100000
2021-05-25 04:39:52,987 epoch 5 - iter 512/643 - loss 0.56504810 - samples/sec: 55.25 - lr: 0.100000
2021-05-25 04:40:11,726 epoch 5 - iter 576/643 - loss 0.55646748 - samples/sec: 54.65 - lr: 0.100000
2021-05-25 04:40:30,387 epoch 5 - iter 640/643 - loss 0.55562035 - samples/sec: 54.88 - lr: 0.100000
2021-05-25 04:40:31,231 ----------------------------------------------------------------------------------------------------
2021-05-25 04:40:31,231 EPOCH 5 done: loss 0.5568 - lr 0.1000000
2021-05-25 04:40:40,091 DEV : loss 0.31152868270874023 - score 0.9474
2021-05-25 04:40:40,238 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:40:49,242 ----------------------------------------------------------------------------------------------------
2021-05-25 04:41:07,902 epoch 6 - iter 64/643 - loss 0.51986618 - samples/sec: 54.89 - lr: 0.100000
2021-05-25 04:41:26,669 epoch 6 - iter 128/643 - loss 0.51714911 - samples/sec: 54.57 - lr: 0.100000
2021-05-25 04:41:45,387 epoch 6 - iter 192/643 - loss 0.51601869 - samples/sec: 54.71 - lr: 0.100000
2021-05-25 04:42:04,015 epoch 6 - iter 256/643 - loss 0.51571806 - samples/sec: 54.98 - lr: 0.100000
2021-05-25 04:42:22,793 epoch 6 - iter 320/643 - loss 0.52115533 - samples/sec: 54.54 - lr: 0.100000
2021-05-25 04:42:41,520 epoch 6 - iter 384/643 - loss 0.51003706 - samples/sec: 54.69 - lr: 0.100000
2021-05-25 04:43:00,216 epoch 6 - iter 448/643 - loss 0.51714164 - samples/sec: 54.78 - lr: 0.100000
2021-05-25 04:43:19,025 epoch 6 - iter 512/643 - loss 0.52059306 - samples/sec: 54.45 - lr: 0.100000
2021-05-25 04:43:37,717 epoch 6 - iter 576/643 - loss 0.52348877 - samples/sec: 54.79 - lr: 0.100000
2021-05-25 04:43:56,529 epoch 6 - iter 640/643 - loss 0.52090661 - samples/sec: 54.44 - lr: 0.100000
2021-05-25 04:43:57,394 ----------------------------------------------------------------------------------------------------
2021-05-25 04:43:57,394 EPOCH 6 done: loss 0.5219 - lr 0.1000000
2021-05-25 04:44:07,080 DEV : loss 0.34720510244369507 - score 0.9478
2021-05-25 04:44:07,226 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:44:17,015 ----------------------------------------------------------------------------------------------------
2021-05-25 04:44:35,652 epoch 7 - iter 64/643 - loss 0.48436438 - samples/sec: 54.96 - lr: 0.100000
2021-05-25 04:44:54,230 epoch 7 - iter 128/643 - loss 0.47366950 - samples/sec: 55.13 - lr: 0.100000
2021-05-25 04:45:12,867 epoch 7 - iter 192/643 - loss 0.50621617 - samples/sec: 54.95 - lr: 0.100000
2021-05-25 04:45:31,598 epoch 7 - iter 256/643 - loss 0.52138013 - samples/sec: 54.68 - lr: 0.100000
2021-05-25 04:45:50,388 epoch 7 - iter 320/643 - loss 0.50557559 - samples/sec: 54.50 - lr: 0.100000
2021-05-25 04:46:09,088 epoch 7 - iter 384/643 - loss 0.49887432 - samples/sec: 54.77 - lr: 0.100000
2021-05-25 04:46:27,912 epoch 7 - iter 448/643 - loss 0.49928703 - samples/sec: 54.41 - lr: 0.100000
2021-05-25 04:46:46,673 epoch 7 - iter 512/643 - loss 0.49996345 - samples/sec: 54.59 - lr: 0.100000
2021-05-25 04:47:05,345 epoch 7 - iter 576/643 - loss 0.50182677 - samples/sec: 54.85 - lr: 0.100000
2021-05-25 04:47:24,187 epoch 7 - iter 640/643 - loss 0.49914251 - samples/sec: 54.35 - lr: 0.100000
2021-05-25 04:47:25,039 ----------------------------------------------------------------------------------------------------
2021-05-25 04:47:25,040 EPOCH 7 done: loss 0.4979 - lr 0.1000000
2021-05-25 04:47:33,872 DEV : loss 0.31701764464378357 - score 0.9394
2021-05-25 04:47:34,018 BAD EPOCHS (no improvement): 1
2021-05-25 04:47:34,019 ----------------------------------------------------------------------------------------------------
2021-05-25 04:47:52,647 epoch 8 - iter 64/643 - loss 0.45872492 - samples/sec: 54.98 - lr: 0.100000
2021-05-25 04:48:11,331 epoch 8 - iter 128/643 - loss 0.47896745 - samples/sec: 54.81 - lr: 0.100000
2021-05-25 04:48:29,961 epoch 8 - iter 192/643 - loss 0.46338734 - samples/sec: 54.97 - lr: 0.100000
2021-05-25 04:48:48,671 epoch 8 - iter 256/643 - loss 0.48720228 - samples/sec: 54.74 - lr: 0.100000
2021-05-25 04:49:07,424 epoch 8 - iter 320/643 - loss 0.47730664 - samples/sec: 54.61 - lr: 0.100000
2021-05-25 04:49:26,147 epoch 8 - iter 384/643 - loss 0.47142823 - samples/sec: 54.70 - lr: 0.100000
2021-05-25 04:49:44,714 epoch 8 - iter 448/643 - loss 0.46211291 - samples/sec: 55.16 - lr: 0.100000
2021-05-25 04:50:03,574 epoch 8 - iter 512/643 - loss 0.46655173 - samples/sec: 54.30 - lr: 0.100000
2021-05-25 04:50:22,257 epoch 8 - iter 576/643 - loss 0.46455043 - samples/sec: 54.81 - lr: 0.100000
2021-05-25 04:50:40,860 epoch 8 - iter 640/643 - loss 0.45917715 - samples/sec: 55.05 - lr: 0.100000
2021-05-25 04:50:41,701 ----------------------------------------------------------------------------------------------------
2021-05-25 04:50:41,701 EPOCH 8 done: loss 0.4600 - lr 0.1000000
2021-05-25 04:50:50,544 DEV : loss 0.3043358027935028 - score 0.951
2021-05-25 04:50:50,691 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:51:00,436 ----------------------------------------------------------------------------------------------------
2021-05-25 04:51:19,200 epoch 9 - iter 64/643 - loss 0.39300245 - samples/sec: 54.58 - lr: 0.100000
2021-05-25 04:51:37,998 epoch 9 - iter 128/643 - loss 0.43436948 - samples/sec: 54.48 - lr: 0.100000
2021-05-25 04:51:56,684 epoch 9 - iter 192/643 - loss 0.42678179 - samples/sec: 54.81 - lr: 0.100000
2021-05-25 04:52:15,378 epoch 9 - iter 256/643 - loss 0.44056030 - samples/sec: 54.79 - lr: 0.100000
2021-05-25 04:52:34,081 epoch 9 - iter 320/643 - loss 0.45028481 - samples/sec: 54.76 - lr: 0.100000
2021-05-25 04:52:52,722 epoch 9 - iter 384/643 - loss 0.44455965 - samples/sec: 54.94 - lr: 0.100000
2021-05-25 04:53:11,277 epoch 9 - iter 448/643 - loss 0.43816462 - samples/sec: 55.20 - lr: 0.100000
2021-05-25 04:53:29,973 epoch 9 - iter 512/643 - loss 0.43295829 - samples/sec: 54.78 - lr: 0.100000
2021-05-25 04:53:48,666 epoch 9 - iter 576/643 - loss 0.43650201 - samples/sec: 54.79 - lr: 0.100000
2021-05-25 04:54:07,334 epoch 9 - iter 640/643 - loss 0.43986296 - samples/sec: 54.86 - lr: 0.100000
2021-05-25 04:54:08,182 ----------------------------------------------------------------------------------------------------
2021-05-25 04:54:08,182 EPOCH 9 done: loss 0.4389 - lr 0.1000000
2021-05-25 04:54:17,027 DEV : loss 0.2832014858722687 - score 0.9447
2021-05-25 04:54:17,173 BAD EPOCHS (no improvement): 1
2021-05-25 04:54:17,173 ----------------------------------------------------------------------------------------------------
2021-05-25 04:54:35,861 epoch 10 - iter 64/643 - loss 0.40067180 - samples/sec: 54.80 - lr: 0.100000
2021-05-25 04:54:54,580 epoch 10 - iter 128/643 - loss 0.40385896 - samples/sec: 54.71 - lr: 0.100000
2021-05-25 04:55:13,365 epoch 10 - iter 192/643 - loss 0.42641042 - samples/sec: 54.52 - lr: 0.100000
2021-05-25 04:55:31,895 epoch 10 - iter 256/643 - loss 0.42215224 - samples/sec: 55.27 - lr: 0.100000
2021-05-25 04:55:50,508 epoch 10 - iter 320/643 - loss 0.43299234 - samples/sec: 55.02 - lr: 0.100000
2021-05-25 04:56:09,109 epoch 10 - iter 384/643 - loss 0.43810638 - samples/sec: 55.06 - lr: 0.100000
2021-05-25 04:56:27,636 epoch 10 - iter 448/643 - loss 0.42431922 - samples/sec: 55.28 - lr: 0.100000
2021-05-25 04:56:46,146 epoch 10 - iter 512/643 - loss 0.42169878 - samples/sec: 55.33 - lr: 0.100000
2021-05-25 04:57:04,806 epoch 10 - iter 576/643 - loss 0.41584729 - samples/sec: 54.88 - lr: 0.100000
2021-05-25 04:57:23,559 epoch 10 - iter 640/643 - loss 0.41472331 - samples/sec: 54.61 - lr: 0.100000
2021-05-25 04:57:24,417 ----------------------------------------------------------------------------------------------------
2021-05-25 04:57:24,417 EPOCH 10 done: loss 0.4151 - lr 0.1000000
2021-05-25 04:57:34,160 DEV : loss 0.2762599289417267 - score 0.962
2021-05-25 04:57:34,308 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 04:57:44,054 ----------------------------------------------------------------------------------------------------
2021-05-25 04:58:02,818 epoch 11 - iter 64/643 - loss 0.35521194 - samples/sec: 54.58 - lr: 0.100000
2021-05-25 04:58:21,531 epoch 11 - iter 128/643 - loss 0.41785888 - samples/sec: 54.73 - lr: 0.100000
2021-05-25 04:58:40,238 epoch 11 - iter 192/643 - loss 0.39961614 - samples/sec: 54.75 - lr: 0.100000
2021-05-25 04:58:59,026 epoch 11 - iter 256/643 - loss 0.40880307 - samples/sec: 54.51 - lr: 0.100000
2021-05-25 04:59:17,683 epoch 11 - iter 320/643 - loss 0.41895291 - samples/sec: 54.89 - lr: 0.100000
2021-05-25 04:59:36,348 epoch 11 - iter 384/643 - loss 0.43317422 - samples/sec: 54.87 - lr: 0.100000
2021-05-25 04:59:55,063 epoch 11 - iter 448/643 - loss 0.43102854 - samples/sec: 54.72 - lr: 0.100000
2021-05-25 05:00:13,751 epoch 11 - iter 512/643 - loss 0.42537571 - samples/sec: 54.80 - lr: 0.100000
2021-05-25 05:00:32,523 epoch 11 - iter 576/643 - loss 0.42242064 - samples/sec: 54.56 - lr: 0.100000
2021-05-25 05:00:51,156 epoch 11 - iter 640/643 - loss 0.42029324 - samples/sec: 54.96 - lr: 0.100000
2021-05-25 05:00:51,990 ----------------------------------------------------------------------------------------------------
2021-05-25 05:00:51,990 EPOCH 11 done: loss 0.4195 - lr 0.1000000
2021-05-25 05:01:00,816 DEV : loss 0.287256121635437 - score 0.958
2021-05-25 05:01:00,962 BAD EPOCHS (no improvement): 1
2021-05-25 05:01:00,962 ----------------------------------------------------------------------------------------------------
2021-05-25 05:01:19,644 epoch 12 - iter 64/643 - loss 0.41361442 - samples/sec: 54.82 - lr: 0.100000
2021-05-25 05:01:38,425 epoch 12 - iter 128/643 - loss 0.42421097 - samples/sec: 54.53 - lr: 0.100000
2021-05-25 05:01:57,021 epoch 12 - iter 192/643 - loss 0.39784067 - samples/sec: 55.07 - lr: 0.100000
2021-05-25 05:02:15,749 epoch 12 - iter 256/643 - loss 0.40884962 - samples/sec: 54.68 - lr: 0.100000
2021-05-25 05:02:34,375 epoch 12 - iter 320/643 - loss 0.41257577 - samples/sec: 54.99 - lr: 0.100000
2021-05-25 05:02:53,013 epoch 12 - iter 384/643 - loss 0.41846399 - samples/sec: 54.95 - lr: 0.100000
2021-05-25 05:03:11,740 epoch 12 - iter 448/643 - loss 0.42295672 - samples/sec: 54.69 - lr: 0.100000
2021-05-25 05:03:30,400 epoch 12 - iter 512/643 - loss 0.41991213 - samples/sec: 54.88 - lr: 0.100000
2021-05-25 05:03:49,146 epoch 12 - iter 576/643 - loss 0.41204161 - samples/sec: 54.63 - lr: 0.100000
2021-05-25 05:04:07,847 epoch 12 - iter 640/643 - loss 0.40849099 - samples/sec: 54.76 - lr: 0.100000
2021-05-25 05:04:08,698 ----------------------------------------------------------------------------------------------------
2021-05-25 05:04:08,698 EPOCH 12 done: loss 0.4069 - lr 0.1000000
2021-05-25 05:04:17,545 DEV : loss 0.2538095712661743 - score 0.9563
2021-05-25 05:04:17,692 BAD EPOCHS (no improvement): 2
2021-05-25 05:04:17,692 ----------------------------------------------------------------------------------------------------
2021-05-25 05:04:36,415 epoch 13 - iter 64/643 - loss 0.41643200 - samples/sec: 54.70 - lr: 0.100000
2021-05-25 05:04:55,100 epoch 13 - iter 128/643 - loss 0.37693109 - samples/sec: 54.81 - lr: 0.100000
2021-05-25 05:05:13,792 epoch 13 - iter 192/643 - loss 0.40061226 - samples/sec: 54.79 - lr: 0.100000
2021-05-25 05:05:32,491 epoch 13 - iter 256/643 - loss 0.41124874 - samples/sec: 54.77 - lr: 0.100000
2021-05-25 05:05:51,110 epoch 13 - iter 320/643 - loss 0.41065607 - samples/sec: 55.00 - lr: 0.100000
2021-05-25 05:06:09,825 epoch 13 - iter 384/643 - loss 0.39851978 - samples/sec: 54.72 - lr: 0.100000
2021-05-25 05:06:28,555 epoch 13 - iter 448/643 - loss 0.39802989 - samples/sec: 54.68 - lr: 0.100000
2021-05-25 05:06:47,237 epoch 13 - iter 512/643 - loss 0.39337835 - samples/sec: 54.82 - lr: 0.100000
2021-05-25 05:07:05,952 epoch 13 - iter 576/643 - loss 0.39457589 - samples/sec: 54.72 - lr: 0.100000
2021-05-25 05:07:24,617 epoch 13 - iter 640/643 - loss 0.39569931 - samples/sec: 54.87 - lr: 0.100000
2021-05-25 05:07:25,460 ----------------------------------------------------------------------------------------------------
2021-05-25 05:07:25,460 EPOCH 13 done: loss 0.3960 - lr 0.1000000
2021-05-25 05:07:34,311 DEV : loss 0.2715342342853546 - score 0.9517
2021-05-25 05:07:34,458 BAD EPOCHS (no improvement): 3
2021-05-25 05:07:34,458 ----------------------------------------------------------------------------------------------------
2021-05-25 05:07:53,108 epoch 14 - iter 64/643 - loss 0.36580232 - samples/sec: 54.92 - lr: 0.100000
2021-05-25 05:08:11,778 epoch 14 - iter 128/643 - loss 0.35694555 - samples/sec: 54.85 - lr: 0.100000
2021-05-25 05:08:30,341 epoch 14 - iter 192/643 - loss 0.37818531 - samples/sec: 55.17 - lr: 0.100000
2021-05-25 05:08:48,917 epoch 14 - iter 256/643 - loss 0.39276554 - samples/sec: 55.13 - lr: 0.100000
2021-05-25 05:09:07,523 epoch 14 - iter 320/643 - loss 0.38525243 - samples/sec: 55.04 - lr: 0.100000
2021-05-25 05:09:26,187 epoch 14 - iter 384/643 - loss 0.38517192 - samples/sec: 54.87 - lr: 0.100000
2021-05-25 05:09:44,967 epoch 14 - iter 448/643 - loss 0.38323768 - samples/sec: 54.53 - lr: 0.100000
2021-05-25 05:10:03,702 epoch 14 - iter 512/643 - loss 0.38003004 - samples/sec: 54.66 - lr: 0.100000
2021-05-25 05:10:22,445 epoch 14 - iter 576/643 - loss 0.37944616 - samples/sec: 54.64 - lr: 0.100000
2021-05-25 05:10:41,185 epoch 14 - iter 640/643 - loss 0.37410181 - samples/sec: 54.65 - lr: 0.100000
2021-05-25 05:10:42,025 ----------------------------------------------------------------------------------------------------
2021-05-25 05:10:42,026 EPOCH 14 done: loss 0.3739 - lr 0.1000000
2021-05-25 05:10:50,862 DEV : loss 0.2586277723312378 - score 0.95
Epoch    14: reducing learning rate of group 0 to 5.0000e-02.
2021-05-25 05:10:51,009 BAD EPOCHS (no improvement): 4
2021-05-25 05:10:51,009 ----------------------------------------------------------------------------------------------------
2021-05-25 05:11:09,736 epoch 15 - iter 64/643 - loss 0.27576552 - samples/sec: 54.69 - lr: 0.050000
2021-05-25 05:11:28,487 epoch 15 - iter 128/643 - loss 0.28146233 - samples/sec: 54.62 - lr: 0.050000
2021-05-25 05:11:47,173 epoch 15 - iter 192/643 - loss 0.30723925 - samples/sec: 54.81 - lr: 0.050000
2021-05-25 05:12:06,736 epoch 15 - iter 256/643 - loss 0.31125763 - samples/sec: 52.35 - lr: 0.050000
2021-05-25 05:12:25,404 epoch 15 - iter 320/643 - loss 0.32043079 - samples/sec: 54.86 - lr: 0.050000
2021-05-25 05:12:44,149 epoch 15 - iter 384/643 - loss 0.32562155 - samples/sec: 54.64 - lr: 0.050000
2021-05-25 05:13:02,864 epoch 15 - iter 448/643 - loss 0.31606429 - samples/sec: 54.72 - lr: 0.050000
2021-05-25 05:13:21,368 epoch 15 - iter 512/643 - loss 0.31765046 - samples/sec: 55.35 - lr: 0.050000
2021-05-25 05:13:39,888 epoch 15 - iter 576/643 - loss 0.31140505 - samples/sec: 55.30 - lr: 0.050000
2021-05-25 05:13:58,482 epoch 15 - iter 640/643 - loss 0.30865538 - samples/sec: 55.08 - lr: 0.050000
2021-05-25 05:13:59,309 ----------------------------------------------------------------------------------------------------
2021-05-25 05:13:59,309 EPOCH 15 done: loss 0.3088 - lr 0.0500000
2021-05-25 05:14:08,137 DEV : loss 0.2544015347957611 - score 0.9577
2021-05-25 05:14:08,284 BAD EPOCHS (no improvement): 1
2021-05-25 05:14:08,284 ----------------------------------------------------------------------------------------------------
2021-05-25 05:14:26,992 epoch 16 - iter 64/643 - loss 0.30017818 - samples/sec: 54.74 - lr: 0.050000
2021-05-25 05:14:45,762 epoch 16 - iter 128/643 - loss 0.28821829 - samples/sec: 54.56 - lr: 0.050000
2021-05-25 05:15:04,538 epoch 16 - iter 192/643 - loss 0.27973528 - samples/sec: 54.54 - lr: 0.050000
2021-05-25 05:15:23,207 epoch 16 - iter 256/643 - loss 0.29302904 - samples/sec: 54.86 - lr: 0.050000
2021-05-25 05:15:42,001 epoch 16 - iter 320/643 - loss 0.29760460 - samples/sec: 54.49 - lr: 0.050000
2021-05-25 05:16:00,700 epoch 16 - iter 384/643 - loss 0.30157917 - samples/sec: 54.77 - lr: 0.050000
2021-05-25 05:16:19,399 epoch 16 - iter 448/643 - loss 0.30213550 - samples/sec: 54.77 - lr: 0.050000
2021-05-25 05:16:38,167 epoch 16 - iter 512/643 - loss 0.30274582 - samples/sec: 54.57 - lr: 0.050000
2021-05-25 05:16:56,774 epoch 16 - iter 576/643 - loss 0.29990563 - samples/sec: 55.04 - lr: 0.050000
2021-05-25 05:17:15,467 epoch 16 - iter 640/643 - loss 0.29722336 - samples/sec: 54.79 - lr: 0.050000
2021-05-25 05:17:16,320 ----------------------------------------------------------------------------------------------------
2021-05-25 05:17:16,320 EPOCH 16 done: loss 0.2974 - lr 0.0500000
2021-05-25 05:17:25,133 DEV : loss 0.22980493307113647 - score 0.9623
2021-05-25 05:17:25,281 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:17:34,857 ----------------------------------------------------------------------------------------------------
2021-05-25 05:17:53,476 epoch 17 - iter 64/643 - loss 0.26547919 - samples/sec: 55.01 - lr: 0.050000
2021-05-25 05:18:12,172 epoch 17 - iter 128/643 - loss 0.28294300 - samples/sec: 54.78 - lr: 0.050000
2021-05-25 05:18:30,813 epoch 17 - iter 192/643 - loss 0.28445776 - samples/sec: 54.94 - lr: 0.050000
2021-05-25 05:18:49,487 epoch 17 - iter 256/643 - loss 0.29566779 - samples/sec: 54.84 - lr: 0.050000
2021-05-25 05:19:08,166 epoch 17 - iter 320/643 - loss 0.28459531 - samples/sec: 54.83 - lr: 0.050000
2021-05-25 05:19:26,788 epoch 17 - iter 384/643 - loss 0.28388733 - samples/sec: 55.00 - lr: 0.050000
2021-05-25 05:19:45,332 epoch 17 - iter 448/643 - loss 0.28686930 - samples/sec: 55.23 - lr: 0.050000
2021-05-25 05:20:04,007 epoch 17 - iter 512/643 - loss 0.28121185 - samples/sec: 54.84 - lr: 0.050000
2021-05-25 05:20:22,425 epoch 17 - iter 576/643 - loss 0.28086433 - samples/sec: 55.60 - lr: 0.050000
2021-05-25 05:20:41,035 epoch 17 - iter 640/643 - loss 0.28558658 - samples/sec: 55.03 - lr: 0.050000
2021-05-25 05:20:41,883 ----------------------------------------------------------------------------------------------------
2021-05-25 05:20:41,883 EPOCH 17 done: loss 0.2863 - lr 0.0500000
2021-05-25 05:20:50,696 DEV : loss 0.24005508422851562 - score 0.9608
2021-05-25 05:20:50,841 BAD EPOCHS (no improvement): 1
2021-05-25 05:20:50,842 ----------------------------------------------------------------------------------------------------
2021-05-25 05:21:09,496 epoch 18 - iter 64/643 - loss 0.33247799 - samples/sec: 54.90 - lr: 0.050000
2021-05-25 05:21:28,165 epoch 18 - iter 128/643 - loss 0.30729954 - samples/sec: 54.86 - lr: 0.050000
2021-05-25 05:21:46,829 epoch 18 - iter 192/643 - loss 0.29078822 - samples/sec: 54.87 - lr: 0.050000
2021-05-25 05:22:05,504 epoch 18 - iter 256/643 - loss 0.27578911 - samples/sec: 54.84 - lr: 0.050000
2021-05-25 05:22:24,172 epoch 18 - iter 320/643 - loss 0.28404821 - samples/sec: 54.86 - lr: 0.050000
2021-05-25 05:22:42,790 epoch 18 - iter 384/643 - loss 0.28039128 - samples/sec: 55.01 - lr: 0.050000
2021-05-25 05:23:01,457 epoch 18 - iter 448/643 - loss 0.28138255 - samples/sec: 54.86 - lr: 0.050000
2021-05-25 05:23:20,112 epoch 18 - iter 512/643 - loss 0.28351134 - samples/sec: 54.90 - lr: 0.050000
2021-05-25 05:23:38,877 epoch 18 - iter 576/643 - loss 0.28165279 - samples/sec: 54.58 - lr: 0.050000
2021-05-25 05:23:57,517 epoch 18 - iter 640/643 - loss 0.28009616 - samples/sec: 54.94 - lr: 0.050000
2021-05-25 05:23:58,357 ----------------------------------------------------------------------------------------------------
2021-05-25 05:23:58,357 EPOCH 18 done: loss 0.2793 - lr 0.0500000
2021-05-25 05:24:07,210 DEV : loss 0.26591455936431885 - score 0.9597
2021-05-25 05:24:07,359 BAD EPOCHS (no improvement): 2
2021-05-25 05:24:07,359 ----------------------------------------------------------------------------------------------------
2021-05-25 05:24:26,115 epoch 19 - iter 64/643 - loss 0.26583695 - samples/sec: 54.60 - lr: 0.050000
2021-05-25 05:24:44,785 epoch 19 - iter 128/643 - loss 0.24965878 - samples/sec: 54.85 - lr: 0.050000
2021-05-25 05:25:03,391 epoch 19 - iter 192/643 - loss 0.24264479 - samples/sec: 55.04 - lr: 0.050000
2021-05-25 05:25:21,958 epoch 19 - iter 256/643 - loss 0.25522508 - samples/sec: 55.16 - lr: 0.050000
2021-05-25 05:25:40,442 epoch 19 - iter 320/643 - loss 0.26677832 - samples/sec: 55.41 - lr: 0.050000
2021-05-25 05:25:59,082 epoch 19 - iter 384/643 - loss 0.26936192 - samples/sec: 54.94 - lr: 0.050000
2021-05-25 05:26:17,787 epoch 19 - iter 448/643 - loss 0.26503476 - samples/sec: 54.75 - lr: 0.050000
2021-05-25 05:26:36,412 epoch 19 - iter 512/643 - loss 0.26050657 - samples/sec: 54.99 - lr: 0.050000
2021-05-25 05:26:55,155 epoch 19 - iter 576/643 - loss 0.26472315 - samples/sec: 54.64 - lr: 0.050000
2021-05-25 05:27:13,935 epoch 19 - iter 640/643 - loss 0.26249273 - samples/sec: 54.53 - lr: 0.050000
2021-05-25 05:27:14,781 ----------------------------------------------------------------------------------------------------
2021-05-25 05:27:14,781 EPOCH 19 done: loss 0.2647 - lr 0.0500000
2021-05-25 05:27:24,497 DEV : loss 0.318700909614563 - score 0.9525
2021-05-25 05:27:24,646 BAD EPOCHS (no improvement): 3
2021-05-25 05:27:24,646 ----------------------------------------------------------------------------------------------------
2021-05-25 05:27:43,344 epoch 20 - iter 64/643 - loss 0.21108368 - samples/sec: 54.77 - lr: 0.050000
2021-05-25 05:28:01,948 epoch 20 - iter 128/643 - loss 0.24846039 - samples/sec: 55.05 - lr: 0.050000
2021-05-25 05:28:20,752 epoch 20 - iter 192/643 - loss 0.25520145 - samples/sec: 54.47 - lr: 0.050000
2021-05-25 05:28:39,555 epoch 20 - iter 256/643 - loss 0.27321634 - samples/sec: 54.46 - lr: 0.050000
2021-05-25 05:28:58,330 epoch 20 - iter 320/643 - loss 0.27050684 - samples/sec: 54.55 - lr: 0.050000
2021-05-25 05:29:17,023 epoch 20 - iter 384/643 - loss 0.27644291 - samples/sec: 54.79 - lr: 0.050000
2021-05-25 05:29:35,731 epoch 20 - iter 448/643 - loss 0.27097740 - samples/sec: 54.74 - lr: 0.050000
2021-05-25 05:29:54,542 epoch 20 - iter 512/643 - loss 0.26444075 - samples/sec: 54.44 - lr: 0.050000
2021-05-25 05:30:13,251 epoch 20 - iter 576/643 - loss 0.26516209 - samples/sec: 54.74 - lr: 0.050000
2021-05-25 05:30:31,959 epoch 20 - iter 640/643 - loss 0.26540713 - samples/sec: 54.74 - lr: 0.050000
2021-05-25 05:30:32,810 ----------------------------------------------------------------------------------------------------
2021-05-25 05:30:32,810 EPOCH 20 done: loss 0.2645 - lr 0.0500000
2021-05-25 05:30:41,636 DEV : loss 0.22297751903533936 - score 0.9618
Epoch    20: reducing learning rate of group 0 to 2.5000e-02.
2021-05-25 05:30:41,783 BAD EPOCHS (no improvement): 4
2021-05-25 05:30:41,783 ----------------------------------------------------------------------------------------------------
2021-05-25 05:31:00,435 epoch 21 - iter 64/643 - loss 0.29363754 - samples/sec: 54.91 - lr: 0.025000
2021-05-25 05:31:19,072 epoch 21 - iter 128/643 - loss 0.27476829 - samples/sec: 54.95 - lr: 0.025000
2021-05-25 05:31:37,829 epoch 21 - iter 192/643 - loss 0.27071908 - samples/sec: 54.60 - lr: 0.025000
2021-05-25 05:31:56,505 epoch 21 - iter 256/643 - loss 0.25885662 - samples/sec: 54.84 - lr: 0.025000
2021-05-25 05:32:15,179 epoch 21 - iter 320/643 - loss 0.25737821 - samples/sec: 54.84 - lr: 0.025000
2021-05-25 05:32:33,815 epoch 21 - iter 384/643 - loss 0.25213184 - samples/sec: 54.96 - lr: 0.025000
2021-05-25 05:32:52,622 epoch 21 - iter 448/643 - loss 0.25658363 - samples/sec: 54.45 - lr: 0.025000
2021-05-25 05:33:11,440 epoch 21 - iter 512/643 - loss 0.25165153 - samples/sec: 54.42 - lr: 0.025000
2021-05-25 05:33:30,261 epoch 21 - iter 576/643 - loss 0.25435908 - samples/sec: 54.41 - lr: 0.025000
2021-05-25 05:33:49,050 epoch 21 - iter 640/643 - loss 0.25289461 - samples/sec: 54.51 - lr: 0.025000
2021-05-25 05:33:49,901 ----------------------------------------------------------------------------------------------------
2021-05-25 05:33:49,902 EPOCH 21 done: loss 0.2535 - lr 0.0250000
2021-05-25 05:33:58,714 DEV : loss 0.2168528288602829 - score 0.9629
2021-05-25 05:33:58,862 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:34:08,537 ----------------------------------------------------------------------------------------------------
2021-05-25 05:34:27,356 epoch 22 - iter 64/643 - loss 0.26758865 - samples/sec: 54.43 - lr: 0.025000
2021-05-25 05:34:46,122 epoch 22 - iter 128/643 - loss 0.27199335 - samples/sec: 54.57 - lr: 0.025000
2021-05-25 05:35:05,010 epoch 22 - iter 192/643 - loss 0.25965652 - samples/sec: 54.22 - lr: 0.025000
2021-05-25 05:35:23,804 epoch 22 - iter 256/643 - loss 0.25241834 - samples/sec: 54.49 - lr: 0.025000
2021-05-25 05:35:42,572 epoch 22 - iter 320/643 - loss 0.24590593 - samples/sec: 54.57 - lr: 0.025000
2021-05-25 05:36:01,215 epoch 22 - iter 384/643 - loss 0.24891106 - samples/sec: 54.93 - lr: 0.025000
2021-05-25 05:36:19,901 epoch 22 - iter 448/643 - loss 0.25159025 - samples/sec: 54.81 - lr: 0.025000
2021-05-25 05:36:38,650 epoch 22 - iter 512/643 - loss 0.24575751 - samples/sec: 54.62 - lr: 0.025000
2021-05-25 05:36:57,258 epoch 22 - iter 576/643 - loss 0.24335703 - samples/sec: 55.04 - lr: 0.025000
2021-05-25 05:37:15,844 epoch 22 - iter 640/643 - loss 0.24427868 - samples/sec: 55.10 - lr: 0.025000
2021-05-25 05:37:16,696 ----------------------------------------------------------------------------------------------------
2021-05-25 05:37:16,697 EPOCH 22 done: loss 0.2455 - lr 0.0250000
2021-05-25 05:37:25,520 DEV : loss 0.2257300168275833 - score 0.9656
2021-05-25 05:37:25,668 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:37:35,627 ----------------------------------------------------------------------------------------------------
2021-05-25 05:37:54,299 epoch 23 - iter 64/643 - loss 0.26628777 - samples/sec: 54.85 - lr: 0.025000
2021-05-25 05:38:12,883 epoch 23 - iter 128/643 - loss 0.24540230 - samples/sec: 55.11 - lr: 0.025000
2021-05-25 05:38:31,327 epoch 23 - iter 192/643 - loss 0.23592645 - samples/sec: 55.52 - lr: 0.025000
2021-05-25 05:38:49,824 epoch 23 - iter 256/643 - loss 0.23815788 - samples/sec: 55.37 - lr: 0.025000
2021-05-25 05:39:08,341 epoch 23 - iter 320/643 - loss 0.23562722 - samples/sec: 55.31 - lr: 0.025000
2021-05-25 05:39:26,857 epoch 23 - iter 384/643 - loss 0.24162708 - samples/sec: 55.31 - lr: 0.025000
2021-05-25 05:39:45,460 epoch 23 - iter 448/643 - loss 0.24366324 - samples/sec: 55.05 - lr: 0.025000
2021-05-25 05:40:04,051 epoch 23 - iter 512/643 - loss 0.24257360 - samples/sec: 55.09 - lr: 0.025000
2021-05-25 05:40:22,595 epoch 23 - iter 576/643 - loss 0.24728037 - samples/sec: 55.23 - lr: 0.025000
2021-05-25 05:40:41,206 epoch 23 - iter 640/643 - loss 0.24572586 - samples/sec: 55.03 - lr: 0.025000
2021-05-25 05:40:42,044 ----------------------------------------------------------------------------------------------------
2021-05-25 05:40:42,044 EPOCH 23 done: loss 0.2454 - lr 0.0250000
2021-05-25 05:40:51,790 DEV : loss 0.23496700823307037 - score 0.9672
2021-05-25 05:40:51,939 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:41:01,709 ----------------------------------------------------------------------------------------------------
2021-05-25 05:41:20,272 epoch 24 - iter 64/643 - loss 0.24805971 - samples/sec: 55.17 - lr: 0.025000
2021-05-25 05:41:38,894 epoch 24 - iter 128/643 - loss 0.24568706 - samples/sec: 55.00 - lr: 0.025000
2021-05-25 05:41:57,548 epoch 24 - iter 192/643 - loss 0.24672064 - samples/sec: 54.90 - lr: 0.025000
2021-05-25 05:42:16,030 epoch 24 - iter 256/643 - loss 0.25108597 - samples/sec: 55.41 - lr: 0.025000
2021-05-25 05:42:34,499 epoch 24 - iter 320/643 - loss 0.24601266 - samples/sec: 55.45 - lr: 0.025000
2021-05-25 05:42:53,020 epoch 24 - iter 384/643 - loss 0.25414017 - samples/sec: 55.30 - lr: 0.025000
2021-05-25 05:43:11,567 epoch 24 - iter 448/643 - loss 0.25122069 - samples/sec: 55.22 - lr: 0.025000
2021-05-25 05:43:30,078 epoch 24 - iter 512/643 - loss 0.24998314 - samples/sec: 55.33 - lr: 0.025000
2021-05-25 05:43:48,689 epoch 24 - iter 576/643 - loss 0.24075972 - samples/sec: 55.03 - lr: 0.025000
2021-05-25 05:44:07,305 epoch 24 - iter 640/643 - loss 0.23986524 - samples/sec: 55.01 - lr: 0.025000
2021-05-25 05:44:08,156 ----------------------------------------------------------------------------------------------------
2021-05-25 05:44:08,156 EPOCH 24 done: loss 0.2394 - lr 0.0250000
2021-05-25 05:44:16,975 DEV : loss 0.20621338486671448 - score 0.9685
2021-05-25 05:44:17,122 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:44:26,437 ----------------------------------------------------------------------------------------------------
2021-05-25 05:44:45,167 epoch 25 - iter 64/643 - loss 0.20953632 - samples/sec: 54.68 - lr: 0.025000
2021-05-25 05:45:03,735 epoch 25 - iter 128/643 - loss 0.20474979 - samples/sec: 55.16 - lr: 0.025000
2021-05-25 05:45:22,378 epoch 25 - iter 192/643 - loss 0.20300206 - samples/sec: 54.93 - lr: 0.025000
2021-05-25 05:45:41,108 epoch 25 - iter 256/643 - loss 0.21109097 - samples/sec: 54.68 - lr: 0.025000
2021-05-25 05:45:59,849 epoch 25 - iter 320/643 - loss 0.21950398 - samples/sec: 54.65 - lr: 0.025000
2021-05-25 05:46:18,599 epoch 25 - iter 384/643 - loss 0.21828773 - samples/sec: 54.62 - lr: 0.025000
2021-05-25 05:46:37,179 epoch 25 - iter 448/643 - loss 0.22135209 - samples/sec: 55.12 - lr: 0.025000
2021-05-25 05:46:55,841 epoch 25 - iter 512/643 - loss 0.21669044 - samples/sec: 54.88 - lr: 0.025000
2021-05-25 05:47:14,598 epoch 25 - iter 576/643 - loss 0.21796994 - samples/sec: 54.60 - lr: 0.025000
2021-05-25 05:47:33,329 epoch 25 - iter 640/643 - loss 0.22041877 - samples/sec: 54.68 - lr: 0.025000
2021-05-25 05:47:34,180 ----------------------------------------------------------------------------------------------------
2021-05-25 05:47:34,180 EPOCH 25 done: loss 0.2201 - lr 0.0250000
2021-05-25 05:47:42,997 DEV : loss 0.19328494369983673 - score 0.9675
2021-05-25 05:47:43,144 BAD EPOCHS (no improvement): 1
2021-05-25 05:47:43,144 ----------------------------------------------------------------------------------------------------
2021-05-25 05:48:01,809 epoch 26 - iter 64/643 - loss 0.21330028 - samples/sec: 54.87 - lr: 0.025000
2021-05-25 05:48:20,531 epoch 26 - iter 128/643 - loss 0.21107527 - samples/sec: 54.70 - lr: 0.025000
2021-05-25 05:48:39,174 epoch 26 - iter 192/643 - loss 0.21590953 - samples/sec: 54.94 - lr: 0.025000
2021-05-25 05:48:57,938 epoch 26 - iter 256/643 - loss 0.21485017 - samples/sec: 54.58 - lr: 0.025000
2021-05-25 05:49:16,709 epoch 26 - iter 320/643 - loss 0.21435621 - samples/sec: 54.56 - lr: 0.025000
2021-05-25 05:49:35,416 epoch 26 - iter 384/643 - loss 0.22116380 - samples/sec: 54.75 - lr: 0.025000
2021-05-25 05:49:54,107 epoch 26 - iter 448/643 - loss 0.22474334 - samples/sec: 54.79 - lr: 0.025000
2021-05-25 05:50:12,720 epoch 26 - iter 512/643 - loss 0.22904666 - samples/sec: 55.02 - lr: 0.025000
2021-05-25 05:50:31,358 epoch 26 - iter 576/643 - loss 0.22965720 - samples/sec: 54.95 - lr: 0.025000
2021-05-25 05:50:50,016 epoch 26 - iter 640/643 - loss 0.23027823 - samples/sec: 54.89 - lr: 0.025000
2021-05-25 05:50:50,860 ----------------------------------------------------------------------------------------------------
2021-05-25 05:50:50,861 EPOCH 26 done: loss 0.2301 - lr 0.0250000
2021-05-25 05:50:59,715 DEV : loss 0.22473303973674774 - score 0.9687
2021-05-25 05:50:59,863 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 05:51:08,917 ----------------------------------------------------------------------------------------------------
2021-05-25 05:51:27,587 epoch 27 - iter 64/643 - loss 0.23714356 - samples/sec: 54.86 - lr: 0.025000
2021-05-25 05:51:46,223 epoch 27 - iter 128/643 - loss 0.23191654 - samples/sec: 54.95 - lr: 0.025000
2021-05-25 05:52:04,860 epoch 27 - iter 192/643 - loss 0.23811985 - samples/sec: 54.95 - lr: 0.025000
2021-05-25 05:52:23,559 epoch 27 - iter 256/643 - loss 0.22942125 - samples/sec: 54.77 - lr: 0.025000
2021-05-25 05:52:42,325 epoch 27 - iter 320/643 - loss 0.23173691 - samples/sec: 54.58 - lr: 0.025000
2021-05-25 05:53:00,937 epoch 27 - iter 384/643 - loss 0.23955852 - samples/sec: 55.02 - lr: 0.025000
2021-05-25 05:53:19,596 epoch 27 - iter 448/643 - loss 0.23442921 - samples/sec: 54.89 - lr: 0.025000
2021-05-25 05:53:38,271 epoch 27 - iter 512/643 - loss 0.23171943 - samples/sec: 54.84 - lr: 0.025000
2021-05-25 05:53:56,924 epoch 27 - iter 576/643 - loss 0.22664259 - samples/sec: 54.90 - lr: 0.025000
2021-05-25 05:54:15,594 epoch 27 - iter 640/643 - loss 0.23086351 - samples/sec: 54.86 - lr: 0.025000
2021-05-25 05:54:16,443 ----------------------------------------------------------------------------------------------------
2021-05-25 05:54:16,443 EPOCH 27 done: loss 0.2305 - lr 0.0250000
2021-05-25 05:54:25,279 DEV : loss 0.19143499433994293 - score 0.9676
2021-05-25 05:54:25,426 BAD EPOCHS (no improvement): 1
2021-05-25 05:54:25,426 ----------------------------------------------------------------------------------------------------
2021-05-25 05:54:44,020 epoch 28 - iter 64/643 - loss 0.21232568 - samples/sec: 55.08 - lr: 0.025000
2021-05-25 05:55:02,637 epoch 28 - iter 128/643 - loss 0.20544085 - samples/sec: 55.01 - lr: 0.025000
2021-05-25 05:55:21,305 epoch 28 - iter 192/643 - loss 0.22146115 - samples/sec: 54.86 - lr: 0.025000
2021-05-25 05:55:39,919 epoch 28 - iter 256/643 - loss 0.22626108 - samples/sec: 55.02 - lr: 0.025000
2021-05-25 05:55:58,667 epoch 28 - iter 320/643 - loss 0.22586132 - samples/sec: 54.63 - lr: 0.025000
2021-05-25 05:56:17,342 epoch 28 - iter 384/643 - loss 0.22553126 - samples/sec: 54.84 - lr: 0.025000
2021-05-25 05:56:35,991 epoch 28 - iter 448/643 - loss 0.22247467 - samples/sec: 54.92 - lr: 0.025000
2021-05-25 05:56:54,669 epoch 28 - iter 512/643 - loss 0.22620154 - samples/sec: 54.83 - lr: 0.025000
2021-05-25 05:57:13,201 epoch 28 - iter 576/643 - loss 0.22485840 - samples/sec: 55.26 - lr: 0.025000
2021-05-25 05:57:32,829 epoch 28 - iter 640/643 - loss 0.22561147 - samples/sec: 52.18 - lr: 0.025000
2021-05-25 05:57:33,691 ----------------------------------------------------------------------------------------------------
2021-05-25 05:57:33,692 EPOCH 28 done: loss 0.2251 - lr 0.0250000
2021-05-25 05:57:42,497 DEV : loss 0.197299063205719 - score 0.9671
2021-05-25 05:57:42,643 BAD EPOCHS (no improvement): 2
2021-05-25 05:57:42,644 ----------------------------------------------------------------------------------------------------
2021-05-25 05:58:01,346 epoch 29 - iter 64/643 - loss 0.23507019 - samples/sec: 54.76 - lr: 0.025000
2021-05-25 05:58:20,000 epoch 29 - iter 128/643 - loss 0.20302840 - samples/sec: 54.90 - lr: 0.025000
2021-05-25 05:58:38,657 epoch 29 - iter 192/643 - loss 0.19971268 - samples/sec: 54.89 - lr: 0.025000
2021-05-25 05:58:57,234 epoch 29 - iter 256/643 - loss 0.20530189 - samples/sec: 55.13 - lr: 0.025000
2021-05-25 05:59:15,785 epoch 29 - iter 320/643 - loss 0.20970985 - samples/sec: 55.21 - lr: 0.025000
2021-05-25 05:59:34,474 epoch 29 - iter 384/643 - loss 0.22182852 - samples/sec: 54.80 - lr: 0.025000
2021-05-25 05:59:53,190 epoch 29 - iter 448/643 - loss 0.22328268 - samples/sec: 54.72 - lr: 0.025000
2021-05-25 06:00:11,879 epoch 29 - iter 512/643 - loss 0.22466526 - samples/sec: 54.80 - lr: 0.025000
2021-05-25 06:00:30,548 epoch 29 - iter 576/643 - loss 0.22625936 - samples/sec: 54.85 - lr: 0.025000
2021-05-25 06:00:49,070 epoch 29 - iter 640/643 - loss 0.22392833 - samples/sec: 55.29 - lr: 0.025000
2021-05-25 06:00:49,914 ----------------------------------------------------------------------------------------------------
2021-05-25 06:00:49,915 EPOCH 29 done: loss 0.2239 - lr 0.0250000
2021-05-25 06:00:58,748 DEV : loss 0.190947487950325 - score 0.9669
2021-05-25 06:00:58,896 BAD EPOCHS (no improvement): 3
2021-05-25 06:00:58,896 ----------------------------------------------------------------------------------------------------
2021-05-25 06:01:17,439 epoch 30 - iter 64/643 - loss 0.20980622 - samples/sec: 55.23 - lr: 0.025000
2021-05-25 06:01:35,864 epoch 30 - iter 128/643 - loss 0.20701647 - samples/sec: 55.59 - lr: 0.025000
2021-05-25 06:01:54,290 epoch 30 - iter 192/643 - loss 0.22143557 - samples/sec: 55.58 - lr: 0.025000
2021-05-25 06:02:12,823 epoch 30 - iter 256/643 - loss 0.22147189 - samples/sec: 55.26 - lr: 0.025000
2021-05-25 06:02:31,300 epoch 30 - iter 320/643 - loss 0.22139923 - samples/sec: 55.43 - lr: 0.025000
2021-05-25 06:02:49,899 epoch 30 - iter 384/643 - loss 0.22461217 - samples/sec: 55.07 - lr: 0.025000
2021-05-25 06:03:08,478 epoch 30 - iter 448/643 - loss 0.22285056 - samples/sec: 55.12 - lr: 0.025000
2021-05-25 06:03:26,960 epoch 30 - iter 512/643 - loss 0.22709726 - samples/sec: 55.41 - lr: 0.025000
2021-05-25 06:03:45,498 epoch 30 - iter 576/643 - loss 0.22562230 - samples/sec: 55.24 - lr: 0.025000
2021-05-25 06:04:04,055 epoch 30 - iter 640/643 - loss 0.23015411 - samples/sec: 55.19 - lr: 0.025000
2021-05-25 06:04:04,904 ----------------------------------------------------------------------------------------------------
2021-05-25 06:04:04,905 EPOCH 30 done: loss 0.2296 - lr 0.0250000
2021-05-25 06:04:13,740 DEV : loss 0.1950254738330841 - score 0.971
2021-05-25 06:04:13,888 BAD EPOCHS (no improvement): 0
saving best model
2021-05-25 06:04:24,568 ----------------------------------------------------------------------------------------------------
2021-05-25 06:04:24,568 Testing using best model ...
2021-05-25 06:04:24,569 loading file /home/shabnam/data/codes/data/dstrpt/2019-output/rus.rst.rrt/best-model.pt
2021-05-25 06:05:08,365 0.9604	0.9683	0.9644
2021-05-25 06:05:08,365 
Results:
- F1-score (micro) 0.9644
- F1-score (macro) 0.9644

By class:
SENT       tp: 1285 - fp: 53 - fn: 42 - precision: 0.9604 - recall: 0.9683 - f1-score: 0.9644
2021-05-25 06:05:08,366 ----------------------------------------------------------------------------------------------------
